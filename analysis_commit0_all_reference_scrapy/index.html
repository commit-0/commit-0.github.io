
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis commit0 all reference scrapy - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reference-gold-scrapy" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis commit0 all reference scrapy
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytest-summary-for-test-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Pytest Summary for test tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytests:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytests:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_commandspygenspidercommandtesttest_template" class="md-nav__link">
    <span class="md-ellipsis">
      test_commands.py::GenspiderCommandTest::test_template
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commandspygenspidercommandtesttest_template_basic" class="md-nav__link">
    <span class="md-ellipsis">
      test_commands.py::GenspiderCommandTest::test_template_basic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commandspygenspidercommandtesttest_template_csvfeed" class="md-nav__link">
    <span class="md-ellipsis">
      test_commands.py::GenspiderCommandTest::test_template_csvfeed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commandspygenspidercommandtesttest_template_xmlfeed" class="md-nav__link">
    <span class="md-ellipsis">
      test_commands.py::GenspiderCommandTest::test_template_xmlfeed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commandspygenspiderstandalonecommandtesttest_same_name_as_existing_file" class="md-nav__link">
    <span class="md-ellipsis">
      test_commands.py::GenspiderStandaloneCommandTest::test_same_name_as_existing_file
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_crawlpycrawlspidertestcasetest_response_ssl_certificate_empty_response" class="md-nav__link">
    <span class="md-ellipsis">
      test_crawl.py::CrawlSpiderTestCase::test_response_ssl_certificate_empty_response
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttptestcasetest_download_head" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::HttpTestCase::test_download_head
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttptestcasetest_get_duplicate_header" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::HttpTestCase::test_get_duplicate_header
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttptestcasetest_host_header_not_in_request_headers" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::HttpTestCase::test_host_header_not_in_request_headers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttptestcasetest_payload" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::HttpTestCase::test_payload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttptestcasetest_response_header_content_length" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::HttpTestCase::test_response_header_content_length
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_content_length_zero_bodyless_post_request_headers" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_content_length_zero_bodyless_post_request_headers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_payload" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_payload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_protocol" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_protocol
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_redirect_status" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_redirect_status
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_redirect_status_head" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_redirect_status_head
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_response_class_from_body" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_response_class_from_body
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps10testcasetest_response_header_content_length" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https10TestCase::test_response_header_content_length
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_content_length_zero_bodyless_post_only_one" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_content_length_zero_bodyless_post_only_one
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_download_head" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_download_head
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_download_with_maxsize" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_download_with_maxsize_per_req" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize_per_req
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_download_with_small_maxsize_per_spider" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_download_with_small_maxsize_per_spider
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_host_header_not_in_request_headers" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_host_header_not_in_request_headers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_response_class_choosing_request" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_response_class_choosing_request
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11testcasetest_response_header_content_length" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11TestCase::test_response_header_content_length
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps11wronghostnametestcasetest_download_broken_chunked_content_cause_data_loss" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https11WrongHostnameTestCase::test_download_broken_chunked_content_cause_data_loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttps11invaliddnspatterntest_timeout_download_from_spider_nodata_rcvd" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Https11InvalidDNSPattern::test_timeout_download_from_spider_nodata_rcvd
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyhttp11proxytestcasetest_download_with_proxy_without_http_scheme" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::Http11ProxyTestCase::test_download_with_proxy_without_http_scheme
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspybaseftptestcasetest_ftp_download_nonexistent" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_nonexistent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspybaseftptestcasetest_ftp_download_success" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_success
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspybaseftptestcasetest_ftp_local_filename" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::BaseFTPTestCase::test_ftp_local_filename
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspybaseftptestcasetest_response_class_from_body" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::BaseFTPTestCase::test_response_class_from_body
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyftptestcasetest_ftp_download_path_with_spaces" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::FTPTestCase::test_ftp_download_path_with_spaces
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyftptestcasetest_ftp_download_success" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::FTPTestCase::test_ftp_download_success
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyftptestcasetest_response_class_from_url" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::FTPTestCase::test_response_class_from_url
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyanonymousftptestcasetest_ftp_download_nonexistent" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::AnonymousFTPTestCase::test_ftp_download_nonexistent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlerspyanonymousftptestcasetest_response_class_from_url" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers.py::AnonymousFTPTestCase::test_response_class_from_url
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlers_http2pyhttp11proxytestcasetest_download_with_proxy" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_with_proxy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlers_http2pyhttp11proxytestcasetest_download_without_proxy" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_without_proxy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlers_http2pyhttps11testcasetest_download_broken_chunked_content_cause_data_loss" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers_http2.py::Https11TestCase::test_download_broken_chunked_content_cause_data_loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloader_handlers_http2pyhttps11testcasetest_timeout_download_from_spider_nodata_rcvd" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloader_handlers_http2.py::Https11TestCase::test_timeout_download_from_spider_nodata_rcvd
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_keep_cookie_from_default_request_headers_middleware" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_from_default_request_headers_middleware
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_keep_cookie_header" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_header
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_request_headers_cookie_encoding" class="md-nav__link">
    <span class="md-ellipsis">
      test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_request_headers_cookie_encoding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_enginepyenginetesttest_crawler_dupefilter" class="md-nav__link">
    <span class="md-ellipsis">
      test_engine.py::EngineTest::test_crawler_dupefilter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_engine_stop_download_bytespyenginetesttest_crawler_change_close_reason_on_idle" class="md-nav__link">
    <span class="md-ellipsis">
      test_engine_stop_download_bytes.py::EngineTest::test_crawler_change_close_reason_on_idle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_engine_stop_download_bytespyenginetesttest_crawler_itemerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_engine_stop_download_bytes.py::EngineTest::test_crawler_itemerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_engine_stop_download_bytespybytesreceivedenginetesttest_crawler_itemerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_engine_stop_download_bytes.py::BytesReceivedEngineTest::test_crawler_itemerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_engine_stop_download_headerspyenginetesttest_crawler_change_close_reason_on_idle" class="md-nav__link">
    <span class="md-ellipsis">
      test_engine_stop_download_headers.py::EngineTest::test_crawler_change_close_reason_on_idle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initpybasesettingstesttest_update_iterable" class="md-nav__link">
    <span class="md-ellipsis">
      init.py::BaseSettingsTest::test_update_iterable
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initpybasesettingstesttest_update_kwargs" class="md-nav__link">
    <span class="md-ellipsis">
      init.py::BaseSettingsTest::test_update_kwargs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespymarshalfifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize1marshalfifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize1MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize2marshalfifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize2MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize3marshalfifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize3MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize4marshalfifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize4MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespypicklefifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize1picklefifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize1PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize2picklefifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize2PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize3picklefifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize3PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespychunksize4picklefifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::ChunkSize4PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespymarshallifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::MarshalLifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_squeuespypicklelifodiskqueuetesttest_non_bytes_raises_typeerror" class="md-nav__link">
    <span class="md-ellipsis">
      test_squeues.py::PickleLifoDiskQueueTest::test_non_bytes_raises_typeerror
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_utils_deferpyasyncdeftestsuitetesttest_deferred_f_from_coro_f_xfail" class="md-nav__link">
    <span class="md-ellipsis">
      test_utils_defer.py::AsyncDefTestsuiteTest::test_deferred_f_from_coro_f_xfail
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_utils_requestpyrequestfingerprinttesttest_part_separation" class="md-nav__link">
    <span class="md-ellipsis">
      test_utils_request.py::RequestFingerprintTest::test_part_separation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_utils_requestpyrequestfingerprintasbytestesttest_part_separation" class="md-nav__link">
    <span class="md-ellipsis">
      test_utils_request.py::RequestFingerprintAsBytesTest::test_part_separation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_commit0_all_reference">back to Reference (Gold) summary</a></p>
<h1 id="reference-gold-scrapy"><strong>Reference (Gold)</strong>: scrapy</h1>
<h2 id="pytest-summary-for-test-tests">Pytest Summary for test <code>tests</code></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">status</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">2858</td>
</tr>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">33</td>
</tr>
<tr>
<td style="text-align: left;">skipped</td>
<td style="text-align: center;">371</td>
</tr>
<tr>
<td style="text-align: left;">xfailed</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: left;">error</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">3296</td>
</tr>
<tr>
<td style="text-align: left;">collected</td>
<td style="text-align: center;">3296</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytests">Failed pytests:</h2>
<h3 id="test_commandspygenspidercommandtesttest_template">test_commands.py::GenspiderCommandTest::test_template</h3>
<details><summary> <pre>test_commands.py::GenspiderCommandTest::test_template</pre></summary><pre>
self = <tests.test_commands.GenspiderCommandTest testMethod=test_template>
tplname = 'crawl'

    def test_template(self, tplname="crawl"):
        args = [f"--template={tplname}"] if tplname else []
        spname = "test_spider"
        spmodule = f"{self.project_name}.spiders.{spname}"
        p, out, err = self.proc("genspider", spname, "test.com", *args)
        self.assertIn(
            f"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}",
            out,
        )
        self.assertTrue(Path(self.proj_mod_path, "spiders", "test_spider.py").exists())
        modify_time_before = (
            Path(self.proj_mod_path, "spiders", "test_spider.py").stat().st_mtime
        )
        p, out, err = self.proc("genspider", spname, "test.com", *args)
        self.assertIn(f"Spider {spname!r} already exists in module", out)
        modify_time_after = (
            Path(self.proj_mod_path, "spiders", "test_spider.py").stat().st_mtime
        )
>       self.assertEqual(modify_time_after, modify_time_before)

/testbed/tests/test_commands.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:444: in assertEqual
    super().assertEqual(first, second, msg)
E   twisted.trial.unittest.FailTest: 1727385204.784021 != 1727385204.785175
</pre>
</details>
<h3 id="test_commandspygenspidercommandtesttest_template_basic">test_commands.py::GenspiderCommandTest::test_template_basic</h3>
<details><summary> <pre>test_commands.py::GenspiderCommandTest::test_template_basic</pre></summary><pre>
self = <tests.test_commands.GenspiderCommandTest testMethod=test_template_basic>

    def test_template_basic(self):
>       self.test_template("basic")

/testbed/tests/test_commands.py:475: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_commands.py:472: in test_template
    self.assertEqual(modify_time_after, modify_time_before)
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:444: in assertEqual
    super().assertEqual(first, second, msg)
E   twisted.trial.unittest.FailTest: 1727385206.4120243 != 1727385206.4133556
</pre>
</details>
<h3 id="test_commandspygenspidercommandtesttest_template_csvfeed">test_commands.py::GenspiderCommandTest::test_template_csvfeed</h3>
<details><summary> <pre>test_commands.py::GenspiderCommandTest::test_template_csvfeed</pre></summary><pre>
self = <tests.test_commands.GenspiderCommandTest testMethod=test_template_csvfeed>

    def test_template_csvfeed(self):
>       self.test_template("csvfeed")

/testbed/tests/test_commands.py:478: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_commands.py:472: in test_template
    self.assertEqual(modify_time_after, modify_time_before)
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:444: in assertEqual
    super().assertEqual(first, second, msg)
E   twisted.trial.unittest.FailTest: 1727385208.0210276 != 1727385208.0219028
</pre>
</details>
<h3 id="test_commandspygenspidercommandtesttest_template_xmlfeed">test_commands.py::GenspiderCommandTest::test_template_xmlfeed</h3>
<details><summary> <pre>test_commands.py::GenspiderCommandTest::test_template_xmlfeed</pre></summary><pre>
self = <tests.test_commands.GenspiderCommandTest testMethod=test_template_xmlfeed>

    def test_template_xmlfeed(self):
>       self.test_template("xmlfeed")

/testbed/tests/test_commands.py:481: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_commands.py:472: in test_template
    self.assertEqual(modify_time_after, modify_time_before)
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:444: in assertEqual
    super().assertEqual(first, second, msg)
E   twisted.trial.unittest.FailTest: 1727385210.770033 != 1727385210.7708642
</pre>
</details>
<h3 id="test_commandspygenspiderstandalonecommandtesttest_same_name_as_existing_file">test_commands.py::GenspiderStandaloneCommandTest::test_same_name_as_existing_file</h3>
<details><summary> <pre>test_commands.py::GenspiderStandaloneCommandTest::test_same_name_as_existing_file</pre></summary><pre>
self = <tests.test_commands.GenspiderStandaloneCommandTest testMethod=test_same_name_as_existing_file>
force = False

    def test_same_name_as_existing_file(self, force=False):
        file_name = "example"
        file_path = Path(self.temp_path, file_name + ".py")
        p, out, err = self.proc("genspider", file_name, "example.com")
        self.assertIn(f"Created spider {file_name!r} using template 'basic' ", out)
        assert file_path.exists()
        modify_time_before = file_path.stat().st_mtime
        file_contents_before = file_path.read_text(encoding="utf-8")

        if force:
            # use different template to ensure contents were changed
            p, out, err = self.proc(
                "genspider", "--force", "-t", "crawl", file_name, "example.com"
            )
            self.assertIn(f"Created spider {file_name!r} using template 'crawl' ", out)
            modify_time_after = file_path.stat().st_mtime
            self.assertNotEqual(modify_time_after, modify_time_before)
            file_contents_after = file_path.read_text(encoding="utf-8")
            self.assertNotEqual(file_contents_after, file_contents_before)
        else:
            p, out, err = self.proc("genspider", file_name, "example.com")
            self.assertIn(
                f"{Path(self.temp_path, file_name + '.py').resolve()} already exists",
                out,
            )
            modify_time_after = file_path.stat().st_mtime
>           self.assertEqual(modify_time_after, modify_time_before)

/testbed/tests/test_commands.py:641: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:444: in assertEqual
    super().assertEqual(first, second, msg)
E   twisted.trial.unittest.FailTest: 1727385214.8590412 != 1727385214.8595073
</pre>
</details>
<h3 id="test_crawlpycrawlspidertestcasetest_response_ssl_certificate_empty_response">test_crawl.py::CrawlSpiderTestCase::test_response_ssl_certificate_empty_response</h3>
<details><summary> <pre>test_crawl.py::CrawlSpiderTestCase::test_response_ssl_certificate_empty_response</pre></summary><pre>
self = <tests.test_crawl.CrawlSpiderTestCase testMethod=test_response_ssl_certificate_empty_response>

    @mark.xfail(reason="Responses with no body return early and contain no certificate")
    @defer.inlineCallbacks
    def test_response_ssl_certificate_empty_response(self):
        crawler = get_crawler(SingleRequestSpider)
        url = self.mockserver.url("/status?n=200", is_secure=True)
        yield crawler.crawl(seed=url, mockserver=self.mockserver)
        cert = crawler.spider.meta["responses"][0].certificate
>       self.assertIsInstance(cert, Certificate)

/testbed/tests/test_crawl.py:629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:666: in assertIsInstance
    self.fail(f"{instance!r} is not an instance of {classOrTuple}{suffix}")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.test_crawl.CrawlSpiderTestCase testMethod=test_response_ssl_certificate_empty_response>
msg = "None is not an instance of <class 'twisted.internet._sslverify.Certificate'>"

    def fail(self, msg: Optional[object] = None) -> NoReturn:
        """
        Absolutely fail the test.  Do not pass go, do not collect $200.

        @param msg: the message that will be displayed as the reason for the
        failure
        """
>       raise self.failureException(msg)
E       twisted.trial.unittest.FailTest: None is not an instance of <class 'twisted.internet._sslverify.Certificate'>

/testbed/.venv/lib/python3.12/site-packages/twisted/trial/_synctest.py:381: FailTest
</pre>
</details>
<h3 id="test_downloader_handlerspyhttptestcasetest_download_head">test_downloader_handlers.py::HttpTestCase::test_download_head</h3>
<details><summary> <pre>test_downloader_handlers.py::HttpTestCase::test_download_head</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 32594>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttptestcasetest_get_duplicate_header">test_downloader_handlers.py::HttpTestCase::test_get_duplicate_header</h3>
<details><summary> <pre>test_downloader_handlers.py::HttpTestCase::test_get_duplicate_header</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 57859>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttptestcasetest_host_header_not_in_request_headers">test_downloader_handlers.py::HttpTestCase::test_host_header_not_in_request_headers</h3>
<details><summary> <pre>test_downloader_handlers.py::HttpTestCase::test_host_header_not_in_request_headers</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 41027>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttptestcasetest_payload">test_downloader_handlers.py::HttpTestCase::test_payload</h3>
<details><summary> <pre>test_downloader_handlers.py::HttpTestCase::test_payload</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 36633>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttptestcasetest_response_header_content_length">test_downloader_handlers.py::HttpTestCase::test_response_header_content_length</h3>
<details><summary> <pre>test_downloader_handlers.py::HttpTestCase::test_response_header_content_length</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 63524>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_content_length_zero_bodyless_post_request_headers">test_downloader_handlers.py::Https10TestCase::test_content_length_zero_bodyless_post_request_headers</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_content_length_zero_bodyless_post_request_headers</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 41794>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_payload">test_downloader_handlers.py::Https10TestCase::test_payload</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_payload</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 45001>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_protocol">test_downloader_handlers.py::Https10TestCase::test_protocol</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_protocol</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 40109>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_redirect_status">test_downloader_handlers.py::Https10TestCase::test_redirect_status</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_redirect_status</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 49534>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_redirect_status_head">test_downloader_handlers.py::Https10TestCase::test_redirect_status_head</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_redirect_status_head</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 17284>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_response_class_from_body">test_downloader_handlers.py::Https10TestCase::test_response_class_from_body</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_response_class_from_body</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 23417>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps10testcasetest_response_header_content_length">test_downloader_handlers.py::Https10TestCase::test_response_header_content_length</h3>
<details><summary> <pre>test_downloader_handlers.py::Https10TestCase::test_response_header_content_length</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 62755>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_content_length_zero_bodyless_post_only_one">test_downloader_handlers.py::Http11TestCase::test_content_length_zero_bodyless_post_only_one</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_content_length_zero_bodyless_post_only_one</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 23794>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_download_head">test_downloader_handlers.py::Http11TestCase::test_download_head</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_download_head</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 34442>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_download_with_maxsize">test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 53867>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_download_with_maxsize_per_req">test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize_per_req</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_download_with_maxsize_per_req</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 31328>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_download_with_small_maxsize_per_spider">test_downloader_handlers.py::Http11TestCase::test_download_with_small_maxsize_per_spider</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_download_with_small_maxsize_per_spider</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 36136>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_host_header_not_in_request_headers">test_downloader_handlers.py::Http11TestCase::test_host_header_not_in_request_headers</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_host_header_not_in_request_headers</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 24706>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_response_class_choosing_request">test_downloader_handlers.py::Http11TestCase::test_response_class_choosing_request</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_response_class_choosing_request</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 45725>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11testcasetest_response_header_content_length">test_downloader_handlers.py::Http11TestCase::test_response_header_content_length</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11TestCase::test_response_header_content_length</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 42301>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps11wronghostnametestcasetest_download_broken_chunked_content_cause_data_loss">test_downloader_handlers.py::Https11WrongHostnameTestCase::test_download_broken_chunked_content_cause_data_loss</h3>
<details><summary> <pre>test_downloader_handlers.py::Https11WrongHostnameTestCase::test_download_broken_chunked_content_cause_data_loss</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 26713>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttps11invaliddnspatterntest_timeout_download_from_spider_nodata_rcvd">test_downloader_handlers.py::Https11InvalidDNSPattern::test_timeout_download_from_spider_nodata_rcvd</h3>
<details><summary> <pre>test_downloader_handlers.py::Https11InvalidDNSPattern::test_timeout_download_from_spider_nodata_rcvd</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 42440>
</pre>
</details>
<h3 id="test_downloader_handlerspyhttp11proxytestcasetest_download_with_proxy_without_http_scheme">test_downloader_handlers.py::Http11ProxyTestCase::test_download_with_proxy_without_http_scheme</h3>
<details><summary> <pre>test_downloader_handlers.py::Http11ProxyTestCase::test_download_with_proxy_without_http_scheme</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 19775>
</pre>
</details>
<h3 id="test_downloader_handlerspybaseftptestcasetest_ftp_download_nonexistent">test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_nonexistent</h3>
<details><summary> <pre>test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_nonexistent</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f2969ca0 [-4.00543212890625e-05s] called=0 cancelled=0 Deferred.callback(None)>
</pre>
</details>
<h3 id="test_downloader_handlerspybaseftptestcasetest_ftp_download_success">test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_success</h3>
<details><summary> <pre>test_downloader_handlers.py::BaseFTPTestCase::test_ftp_download_success</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f1ba1df0 [599.9983518123627s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspybaseftptestcasetest_ftp_local_filename">test_downloader_handlers.py::BaseFTPTestCase::test_ftp_local_filename</h3>
<details><summary> <pre>test_downloader_handlers.py::BaseFTPTestCase::test_ftp_local_filename</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f18ee750 [599.9982721805573s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspybaseftptestcasetest_response_class_from_body">test_downloader_handlers.py::BaseFTPTestCase::test_response_class_from_body</h3>
<details><summary> <pre>test_downloader_handlers.py::BaseFTPTestCase::test_response_class_from_body</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f171a630 [599.9986047744751s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspyftptestcasetest_ftp_download_path_with_spaces">test_downloader_handlers.py::FTPTestCase::test_ftp_download_path_with_spaces</h3>
<details><summary> <pre>test_downloader_handlers.py::FTPTestCase::test_ftp_download_path_with_spaces</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f1811cd0 [599.9986138343811s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspyftptestcasetest_ftp_download_success">test_downloader_handlers.py::FTPTestCase::test_ftp_download_success</h3>
<details><summary> <pre>test_downloader_handlers.py::FTPTestCase::test_ftp_download_success</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f176d9d0 [599.9986596107483s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspyftptestcasetest_response_class_from_url">test_downloader_handlers.py::FTPTestCase::test_response_class_from_url</h3>
<details><summary> <pre>test_downloader_handlers.py::FTPTestCase::test_response_class_from_url</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f17c86b0 [599.9920456409454s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspyanonymousftptestcasetest_ftp_download_nonexistent">test_downloader_handlers.py::AnonymousFTPTestCase::test_ftp_download_nonexistent</h3>
<details><summary> <pre>test_downloader_handlers.py::AnonymousFTPTestCase::test_ftp_download_nonexistent</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f17a99d0 [599.9989242553711s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlerspyanonymousftptestcasetest_response_class_from_url">test_downloader_handlers.py::AnonymousFTPTestCase::test_response_class_from_url</h3>
<details><summary> <pre>test_downloader_handlers.py::AnonymousFTPTestCase::test_response_class_from_url</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22f1749f70 [599.9985325336456s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_downloader_handlers_http2pyhttp11proxytestcasetest_download_with_proxy">test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_with_proxy</h3>
<details><summary> <pre>test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_with_proxy</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 59116>
</pre>
</details>
<h3 id="test_downloader_handlers_http2pyhttp11proxytestcasetest_download_without_proxy">test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_without_proxy</h3>
<details><summary> <pre>test_downloader_handlers_http2.py::Http11ProxyTestCase::test_download_without_proxy</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<ProtocolWrapper #0 on 40788>
</pre>
</details>
<h3 id="test_downloader_handlers_http2pyhttps11testcasetest_download_broken_chunked_content_cause_data_loss">test_downloader_handlers_http2.py::Https11TestCase::test_download_broken_chunked_content_cause_data_loss</h3>
<details><summary> <pre>test_downloader_handlers_http2.py::Https11TestCase::test_download_broken_chunked_content_cause_data_loss</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 40265>
</pre>
</details>
<h3 id="test_downloader_handlers_http2pyhttps11testcasetest_timeout_download_from_spider_nodata_rcvd">test_downloader_handlers_http2.py::Https11TestCase::test_timeout_download_from_spider_nodata_rcvd</h3>
<details><summary> <pre>test_downloader_handlers_http2.py::Https11TestCase::test_timeout_download_from_spider_nodata_rcvd</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
Selectables:
<BufferingTLSTransport #0 on 47135>
</pre>
</details>
<h3 id="test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_keep_cookie_from_default_request_headers_middleware">test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_from_default_request_headers_middleware</h3>
<details><summary> <pre>test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_from_default_request_headers_middleware</pre></summary><pre>
self = <tests.test_downloadermiddleware_cookies.CookiesMiddlewareTest testMethod=test_keep_cookie_from_default_request_headers_middleware>

    @pytest.mark.xfail(reason="Cookie header is not currently being processed")
    def test_keep_cookie_from_default_request_headers_middleware(self):
        DEFAULT_REQUEST_HEADERS = dict(Cookie="default=value; asdf=qwerty")
        mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())
        # overwrite with values from 'cookies' request argument
        req1 = Request("http://example.org", cookies={"default": "something"})
        assert mw_default_headers.process_request(req1, self.spider) is None
        assert self.mw.process_request(req1, self.spider) is None
>       self.assertCookieValEqual(
            req1.headers["Cookie"], b"default=something; asdf=qwerty"
        )

/testbed/tests/test_downloadermiddleware_cookies.py:329: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_downloadermiddleware_cookies.py:59: in assertCookieValEqual
    return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)
E   AssertionError: Lists differ: [b'default=something'] != [b'asdf=qwerty', b'default=something']
E   
E   First differing element 0:
E   b'default=something'
E   b'asdf=qwerty'
E   
E   Second list contains 1 additional elements.
E   First extra element 1:
E   b'default=something'
E   
E   - [b'default=something']
E   + [b'asdf=qwerty', b'default=something']
</pre>
</details>
<h3 id="test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_keep_cookie_header">test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_header</h3>
<details><summary> <pre>test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_keep_cookie_header</pre></summary><pre>
self = <tests.test_downloadermiddleware_cookies.CookiesMiddlewareTest testMethod=test_keep_cookie_header>

    @pytest.mark.xfail(reason="Cookie header is not currently being processed")
    def test_keep_cookie_header(self):
        # keep only cookies from 'Cookie' request header
        req1 = Request("http://scrapytest.org", headers={"Cookie": "a=b; c=d"})
        assert self.mw.process_request(req1, self.spider) is None
>       self.assertCookieValEqual(req1.headers["Cookie"], "a=b; c=d")

/testbed/tests/test_downloadermiddleware_cookies.py:345: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/scrapy/http/headers.py:49: in __getitem__
    return super().__getitem__(key)[-1]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = {}, key = 'Cookie'

    def __getitem__(self, key):
>       return dict.__getitem__(self, self.normkey(key))
E       KeyError: b'Cookie'

/testbed/scrapy/utils/datatypes.py:41: KeyError
</pre>
</details>
<h3 id="test_downloadermiddleware_cookiespycookiesmiddlewaretesttest_request_headers_cookie_encoding">test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_request_headers_cookie_encoding</h3>
<details><summary> <pre>test_downloadermiddleware_cookies.py::CookiesMiddlewareTest::test_request_headers_cookie_encoding</pre></summary><pre>
self = <tests.test_downloadermiddleware_cookies.CookiesMiddlewareTest testMethod=test_request_headers_cookie_encoding>

    @pytest.mark.xfail(reason="Cookie header is not currently being processed")
    def test_request_headers_cookie_encoding(self):
        # 1) UTF8-encoded bytes
        req1 = Request("http://example.org", headers={"Cookie": "a=á".encode("utf8")})
        assert self.mw.process_request(req1, self.spider) is None
>       self.assertCookieValEqual(req1.headers["Cookie"], b"a=\xc3\xa1")

/testbed/tests/test_downloadermiddleware_cookies.py:382: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/scrapy/http/headers.py:49: in __getitem__
    return super().__getitem__(key)[-1]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = {}, key = 'Cookie'

    def __getitem__(self, key):
>       return dict.__getitem__(self, self.normkey(key))
E       KeyError: b'Cookie'

/testbed/scrapy/utils/datatypes.py:41: KeyError
</pre>
</details>
<h3 id="test_enginepyenginetesttest_crawler_dupefilter">test_engine.py::EngineTest::test_crawler_dupefilter</h3>
<details><summary> <pre>test_engine.py::EngineTest::test_crawler_dupefilter</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22e9bb6600 [59.87262201309204s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9bb4860 [59.873056411743164s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9bb6d50 [59.87299036979675s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_engine_stop_download_bytespyenginetesttest_crawler_change_close_reason_on_idle">test_engine_stop_download_bytes.py::EngineTest::test_crawler_change_close_reason_on_idle</h3>
<details><summary> <pre>test_engine_stop_download_bytes.py::EngineTest::test_crawler_change_close_reason_on_idle</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22e9e1db80 [59.779688119888306s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9965b80 [59.878074645996094s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9e1f080 [59.8785035610199s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9e1e240 [59.878971338272095s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_engine_stop_download_bytespyenginetesttest_crawler_itemerror">test_engine_stop_download_bytes.py::EngineTest::test_crawler_itemerror</h3>
<details><summary> <pre>test_engine_stop_download_bytes.py::EngineTest::test_crawler_itemerror</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22ea142f90 [59.87554430961609s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea141bb0 [59.87593984603882s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea140fb0 [59.875999450683594s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_engine_stop_download_bytespybytesreceivedenginetesttest_crawler_itemerror">test_engine_stop_download_bytes.py::BytesReceivedEngineTest::test_crawler_itemerror</h3>
<details><summary> <pre>test_engine_stop_download_bytes.py::BytesReceivedEngineTest::test_crawler_itemerror</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22e9ec9ca0 [59.77031850814819s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9eca660 [59.88174772262573s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22e9ec8710 [59.88219618797302s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea0fdee0 [59.882126569747925s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="test_engine_stop_download_headerspyenginetesttest_crawler_change_close_reason_on_idle">test_engine_stop_download_headers.py::EngineTest::test_crawler_change_close_reason_on_idle</h3>
<details><summary> <pre>test_engine_stop_download_headers.py::EngineTest::test_crawler_change_close_reason_on_idle</pre></summary><pre>
'NoneType' object is not iterable

During handling of the above exception, another exception occurred:
NOTE: Incompatible Exception Representation, displaying natively:

twisted.trial.util.DirtyReactorAggregateError: Reactor was unclean.
DelayedCalls: (set twisted.internet.base.DelayedCall.debug = True to debug)
<DelayedCall 0x7f22ea0861b0 [59.760512828826904s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea086e10 [59.86344361305237s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea085d60 [59.8638379573822s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
<DelayedCall 0x7f22ea086780 [59.86390233039856s] called=0 cancelled=0 TimeoutMixin.__timedOut()>
</pre>
</details>
<h3 id="initpybasesettingstesttest_update_iterable"><strong>init</strong>.py::BaseSettingsTest::test_update_iterable</h3>
<details><summary> <pre>__init__.py::BaseSettingsTest::test_update_iterable</pre></summary><pre>
self = <tests.test_settings.BaseSettingsTest testMethod=test_update_iterable>

    @pytest.mark.xfail(
        raises=AttributeError,
        reason="BaseSettings.update doesn't support iterable input",
    )
    def test_update_iterable(self):
        settings = BaseSettings({"key": 0})
>       settings.update([("key", 1)])

/testbed/tests/test_settings/__init__.py:217: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <scrapy.settings.BaseSettings object at 0x7f22f3afb410>
values = [('key', 1)], priority = 'project'

    def update(self, values: _SettingsInputT, priority: Union[int, str] = "project") -> None:  # type: ignore[override]
        """
        Store key/value pairs with a given priority.

        This is a helper function that calls
        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``
        with the provided ``priority``.

        If ``values`` is a string, it is assumed to be JSON-encoded and parsed
        into a dict with ``json.loads()`` first. If it is a
        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities
        will be used and the ``priority`` parameter ignored. This allows
        inserting/updating settings with different priorities with a single
        command.

        :param values: the settings names and values
        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`

        :param priority: the priority of the settings. Should be a key of
            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
        :type priority: str or int
        """
        self._assert_mutability()
        if isinstance(values, str):
            values = cast(dict, json.loads(values))
        if values is not None:
            if isinstance(values, BaseSettings):
                for name, value in values.items():
                    self.set(name, value, cast(int, values.getpriority(name)))
            else:
>               for name, value in values.items():
E               AttributeError: 'list' object has no attribute 'items'

/testbed/scrapy/settings/__init__.py:421: AttributeError
</pre>
</details>
<h3 id="initpybasesettingstesttest_update_kwargs"><strong>init</strong>.py::BaseSettingsTest::test_update_kwargs</h3>
<details><summary> <pre>__init__.py::BaseSettingsTest::test_update_kwargs</pre></summary><pre>
self = <tests.test_settings.BaseSettingsTest testMethod=test_update_kwargs>

    @pytest.mark.xfail(
        raises=TypeError, reason="BaseSettings.update doesn't support kwargs input"
    )
    def test_update_kwargs(self):
        settings = BaseSettings({"key": 0})
>       settings.update(key=1)  # pylint: disable=unexpected-keyword-arg
E       TypeError: BaseSettings.update() got an unexpected keyword argument 'key'

/testbed/tests/test_settings/__init__.py:209: TypeError
</pre>
</details>
<h3 id="test_squeuespymarshalfifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.MarshalFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize1marshalfifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize1MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize1MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize1MarshalFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize2marshalfifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize2MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize2MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize2MarshalFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize3marshalfifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize3MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize3MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize3MarshalFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize4marshalfifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize4MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize4MarshalFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize4MarshalFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespypicklefifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.PickleFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize1picklefifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize1PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize1PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize1PickleFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize2picklefifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize2PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize2PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize2PickleFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize3picklefifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize3PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize3PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize3PickleFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespychunksize4picklefifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::ChunkSize4PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::ChunkSize4PickleFifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.ChunkSize4PickleFifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespymarshallifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::MarshalLifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::MarshalLifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.MarshalLifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_squeuespypicklelifodiskqueuetesttest_non_bytes_raises_typeerror">test_squeues.py::PickleLifoDiskQueueTest::test_non_bytes_raises_typeerror</h3>
<details><summary> <pre>test_squeues.py::PickleLifoDiskQueueTest::test_non_bytes_raises_typeerror</pre></summary><pre>
self = <tests.test_squeues.PickleLifoDiskQueueTest testMethod=test_non_bytes_raises_typeerror>

    @pytest.mark.xfail(
        reason="Reenable once Scrapy.squeues stops extending from this testsuite"
    )
    def test_non_bytes_raises_typeerror(self):
        q = self.queue()
>       self.assertRaises(TypeError, q.push, 0)
E       AssertionError: TypeError not raised by push

/testbed/.venv/lib/python3.12/site-packages/queuelib/tests/test_queue.py:223: AssertionError
</pre>
</details>
<h3 id="test_utils_deferpyasyncdeftestsuitetesttest_deferred_f_from_coro_f_xfail">test_utils_defer.py::AsyncDefTestsuiteTest::test_deferred_f_from_coro_f_xfail</h3>
<details><summary> <pre>test_utils_defer.py::AsyncDefTestsuiteTest::test_deferred_f_from_coro_f_xfail</pre></summary><pre>
self = <tests.test_utils_defer.AsyncDefTestsuiteTest testMethod=test_deferred_f_from_coro_f_xfail>

    @mark.xfail(reason="Checks that the test is actually executed", strict=True)
    @deferred_f_from_coro_f
    async def test_deferred_f_from_coro_f_xfail(self):
>       raise Exception("This is expected to be raised")
E       Exception: This is expected to be raised

/testbed/tests/test_utils_defer.py:171: Exception
</pre>
</details>
<h3 id="test_utils_requestpyrequestfingerprinttesttest_part_separation">test_utils_request.py::RequestFingerprintTest::test_part_separation</h3>
<details><summary> <pre>test_utils_request.py::RequestFingerprintTest::test_part_separation</pre></summary><pre>
self = <tests.test_utils_request.RequestFingerprintTest testMethod=test_part_separation>

    @pytest.mark.xfail(reason="known bug kept for backward compatibility", strict=True)
    def test_part_separation(self):
>       super().test_part_separation()

/testbed/tests/test_utils_request.py:325: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_utils_request.py:224: in test_part_separation
    self.assertNotEqual(fp1, fp2)
E   AssertionError: '4e38b5ad81c4739738db8a4e3573c22aba5c5c28' == '4e38b5ad81c4739738db8a4e3573c22aba5c5c28'
</pre>
</details>
<h3 id="test_utils_requestpyrequestfingerprintasbytestesttest_part_separation">test_utils_request.py::RequestFingerprintAsBytesTest::test_part_separation</h3>
<details><summary> <pre>test_utils_request.py::RequestFingerprintAsBytesTest::test_part_separation</pre></summary><pre>
self = <tests.test_utils_request.RequestFingerprintAsBytesTest testMethod=test_part_separation>

    @pytest.mark.xfail(reason="known bug kept for backward compatibility", strict=True)
    def test_part_separation(self):
>       super().test_part_separation()

/testbed/tests/test_utils_request.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/testbed/tests/test_utils_request.py:224: in test_part_separation
    self.assertNotEqual(fp1, fp2)
E   AssertionError: b'N8\xb5\xad\x81\xc4s\x978\xdb\x8aN5s\xc2*\xba\\\\(' == b'N8\xb5\xad\x81\xc4s\x978\xdb\x8aN5s\xc2*\xba\\\\('
</pre>
</details>

<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/scrapy/addons.py b/scrapy/addons.py</span>
<span class="gh">index b20d143a9..9060d4f3f 100644</span>
<span class="gd">--- a/scrapy/addons.py</span>
<span class="gi">+++ b/scrapy/addons.py</span>
<span class="gu">@@ -1,28 +1,53 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>from typing import TYPE_CHECKING, Any, List
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.conf import build_component_list
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class AddonManager:
<span class="w"> </span>    &quot;&quot;&quot;This class facilitates loading and storing :ref:`topics-addons`.&quot;&quot;&quot;

<span class="gd">-    def __init__(self, crawler: &#39;Crawler&#39;) -&gt;None:</span>
<span class="gd">-        self.crawler: &#39;Crawler&#39; = crawler</span>
<span class="gi">+    def __init__(self, crawler: &quot;Crawler&quot;) -&gt; None:</span>
<span class="gi">+        self.crawler: &quot;Crawler&quot; = crawler</span>
<span class="w"> </span>        self.addons: List[Any] = []

<span class="gd">-    def load_settings(self, settings: Settings) -&gt;None:</span>
<span class="gi">+    def load_settings(self, settings: Settings) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Load add-ons and configurations from a settings object and apply them.

<span class="w"> </span>        This will load the add-on for every add-on path in the
<span class="w"> </span>        ``ADDONS`` setting and execute their ``update_settings`` methods.

<span class="gd">-        :param settings: The :class:`~scrapy.settings.Settings` object from             which to read the add-on configuration</span>
<span class="gi">+        :param settings: The :class:`~scrapy.settings.Settings` object from \</span>
<span class="gi">+            which to read the add-on configuration</span>
<span class="w"> </span>        :type settings: :class:`~scrapy.settings.Settings`
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for clspath in build_component_list(settings[&quot;ADDONS&quot;]):</span>
<span class="gi">+            try:</span>
<span class="gi">+                addoncls = load_object(clspath)</span>
<span class="gi">+                addon = create_instance(</span>
<span class="gi">+                    addoncls, settings=settings, crawler=self.crawler</span>
<span class="gi">+                )</span>
<span class="gi">+                addon.update_settings(settings)</span>
<span class="gi">+                self.addons.append(addon)</span>
<span class="gi">+            except NotConfigured as e:</span>
<span class="gi">+                if e.args:</span>
<span class="gi">+                    logger.warning(</span>
<span class="gi">+                        &quot;Disabled %(clspath)s: %(eargs)s&quot;,</span>
<span class="gi">+                        {&quot;clspath&quot;: clspath, &quot;eargs&quot;: e.args[0]},</span>
<span class="gi">+                        extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+                    )</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Enabled addons:\n%(addons)s&quot;,</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;addons&quot;: self.addons,</span>
<span class="gi">+            },</span>
<span class="gi">+            extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py</span>
<span class="gh">index bf63f266a..6580ba9ce 100644</span>
<span class="gd">--- a/scrapy/cmdline.py</span>
<span class="gi">+++ b/scrapy/cmdline.py</span>
<span class="gu">@@ -4,6 +4,7 @@ import inspect</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>from importlib.metadata import entry_points
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter
<span class="w"> </span>from scrapy.crawler import CrawlerProcess
<span class="gu">@@ -14,11 +15,175 @@ from scrapy.utils.python import garbage_collect</span>


<span class="w"> </span>class ScrapyArgumentParser(argparse.ArgumentParser):
<span class="gd">-    pass</span>
<span class="gi">+    def _parse_optional(self, arg_string):</span>
<span class="gi">+        # if starts with -: it means that is a parameter not a argument</span>
<span class="gi">+        if arg_string[:2] == &quot;-:&quot;:</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        return super()._parse_optional(arg_string)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _iter_command_classes(module_name):</span>
<span class="gi">+    # TODO: add `name` attribute to commands and merge this function with</span>
<span class="gi">+    # scrapy.utils.spider.iter_spider_classes</span>
<span class="gi">+    for module in walk_modules(module_name):</span>
<span class="gi">+        for obj in vars(module).values():</span>
<span class="gi">+            if (</span>
<span class="gi">+                inspect.isclass(obj)</span>
<span class="gi">+                and issubclass(obj, ScrapyCommand)</span>
<span class="gi">+                and obj.__module__ == module.__name__</span>
<span class="gi">+                and obj not in (ScrapyCommand, BaseRunSpiderCommand)</span>
<span class="gi">+            ):</span>
<span class="gi">+                yield obj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_commands_from_module(module, inproject):</span>
<span class="gi">+    d = {}</span>
<span class="gi">+    for cmd in _iter_command_classes(module):</span>
<span class="gi">+        if inproject or not cmd.requires_project:</span>
<span class="gi">+            cmdname = cmd.__module__.split(&quot;.&quot;)[-1]</span>
<span class="gi">+            d[cmdname] = cmd()</span>
<span class="gi">+    return d</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_commands_from_entry_points(inproject, group=&quot;scrapy.commands&quot;):</span>
<span class="gi">+    cmds = {}</span>
<span class="gi">+    if sys.version_info &gt;= (3, 10):</span>
<span class="gi">+        eps = entry_points(group=group)</span>
<span class="gi">+    else:</span>
<span class="gi">+        eps = entry_points().get(group, ())</span>
<span class="gi">+    for entry_point in eps:</span>
<span class="gi">+        obj = entry_point.load()</span>
<span class="gi">+        if inspect.isclass(obj):</span>
<span class="gi">+            cmds[entry_point.name] = obj()</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise Exception(f&quot;Invalid entry point {entry_point.name}&quot;)</span>
<span class="gi">+    return cmds</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_commands_dict(settings, inproject):</span>
<span class="gi">+    cmds = _get_commands_from_module(&quot;scrapy.commands&quot;, inproject)</span>
<span class="gi">+    cmds.update(_get_commands_from_entry_points(inproject))</span>
<span class="gi">+    cmds_module = settings[&quot;COMMANDS_MODULE&quot;]</span>
<span class="gi">+    if cmds_module:</span>
<span class="gi">+        cmds.update(_get_commands_from_module(cmds_module, inproject))</span>
<span class="gi">+    return cmds</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _pop_command_name(argv):</span>
<span class="gi">+    i = 0</span>
<span class="gi">+    for arg in argv[1:]:</span>
<span class="gi">+        if not arg.startswith(&quot;-&quot;):</span>
<span class="gi">+            del argv[i]</span>
<span class="gi">+            return arg</span>
<span class="gi">+        i += 1</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _print_header(settings, inproject):</span>
<span class="gi">+    version = scrapy.__version__</span>
<span class="gi">+    if inproject:</span>
<span class="gi">+        print(f&quot;Scrapy {version} - active project: {settings[&#39;BOT_NAME&#39;]}\n&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    else:</span>
<span class="gi">+        print(f&quot;Scrapy {version} - no active project\n&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _print_commands(settings, inproject):</span>
<span class="gi">+    _print_header(settings, inproject)</span>
<span class="gi">+    print(&quot;Usage:&quot;)</span>
<span class="gi">+    print(&quot;  scrapy &lt;command&gt; [options] [args]\n&quot;)</span>
<span class="gi">+    print(&quot;Available commands:&quot;)</span>
<span class="gi">+    cmds = _get_commands_dict(settings, inproject)</span>
<span class="gi">+    for cmdname, cmdclass in sorted(cmds.items()):</span>
<span class="gi">+        print(f&quot;  {cmdname:&lt;13} {cmdclass.short_desc()}&quot;)</span>
<span class="gi">+    if not inproject:</span>
<span class="gi">+        print()</span>
<span class="gi">+        print(&quot;  [ more ]      More commands available when run from project directory&quot;)</span>
<span class="gi">+    print()</span>
<span class="gi">+    print(&#39;Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command&#39;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _print_unknown_command(settings, cmdname, inproject):</span>
<span class="gi">+    _print_header(settings, inproject)</span>
<span class="gi">+    print(f&quot;Unknown command: {cmdname}\n&quot;)</span>
<span class="gi">+    print(&#39;Use &quot;scrapy&quot; to see available commands&#39;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _run_print_help(parser, func, *a, **kw):</span>
<span class="gi">+    try:</span>
<span class="gi">+        func(*a, **kw)</span>
<span class="gi">+    except UsageError as e:</span>
<span class="gi">+        if str(e):</span>
<span class="gi">+            parser.error(str(e))</span>
<span class="gi">+        if e.print_help:</span>
<span class="gi">+            parser.print_help()</span>
<span class="gi">+        sys.exit(2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def execute(argv=None, settings=None):</span>
<span class="gi">+    if argv is None:</span>
<span class="gi">+        argv = sys.argv</span>
<span class="gi">+</span>
<span class="gi">+    if settings is None:</span>
<span class="gi">+        settings = get_project_settings()</span>
<span class="gi">+        # set EDITOR from environment if available</span>
<span class="gi">+        try:</span>
<span class="gi">+            editor = os.environ[&quot;EDITOR&quot;]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass</span>
<span class="gi">+        else:</span>
<span class="gi">+            settings[&quot;EDITOR&quot;] = editor</span>
<span class="gi">+</span>
<span class="gi">+    inproject = inside_project()</span>
<span class="gi">+    cmds = _get_commands_dict(settings, inproject)</span>
<span class="gi">+    cmdname = _pop_command_name(argv)</span>
<span class="gi">+    if not cmdname:</span>
<span class="gi">+        _print_commands(settings, inproject)</span>
<span class="gi">+        sys.exit(0)</span>
<span class="gi">+    elif cmdname not in cmds:</span>
<span class="gi">+        _print_unknown_command(settings, cmdname, inproject)</span>
<span class="gi">+        sys.exit(2)</span>
<span class="gi">+</span>
<span class="gi">+    cmd = cmds[cmdname]</span>
<span class="gi">+    parser = ScrapyArgumentParser(</span>
<span class="gi">+        formatter_class=ScrapyHelpFormatter,</span>
<span class="gi">+        usage=f&quot;scrapy {cmdname} {cmd.syntax()}&quot;,</span>
<span class="gi">+        conflict_handler=&quot;resolve&quot;,</span>
<span class="gi">+        description=cmd.long_desc(),</span>
<span class="gi">+    )</span>
<span class="gi">+    settings.setdict(cmd.default_settings, priority=&quot;command&quot;)</span>
<span class="gi">+    cmd.settings = settings</span>
<span class="gi">+    cmd.add_options(parser)</span>
<span class="gi">+    opts, args = parser.parse_known_args(args=argv[1:])</span>
<span class="gi">+    _run_print_help(parser, cmd.process_options, args, opts)</span>
<span class="gi">+</span>
<span class="gi">+    cmd.crawler_process = CrawlerProcess(settings)</span>
<span class="gi">+    _run_print_help(parser, _run_command, cmd, args, opts)</span>
<span class="gi">+    sys.exit(cmd.exitcode)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _run_command(cmd, args, opts):</span>
<span class="gi">+    if opts.profile:</span>
<span class="gi">+        _run_command_profiled(cmd, args, opts)</span>
<span class="gi">+    else:</span>
<span class="gi">+        cmd.run(args, opts)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _run_command_profiled(cmd, args, opts):</span>
<span class="gi">+    if opts.profile:</span>
<span class="gi">+        sys.stderr.write(f&quot;scrapy: writing cProfile stats to {opts.profile!r}\n&quot;)</span>
<span class="gi">+    loc = locals()</span>
<span class="gi">+    p = cProfile.Profile()</span>
<span class="gi">+    p.runctx(&quot;cmd.run(args, opts)&quot;, globals(), loc)</span>
<span class="gi">+    if opts.profile:</span>
<span class="gi">+        p.dump_stats(opts.profile)</span>


<span class="gd">-if __name__ == &#39;__main__&#39;:</span>
<span class="gi">+if __name__ == &quot;__main__&quot;:</span>
<span class="w"> </span>    try:
<span class="w"> </span>        execute()
<span class="w"> </span>    finally:
<span class="gi">+        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:</span>
<span class="gi">+        # http://doc.pypy.org/en/latest/cpython_differences.html</span>
<span class="gi">+        # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies</span>
<span class="w"> </span>        garbage_collect()
<span class="gh">diff --git a/scrapy/commands/bench.py b/scrapy/commands/bench.py</span>
<span class="gh">index 1c049d02b..e1ccdc451 100644</span>
<span class="gd">--- a/scrapy/commands/bench.py</span>
<span class="gi">+++ b/scrapy/commands/bench.py</span>
<span class="gu">@@ -2,23 +2,34 @@ import subprocess</span>
<span class="w"> </span>import sys
<span class="w"> </span>import time
<span class="w"> </span>from urllib.parse import urlencode
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.linkextractors import LinkExtractor


<span class="w"> </span>class Command(ScrapyCommand):
<span class="gd">-    default_settings = {&#39;LOG_LEVEL&#39;: &#39;INFO&#39;, &#39;LOGSTATS_INTERVAL&#39;: 1,</span>
<span class="gd">-        &#39;CLOSESPIDER_TIMEOUT&#39;: 10}</span>
<span class="gi">+    default_settings = {</span>
<span class="gi">+        &quot;LOG_LEVEL&quot;: &quot;INFO&quot;,</span>
<span class="gi">+        &quot;LOGSTATS_INTERVAL&quot;: 1,</span>
<span class="gi">+        &quot;CLOSESPIDER_TIMEOUT&quot;: 10,</span>
<span class="gi">+    }</span>

<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Run quick benchmark test&quot;</span>

<span class="gd">-class _BenchServer:</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        with _BenchServer():</span>
<span class="gi">+            self.crawler_process.crawl(_BenchSpider, total=100000)</span>
<span class="gi">+            self.crawler_process.start()</span>

<span class="gi">+</span>
<span class="gi">+class _BenchServer:</span>
<span class="w"> </span>    def __enter__(self):
<span class="w"> </span>        from scrapy.utils.test import get_testenv
<span class="gd">-        pargs = [sys.executable, &#39;-u&#39;, &#39;-m&#39;, &#39;scrapy.utils.benchserver&#39;]</span>
<span class="gd">-        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE, env=</span>
<span class="gd">-            get_testenv())</span>
<span class="gi">+</span>
<span class="gi">+        pargs = [sys.executable, &quot;-u&quot;, &quot;-m&quot;, &quot;scrapy.utils.benchserver&quot;]</span>
<span class="gi">+        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE, env=get_testenv())</span>
<span class="w"> </span>        self.proc.stdout.readline()

<span class="w"> </span>    def __exit__(self, exc_type, exc_value, traceback):
<span class="gu">@@ -29,8 +40,18 @@ class _BenchServer:</span>

<span class="w"> </span>class _BenchSpider(scrapy.Spider):
<span class="w"> </span>    &quot;&quot;&quot;A spider that follows all links&quot;&quot;&quot;
<span class="gd">-    name = &#39;follow&#39;</span>
<span class="gi">+</span>
<span class="gi">+    name = &quot;follow&quot;</span>
<span class="w"> </span>    total = 10000
<span class="w"> </span>    show = 20
<span class="gd">-    baseurl = &#39;http://localhost:8998&#39;</span>
<span class="gi">+    baseurl = &quot;http://localhost:8998&quot;</span>
<span class="w"> </span>    link_extractor = LinkExtractor()
<span class="gi">+</span>
<span class="gi">+    def start_requests(self):</span>
<span class="gi">+        qargs = {&quot;total&quot;: self.total, &quot;show&quot;: self.show}</span>
<span class="gi">+        url = f&quot;{self.baseurl}?{urlencode(qargs, doseq=True)}&quot;</span>
<span class="gi">+        return [scrapy.Request(url, dont_filter=True)]</span>
<span class="gi">+</span>
<span class="gi">+    def parse(self, response):</span>
<span class="gi">+        for link in self.link_extractor.extract_links(response):</span>
<span class="gi">+            yield scrapy.Request(link.url, callback=self.parse)</span>
<span class="gh">diff --git a/scrapy/commands/check.py b/scrapy/commands/check.py</span>
<span class="gh">index 7d6b7e3ed..de54ca4d3 100644</span>
<span class="gd">--- a/scrapy/commands/check.py</span>
<span class="gi">+++ b/scrapy/commands/check.py</span>
<span class="gu">@@ -2,6 +2,7 @@ import time</span>
<span class="w"> </span>from collections import defaultdict
<span class="w"> </span>from unittest import TextTestResult as _TextTestResult
<span class="w"> </span>from unittest import TextTestRunner
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.contracts import ContractsManager
<span class="w"> </span>from scrapy.utils.conf import build_component_list
<span class="gu">@@ -9,9 +10,99 @@ from scrapy.utils.misc import load_object, set_environ</span>


<span class="w"> </span>class TextTestResult(_TextTestResult):
<span class="gd">-    pass</span>
<span class="gi">+    def printSummary(self, start, stop):</span>
<span class="gi">+        write = self.stream.write</span>
<span class="gi">+        writeln = self.stream.writeln</span>
<span class="gi">+</span>
<span class="gi">+        run = self.testsRun</span>
<span class="gi">+        plural = &quot;s&quot; if run != 1 else &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+        writeln(self.separator2)</span>
<span class="gi">+        writeln(f&quot;Ran {run} contract{plural} in {stop - start:.3f}s&quot;)</span>
<span class="gi">+        writeln()</span>
<span class="gi">+</span>
<span class="gi">+        infos = []</span>
<span class="gi">+        if not self.wasSuccessful():</span>
<span class="gi">+            write(&quot;FAILED&quot;)</span>
<span class="gi">+            failed, errored = map(len, (self.failures, self.errors))</span>
<span class="gi">+            if failed:</span>
<span class="gi">+                infos.append(f&quot;failures={failed}&quot;)</span>
<span class="gi">+            if errored:</span>
<span class="gi">+                infos.append(f&quot;errors={errored}&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            write(&quot;OK&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if infos:</span>
<span class="gi">+            writeln(f&quot; ({&#39;, &#39;.join(infos)})&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            write(&quot;\n&quot;)</span>


<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = True
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;spider&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Check spider contracts&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-l&quot;,</span>
<span class="gi">+            &quot;--list&quot;,</span>
<span class="gi">+            dest=&quot;list&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;only list contracts, without checking them&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-v&quot;,</span>
<span class="gi">+            &quot;--verbose&quot;,</span>
<span class="gi">+            dest=&quot;verbose&quot;,</span>
<span class="gi">+            default=False,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;print contract tests for all spiders&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        # load contracts</span>
<span class="gi">+        contracts = build_component_list(self.settings.getwithbase(&quot;SPIDER_CONTRACTS&quot;))</span>
<span class="gi">+        conman = ContractsManager(load_object(c) for c in contracts)</span>
<span class="gi">+        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)</span>
<span class="gi">+        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)</span>
<span class="gi">+</span>
<span class="gi">+        # contract requests</span>
<span class="gi">+        contract_reqs = defaultdict(list)</span>
<span class="gi">+</span>
<span class="gi">+        spider_loader = self.crawler_process.spider_loader</span>
<span class="gi">+</span>
<span class="gi">+        with set_environ(SCRAPY_CHECK=&quot;true&quot;):</span>
<span class="gi">+            for spidername in args or spider_loader.list():</span>
<span class="gi">+                spidercls = spider_loader.load(spidername)</span>
<span class="gi">+                spidercls.start_requests = lambda s: conman.from_spider(s, result)</span>
<span class="gi">+</span>
<span class="gi">+                tested_methods = conman.tested_methods_from_spidercls(spidercls)</span>
<span class="gi">+                if opts.list:</span>
<span class="gi">+                    for method in tested_methods:</span>
<span class="gi">+                        contract_reqs[spidercls.name].append(method)</span>
<span class="gi">+                elif tested_methods:</span>
<span class="gi">+                    self.crawler_process.crawl(spidercls)</span>
<span class="gi">+</span>
<span class="gi">+            # start checks</span>
<span class="gi">+            if opts.list:</span>
<span class="gi">+                for spider, methods in sorted(contract_reqs.items()):</span>
<span class="gi">+                    if not methods and not opts.verbose:</span>
<span class="gi">+                        continue</span>
<span class="gi">+                    print(spider)</span>
<span class="gi">+                    for method in sorted(methods):</span>
<span class="gi">+                        print(f&quot;  * {method}&quot;)</span>
<span class="gi">+            else:</span>
<span class="gi">+                start = time.time()</span>
<span class="gi">+                self.crawler_process.start()</span>
<span class="gi">+                stop = time.time()</span>
<span class="gi">+</span>
<span class="gi">+                result.printErrors()</span>
<span class="gi">+                result.printSummary(start, stop)</span>
<span class="gi">+                self.exitcode = int(not result.wasSuccessful())</span>
<span class="gh">diff --git a/scrapy/commands/crawl.py b/scrapy/commands/crawl.py</span>
<span class="gh">index 2348fd64d..2f0f1c7b9 100644</span>
<span class="gd">--- a/scrapy/commands/crawl.py</span>
<span class="gi">+++ b/scrapy/commands/crawl.py</span>
<span class="gu">@@ -4,3 +4,34 @@ from scrapy.exceptions import UsageError</span>

<span class="w"> </span>class Command(BaseRunSpiderCommand):
<span class="w"> </span>    requires_project = True
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;spider&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Run a spider&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if len(args) &lt; 1:</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+        elif len(args) &gt; 1:</span>
<span class="gi">+            raise UsageError(</span>
<span class="gi">+                &quot;running &#39;scrapy crawl&#39; with more than one spider is not supported&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        spname = args[0]</span>
<span class="gi">+</span>
<span class="gi">+        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)</span>
<span class="gi">+</span>
<span class="gi">+        if getattr(crawl_defer, &quot;result&quot;, None) is not None and issubclass(</span>
<span class="gi">+            crawl_defer.result.type, Exception</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.exitcode = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.crawler_process.start()</span>
<span class="gi">+</span>
<span class="gi">+            if (</span>
<span class="gi">+                self.crawler_process.bootstrap_failed</span>
<span class="gi">+                or hasattr(self.crawler_process, &quot;has_exception&quot;)</span>
<span class="gi">+                and self.crawler_process.has_exception</span>
<span class="gi">+            ):</span>
<span class="gi">+                self.exitcode = 1</span>
<span class="gh">diff --git a/scrapy/commands/edit.py b/scrapy/commands/edit.py</span>
<span class="gh">index ce7b67cc7..03a8ed5c7 100644</span>
<span class="gd">--- a/scrapy/commands/edit.py</span>
<span class="gi">+++ b/scrapy/commands/edit.py</span>
<span class="gu">@@ -1,9 +1,40 @@</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.exceptions import UsageError


<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = True
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;&lt;spider&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Edit spider&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def long_desc(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            &quot;Edit a spider using the editor defined in the EDITOR environment&quot;</span>
<span class="gi">+            &quot; variable or else the EDITOR setting&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _err(self, msg):</span>
<span class="gi">+        sys.stderr.write(msg + os.linesep)</span>
<span class="gi">+        self.exitcode = 1</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if len(args) != 1:</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+</span>
<span class="gi">+        editor = self.settings[&quot;EDITOR&quot;]</span>
<span class="gi">+        try:</span>
<span class="gi">+            spidercls = self.crawler_process.spider_loader.load(args[0])</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            return self._err(f&quot;Spider not found: {args[0]}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        sfile = sys.modules[spidercls.__module__].__file__</span>
<span class="gi">+        sfile = sfile.replace(&quot;.pyc&quot;, &quot;.py&quot;)</span>
<span class="gi">+        self.exitcode = os.system(f&#39;{editor} &quot;{sfile}&quot;&#39;)</span>
<span class="gh">diff --git a/scrapy/commands/fetch.py b/scrapy/commands/fetch.py</span>
<span class="gh">index 59dcdb771..cdb7ad4ae 100644</span>
<span class="gd">--- a/scrapy/commands/fetch.py</span>
<span class="gi">+++ b/scrapy/commands/fetch.py</span>
<span class="gu">@@ -1,7 +1,9 @@</span>
<span class="w"> </span>import sys
<span class="w"> </span>from argparse import Namespace
<span class="w"> </span>from typing import List, Type
<span class="gi">+</span>
<span class="w"> </span>from w3lib.url import is_url
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.exceptions import UsageError
<span class="gu">@@ -12,3 +14,74 @@ from scrapy.utils.spider import DefaultSpider, spidercls_for_request</span>

<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = False
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;url&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Fetch a URL using the Scrapy downloader&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def long_desc(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            &quot;Fetch a URL using the Scrapy downloader and print its content&quot;</span>
<span class="gi">+            &quot; to stdout. You may want to use --nolog to disable logging&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(&quot;--spider&quot;, dest=&quot;spider&quot;, help=&quot;use this spider&quot;)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--headers&quot;,</span>
<span class="gi">+            dest=&quot;headers&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;print response HTTP headers instead of body&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--no-redirect&quot;,</span>
<span class="gi">+            dest=&quot;no_redirect&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            default=False,</span>
<span class="gi">+            help=&quot;do not handle HTTP 3xx status codes and print response as-is&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _print_headers(self, headers, prefix):</span>
<span class="gi">+        for key, values in headers.items():</span>
<span class="gi">+            for value in values:</span>
<span class="gi">+                self._print_bytes(prefix + b&quot; &quot; + key + b&quot;: &quot; + value)</span>
<span class="gi">+</span>
<span class="gi">+    def _print_response(self, response, opts):</span>
<span class="gi">+        if opts.headers:</span>
<span class="gi">+            self._print_headers(response.request.headers, b&quot;&gt;&quot;)</span>
<span class="gi">+            print(&quot;&gt;&quot;)</span>
<span class="gi">+            self._print_headers(response.headers, b&quot;&lt;&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._print_bytes(response.body)</span>
<span class="gi">+</span>
<span class="gi">+    def _print_bytes(self, bytes_):</span>
<span class="gi">+        sys.stdout.buffer.write(bytes_ + b&quot;\n&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args: List[str], opts: Namespace) -&gt; None:</span>
<span class="gi">+        if len(args) != 1 or not is_url(args[0]):</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+        request = Request(</span>
<span class="gi">+            args[0],</span>
<span class="gi">+            callback=self._print_response,</span>
<span class="gi">+            cb_kwargs={&quot;opts&quot;: opts},</span>
<span class="gi">+            dont_filter=True,</span>
<span class="gi">+        )</span>
<span class="gi">+        # by default, let the framework handle redirects,</span>
<span class="gi">+        # i.e. command handles all codes expect 3xx</span>
<span class="gi">+        if not opts.no_redirect:</span>
<span class="gi">+            request.meta[&quot;handle_httpstatus_list&quot;] = SequenceExclude(range(300, 400))</span>
<span class="gi">+        else:</span>
<span class="gi">+            request.meta[&quot;handle_httpstatus_all&quot;] = True</span>
<span class="gi">+</span>
<span class="gi">+        spidercls: Type[Spider] = DefaultSpider</span>
<span class="gi">+        assert self.crawler_process</span>
<span class="gi">+        spider_loader = self.crawler_process.spider_loader</span>
<span class="gi">+        if opts.spider:</span>
<span class="gi">+            spidercls = spider_loader.load(opts.spider)</span>
<span class="gi">+        else:</span>
<span class="gi">+            spidercls = spidercls_for_request(spider_loader, request, spidercls)</span>
<span class="gi">+        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])</span>
<span class="gi">+        self.crawler_process.start()</span>
<span class="gh">diff --git a/scrapy/commands/genspider.py b/scrapy/commands/genspider.py</span>
<span class="gh">index 5c5068083..68cbe8ff6 100644</span>
<span class="gd">--- a/scrapy/commands/genspider.py</span>
<span class="gi">+++ b/scrapy/commands/genspider.py</span>
<span class="gu">@@ -5,6 +5,7 @@ from importlib import import_module</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import Optional, cast
<span class="w"> </span>from urllib.parse import urlparse
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.exceptions import UsageError
<span class="gu">@@ -16,23 +17,186 @@ def sanitize_module_name(module_name):</span>
<span class="w"> </span>    with underscores and prefixing it with a letter if it doesn&#39;t start
<span class="w"> </span>    with one
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    module_name = module_name.replace(&quot;-&quot;, &quot;_&quot;).replace(&quot;.&quot;, &quot;_&quot;)</span>
<span class="gi">+    if module_name[0] not in string.ascii_letters:</span>
<span class="gi">+        module_name = &quot;a&quot; + module_name</span>
<span class="gi">+    return module_name</span>


<span class="w"> </span>def extract_domain(url):
<span class="w"> </span>    &quot;&quot;&quot;Extract domain name from URL string&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    o = urlparse(url)</span>
<span class="gi">+    if o.scheme == &quot;&quot; and o.netloc == &quot;&quot;:</span>
<span class="gi">+        o = urlparse(&quot;//&quot; + url.lstrip(&quot;/&quot;))</span>
<span class="gi">+    return o.netloc</span>


<span class="w"> </span>def verify_url_scheme(url):
<span class="w"> </span>    &quot;&quot;&quot;Check url for scheme and insert https if none found.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    parsed = urlparse(url)</span>
<span class="gi">+    if parsed.scheme == &quot;&quot; and parsed.netloc == &quot;&quot;:</span>
<span class="gi">+        parsed = urlparse(&quot;//&quot; + url)._replace(scheme=&quot;https&quot;)</span>
<span class="gi">+    return parsed.geturl()</span>


<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = False
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;name&gt; &lt;domain&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Generate new spider using pre-defined templates&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-l&quot;,</span>
<span class="gi">+            &quot;--list&quot;,</span>
<span class="gi">+            dest=&quot;list&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;List available templates&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-e&quot;,</span>
<span class="gi">+            &quot;--edit&quot;,</span>
<span class="gi">+            dest=&quot;edit&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;Edit spider after creating it&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-d&quot;,</span>
<span class="gi">+            &quot;--dump&quot;,</span>
<span class="gi">+            dest=&quot;dump&quot;,</span>
<span class="gi">+            metavar=&quot;TEMPLATE&quot;,</span>
<span class="gi">+            help=&quot;Dump template to standard output&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-t&quot;,</span>
<span class="gi">+            &quot;--template&quot;,</span>
<span class="gi">+            dest=&quot;template&quot;,</span>
<span class="gi">+            default=&quot;basic&quot;,</span>
<span class="gi">+            help=&quot;Uses a custom template.&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--force&quot;,</span>
<span class="gi">+            dest=&quot;force&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;If the spider already exists, overwrite it with the template&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if opts.list:</span>
<span class="gi">+            self._list_templates()</span>
<span class="gi">+            return</span>
<span class="gi">+        if opts.dump:</span>
<span class="gi">+            template_file = self._find_template(opts.dump)</span>
<span class="gi">+            if template_file:</span>
<span class="gi">+                print(template_file.read_text(encoding=&quot;utf-8&quot;))</span>
<span class="gi">+            return</span>
<span class="gi">+        if len(args) != 2:</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+</span>
<span class="gi">+        name, url = args[0:2]</span>
<span class="gi">+        url = verify_url_scheme(url)</span>
<span class="gi">+        module = sanitize_module_name(name)</span>
<span class="gi">+</span>
<span class="gi">+        if self.settings.get(&quot;BOT_NAME&quot;) == module:</span>
<span class="gi">+            print(&quot;Cannot create a spider with the same name as your project&quot;)</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if not opts.force and self._spider_exists(name):</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        template_file = self._find_template(opts.template)</span>
<span class="gi">+        if template_file:</span>
<span class="gi">+            self._genspider(module, name, url, opts.template, template_file)</span>
<span class="gi">+            if opts.edit:</span>
<span class="gi">+                self.exitcode = os.system(f&#39;scrapy edit &quot;{name}&quot;&#39;)</span>

<span class="w"> </span>    def _genspider(self, module, name, url, template_name, template_file):
<span class="w"> </span>        &quot;&quot;&quot;Generate the spider module, based on the given template&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        capitalized_module = &quot;&quot;.join(s.capitalize() for s in module.split(&quot;_&quot;))</span>
<span class="gi">+        domain = extract_domain(url)</span>
<span class="gi">+        tvars = {</span>
<span class="gi">+            &quot;project_name&quot;: self.settings.get(&quot;BOT_NAME&quot;),</span>
<span class="gi">+            &quot;ProjectName&quot;: string_camelcase(self.settings.get(&quot;BOT_NAME&quot;)),</span>
<span class="gi">+            &quot;module&quot;: module,</span>
<span class="gi">+            &quot;name&quot;: name,</span>
<span class="gi">+            &quot;url&quot;: url,</span>
<span class="gi">+            &quot;domain&quot;: domain,</span>
<span class="gi">+            &quot;classname&quot;: f&quot;{capitalized_module}Spider&quot;,</span>
<span class="gi">+        }</span>
<span class="gi">+        if self.settings.get(&quot;NEWSPIDER_MODULE&quot;):</span>
<span class="gi">+            spiders_module = import_module(self.settings[&quot;NEWSPIDER_MODULE&quot;])</span>
<span class="gi">+            spiders_dir = Path(spiders_module.__file__).parent.resolve()</span>
<span class="gi">+        else:</span>
<span class="gi">+            spiders_module = None</span>
<span class="gi">+            spiders_dir = Path(&quot;.&quot;)</span>
<span class="gi">+        spider_file = f&quot;{spiders_dir / module}.py&quot;</span>
<span class="gi">+        shutil.copyfile(template_file, spider_file)</span>
<span class="gi">+        render_templatefile(spider_file, **tvars)</span>
<span class="gi">+        print(</span>
<span class="gi">+            f&quot;Created spider {name!r} using template {template_name!r} &quot;,</span>
<span class="gi">+            end=(&quot;&quot; if spiders_module else &quot;\n&quot;),</span>
<span class="gi">+        )</span>
<span class="gi">+        if spiders_module:</span>
<span class="gi">+            print(f&quot;in module:\n  {spiders_module.__name__}.{module}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _find_template(self, template: str) -&gt; Optional[Path]:</span>
<span class="gi">+        template_file = Path(self.templates_dir, f&quot;{template}.tmpl&quot;)</span>
<span class="gi">+        if template_file.exists():</span>
<span class="gi">+            return template_file</span>
<span class="gi">+        print(f&quot;Unable to find template: {template}\n&quot;)</span>
<span class="gi">+        print(&#39;Use &quot;scrapy genspider --list&quot; to see all available templates.&#39;)</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    def _list_templates(self):</span>
<span class="gi">+        print(&quot;Available templates:&quot;)</span>
<span class="gi">+        for file in sorted(Path(self.templates_dir).iterdir()):</span>
<span class="gi">+            if file.suffix == &quot;.tmpl&quot;:</span>
<span class="gi">+                print(f&quot;  {file.stem}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _spider_exists(self, name: str) -&gt; bool:</span>
<span class="gi">+        if not self.settings.get(&quot;NEWSPIDER_MODULE&quot;):</span>
<span class="gi">+            # if run as a standalone command and file with same filename already exists</span>
<span class="gi">+            path = Path(name + &quot;.py&quot;)</span>
<span class="gi">+            if path.exists():</span>
<span class="gi">+                print(f&quot;{path.resolve()} already exists&quot;)</span>
<span class="gi">+                return True</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        assert (</span>
<span class="gi">+            self.crawler_process is not None</span>
<span class="gi">+        ), &quot;crawler_process must be set before calling run&quot;</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            spidercls = self.crawler_process.spider_loader.load(name)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass</span>
<span class="gi">+        else:</span>
<span class="gi">+            # if spider with same name exists</span>
<span class="gi">+            print(f&quot;Spider {name!r} already exists in module:&quot;)</span>
<span class="gi">+            print(f&quot;  {spidercls.__module__}&quot;)</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        # a file with the same name exists in the target directory</span>
<span class="gi">+        spiders_module = import_module(self.settings[&quot;NEWSPIDER_MODULE&quot;])</span>
<span class="gi">+        spiders_dir = Path(cast(str, spiders_module.__file__)).parent</span>
<span class="gi">+        spiders_dir_abs = spiders_dir.resolve()</span>
<span class="gi">+        path = spiders_dir_abs / (name + &quot;.py&quot;)</span>
<span class="gi">+        if path.exists():</span>
<span class="gi">+            print(f&quot;{path} already exists&quot;)</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def templates_dir(self) -&gt; str:</span>
<span class="gi">+        return str(</span>
<span class="gi">+            Path(</span>
<span class="gi">+                self.settings[&quot;TEMPLATES_DIR&quot;] or Path(scrapy.__path__[0], &quot;templates&quot;),</span>
<span class="gi">+                &quot;spiders&quot;,</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/commands/list.py b/scrapy/commands/list.py</span>
<span class="gh">index 59078bd88..2f5032360 100644</span>
<span class="gd">--- a/scrapy/commands/list.py</span>
<span class="gi">+++ b/scrapy/commands/list.py</span>
<span class="gu">@@ -3,4 +3,11 @@ from scrapy.commands import ScrapyCommand</span>

<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = True
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False}</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;List available spiders&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        for s in sorted(self.crawler_process.spider_loader.list()):</span>
<span class="gi">+            print(s)</span>
<span class="gh">diff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py</span>
<span class="gh">index 63e47a92b..c9f8586d3 100644</span>
<span class="gd">--- a/scrapy/commands/parse.py</span>
<span class="gi">+++ b/scrapy/commands/parse.py</span>
<span class="gu">@@ -3,9 +3,11 @@ import inspect</span>
<span class="w"> </span>import json
<span class="w"> </span>import logging
<span class="w"> </span>from typing import Dict
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter, is_item
<span class="w"> </span>from twisted.internet.defer import maybeDeferred
<span class="w"> </span>from w3lib.url import is_url
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import BaseRunSpiderCommand
<span class="w"> </span>from scrapy.exceptions import UsageError
<span class="w"> </span>from scrapy.http import Request
<span class="gu">@@ -15,12 +17,338 @@ from scrapy.utils.defer import aiter_errback, deferred_from_coro</span>
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info
<span class="w"> </span>from scrapy.utils.misc import arg_to_iter
<span class="w"> </span>from scrapy.utils.spider import spidercls_for_request
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class Command(BaseRunSpiderCommand):
<span class="w"> </span>    requires_project = True
<span class="gi">+</span>
<span class="w"> </span>    spider = None
<span class="w"> </span>    items: Dict[int, list] = {}
<span class="w"> </span>    requests: Dict[int, list] = {}
<span class="gi">+</span>
<span class="w"> </span>    first_response = None
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;url&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Parse URL (using its spider) and print the results&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        BaseRunSpiderCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--spider&quot;,</span>
<span class="gi">+            dest=&quot;spider&quot;,</span>
<span class="gi">+            default=None,</span>
<span class="gi">+            help=&quot;use this spider without looking for one&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--pipelines&quot;, action=&quot;store_true&quot;, help=&quot;process items through pipelines&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--nolinks&quot;,</span>
<span class="gi">+            dest=&quot;nolinks&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;don&#39;t show links to follow (extracted requests)&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--noitems&quot;,</span>
<span class="gi">+            dest=&quot;noitems&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;don&#39;t show scraped items&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--nocolour&quot;,</span>
<span class="gi">+            dest=&quot;nocolour&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;avoid using pygments to colorize the output&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-r&quot;,</span>
<span class="gi">+            &quot;--rules&quot;,</span>
<span class="gi">+            dest=&quot;rules&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;use CrawlSpider rules to discover the callback&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-c&quot;,</span>
<span class="gi">+            &quot;--callback&quot;,</span>
<span class="gi">+            dest=&quot;callback&quot;,</span>
<span class="gi">+            help=&quot;use this callback for parsing, instead looking for a callback&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-m&quot;,</span>
<span class="gi">+            &quot;--meta&quot;,</span>
<span class="gi">+            dest=&quot;meta&quot;,</span>
<span class="gi">+            help=&quot;inject extra meta into the Request, it must be a valid raw json string&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--cbkwargs&quot;,</span>
<span class="gi">+            dest=&quot;cbkwargs&quot;,</span>
<span class="gi">+            help=&quot;inject extra callback kwargs into the Request, it must be a valid raw json string&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-d&quot;,</span>
<span class="gi">+            &quot;--depth&quot;,</span>
<span class="gi">+            dest=&quot;depth&quot;,</span>
<span class="gi">+            type=int,</span>
<span class="gi">+            default=1,</span>
<span class="gi">+            help=&quot;maximum depth for parsing requests [default: %(default)s]&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-v&quot;,</span>
<span class="gi">+            &quot;--verbose&quot;,</span>
<span class="gi">+            dest=&quot;verbose&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;print each depth level one by one&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def max_level(self):</span>
<span class="gi">+        max_items, max_requests = 0, 0</span>
<span class="gi">+        if self.items:</span>
<span class="gi">+            max_items = max(self.items)</span>
<span class="gi">+        if self.requests:</span>
<span class="gi">+            max_requests = max(self.requests)</span>
<span class="gi">+        return max(max_items, max_requests)</span>
<span class="gi">+</span>
<span class="gi">+    def handle_exception(self, _failure):</span>
<span class="gi">+        logger.error(</span>
<span class="gi">+            &quot;An error is caught while iterating the async iterable&quot;,</span>
<span class="gi">+            exc_info=failure_to_exc_info(_failure),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def iterate_spider_output(self, result):</span>
<span class="gi">+        if inspect.isasyncgen(result):</span>
<span class="gi">+            d = deferred_from_coro(</span>
<span class="gi">+                collect_asyncgen(aiter_errback(result, self.handle_exception))</span>
<span class="gi">+            )</span>
<span class="gi">+            d.addCallback(self.iterate_spider_output)</span>
<span class="gi">+            return d</span>
<span class="gi">+        if inspect.iscoroutine(result):</span>
<span class="gi">+            d = deferred_from_coro(result)</span>
<span class="gi">+            d.addCallback(self.iterate_spider_output)</span>
<span class="gi">+            return d</span>
<span class="gi">+        return arg_to_iter(deferred_from_coro(result))</span>
<span class="gi">+</span>
<span class="gi">+    def add_items(self, lvl, new_items):</span>
<span class="gi">+        old_items = self.items.get(lvl, [])</span>
<span class="gi">+        self.items[lvl] = old_items + new_items</span>
<span class="gi">+</span>
<span class="gi">+    def add_requests(self, lvl, new_reqs):</span>
<span class="gi">+        old_reqs = self.requests.get(lvl, [])</span>
<span class="gi">+        self.requests[lvl] = old_reqs + new_reqs</span>
<span class="gi">+</span>
<span class="gi">+    def print_items(self, lvl=None, colour=True):</span>
<span class="gi">+        if lvl is None:</span>
<span class="gi">+            items = [item for lst in self.items.values() for item in lst]</span>
<span class="gi">+        else:</span>
<span class="gi">+            items = self.items.get(lvl, [])</span>
<span class="gi">+</span>
<span class="gi">+        print(&quot;# Scraped Items &quot;, &quot;-&quot; * 60)</span>
<span class="gi">+        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)</span>
<span class="gi">+</span>
<span class="gi">+    def print_requests(self, lvl=None, colour=True):</span>
<span class="gi">+        if lvl is None:</span>
<span class="gi">+            if self.requests:</span>
<span class="gi">+                requests = self.requests[max(self.requests)]</span>
<span class="gi">+            else:</span>
<span class="gi">+                requests = []</span>
<span class="gi">+        else:</span>
<span class="gi">+            requests = self.requests.get(lvl, [])</span>
<span class="gi">+</span>
<span class="gi">+        print(&quot;# Requests &quot;, &quot;-&quot; * 65)</span>
<span class="gi">+        display.pprint(requests, colorize=colour)</span>
<span class="gi">+</span>
<span class="gi">+    def print_results(self, opts):</span>
<span class="gi">+        colour = not opts.nocolour</span>
<span class="gi">+</span>
<span class="gi">+        if opts.verbose:</span>
<span class="gi">+            for level in range(1, self.max_level + 1):</span>
<span class="gi">+                print(f&quot;\n&gt;&gt;&gt; DEPTH LEVEL: {level} &lt;&lt;&lt;&quot;)</span>
<span class="gi">+                if not opts.noitems:</span>
<span class="gi">+                    self.print_items(level, colour)</span>
<span class="gi">+                if not opts.nolinks:</span>
<span class="gi">+                    self.print_requests(level, colour)</span>
<span class="gi">+        else:</span>
<span class="gi">+            print(f&quot;\n&gt;&gt;&gt; STATUS DEPTH LEVEL {self.max_level} &lt;&lt;&lt;&quot;)</span>
<span class="gi">+            if not opts.noitems:</span>
<span class="gi">+                self.print_items(colour=colour)</span>
<span class="gi">+            if not opts.nolinks:</span>
<span class="gi">+                self.print_requests(colour=colour)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_items_and_requests(self, spider_output, opts, depth, spider, callback):</span>
<span class="gi">+        items, requests = [], []</span>
<span class="gi">+        for x in spider_output:</span>
<span class="gi">+            if is_item(x):</span>
<span class="gi">+                items.append(x)</span>
<span class="gi">+            elif isinstance(x, Request):</span>
<span class="gi">+                requests.append(x)</span>
<span class="gi">+        return items, requests, opts, depth, spider, callback</span>
<span class="gi">+</span>
<span class="gi">+    def run_callback(self, response, callback, cb_kwargs=None):</span>
<span class="gi">+        cb_kwargs = cb_kwargs or {}</span>
<span class="gi">+        d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def get_callback_from_rules(self, spider, response):</span>
<span class="gi">+        if getattr(spider, &quot;rules&quot;, None):</span>
<span class="gi">+            for rule in spider.rules:</span>
<span class="gi">+                if rule.link_extractor.matches(response.url):</span>
<span class="gi">+                    return rule.callback or &quot;parse&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;No CrawlSpider rules found in spider %(spider)r, &quot;</span>
<span class="gi">+                &quot;please specify a callback to use for parsing&quot;,</span>
<span class="gi">+                {&quot;spider&quot;: spider.name},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def set_spidercls(self, url, opts):</span>
<span class="gi">+        spider_loader = self.crawler_process.spider_loader</span>
<span class="gi">+        if opts.spider:</span>
<span class="gi">+            try:</span>
<span class="gi">+                self.spidercls = spider_loader.load(opts.spider)</span>
<span class="gi">+            except KeyError:</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    &quot;Unable to find spider: %(spider)s&quot;, {&quot;spider&quot;: opts.spider}</span>
<span class="gi">+                )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.spidercls = spidercls_for_request(spider_loader, Request(url))</span>
<span class="gi">+            if not self.spidercls:</span>
<span class="gi">+                logger.error(&quot;Unable to find spider for: %(url)s&quot;, {&quot;url&quot;: url})</span>
<span class="gi">+</span>
<span class="gi">+        def _start_requests(spider):</span>
<span class="gi">+            yield self.prepare_request(spider, Request(url), opts)</span>
<span class="gi">+</span>
<span class="gi">+        if self.spidercls:</span>
<span class="gi">+            self.spidercls.start_requests = _start_requests</span>
<span class="gi">+</span>
<span class="gi">+    def start_parsing(self, url, opts):</span>
<span class="gi">+        self.crawler_process.crawl(self.spidercls, **opts.spargs)</span>
<span class="gi">+        self.pcrawler = list(self.crawler_process.crawlers)[0]</span>
<span class="gi">+        self.crawler_process.start()</span>
<span class="gi">+</span>
<span class="gi">+        if not self.first_response:</span>
<span class="gi">+            logger.error(&quot;No response downloaded for: %(url)s&quot;, {&quot;url&quot;: url})</span>
<span class="gi">+</span>
<span class="gi">+    def scraped_data(self, args):</span>
<span class="gi">+        items, requests, opts, depth, spider, callback = args</span>
<span class="gi">+        if opts.pipelines:</span>
<span class="gi">+            itemproc = self.pcrawler.engine.scraper.itemproc</span>
<span class="gi">+            for item in items:</span>
<span class="gi">+                itemproc.process_item(item, spider)</span>
<span class="gi">+        self.add_items(depth, items)</span>
<span class="gi">+        self.add_requests(depth, requests)</span>
<span class="gi">+</span>
<span class="gi">+        scraped_data = items if opts.output else []</span>
<span class="gi">+        if depth &lt; opts.depth:</span>
<span class="gi">+            for req in requests:</span>
<span class="gi">+                req.meta[&quot;_depth&quot;] = depth + 1</span>
<span class="gi">+                req.meta[&quot;_callback&quot;] = req.callback</span>
<span class="gi">+                req.callback = callback</span>
<span class="gi">+            scraped_data += requests</span>
<span class="gi">+</span>
<span class="gi">+        return scraped_data</span>
<span class="gi">+</span>
<span class="gi">+    def _get_callback(self, *, spider, opts, response=None):</span>
<span class="gi">+        cb = None</span>
<span class="gi">+        if response:</span>
<span class="gi">+            cb = response.meta[&quot;_callback&quot;]</span>
<span class="gi">+        if not cb:</span>
<span class="gi">+            if opts.callback:</span>
<span class="gi">+                cb = opts.callback</span>
<span class="gi">+            elif response and opts.rules and self.first_response == response:</span>
<span class="gi">+                cb = self.get_callback_from_rules(spider, response)</span>
<span class="gi">+                if not cb:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;Cannot find a rule that matches {response.url!r} in spider: &quot;</span>
<span class="gi">+                        f&quot;{spider.name}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+            else:</span>
<span class="gi">+                cb = &quot;parse&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if not callable(cb):</span>
<span class="gi">+            cb_method = getattr(spider, cb, None)</span>
<span class="gi">+            if callable(cb_method):</span>
<span class="gi">+                cb = cb_method</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;Cannot find callback {cb!r} in spider: {spider.name}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+        return cb</span>
<span class="gi">+</span>
<span class="gi">+    def prepare_request(self, spider, request, opts):</span>
<span class="gi">+        def callback(response, **cb_kwargs):</span>
<span class="gi">+            # memorize first request</span>
<span class="gi">+            if not self.first_response:</span>
<span class="gi">+                self.first_response = response</span>
<span class="gi">+</span>
<span class="gi">+            cb = self._get_callback(spider=spider, opts=opts, response=response)</span>
<span class="gi">+</span>
<span class="gi">+            # parse items and requests</span>
<span class="gi">+            depth = response.meta[&quot;_depth&quot;]</span>
<span class="gi">+</span>
<span class="gi">+            d = self.run_callback(response, cb, cb_kwargs)</span>
<span class="gi">+            d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)</span>
<span class="gi">+            d.addCallback(self.scraped_data)</span>
<span class="gi">+            return d</span>
<span class="gi">+</span>
<span class="gi">+        # update request meta if any extra meta was passed through the --meta/-m opts.</span>
<span class="gi">+        if opts.meta:</span>
<span class="gi">+            request.meta.update(opts.meta)</span>
<span class="gi">+</span>
<span class="gi">+        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.</span>
<span class="gi">+        if opts.cbkwargs:</span>
<span class="gi">+            request.cb_kwargs.update(opts.cbkwargs)</span>
<span class="gi">+</span>
<span class="gi">+        request.meta[&quot;_depth&quot;] = 1</span>
<span class="gi">+        request.meta[&quot;_callback&quot;] = request.callback</span>
<span class="gi">+        if not request.callback and not opts.rules:</span>
<span class="gi">+            cb = self._get_callback(spider=spider, opts=opts)</span>
<span class="gi">+            functools.update_wrapper(callback, cb)</span>
<span class="gi">+        request.callback = callback</span>
<span class="gi">+        return request</span>
<span class="gi">+</span>
<span class="gi">+    def process_options(self, args, opts):</span>
<span class="gi">+        BaseRunSpiderCommand.process_options(self, args, opts)</span>
<span class="gi">+</span>
<span class="gi">+        self.process_request_meta(opts)</span>
<span class="gi">+        self.process_request_cb_kwargs(opts)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request_meta(self, opts):</span>
<span class="gi">+        if opts.meta:</span>
<span class="gi">+            try:</span>
<span class="gi">+                opts.meta = json.loads(opts.meta)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                raise UsageError(</span>
<span class="gi">+                    &quot;Invalid -m/--meta value, pass a valid json string to -m or --meta. &quot;</span>
<span class="gi">+                    &#39;Example: --meta=\&#39;{&quot;foo&quot; : &quot;bar&quot;}\&#39;&#39;,</span>
<span class="gi">+                    print_help=False,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    def process_request_cb_kwargs(self, opts):</span>
<span class="gi">+        if opts.cbkwargs:</span>
<span class="gi">+            try:</span>
<span class="gi">+                opts.cbkwargs = json.loads(opts.cbkwargs)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                raise UsageError(</span>
<span class="gi">+                    &quot;Invalid --cbkwargs value, pass a valid json string to --cbkwargs. &quot;</span>
<span class="gi">+                    &#39;Example: --cbkwargs=\&#39;{&quot;foo&quot; : &quot;bar&quot;}\&#39;&#39;,</span>
<span class="gi">+                    print_help=False,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        # parse arguments</span>
<span class="gi">+        if not len(args) == 1 or not is_url(args[0]):</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+        else:</span>
<span class="gi">+            url = args[0]</span>
<span class="gi">+</span>
<span class="gi">+        # prepare spidercls</span>
<span class="gi">+        self.set_spidercls(url, opts)</span>
<span class="gi">+</span>
<span class="gi">+        if self.spidercls and opts.depth &gt; 0:</span>
<span class="gi">+            self.start_parsing(url, opts)</span>
<span class="gi">+            self.print_results(opts)</span>
<span class="gh">diff --git a/scrapy/commands/runspider.py b/scrapy/commands/runspider.py</span>
<span class="gh">index d6b20ae5f..58ed89a81 100644</span>
<span class="gd">--- a/scrapy/commands/runspider.py</span>
<span class="gi">+++ b/scrapy/commands/runspider.py</span>
<span class="gu">@@ -4,11 +4,55 @@ from os import PathLike</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from types import ModuleType
<span class="w"> </span>from typing import Union
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import BaseRunSpiderCommand
<span class="w"> </span>from scrapy.exceptions import UsageError
<span class="w"> </span>from scrapy.utils.spider import iter_spider_classes


<span class="gi">+def _import_file(filepath: Union[str, PathLike]) -&gt; ModuleType:</span>
<span class="gi">+    abspath = Path(filepath).resolve()</span>
<span class="gi">+    if abspath.suffix not in (&quot;.py&quot;, &quot;.pyw&quot;):</span>
<span class="gi">+        raise ValueError(f&quot;Not a Python source file: {abspath}&quot;)</span>
<span class="gi">+    dirname = str(abspath.parent)</span>
<span class="gi">+    sys.path = [dirname] + sys.path</span>
<span class="gi">+    try:</span>
<span class="gi">+        module = import_module(abspath.stem)</span>
<span class="gi">+    finally:</span>
<span class="gi">+        sys.path.pop(0)</span>
<span class="gi">+    return module</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class Command(BaseRunSpiderCommand):
<span class="w"> </span>    requires_project = False
<span class="gd">-    default_settings = {&#39;SPIDER_LOADER_WARN_ONLY&#39;: True}</span>
<span class="gi">+    default_settings = {&quot;SPIDER_LOADER_WARN_ONLY&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options] &lt;spider_file&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Run a self-contained spider (without creating a project)&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def long_desc(self):</span>
<span class="gi">+        return &quot;Run the spider defined in the given file&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if len(args) != 1:</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+        filename = Path(args[0])</span>
<span class="gi">+        if not filename.exists():</span>
<span class="gi">+            raise UsageError(f&quot;File not found: {filename}\n&quot;)</span>
<span class="gi">+        try:</span>
<span class="gi">+            module = _import_file(filename)</span>
<span class="gi">+        except (ImportError, ValueError) as e:</span>
<span class="gi">+            raise UsageError(f&quot;Unable to load {str(filename)!r}: {e}\n&quot;)</span>
<span class="gi">+        spclasses = list(iter_spider_classes(module))</span>
<span class="gi">+        if not spclasses:</span>
<span class="gi">+            raise UsageError(f&quot;No spider found in file: {filename}\n&quot;)</span>
<span class="gi">+        spidercls = spclasses.pop()</span>
<span class="gi">+</span>
<span class="gi">+        self.crawler_process.crawl(spidercls, **opts.spargs)</span>
<span class="gi">+        self.crawler_process.start()</span>
<span class="gi">+</span>
<span class="gi">+        if self.crawler_process.bootstrap_failed:</span>
<span class="gi">+            self.exitcode = 1</span>
<span class="gh">diff --git a/scrapy/commands/settings.py b/scrapy/commands/settings.py</span>
<span class="gh">index 017f56138..318187204 100644</span>
<span class="gd">--- a/scrapy/commands/settings.py</span>
<span class="gi">+++ b/scrapy/commands/settings.py</span>
<span class="gu">@@ -1,8 +1,62 @@</span>
<span class="w"> </span>import json
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.settings import BaseSettings


<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = False
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False, &#39;SPIDER_LOADER_WARN_ONLY&#39;: True}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False, &quot;SPIDER_LOADER_WARN_ONLY&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[options]&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Get settings values&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--get&quot;, dest=&quot;get&quot;, metavar=&quot;SETTING&quot;, help=&quot;print raw setting value&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--getbool&quot;,</span>
<span class="gi">+            dest=&quot;getbool&quot;,</span>
<span class="gi">+            metavar=&quot;SETTING&quot;,</span>
<span class="gi">+            help=&quot;print setting value, interpreted as a boolean&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--getint&quot;,</span>
<span class="gi">+            dest=&quot;getint&quot;,</span>
<span class="gi">+            metavar=&quot;SETTING&quot;,</span>
<span class="gi">+            help=&quot;print setting value, interpreted as an integer&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--getfloat&quot;,</span>
<span class="gi">+            dest=&quot;getfloat&quot;,</span>
<span class="gi">+            metavar=&quot;SETTING&quot;,</span>
<span class="gi">+            help=&quot;print setting value, interpreted as a float&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--getlist&quot;,</span>
<span class="gi">+            dest=&quot;getlist&quot;,</span>
<span class="gi">+            metavar=&quot;SETTING&quot;,</span>
<span class="gi">+            help=&quot;print setting value, interpreted as a list&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        settings = self.crawler_process.settings</span>
<span class="gi">+        if opts.get:</span>
<span class="gi">+            s = settings.get(opts.get)</span>
<span class="gi">+            if isinstance(s, BaseSettings):</span>
<span class="gi">+                print(json.dumps(s.copy_to_dict()))</span>
<span class="gi">+            else:</span>
<span class="gi">+                print(s)</span>
<span class="gi">+        elif opts.getbool:</span>
<span class="gi">+            print(settings.getbool(opts.getbool))</span>
<span class="gi">+        elif opts.getint:</span>
<span class="gi">+            print(settings.getint(opts.getint))</span>
<span class="gi">+        elif opts.getfloat:</span>
<span class="gi">+            print(settings.getfloat(opts.getfloat))</span>
<span class="gi">+        elif opts.getlist:</span>
<span class="gi">+            print(settings.getlist(opts.getlist))</span>
<span class="gh">diff --git a/scrapy/commands/shell.py b/scrapy/commands/shell.py</span>
<span class="gh">index fadb2c519..12e37babc 100644</span>
<span class="gd">--- a/scrapy/commands/shell.py</span>
<span class="gi">+++ b/scrapy/commands/shell.py</span>
<span class="gu">@@ -6,6 +6,7 @@ See documentation in docs/topics/shell.rst</span>
<span class="w"> </span>from argparse import Namespace
<span class="w"> </span>from threading import Thread
<span class="w"> </span>from typing import List, Type
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.http import Request
<span class="gu">@@ -16,11 +17,80 @@ from scrapy.utils.url import guess_scheme</span>

<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = False
<span class="gd">-    default_settings = {&#39;KEEP_ALIVE&#39;: True, &#39;LOGSTATS_INTERVAL&#39;: 0,</span>
<span class="gd">-        &#39;DUPEFILTER_CLASS&#39;: &#39;scrapy.dupefilters.BaseDupeFilter&#39;}</span>
<span class="gi">+    default_settings = {</span>
<span class="gi">+        &quot;KEEP_ALIVE&quot;: True,</span>
<span class="gi">+        &quot;LOGSTATS_INTERVAL&quot;: 0,</span>
<span class="gi">+        &quot;DUPEFILTER_CLASS&quot;: &quot;scrapy.dupefilters.BaseDupeFilter&quot;,</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[url|file]&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Interactive scraping console&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def long_desc(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            &quot;Interactive console for scraping the given url or file. &quot;</span>
<span class="gi">+            &quot;Use ./file.html syntax or full path for local file.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;-c&quot;,</span>
<span class="gi">+            dest=&quot;code&quot;,</span>
<span class="gi">+            help=&quot;evaluate the code in the shell, print the result and exit&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        parser.add_argument(&quot;--spider&quot;, dest=&quot;spider&quot;, help=&quot;use this spider&quot;)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--no-redirect&quot;,</span>
<span class="gi">+            dest=&quot;no_redirect&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            default=False,</span>
<span class="gi">+            help=&quot;do not handle HTTP 3xx status codes and print response as-is&quot;,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def update_vars(self, vars):
<span class="w"> </span>        &quot;&quot;&quot;You can use this function to update the Scrapy objects that will be
<span class="w"> </span>        available in the shell
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        pass
<span class="gi">+</span>
<span class="gi">+    def run(self, args: List[str], opts: Namespace) -&gt; None:</span>
<span class="gi">+        url = args[0] if args else None</span>
<span class="gi">+        if url:</span>
<span class="gi">+            # first argument may be a local file</span>
<span class="gi">+            url = guess_scheme(url)</span>
<span class="gi">+</span>
<span class="gi">+        assert self.crawler_process</span>
<span class="gi">+        spider_loader = self.crawler_process.spider_loader</span>
<span class="gi">+</span>
<span class="gi">+        spidercls: Type[Spider] = DefaultSpider</span>
<span class="gi">+        if opts.spider:</span>
<span class="gi">+            spidercls = spider_loader.load(opts.spider)</span>
<span class="gi">+        elif url:</span>
<span class="gi">+            spidercls = spidercls_for_request(</span>
<span class="gi">+                spider_loader, Request(url), spidercls, log_multiple=True</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # The crawler is created this way since the Shell manually handles the</span>
<span class="gi">+        # crawling engine, so the set up in the crawl method won&#39;t work</span>
<span class="gi">+        crawler = self.crawler_process._create_crawler(spidercls)</span>
<span class="gi">+        crawler._apply_settings()</span>
<span class="gi">+        # The Shell class needs a persistent engine in the crawler</span>
<span class="gi">+        crawler.engine = crawler._create_engine()</span>
<span class="gi">+        crawler.engine.start()</span>
<span class="gi">+</span>
<span class="gi">+        self._start_crawler_thread()</span>
<span class="gi">+</span>
<span class="gi">+        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)</span>
<span class="gi">+        shell.start(url=url, redirect=not opts.no_redirect)</span>
<span class="gi">+</span>
<span class="gi">+    def _start_crawler_thread(self):</span>
<span class="gi">+        t = Thread(</span>
<span class="gi">+            target=self.crawler_process.start,</span>
<span class="gi">+            kwargs={&quot;stop_after_crawl&quot;: False, &quot;install_signal_handlers&quot;: False},</span>
<span class="gi">+        )</span>
<span class="gi">+        t.daemon = True</span>
<span class="gi">+        t.start()</span>
<span class="gh">diff --git a/scrapy/commands/startproject.py b/scrapy/commands/startproject.py</span>
<span class="gh">index c9783c28e..fde609c6f 100644</span>
<span class="gd">--- a/scrapy/commands/startproject.py</span>
<span class="gi">+++ b/scrapy/commands/startproject.py</span>
<span class="gu">@@ -5,19 +5,53 @@ from importlib.util import find_spec</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from shutil import copy2, copystat, ignore_patterns, move
<span class="w"> </span>from stat import S_IWUSR as OWNER_WRITE_PERMISSION
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.commands import ScrapyCommand
<span class="w"> </span>from scrapy.exceptions import UsageError
<span class="w"> </span>from scrapy.utils.template import render_templatefile, string_camelcase
<span class="gd">-TEMPLATES_TO_RENDER = (&#39;scrapy.cfg&#39;,), (&#39;${project_name}&#39;, &#39;settings.py.tmpl&#39;</span>
<span class="gd">-    ), (&#39;${project_name}&#39;, &#39;items.py.tmpl&#39;), (&#39;${project_name}&#39;,</span>
<span class="gd">-    &#39;pipelines.py.tmpl&#39;), (&#39;${project_name}&#39;, &#39;middlewares.py.tmpl&#39;)</span>
<span class="gd">-IGNORE = ignore_patterns(&#39;*.pyc&#39;, &#39;__pycache__&#39;, &#39;.svn&#39;)</span>
<span class="gi">+</span>
<span class="gi">+TEMPLATES_TO_RENDER = (</span>
<span class="gi">+    (&quot;scrapy.cfg&quot;,),</span>
<span class="gi">+    (&quot;${project_name}&quot;, &quot;settings.py.tmpl&quot;),</span>
<span class="gi">+    (&quot;${project_name}&quot;, &quot;items.py.tmpl&quot;),</span>
<span class="gi">+    (&quot;${project_name}&quot;, &quot;pipelines.py.tmpl&quot;),</span>
<span class="gi">+    (&quot;${project_name}&quot;, &quot;middlewares.py.tmpl&quot;),</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+IGNORE = ignore_patterns(&quot;*.pyc&quot;, &quot;__pycache__&quot;, &quot;.svn&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_writable(path):</span>
<span class="gi">+    current_permissions = os.stat(path).st_mode</span>
<span class="gi">+    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)</span>


<span class="w"> </span>class Command(ScrapyCommand):
<span class="w"> </span>    requires_project = False
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False, &#39;SPIDER_LOADER_WARN_ONLY&#39;: True}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False, &quot;SPIDER_LOADER_WARN_ONLY&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;&lt;project_name&gt; [project_dir]&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Create new project&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def _is_valid_name(self, project_name):</span>
<span class="gi">+        def _module_exists(module_name):</span>
<span class="gi">+            spec = find_spec(module_name)</span>
<span class="gi">+            return spec is not None and spec.loader is not None</span>
<span class="gi">+</span>
<span class="gi">+        if not re.search(r&quot;^[_a-zA-Z]\w*$&quot;, project_name):</span>
<span class="gi">+            print(</span>
<span class="gi">+                &quot;Error: Project names must begin with a letter and contain&quot;</span>
<span class="gi">+                &quot; only\nletters, numbers and underscores&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        elif _module_exists(project_name):</span>
<span class="gi">+            print(f&quot;Error: Module {project_name!r} already exists&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return True</span>
<span class="gi">+        return False</span>

<span class="w"> </span>    def _copytree(self, src: Path, dst: Path):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gu">@@ -28,4 +62,77 @@ class Command(ScrapyCommand):</span>
<span class="w"> </span>        More info at:
<span class="w"> </span>        https://github.com/scrapy/scrapy/pull/2005
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ignore = IGNORE</span>
<span class="gi">+        names = [x.name for x in src.iterdir()]</span>
<span class="gi">+        ignored_names = ignore(src, names)</span>
<span class="gi">+</span>
<span class="gi">+        if not dst.exists():</span>
<span class="gi">+            dst.mkdir(parents=True)</span>
<span class="gi">+</span>
<span class="gi">+        for name in names:</span>
<span class="gi">+            if name in ignored_names:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            srcname = src / name</span>
<span class="gi">+            dstname = dst / name</span>
<span class="gi">+            if srcname.is_dir():</span>
<span class="gi">+                self._copytree(srcname, dstname)</span>
<span class="gi">+            else:</span>
<span class="gi">+                copy2(srcname, dstname)</span>
<span class="gi">+                _make_writable(dstname)</span>
<span class="gi">+</span>
<span class="gi">+        copystat(src, dst)</span>
<span class="gi">+        _make_writable(dst)</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if len(args) not in (1, 2):</span>
<span class="gi">+            raise UsageError()</span>
<span class="gi">+</span>
<span class="gi">+        project_name = args[0]</span>
<span class="gi">+</span>
<span class="gi">+        if len(args) == 2:</span>
<span class="gi">+            project_dir = Path(args[1])</span>
<span class="gi">+        else:</span>
<span class="gi">+            project_dir = Path(args[0])</span>
<span class="gi">+</span>
<span class="gi">+        if (project_dir / &quot;scrapy.cfg&quot;).exists():</span>
<span class="gi">+            self.exitcode = 1</span>
<span class="gi">+            print(f&quot;Error: scrapy.cfg already exists in {project_dir.resolve()}&quot;)</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if not self._is_valid_name(project_name):</span>
<span class="gi">+            self.exitcode = 1</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        self._copytree(Path(self.templates_dir), project_dir.resolve())</span>
<span class="gi">+        move(project_dir / &quot;module&quot;, project_dir / project_name)</span>
<span class="gi">+        for paths in TEMPLATES_TO_RENDER:</span>
<span class="gi">+            tplfile = Path(</span>
<span class="gi">+                project_dir,</span>
<span class="gi">+                *(</span>
<span class="gi">+                    string.Template(s).substitute(project_name=project_name)</span>
<span class="gi">+                    for s in paths</span>
<span class="gi">+                ),</span>
<span class="gi">+            )</span>
<span class="gi">+            render_templatefile(</span>
<span class="gi">+                tplfile,</span>
<span class="gi">+                project_name=project_name,</span>
<span class="gi">+                ProjectName=string_camelcase(project_name),</span>
<span class="gi">+            )</span>
<span class="gi">+        print(</span>
<span class="gi">+            f&quot;New Scrapy project &#39;{project_name}&#39;, using template directory &quot;</span>
<span class="gi">+            f&quot;&#39;{self.templates_dir}&#39;, created in:&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        print(f&quot;    {project_dir.resolve()}\n&quot;)</span>
<span class="gi">+        print(&quot;You can start your first spider with:&quot;)</span>
<span class="gi">+        print(f&quot;    cd {project_dir}&quot;)</span>
<span class="gi">+        print(&quot;    scrapy genspider example example.com&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def templates_dir(self) -&gt; str:</span>
<span class="gi">+        return str(</span>
<span class="gi">+            Path(</span>
<span class="gi">+                self.settings[&quot;TEMPLATES_DIR&quot;] or Path(scrapy.__path__[0], &quot;templates&quot;),</span>
<span class="gi">+                &quot;project&quot;,</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/commands/version.py b/scrapy/commands/version.py</span>
<span class="gh">index 409f6ebb2..47582866b 100644</span>
<span class="gd">--- a/scrapy/commands/version.py</span>
<span class="gi">+++ b/scrapy/commands/version.py</span>
<span class="gu">@@ -4,4 +4,29 @@ from scrapy.utils.versions import scrapy_components_versions</span>


<span class="w"> </span>class Command(ScrapyCommand):
<span class="gd">-    default_settings = {&#39;LOG_ENABLED&#39;: False, &#39;SPIDER_LOADER_WARN_ONLY&#39;: True}</span>
<span class="gi">+    default_settings = {&quot;LOG_ENABLED&quot;: False, &quot;SPIDER_LOADER_WARN_ONLY&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+    def syntax(self):</span>
<span class="gi">+        return &quot;[-v]&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Print Scrapy version&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        ScrapyCommand.add_options(self, parser)</span>
<span class="gi">+        parser.add_argument(</span>
<span class="gi">+            &quot;--verbose&quot;,</span>
<span class="gi">+            &quot;-v&quot;,</span>
<span class="gi">+            dest=&quot;verbose&quot;,</span>
<span class="gi">+            action=&quot;store_true&quot;,</span>
<span class="gi">+            help=&quot;also display twisted/python/platform info (useful for bug reports)&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def run(self, args, opts):</span>
<span class="gi">+        if opts.verbose:</span>
<span class="gi">+            versions = scrapy_components_versions()</span>
<span class="gi">+            width = max(len(n) for (n, _) in versions)</span>
<span class="gi">+            for name, version in versions:</span>
<span class="gi">+                print(f&quot;{name:&lt;{width}} : {version}&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            print(f&quot;Scrapy {scrapy.__version__}&quot;)</span>
<span class="gh">diff --git a/scrapy/commands/view.py b/scrapy/commands/view.py</span>
<span class="gh">index 4b95dcfbd..ebdfa10a8 100644</span>
<span class="gd">--- a/scrapy/commands/view.py</span>
<span class="gi">+++ b/scrapy/commands/view.py</span>
<span class="gu">@@ -1,7 +1,21 @@</span>
<span class="w"> </span>import argparse
<span class="gi">+</span>
<span class="w"> </span>from scrapy.commands import fetch
<span class="w"> </span>from scrapy.utils.response import open_in_browser


<span class="w"> </span>class Command(fetch.Command):
<span class="gd">-    pass</span>
<span class="gi">+    def short_desc(self):</span>
<span class="gi">+        return &quot;Open URL in browser, as seen by Scrapy&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def long_desc(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            &quot;Fetch a URL using the Scrapy downloader and show its contents in a browser&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def add_options(self, parser):</span>
<span class="gi">+        super().add_options(parser)</span>
<span class="gi">+        parser.add_argument(&quot;--headers&quot;, help=argparse.SUPPRESS)</span>
<span class="gi">+</span>
<span class="gi">+    def _print_response(self, response, opts):</span>
<span class="gi">+        open_in_browser(response)</span>
<span class="gh">diff --git a/scrapy/contracts/default.py b/scrapy/contracts/default.py</span>
<span class="gh">index 63c140a96..eac702cef 100644</span>
<span class="gd">--- a/scrapy/contracts/default.py</span>
<span class="gi">+++ b/scrapy/contracts/default.py</span>
<span class="gu">@@ -1,15 +1,23 @@</span>
<span class="w"> </span>import json
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter, is_item
<span class="gi">+</span>
<span class="w"> </span>from scrapy.contracts import Contract
<span class="w"> </span>from scrapy.exceptions import ContractFail
<span class="w"> </span>from scrapy.http import Request


<span class="gi">+# contracts</span>
<span class="w"> </span>class UrlContract(Contract):
<span class="w"> </span>    &quot;&quot;&quot;Contract to set the url of the request (mandatory)
<span class="w"> </span>    @url http://scrapy.org
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    name = &#39;url&#39;</span>
<span class="gi">+</span>
<span class="gi">+    name = &quot;url&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def adjust_request_args(self, args):</span>
<span class="gi">+        args[&quot;url&quot;] = self.args[0]</span>
<span class="gi">+        return args</span>


<span class="w"> </span>class CallbackKeywordArgumentsContract(Contract):
<span class="gu">@@ -18,7 +26,12 @@ class CallbackKeywordArgumentsContract(Contract):</span>

<span class="w"> </span>    @cb_kwargs {&quot;arg1&quot;: &quot;some value&quot;}
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    name = &#39;cb_kwargs&#39;</span>
<span class="gi">+</span>
<span class="gi">+    name = &quot;cb_kwargs&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def adjust_request_args(self, args):</span>
<span class="gi">+        args[&quot;cb_kwargs&quot;] = json.loads(&quot; &quot;.join(self.args))</span>
<span class="gi">+        return args</span>


<span class="w"> </span>class ReturnsContract(Contract):
<span class="gu">@@ -33,31 +46,65 @@ class ReturnsContract(Contract):</span>
<span class="w"> </span>    @returns request 2 10
<span class="w"> </span>    @returns request 0 10
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    name = &#39;returns&#39;</span>
<span class="gd">-    object_type_verifiers = {&#39;request&#39;: lambda x: isinstance(x, Request),</span>
<span class="gd">-        &#39;requests&#39;: lambda x: isinstance(x, Request), &#39;item&#39;: is_item,</span>
<span class="gd">-        &#39;items&#39;: is_item}</span>
<span class="gi">+</span>
<span class="gi">+    name = &quot;returns&quot;</span>
<span class="gi">+    object_type_verifiers = {</span>
<span class="gi">+        &quot;request&quot;: lambda x: isinstance(x, Request),</span>
<span class="gi">+        &quot;requests&quot;: lambda x: isinstance(x, Request),</span>
<span class="gi">+        &quot;item&quot;: is_item,</span>
<span class="gi">+        &quot;items&quot;: is_item,</span>
<span class="gi">+    }</span>

<span class="w"> </span>    def __init__(self, *args, **kwargs):
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="gi">+</span>
<span class="w"> </span>        if len(self.args) not in [1, 2, 3]:
<span class="w"> </span>            raise ValueError(
<span class="gd">-                f&#39;Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                f&quot;Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>        self.obj_name = self.args[0] or None
<span class="w"> </span>        self.obj_type_verifier = self.object_type_verifiers[self.obj_name]
<span class="gi">+</span>
<span class="w"> </span>        try:
<span class="w"> </span>            self.min_bound = int(self.args[1])
<span class="w"> </span>        except IndexError:
<span class="w"> </span>            self.min_bound = 1
<span class="gi">+</span>
<span class="w"> </span>        try:
<span class="w"> </span>            self.max_bound = int(self.args[2])
<span class="w"> </span>        except IndexError:
<span class="gd">-            self.max_bound = float(&#39;inf&#39;)</span>
<span class="gi">+            self.max_bound = float(&quot;inf&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def post_process(self, output):</span>
<span class="gi">+        occurrences = 0</span>
<span class="gi">+        for x in output:</span>
<span class="gi">+            if self.obj_type_verifier(x):</span>
<span class="gi">+                occurrences += 1</span>
<span class="gi">+</span>
<span class="gi">+        assertion = self.min_bound &lt;= occurrences &lt;= self.max_bound</span>
<span class="gi">+</span>
<span class="gi">+        if not assertion:</span>
<span class="gi">+            if self.min_bound == self.max_bound:</span>
<span class="gi">+                expected = self.min_bound</span>
<span class="gi">+            else:</span>
<span class="gi">+                expected = f&quot;{self.min_bound}..{self.max_bound}&quot;</span>
<span class="gi">+</span>
<span class="gi">+            raise ContractFail(</span>
<span class="gi">+                f&quot;Returned {occurrences} {self.obj_name}, expected {expected}&quot;</span>
<span class="gi">+            )</span>


<span class="w"> </span>class ScrapesContract(Contract):
<span class="w"> </span>    &quot;&quot;&quot;Contract to check presence of fields in scraped items
<span class="w"> </span>    @scrapes page_name page_body
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    name = &#39;scrapes&#39;</span>
<span class="gi">+</span>
<span class="gi">+    name = &quot;scrapes&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def post_process(self, output):</span>
<span class="gi">+        for x in output:</span>
<span class="gi">+            if is_item(x):</span>
<span class="gi">+                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]</span>
<span class="gi">+                if missing:</span>
<span class="gi">+                    missing_fields = &quot;, &quot;.join(missing)</span>
<span class="gi">+                    raise ContractFail(f&quot;Missing fields: {missing_fields}&quot;)</span>
<span class="gh">diff --git a/scrapy/core/downloader/contextfactory.py b/scrapy/core/downloader/contextfactory.py</span>
<span class="gh">index fe70014d1..909cc273f 100644</span>
<span class="gd">--- a/scrapy/core/downloader/contextfactory.py</span>
<span class="gi">+++ b/scrapy/core/downloader/contextfactory.py</span>
<span class="gu">@@ -1,15 +1,27 @@</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from typing import TYPE_CHECKING, Any, List, Optional
<span class="gi">+</span>
<span class="w"> </span>from OpenSSL import SSL
<span class="w"> </span>from twisted.internet._sslverify import _setAcceptableProtocols
<span class="gd">-from twisted.internet.ssl import AcceptableCiphers, CertificateOptions, optionsForClientTLS, platformTrust</span>
<span class="gi">+from twisted.internet.ssl import (</span>
<span class="gi">+    AcceptableCiphers,</span>
<span class="gi">+    CertificateOptions,</span>
<span class="gi">+    optionsForClientTLS,</span>
<span class="gi">+    platformTrust,</span>
<span class="gi">+)</span>
<span class="w"> </span>from twisted.web.client import BrowserLikePolicyForHTTPS
<span class="w"> </span>from twisted.web.iweb import IPolicyForHTTPS
<span class="w"> </span>from zope.interface.declarations import implementer
<span class="w"> </span>from zope.interface.verify import verifyObject
<span class="gd">-from scrapy.core.downloader.tls import DEFAULT_CIPHERS, ScrapyClientTLSOptions, openssl_methods</span>
<span class="gi">+</span>
<span class="gi">+from scrapy.core.downloader.tls import (</span>
<span class="gi">+    DEFAULT_CIPHERS,</span>
<span class="gi">+    ScrapyClientTLSOptions,</span>
<span class="gi">+    openssl_methods,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from twisted.internet._sslverify import ClientTLSOptions

<span class="gu">@@ -26,19 +38,76 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):</span>
<span class="w"> </span>     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.&#39;
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, method: int=SSL.SSLv23_METHOD, tls_verbose_logging:</span>
<span class="gd">-        bool=False, tls_ciphers: Optional[str]=None, *args: Any, **kwargs: Any</span>
<span class="gd">-        ):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        method: int = SSL.SSLv23_METHOD,</span>
<span class="gi">+        tls_verbose_logging: bool = False,</span>
<span class="gi">+        tls_ciphers: Optional[str] = None,</span>
<span class="gi">+        *args: Any,</span>
<span class="gi">+        **kwargs: Any,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="w"> </span>        self._ssl_method: int = method
<span class="w"> </span>        self.tls_verbose_logging: bool = tls_verbose_logging
<span class="w"> </span>        self.tls_ciphers: AcceptableCiphers
<span class="w"> </span>        if tls_ciphers:
<span class="gd">-            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(</span>
<span class="gd">-                tls_ciphers)</span>
<span class="gi">+            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.tls_ciphers = DEFAULT_CIPHERS

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(</span>
<span class="gi">+        cls,</span>
<span class="gi">+        settings: BaseSettings,</span>
<span class="gi">+        method: int = SSL.SSLv23_METHOD,</span>
<span class="gi">+        *args: Any,</span>
<span class="gi">+        **kwargs: Any,</span>
<span class="gi">+    ):</span>
<span class="gi">+        tls_verbose_logging: bool = settings.getbool(</span>
<span class="gi">+            &quot;DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        tls_ciphers: Optional[str] = settings[&quot;DOWNLOADER_CLIENT_TLS_CIPHERS&quot;]</span>
<span class="gi">+        return cls(  # type: ignore[misc]</span>
<span class="gi">+            method=method,</span>
<span class="gi">+            tls_verbose_logging=tls_verbose_logging,</span>
<span class="gi">+            tls_ciphers=tls_ciphers,</span>
<span class="gi">+            *args,</span>
<span class="gi">+            **kwargs,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def getCertificateOptions(self) -&gt; CertificateOptions:</span>
<span class="gi">+        # setting verify=True will require you to provide CAs</span>
<span class="gi">+        # to verify against; in other words: it&#39;s not that simple</span>
<span class="gi">+</span>
<span class="gi">+        # backward-compatible SSL/TLS method:</span>
<span class="gi">+        #</span>
<span class="gi">+        # * this will respect `method` attribute in often recommended</span>
<span class="gi">+        #   `ScrapyClientContextFactory` subclass</span>
<span class="gi">+        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)</span>
<span class="gi">+        #</span>
<span class="gi">+        # * getattr() for `_ssl_method` attribute for context factories</span>
<span class="gi">+        #   not calling super().__init__</span>
<span class="gi">+        return CertificateOptions(</span>
<span class="gi">+            verify=False,</span>
<span class="gi">+            method=getattr(self, &quot;method&quot;, getattr(self, &quot;_ssl_method&quot;, None)),</span>
<span class="gi">+            fixBrokenPeers=True,</span>
<span class="gi">+            acceptableCiphers=self.tls_ciphers,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # kept for old-style HTTP/1.0 downloader context twisted calls,</span>
<span class="gi">+    # e.g. connectSSL()</span>
<span class="gi">+    def getContext(self, hostname: Any = None, port: Any = None) -&gt; SSL.Context:</span>
<span class="gi">+        ctx = self.getCertificateOptions().getContext()</span>
<span class="gi">+        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT</span>
<span class="gi">+        return ctx</span>
<span class="gi">+</span>
<span class="gi">+    def creatorForNetloc(self, hostname: bytes, port: int) -&gt; &quot;ClientTLSOptions&quot;:</span>
<span class="gi">+        return ScrapyClientTLSOptions(</span>
<span class="gi">+            hostname.decode(&quot;ascii&quot;),</span>
<span class="gi">+            self.getContext(),</span>
<span class="gi">+            verbose_logging=self.tls_verbose_logging,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>

<span class="w"> </span>@implementer(IPolicyForHTTPS)
<span class="w"> </span>class BrowserLikeContextFactory(ScrapyClientContextFactory):
<span class="gu">@@ -59,6 +128,17 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):</span>
<span class="w"> </span>    ``SSLv23_METHOD``) which allows TLS protocol negotiation.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    def creatorForNetloc(self, hostname: bytes, port: int) -&gt; &quot;ClientTLSOptions&quot;:</span>
<span class="gi">+        # trustRoot set to platformTrust() will use the platform&#39;s root CAs.</span>
<span class="gi">+        #</span>
<span class="gi">+        # This means that a website like https://www.cacert.org will be rejected</span>
<span class="gi">+        # by default, since CAcert.org CA certificate is seldom shipped.</span>
<span class="gi">+        return optionsForClientTLS(</span>
<span class="gi">+            hostname=hostname.decode(&quot;ascii&quot;),</span>
<span class="gi">+            trustRoot=platformTrust(),</span>
<span class="gi">+            extraCertificateOptions={&quot;method&quot;: self._ssl_method},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>

<span class="w"> </span>@implementer(IPolicyForHTTPS)
<span class="w"> </span>class AcceptableProtocolsContextFactory:
<span class="gu">@@ -67,8 +147,44 @@ class AcceptableProtocolsContextFactory:</span>
<span class="w"> </span>    negotiation.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]</span>
<span class="gd">-        ):</span>
<span class="gi">+    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]):</span>
<span class="w"> </span>        verifyObject(IPolicyForHTTPS, context_factory)
<span class="w"> </span>        self._wrapped_context_factory: Any = context_factory
<span class="w"> </span>        self._acceptable_protocols: List[bytes] = acceptable_protocols
<span class="gi">+</span>
<span class="gi">+    def creatorForNetloc(self, hostname: bytes, port: int) -&gt; &quot;ClientTLSOptions&quot;:</span>
<span class="gi">+        options: &quot;ClientTLSOptions&quot; = self._wrapped_context_factory.creatorForNetloc(</span>
<span class="gi">+            hostname, port</span>
<span class="gi">+        )</span>
<span class="gi">+        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)</span>
<span class="gi">+        return options</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def load_context_factory_from_settings(settings, crawler):</span>
<span class="gi">+    ssl_method = openssl_methods[settings.get(&quot;DOWNLOADER_CLIENT_TLS_METHOD&quot;)]</span>
<span class="gi">+    context_factory_cls = load_object(settings[&quot;DOWNLOADER_CLIENTCONTEXTFACTORY&quot;])</span>
<span class="gi">+    # try method-aware context factory</span>
<span class="gi">+    try:</span>
<span class="gi">+        context_factory = create_instance(</span>
<span class="gi">+            objcls=context_factory_cls,</span>
<span class="gi">+            settings=settings,</span>
<span class="gi">+            crawler=crawler,</span>
<span class="gi">+            method=ssl_method,</span>
<span class="gi">+        )</span>
<span class="gi">+    except TypeError:</span>
<span class="gi">+        # use context factory defaults</span>
<span class="gi">+        context_factory = create_instance(</span>
<span class="gi">+            objcls=context_factory_cls,</span>
<span class="gi">+            settings=settings,</span>
<span class="gi">+            crawler=crawler,</span>
<span class="gi">+        )</span>
<span class="gi">+        msg = (</span>
<span class="gi">+            f&quot;{settings[&#39;DOWNLOADER_CLIENTCONTEXTFACTORY&#39;]} does not accept &quot;</span>
<span class="gi">+            &quot;a `method` argument (type OpenSSL.SSL method, e.g. &quot;</span>
<span class="gi">+            &quot;OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` &quot;</span>
<span class="gi">+            &quot;argument and/or a `tls_ciphers` argument. Please, upgrade your &quot;</span>
<span class="gi">+            &quot;context factory class to handle them or ignore them.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        warnings.warn(msg)</span>
<span class="gi">+</span>
<span class="gi">+    return context_factory</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/datauri.py b/scrapy/core/downloader/handlers/datauri.py</span>
<span class="gh">index 25a176778..8b78c53c1 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/datauri.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/datauri.py</span>
<span class="gu">@@ -1,4 +1,5 @@</span>
<span class="w"> </span>from w3lib.url import parse_data_uri
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import TextResponse
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="w"> </span>from scrapy.utils.decorators import defers
<span class="gu">@@ -6,3 +7,15 @@ from scrapy.utils.decorators import defers</span>

<span class="w"> </span>class DataURIDownloadHandler:
<span class="w"> </span>    lazy = False
<span class="gi">+</span>
<span class="gi">+    @defers</span>
<span class="gi">+    def download_request(self, request, spider):</span>
<span class="gi">+        uri = parse_data_uri(request.url)</span>
<span class="gi">+        respcls = responsetypes.from_mimetype(uri.media_type)</span>
<span class="gi">+</span>
<span class="gi">+        resp_kwargs = {}</span>
<span class="gi">+        if issubclass(respcls, TextResponse) and uri.media_type.split(&quot;/&quot;)[0] == &quot;text&quot;:</span>
<span class="gi">+            charset = uri.media_type_parameters.get(&quot;charset&quot;)</span>
<span class="gi">+            resp_kwargs[&quot;encoding&quot;] = charset</span>
<span class="gi">+</span>
<span class="gi">+        return respcls(url=request.url, body=uri.data, **resp_kwargs)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/file.py b/scrapy/core/downloader/handlers/file.py</span>
<span class="gh">index 8fa3d2938..4824167da 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/file.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/file.py</span>
<span class="gu">@@ -1,8 +1,17 @@</span>
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>from w3lib.url import file_uri_to_path
<span class="gi">+</span>
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="w"> </span>from scrapy.utils.decorators import defers


<span class="w"> </span>class FileDownloadHandler:
<span class="w"> </span>    lazy = False
<span class="gi">+</span>
<span class="gi">+    @defers</span>
<span class="gi">+    def download_request(self, request, spider):</span>
<span class="gi">+        filepath = file_uri_to_path(request.url)</span>
<span class="gi">+        body = Path(filepath).read_bytes()</span>
<span class="gi">+        respcls = responsetypes.from_args(filename=filepath, body=body)</span>
<span class="gi">+        return respcls(url=request.url, body=body)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/ftp.py b/scrapy/core/downloader/handlers/ftp.py</span>
<span class="gh">index 78ad52f12..4081545ce 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/ftp.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/ftp.py</span>
<span class="gu">@@ -27,11 +27,14 @@ In case of status 200 request, response.headers will come with two keys:</span>
<span class="w"> </span>    &#39;Local Filename&#39; - with the value of the local filename if given
<span class="w"> </span>    &#39;Size&#39; - with size of the downloaded data
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import re
<span class="w"> </span>from io import BytesIO
<span class="w"> </span>from urllib.parse import unquote
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.protocol import ClientCreator, Protocol
<span class="w"> </span>from twisted.protocols.ftp import CommandFailed, FTPClient
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Response
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="gu">@@ -39,21 +42,84 @@ from scrapy.utils.python import to_bytes</span>


<span class="w"> </span>class ReceivedDataProtocol(Protocol):
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, filename=None):
<span class="w"> </span>        self.__filename = filename
<span class="gd">-        self.body = open(filename, &#39;wb&#39;) if filename else BytesIO()</span>
<span class="gi">+        self.body = open(filename, &quot;wb&quot;) if filename else BytesIO()</span>
<span class="w"> </span>        self.size = 0

<span class="gi">+    def dataReceived(self, data):</span>
<span class="gi">+        self.body.write(data)</span>
<span class="gi">+        self.size += len(data)</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def filename(self):</span>
<span class="gi">+        return self.__filename</span>
<span class="gi">+</span>
<span class="gi">+    def close(self):</span>
<span class="gi">+        self.body.close() if self.filename else self.body.seek(0)</span>
<span class="gi">+</span>

<span class="gd">-_CODE_RE = re.compile(&#39;\\d+&#39;)</span>
<span class="gi">+_CODE_RE = re.compile(r&quot;\d+&quot;)</span>


<span class="w"> </span>class FTPDownloadHandler:
<span class="w"> </span>    lazy = False
<span class="gd">-    CODE_MAPPING = {&#39;550&#39;: 404, &#39;default&#39;: 503}</span>
<span class="gi">+</span>
<span class="gi">+    CODE_MAPPING = {</span>
<span class="gi">+        &quot;550&quot;: 404,</span>
<span class="gi">+        &quot;default&quot;: 503,</span>
<span class="gi">+    }</span>

<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.default_user = settings[&#39;FTP_USER&#39;]</span>
<span class="gd">-        self.default_password = settings[&#39;FTP_PASSWORD&#39;]</span>
<span class="gd">-        self.passive_mode = settings[&#39;FTP_PASSIVE_MODE&#39;]</span>
<span class="gi">+        self.default_user = settings[&quot;FTP_USER&quot;]</span>
<span class="gi">+        self.default_password = settings[&quot;FTP_PASSWORD&quot;]</span>
<span class="gi">+        self.passive_mode = settings[&quot;FTP_PASSIVE_MODE&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def download_request(self, request, spider):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        parsed_url = urlparse_cached(request)</span>
<span class="gi">+        user = request.meta.get(&quot;ftp_user&quot;, self.default_user)</span>
<span class="gi">+        password = request.meta.get(&quot;ftp_password&quot;, self.default_password)</span>
<span class="gi">+        passive_mode = (</span>
<span class="gi">+            1 if bool(request.meta.get(&quot;ftp_passive&quot;, self.passive_mode)) else 0</span>
<span class="gi">+        )</span>
<span class="gi">+        creator = ClientCreator(</span>
<span class="gi">+            reactor, FTPClient, user, password, passive=passive_mode</span>
<span class="gi">+        )</span>
<span class="gi">+        dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)</span>
<span class="gi">+        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))</span>
<span class="gi">+</span>
<span class="gi">+    def gotClient(self, client, request, filepath):</span>
<span class="gi">+        self.client = client</span>
<span class="gi">+        protocol = ReceivedDataProtocol(request.meta.get(&quot;ftp_local_filename&quot;))</span>
<span class="gi">+        return client.retrieveFile(filepath, protocol).addCallbacks(</span>
<span class="gi">+            callback=self._build_response,</span>
<span class="gi">+            callbackArgs=(request, protocol),</span>
<span class="gi">+            errback=self._failed,</span>
<span class="gi">+            errbackArgs=(request,),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _build_response(self, result, request, protocol):</span>
<span class="gi">+        self.result = result</span>
<span class="gi">+        protocol.close()</span>
<span class="gi">+        headers = {&quot;local filename&quot;: protocol.filename or &quot;&quot;, &quot;size&quot;: protocol.size}</span>
<span class="gi">+        body = to_bytes(protocol.filename or protocol.body.read())</span>
<span class="gi">+        respcls = responsetypes.from_args(url=request.url, body=body)</span>
<span class="gi">+        return respcls(url=request.url, status=200, body=body, headers=headers)</span>
<span class="gi">+</span>
<span class="gi">+    def _failed(self, result, request):</span>
<span class="gi">+        message = result.getErrorMessage()</span>
<span class="gi">+        if result.type == CommandFailed:</span>
<span class="gi">+            m = _CODE_RE.search(message)</span>
<span class="gi">+            if m:</span>
<span class="gi">+                ftpcode = m.group()</span>
<span class="gi">+                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[&quot;default&quot;])</span>
<span class="gi">+                return Response(</span>
<span class="gi">+                    url=request.url, status=httpcode, body=to_bytes(message)</span>
<span class="gi">+                )</span>
<span class="gi">+        raise result.type(result.value)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/http.py b/scrapy/core/downloader/handlers/http.py</span>
<span class="gh">index a62ecadc7..52535bd8b 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/http.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/http.py</span>
<span class="gu">@@ -1,2 +1,4 @@</span>
<span class="w"> </span>from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler
<span class="gd">-from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler as HTTPDownloadHandler</span>
<span class="gi">+from scrapy.core.downloader.handlers.http11 import (</span>
<span class="gi">+    HTTP11DownloadHandler as HTTPDownloadHandler,</span>
<span class="gi">+)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/http10.py b/scrapy/core/downloader/handlers/http10.py</span>
<span class="gh">index 6f9e8f618..6c1dac4a5 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/http10.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/http10.py</span>
<span class="gu">@@ -8,13 +8,32 @@ class HTTP10DownloadHandler:</span>
<span class="w"> </span>    lazy = False

<span class="w"> </span>    def __init__(self, settings, crawler=None):
<span class="gd">-        self.HTTPClientFactory = load_object(settings[</span>
<span class="gd">-            &#39;DOWNLOADER_HTTPCLIENTFACTORY&#39;])</span>
<span class="gd">-        self.ClientContextFactory = load_object(settings[</span>
<span class="gd">-            &#39;DOWNLOADER_CLIENTCONTEXTFACTORY&#39;])</span>
<span class="gi">+        self.HTTPClientFactory = load_object(settings[&quot;DOWNLOADER_HTTPCLIENTFACTORY&quot;])</span>
<span class="gi">+        self.ClientContextFactory = load_object(</span>
<span class="gi">+            settings[&quot;DOWNLOADER_CLIENTCONTEXTFACTORY&quot;]</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._settings = settings
<span class="w"> </span>        self._crawler = crawler

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings, crawler)</span>
<span class="gi">+</span>
<span class="w"> </span>    def download_request(self, request, spider):
<span class="w"> </span>        &quot;&quot;&quot;Return a deferred for the HTTP download&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        factory = self.HTTPClientFactory(request)</span>
<span class="gi">+        self._connect(factory)</span>
<span class="gi">+        return factory.deferred</span>
<span class="gi">+</span>
<span class="gi">+    def _connect(self, factory):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        host, port = to_unicode(factory.host), factory.port</span>
<span class="gi">+        if factory.scheme == b&quot;https&quot;:</span>
<span class="gi">+            client_context_factory = create_instance(</span>
<span class="gi">+                objcls=self.ClientContextFactory,</span>
<span class="gi">+                settings=self._settings,</span>
<span class="gi">+                crawler=self._crawler,</span>
<span class="gi">+            )</span>
<span class="gi">+            return reactor.connectSSL(host, port, factory, client_context_factory)</span>
<span class="gi">+        return reactor.connectTCP(host, port, factory)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/http11.py b/scrapy/core/downloader/handlers/http11.py</span>
<span class="gh">index 2a58c6f22..c3704de3d 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/http11.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/http11.py</span>
<span class="gu">@@ -1,4 +1,5 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Download handlers for http and https schemes&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import ipaddress
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="gu">@@ -6,15 +7,23 @@ from contextlib import suppress</span>
<span class="w"> </span>from io import BytesIO
<span class="w"> </span>from time import time
<span class="w"> </span>from urllib.parse import urldefrag, urlunparse
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer, protocol, ssl
<span class="w"> </span>from twisted.internet.endpoints import TCP4ClientEndpoint
<span class="w"> </span>from twisted.internet.error import TimeoutError
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gd">-from twisted.web.client import URI, Agent, HTTPConnectionPool, ResponseDone, ResponseFailed</span>
<span class="gi">+from twisted.web.client import (</span>
<span class="gi">+    URI,</span>
<span class="gi">+    Agent,</span>
<span class="gi">+    HTTPConnectionPool,</span>
<span class="gi">+    ResponseDone,</span>
<span class="gi">+    ResponseFailed,</span>
<span class="gi">+)</span>
<span class="w"> </span>from twisted.web.http import PotentialDataLoss, _DataLoss
<span class="w"> </span>from twisted.web.http_headers import Headers as TxHeaders
<span class="w"> </span>from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer
<span class="w"> </span>from zope.interface import implementer
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.core.downloader.contextfactory import load_context_factory_from_settings
<span class="w"> </span>from scrapy.core.downloader.webclient import _parse
<span class="gu">@@ -22,6 +31,7 @@ from scrapy.exceptions import StopDownload</span>
<span class="w"> </span>from scrapy.http import Headers
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="w"> </span>from scrapy.utils.python import to_bytes, to_unicode
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -30,21 +40,58 @@ class HTTP11DownloadHandler:</span>

<span class="w"> </span>    def __init__(self, settings, crawler=None):
<span class="w"> </span>        self._crawler = crawler
<span class="gi">+</span>
<span class="w"> </span>        from twisted.internet import reactor
<span class="gi">+</span>
<span class="w"> </span>        self._pool = HTTPConnectionPool(reactor, persistent=True)
<span class="w"> </span>        self._pool.maxPersistentPerHost = settings.getint(
<span class="gd">-            &#39;CONCURRENT_REQUESTS_PER_DOMAIN&#39;)</span>
<span class="gi">+            &quot;CONCURRENT_REQUESTS_PER_DOMAIN&quot;</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._pool._factory.noisy = False
<span class="gd">-        self._contextFactory = load_context_factory_from_settings(settings,</span>
<span class="gd">-            crawler)</span>
<span class="gd">-        self._default_maxsize = settings.getint(&#39;DOWNLOAD_MAXSIZE&#39;)</span>
<span class="gd">-        self._default_warnsize = settings.getint(&#39;DOWNLOAD_WARNSIZE&#39;)</span>
<span class="gd">-        self._fail_on_dataloss = settings.getbool(&#39;DOWNLOAD_FAIL_ON_DATALOSS&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        self._contextFactory = load_context_factory_from_settings(settings, crawler)</span>
<span class="gi">+        self._default_maxsize = settings.getint(&quot;DOWNLOAD_MAXSIZE&quot;)</span>
<span class="gi">+        self._default_warnsize = settings.getint(&quot;DOWNLOAD_WARNSIZE&quot;)</span>
<span class="gi">+        self._fail_on_dataloss = settings.getbool(&quot;DOWNLOAD_FAIL_ON_DATALOSS&quot;)</span>
<span class="w"> </span>        self._disconnect_timeout = 1

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings, crawler)</span>
<span class="gi">+</span>
<span class="w"> </span>    def download_request(self, request, spider):
<span class="w"> </span>        &quot;&quot;&quot;Return a deferred for the HTTP download&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        agent = ScrapyAgent(</span>
<span class="gi">+            contextFactory=self._contextFactory,</span>
<span class="gi">+            pool=self._pool,</span>
<span class="gi">+            maxsize=getattr(spider, &quot;download_maxsize&quot;, self._default_maxsize),</span>
<span class="gi">+            warnsize=getattr(spider, &quot;download_warnsize&quot;, self._default_warnsize),</span>
<span class="gi">+            fail_on_dataloss=self._fail_on_dataloss,</span>
<span class="gi">+            crawler=self._crawler,</span>
<span class="gi">+        )</span>
<span class="gi">+        return agent.download_request(request)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        d = self._pool.closeCachedConnections()</span>
<span class="gi">+        # closeCachedConnections will hang on network or server issues, so</span>
<span class="gi">+        # we&#39;ll manually timeout the deferred.</span>
<span class="gi">+        #</span>
<span class="gi">+        # Twisted issue addressing this problem can be found here:</span>
<span class="gi">+        # https://twistedmatrix.com/trac/ticket/7738.</span>
<span class="gi">+        #</span>
<span class="gi">+        # closeCachedConnections doesn&#39;t handle external errbacks, so we&#39;ll</span>
<span class="gi">+        # issue a callback after `_disconnect_timeout` seconds.</span>
<span class="gi">+        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])</span>
<span class="gi">+</span>
<span class="gi">+        def cancel_delayed_call(result):</span>
<span class="gi">+            if delayed_call.active():</span>
<span class="gi">+                delayed_call.cancel()</span>
<span class="gi">+            return result</span>
<span class="gi">+</span>
<span class="gi">+        d.addBoth(cancel_delayed_call)</span>
<span class="gi">+        return d</span>


<span class="w"> </span>class TunnelError(Exception):
<span class="gu">@@ -59,13 +106,23 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):</span>
<span class="w"> </span>    with this endpoint comes from the pool and a CONNECT has already been issued
<span class="w"> </span>    for it.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    _truncatedLength = 1000
<span class="gd">-    _responseAnswer = &#39;HTTP/1\\.. (?P&lt;status&gt;\\d{3})(?P&lt;reason&gt;.{,&#39; + str(</span>
<span class="gd">-        _truncatedLength) + &#39;})&#39;</span>
<span class="gi">+    _responseAnswer = (</span>
<span class="gi">+        r&quot;HTTP/1\.. (?P&lt;status&gt;\d{3})(?P&lt;reason&gt;.{,&quot; + str(_truncatedLength) + r&quot;})&quot;</span>
<span class="gi">+    )</span>
<span class="w"> </span>    _responseMatcher = re.compile(_responseAnswer.encode())

<span class="gd">-    def __init__(self, reactor, host, port, proxyConf, contextFactory,</span>
<span class="gd">-        timeout=30, bindAddress=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        reactor,</span>
<span class="gi">+        host,</span>
<span class="gi">+        port,</span>
<span class="gi">+        proxyConf,</span>
<span class="gi">+        contextFactory,</span>
<span class="gi">+        timeout=30,</span>
<span class="gi">+        bindAddress=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf
<span class="w"> </span>        super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)
<span class="w"> </span>        self._tunnelReadyDeferred = defer.Deferred()
<span class="gu">@@ -76,33 +133,83 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):</span>

<span class="w"> </span>    def requestTunnel(self, protocol):
<span class="w"> </span>        &quot;&quot;&quot;Asks the proxy to open a tunnel.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        tunnelReq = tunnel_request_data(</span>
<span class="gi">+            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader</span>
<span class="gi">+        )</span>
<span class="gi">+        protocol.transport.write(tunnelReq)</span>
<span class="gi">+        self._protocolDataReceived = protocol.dataReceived</span>
<span class="gi">+        protocol.dataReceived = self.processProxyResponse</span>
<span class="gi">+        self._protocol = protocol</span>
<span class="gi">+        return protocol</span>

<span class="w"> </span>    def processProxyResponse(self, rcvd_bytes):
<span class="w"> </span>        &quot;&quot;&quot;Processes the response from the proxy. If the tunnel is successfully
<span class="w"> </span>        created, notifies the client that we are ready to send requests. If not
<span class="w"> </span>        raises a TunnelError.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._connectBuffer += rcvd_bytes</span>
<span class="gi">+        # make sure that enough (all) bytes are consumed</span>
<span class="gi">+        # and that we&#39;ve got all HTTP headers (ending with a blank line)</span>
<span class="gi">+        # from the proxy so that we don&#39;t send those bytes to the TLS layer</span>
<span class="gi">+        #</span>
<span class="gi">+        # see https://github.com/scrapy/scrapy/issues/2491</span>
<span class="gi">+        if b&quot;\r\n\r\n&quot; not in self._connectBuffer:</span>
<span class="gi">+            return</span>
<span class="gi">+        self._protocol.dataReceived = self._protocolDataReceived</span>
<span class="gi">+        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)</span>
<span class="gi">+        if respm and int(respm.group(&quot;status&quot;)) == 200:</span>
<span class="gi">+            # set proper Server Name Indication extension</span>
<span class="gi">+            sslOptions = self._contextFactory.creatorForNetloc(</span>
<span class="gi">+                self._tunneledHost, self._tunneledPort</span>
<span class="gi">+            )</span>
<span class="gi">+            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)</span>
<span class="gi">+            self._tunnelReadyDeferred.callback(self._protocol)</span>
<span class="gi">+        else:</span>
<span class="gi">+            if respm:</span>
<span class="gi">+                extra = {</span>
<span class="gi">+                    &quot;status&quot;: int(respm.group(&quot;status&quot;)),</span>
<span class="gi">+                    &quot;reason&quot;: respm.group(&quot;reason&quot;).strip(),</span>
<span class="gi">+                }</span>
<span class="gi">+            else:</span>
<span class="gi">+                extra = rcvd_bytes[: self._truncatedLength]</span>
<span class="gi">+            self._tunnelReadyDeferred.errback(</span>
<span class="gi">+                TunnelError(</span>
<span class="gi">+                    &quot;Could not open CONNECT tunnel with proxy &quot;</span>
<span class="gi">+                    f&quot;{self._host}:{self._port} [{extra!r}]&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>

<span class="w"> </span>    def connectFailed(self, reason):
<span class="w"> </span>        &quot;&quot;&quot;Propagates the errback to the appropriate deferred.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._tunnelReadyDeferred.errback(reason)</span>
<span class="gi">+</span>
<span class="gi">+    def connect(self, protocolFactory):</span>
<span class="gi">+        self._protocolFactory = protocolFactory</span>
<span class="gi">+        connectDeferred = super().connect(protocolFactory)</span>
<span class="gi">+        connectDeferred.addCallback(self.requestTunnel)</span>
<span class="gi">+        connectDeferred.addErrback(self.connectFailed)</span>
<span class="gi">+        return self._tunnelReadyDeferred</span>


<span class="w"> </span>def tunnel_request_data(host, port, proxy_auth_header=None):
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="w"> </span>    Return binary content of a CONNECT request.

<span class="w"> </span>    &gt;&gt;&gt; from scrapy.utils.python import to_unicode as s
<span class="w"> </span>    &gt;&gt;&gt; s(tunnel_request_data(&quot;example.com&quot;, 8080))
<span class="gd">-    &#39;CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n&#39;</span>
<span class="gi">+    &#39;CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\n\r\n&#39;</span>
<span class="w"> </span>    &gt;&gt;&gt; s(tunnel_request_data(&quot;example.com&quot;, 8080, b&quot;123&quot;))
<span class="gd">-    &#39;CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n&#39;</span>
<span class="gi">+    &#39;CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\nProxy-Authorization: 123\r\n\r\n&#39;</span>
<span class="w"> </span>    &gt;&gt;&gt; s(tunnel_request_data(b&quot;example.com&quot;, &quot;8090&quot;))
<span class="gd">-    &#39;CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n&#39;</span>
<span class="gi">+    &#39;CONNECT example.com:8090 HTTP/1.1\r\nHost: example.com:8090\r\n\r\n&#39;</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    host_value = to_bytes(host, encoding=&quot;ascii&quot;) + b&quot;:&quot; + to_bytes(str(port))</span>
<span class="gi">+    tunnel_req = b&quot;CONNECT &quot; + host_value + b&quot; HTTP/1.1\r\n&quot;</span>
<span class="gi">+    tunnel_req += b&quot;Host: &quot; + host_value + b&quot;\r\n&quot;</span>
<span class="gi">+    if proxy_auth_header:</span>
<span class="gi">+        tunnel_req += b&quot;Proxy-Authorization: &quot; + proxy_auth_header + b&quot;\r\n&quot;</span>
<span class="gi">+    tunnel_req += b&quot;\r\n&quot;</span>
<span class="gi">+    return tunnel_req</span>


<span class="w"> </span>class TunnelingAgent(Agent):
<span class="gu">@@ -113,27 +220,75 @@ class TunnelingAgent(Agent):</span>
<span class="w"> </span>    proxy involved.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, reactor, proxyConf, contextFactory=None,</span>
<span class="gd">-        connectTimeout=None, bindAddress=None, pool=None):</span>
<span class="gd">-        super().__init__(reactor, contextFactory, connectTimeout,</span>
<span class="gd">-            bindAddress, pool)</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        reactor,</span>
<span class="gi">+        proxyConf,</span>
<span class="gi">+        contextFactory=None,</span>
<span class="gi">+        connectTimeout=None,</span>
<span class="gi">+        bindAddress=None,</span>
<span class="gi">+        pool=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)</span>
<span class="w"> </span>        self._proxyConf = proxyConf
<span class="w"> </span>        self._contextFactory = contextFactory

<span class="gi">+    def _getEndpoint(self, uri):</span>
<span class="gi">+        return TunnelingTCP4ClientEndpoint(</span>
<span class="gi">+            reactor=self._reactor,</span>
<span class="gi">+            host=uri.host,</span>
<span class="gi">+            port=uri.port,</span>
<span class="gi">+            proxyConf=self._proxyConf,</span>
<span class="gi">+            contextFactory=self._contextFactory,</span>
<span class="gi">+            timeout=self._endpointFactory._connectTimeout,</span>
<span class="gi">+            bindAddress=self._endpointFactory._bindAddress,</span>
<span class="gi">+        )</span>

<span class="gd">-class ScrapyProxyAgent(Agent):</span>
<span class="gi">+    def _requestWithEndpoint(</span>
<span class="gi">+        self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath</span>
<span class="gi">+    ):</span>
<span class="gi">+        # proxy host and port are required for HTTP pool `key`</span>
<span class="gi">+        # otherwise, same remote host connection request could reuse</span>
<span class="gi">+        # a cached tunneled connection to a different proxy</span>
<span class="gi">+        key += self._proxyConf</span>
<span class="gi">+        return super()._requestWithEndpoint(</span>
<span class="gi">+            key=key,</span>
<span class="gi">+            endpoint=endpoint,</span>
<span class="gi">+            method=method,</span>
<span class="gi">+            parsedURI=parsedURI,</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            bodyProducer=bodyProducer,</span>
<span class="gi">+            requestPath=requestPath,</span>
<span class="gi">+        )</span>

<span class="gd">-    def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=</span>
<span class="gd">-        None, pool=None):</span>
<span class="gd">-        super().__init__(reactor=reactor, connectTimeout=connectTimeout,</span>
<span class="gd">-            bindAddress=bindAddress, pool=pool)</span>
<span class="gi">+</span>
<span class="gi">+class ScrapyProxyAgent(Agent):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            reactor=reactor,</span>
<span class="gi">+            connectTimeout=connectTimeout,</span>
<span class="gi">+            bindAddress=bindAddress,</span>
<span class="gi">+            pool=pool,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._proxyURI = URI.fromBytes(proxyURI)

<span class="w"> </span>    def request(self, method, uri, headers=None, bodyProducer=None):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Issue a new request via the configured proxy.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Cache *all* connections under the same key, since we are only</span>
<span class="gi">+        # connecting to a single destination, the proxy:</span>
<span class="gi">+        return self._requestWithEndpoint(</span>
<span class="gi">+            key=(&quot;http-proxy&quot;, self._proxyURI.host, self._proxyURI.port),</span>
<span class="gi">+            endpoint=self._getEndpoint(self._proxyURI),</span>
<span class="gi">+            method=method,</span>
<span class="gi">+            parsedURI=URI.fromBytes(uri),</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            bodyProducer=bodyProducer,</span>
<span class="gi">+            requestPath=uri,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class ScrapyAgent:
<span class="gu">@@ -141,9 +296,17 @@ class ScrapyAgent:</span>
<span class="w"> </span>    _ProxyAgent = ScrapyProxyAgent
<span class="w"> </span>    _TunnelingAgent = TunnelingAgent

<span class="gd">-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=</span>
<span class="gd">-        None, pool=None, maxsize=0, warnsize=0, fail_on_dataloss=True,</span>
<span class="gd">-        crawler=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        contextFactory=None,</span>
<span class="gi">+        connectTimeout=10,</span>
<span class="gi">+        bindAddress=None,</span>
<span class="gi">+        pool=None,</span>
<span class="gi">+        maxsize=0,</span>
<span class="gi">+        warnsize=0,</span>
<span class="gi">+        fail_on_dataloss=True,</span>
<span class="gi">+        crawler=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self._contextFactory = contextFactory
<span class="w"> </span>        self._connectTimeout = connectTimeout
<span class="w"> </span>        self._bindAddress = bindAddress
<span class="gu">@@ -154,19 +317,236 @@ class ScrapyAgent:</span>
<span class="w"> </span>        self._txresponse = None
<span class="w"> </span>        self._crawler = crawler

<span class="gi">+    def _get_agent(self, request, timeout):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        bindaddress = request.meta.get(&quot;bindaddress&quot;) or self._bindAddress</span>
<span class="gi">+        proxy = request.meta.get(&quot;proxy&quot;)</span>
<span class="gi">+        if proxy:</span>
<span class="gi">+            proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)</span>
<span class="gi">+            scheme = _parse(request.url)[0]</span>
<span class="gi">+            proxyHost = to_unicode(proxyHost)</span>
<span class="gi">+            if scheme == b&quot;https&quot;:</span>
<span class="gi">+                proxyAuth = request.headers.get(b&quot;Proxy-Authorization&quot;, None)</span>
<span class="gi">+                proxyConf = (proxyHost, proxyPort, proxyAuth)</span>
<span class="gi">+                return self._TunnelingAgent(</span>
<span class="gi">+                    reactor=reactor,</span>
<span class="gi">+                    proxyConf=proxyConf,</span>
<span class="gi">+                    contextFactory=self._contextFactory,</span>
<span class="gi">+                    connectTimeout=timeout,</span>
<span class="gi">+                    bindAddress=bindaddress,</span>
<span class="gi">+                    pool=self._pool,</span>
<span class="gi">+                )</span>
<span class="gi">+            proxyScheme = proxyScheme or b&quot;http&quot;</span>
<span class="gi">+            proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, &quot;&quot;, &quot;&quot;, &quot;&quot;))</span>
<span class="gi">+            return self._ProxyAgent(</span>
<span class="gi">+                reactor=reactor,</span>
<span class="gi">+                proxyURI=to_bytes(proxyURI, encoding=&quot;ascii&quot;),</span>
<span class="gi">+                connectTimeout=timeout,</span>
<span class="gi">+                bindAddress=bindaddress,</span>
<span class="gi">+                pool=self._pool,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        return self._Agent(</span>
<span class="gi">+            reactor=reactor,</span>
<span class="gi">+            contextFactory=self._contextFactory,</span>
<span class="gi">+            connectTimeout=timeout,</span>
<span class="gi">+            bindAddress=bindaddress,</span>
<span class="gi">+            pool=self._pool,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def download_request(self, request):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        timeout = request.meta.get(&quot;download_timeout&quot;) or self._connectTimeout</span>
<span class="gi">+        agent = self._get_agent(request, timeout)</span>
<span class="gi">+</span>
<span class="gi">+        # request details</span>
<span class="gi">+        url = urldefrag(request.url)[0]</span>
<span class="gi">+        method = to_bytes(request.method)</span>
<span class="gi">+        headers = TxHeaders(request.headers)</span>
<span class="gi">+        if isinstance(agent, self._TunnelingAgent):</span>
<span class="gi">+            headers.removeHeader(b&quot;Proxy-Authorization&quot;)</span>
<span class="gi">+        if request.body:</span>
<span class="gi">+            bodyproducer = _RequestBodyProducer(request.body)</span>
<span class="gi">+        else:</span>
<span class="gi">+            bodyproducer = None</span>
<span class="gi">+        start_time = time()</span>
<span class="gi">+        d = agent.request(</span>
<span class="gi">+            method, to_bytes(url, encoding=&quot;ascii&quot;), headers, bodyproducer</span>
<span class="gi">+        )</span>
<span class="gi">+        # set download latency</span>
<span class="gi">+        d.addCallback(self._cb_latency, request, start_time)</span>
<span class="gi">+        # response body is ready to be consumed</span>
<span class="gi">+        d.addCallback(self._cb_bodyready, request)</span>
<span class="gi">+        d.addCallback(self._cb_bodydone, request, url)</span>
<span class="gi">+        # check download timeout</span>
<span class="gi">+        self._timeout_cl = reactor.callLater(timeout, d.cancel)</span>
<span class="gi">+        d.addBoth(self._cb_timeout, request, url, timeout)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _cb_timeout(self, result, request, url, timeout):</span>
<span class="gi">+        if self._timeout_cl.active():</span>
<span class="gi">+            self._timeout_cl.cancel()</span>
<span class="gi">+            return result</span>
<span class="gi">+        # needed for HTTPS requests, otherwise _ResponseReader doesn&#39;t</span>
<span class="gi">+        # receive connectionLost()</span>
<span class="gi">+        if self._txresponse:</span>
<span class="gi">+            self._txresponse._transport.stopProducing()</span>
<span class="gi">+</span>
<span class="gi">+        raise TimeoutError(f&quot;Getting {url} took longer than {timeout} seconds.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _cb_latency(self, result, request, start_time):</span>
<span class="gi">+        request.meta[&quot;download_latency&quot;] = time() - start_time</span>
<span class="gi">+        return result</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _headers_from_twisted_response(response):</span>
<span class="gi">+        headers = Headers()</span>
<span class="gi">+        if response.length != UNKNOWN_LENGTH:</span>
<span class="gi">+            headers[b&quot;Content-Length&quot;] = str(response.length).encode()</span>
<span class="gi">+        headers.update(response.headers.getAllRawHeaders())</span>
<span class="gi">+        return headers</span>
<span class="gi">+</span>
<span class="gi">+    def _cb_bodyready(self, txresponse, request):</span>
<span class="gi">+        headers_received_result = self._crawler.signals.send_catch_log(</span>
<span class="gi">+            signal=signals.headers_received,</span>
<span class="gi">+            headers=self._headers_from_twisted_response(txresponse),</span>
<span class="gi">+            body_length=txresponse.length,</span>
<span class="gi">+            request=request,</span>
<span class="gi">+            spider=self._crawler.spider,</span>
<span class="gi">+        )</span>
<span class="gi">+        for handler, result in headers_received_result:</span>
<span class="gi">+            if isinstance(result, Failure) and isinstance(result.value, StopDownload):</span>
<span class="gi">+                logger.debug(</span>
<span class="gi">+                    &quot;Download stopped for %(request)s from signal handler %(handler)s&quot;,</span>
<span class="gi">+                    {&quot;request&quot;: request, &quot;handler&quot;: handler.__qualname__},</span>
<span class="gi">+                )</span>
<span class="gi">+                txresponse._transport.stopProducing()</span>
<span class="gi">+                txresponse._transport.loseConnection()</span>
<span class="gi">+                return {</span>
<span class="gi">+                    &quot;txresponse&quot;: txresponse,</span>
<span class="gi">+                    &quot;body&quot;: b&quot;&quot;,</span>
<span class="gi">+                    &quot;flags&quot;: [&quot;download_stopped&quot;],</span>
<span class="gi">+                    &quot;certificate&quot;: None,</span>
<span class="gi">+                    &quot;ip_address&quot;: None,</span>
<span class="gi">+                    &quot;failure&quot;: result if result.value.fail else None,</span>
<span class="gi">+                }</span>
<span class="gi">+</span>
<span class="gi">+        # deliverBody hangs for responses without body</span>
<span class="gi">+        if txresponse.length == 0:</span>
<span class="gi">+            return {</span>
<span class="gi">+                &quot;txresponse&quot;: txresponse,</span>
<span class="gi">+                &quot;body&quot;: b&quot;&quot;,</span>
<span class="gi">+                &quot;flags&quot;: None,</span>
<span class="gi">+                &quot;certificate&quot;: None,</span>
<span class="gi">+                &quot;ip_address&quot;: None,</span>
<span class="gi">+            }</span>
<span class="gi">+</span>
<span class="gi">+        maxsize = request.meta.get(&quot;download_maxsize&quot;, self._maxsize)</span>
<span class="gi">+        warnsize = request.meta.get(&quot;download_warnsize&quot;, self._warnsize)</span>
<span class="gi">+        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1</span>
<span class="gi">+        fail_on_dataloss = request.meta.get(</span>
<span class="gi">+            &quot;download_fail_on_dataloss&quot;, self._fail_on_dataloss</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        if maxsize and expected_size &gt; maxsize:</span>
<span class="gi">+            warning_msg = (</span>
<span class="gi">+                &quot;Cancelling download of %(url)s: expected response &quot;</span>
<span class="gi">+                &quot;size (%(size)s) larger than download max size (%(maxsize)s).&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            warning_args = {</span>
<span class="gi">+                &quot;url&quot;: request.url,</span>
<span class="gi">+                &quot;size&quot;: expected_size,</span>
<span class="gi">+                &quot;maxsize&quot;: maxsize,</span>
<span class="gi">+            }</span>
<span class="gi">+</span>
<span class="gi">+            logger.warning(warning_msg, warning_args)</span>
<span class="gi">+</span>
<span class="gi">+            txresponse._transport.loseConnection()</span>
<span class="gi">+            raise defer.CancelledError(warning_msg % warning_args)</span>
<span class="gi">+</span>
<span class="gi">+        if warnsize and expected_size &gt; warnsize:</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;Expected response size (%(size)s) larger than &quot;</span>
<span class="gi">+                &quot;download warn size (%(warnsize)s) in request %(request)s.&quot;,</span>
<span class="gi">+                {&quot;size&quot;: expected_size, &quot;warnsize&quot;: warnsize, &quot;request&quot;: request},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        def _cancel(_):</span>
<span class="gi">+            # Abort connection immediately.</span>
<span class="gi">+            txresponse._transport._producer.abortConnection()</span>
<span class="gi">+</span>
<span class="gi">+        d = defer.Deferred(_cancel)</span>
<span class="gi">+        txresponse.deliverBody(</span>
<span class="gi">+            _ResponseReader(</span>
<span class="gi">+                finished=d,</span>
<span class="gi">+                txresponse=txresponse,</span>
<span class="gi">+                request=request,</span>
<span class="gi">+                maxsize=maxsize,</span>
<span class="gi">+                warnsize=warnsize,</span>
<span class="gi">+                fail_on_dataloss=fail_on_dataloss,</span>
<span class="gi">+                crawler=self._crawler,</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # save response for timeouts</span>
<span class="gi">+        self._txresponse = txresponse</span>
<span class="gi">+</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _cb_bodydone(self, result, request, url):</span>
<span class="gi">+        headers = self._headers_from_twisted_response(result[&quot;txresponse&quot;])</span>
<span class="gi">+        respcls = responsetypes.from_args(headers=headers, url=url, body=result[&quot;body&quot;])</span>
<span class="gi">+        try:</span>
<span class="gi">+            version = result[&quot;txresponse&quot;].version</span>
<span class="gi">+            protocol = f&quot;{to_unicode(version[0])}/{version[1]}.{version[2]}&quot;</span>
<span class="gi">+        except (AttributeError, TypeError, IndexError):</span>
<span class="gi">+            protocol = None</span>
<span class="gi">+        response = respcls(</span>
<span class="gi">+            url=url,</span>
<span class="gi">+            status=int(result[&quot;txresponse&quot;].code),</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            body=result[&quot;body&quot;],</span>
<span class="gi">+            flags=result[&quot;flags&quot;],</span>
<span class="gi">+            certificate=result[&quot;certificate&quot;],</span>
<span class="gi">+            ip_address=result[&quot;ip_address&quot;],</span>
<span class="gi">+            protocol=protocol,</span>
<span class="gi">+        )</span>
<span class="gi">+        if result.get(&quot;failure&quot;):</span>
<span class="gi">+            result[&quot;failure&quot;].value.response = response</span>
<span class="gi">+            return result[&quot;failure&quot;]</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>

<span class="w"> </span>@implementer(IBodyProducer)
<span class="w"> </span>class _RequestBodyProducer:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, body):
<span class="w"> </span>        self.body = body
<span class="w"> </span>        self.length = len(body)

<span class="gi">+    def startProducing(self, consumer):</span>
<span class="gi">+        consumer.write(self.body)</span>
<span class="gi">+        return defer.succeed(None)</span>

<span class="gd">-class _ResponseReader(protocol.Protocol):</span>
<span class="gi">+    def pauseProducing(self):</span>
<span class="gi">+        pass</span>

<span class="gd">-    def __init__(self, finished, txresponse, request, maxsize, warnsize,</span>
<span class="gd">-        fail_on_dataloss, crawler):</span>
<span class="gi">+    def stopProducing(self):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class _ResponseReader(protocol.Protocol):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        finished,</span>
<span class="gi">+        txresponse,</span>
<span class="gi">+        request,</span>
<span class="gi">+        maxsize,</span>
<span class="gi">+        warnsize,</span>
<span class="gi">+        fail_on_dataloss,</span>
<span class="gi">+        crawler,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self._finished = finished
<span class="w"> </span>        self._txresponse = txresponse
<span class="w"> </span>        self._request = request
<span class="gu">@@ -180,3 +560,108 @@ class _ResponseReader(protocol.Protocol):</span>
<span class="w"> </span>        self._certificate = None
<span class="w"> </span>        self._ip_address = None
<span class="w"> </span>        self._crawler = crawler
<span class="gi">+</span>
<span class="gi">+    def _finish_response(self, flags=None, failure=None):</span>
<span class="gi">+        self._finished.callback(</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;txresponse&quot;: self._txresponse,</span>
<span class="gi">+                &quot;body&quot;: self._bodybuf.getvalue(),</span>
<span class="gi">+                &quot;flags&quot;: flags,</span>
<span class="gi">+                &quot;certificate&quot;: self._certificate,</span>
<span class="gi">+                &quot;ip_address&quot;: self._ip_address,</span>
<span class="gi">+                &quot;failure&quot;: failure,</span>
<span class="gi">+            }</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def connectionMade(self):</span>
<span class="gi">+        if self._certificate is None:</span>
<span class="gi">+            with suppress(AttributeError):</span>
<span class="gi">+                self._certificate = ssl.Certificate(</span>
<span class="gi">+                    self.transport._producer.getPeerCertificate()</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        if self._ip_address is None:</span>
<span class="gi">+            self._ip_address = ipaddress.ip_address(</span>
<span class="gi">+                self.transport._producer.getPeer().host</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def dataReceived(self, bodyBytes):</span>
<span class="gi">+        # This maybe called several times after cancel was called with buffered data.</span>
<span class="gi">+        if self._finished.called:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        self._bodybuf.write(bodyBytes)</span>
<span class="gi">+        self._bytes_received += len(bodyBytes)</span>
<span class="gi">+</span>
<span class="gi">+        bytes_received_result = self._crawler.signals.send_catch_log(</span>
<span class="gi">+            signal=signals.bytes_received,</span>
<span class="gi">+            data=bodyBytes,</span>
<span class="gi">+            request=self._request,</span>
<span class="gi">+            spider=self._crawler.spider,</span>
<span class="gi">+        )</span>
<span class="gi">+        for handler, result in bytes_received_result:</span>
<span class="gi">+            if isinstance(result, Failure) and isinstance(result.value, StopDownload):</span>
<span class="gi">+                logger.debug(</span>
<span class="gi">+                    &quot;Download stopped for %(request)s from signal handler %(handler)s&quot;,</span>
<span class="gi">+                    {&quot;request&quot;: self._request, &quot;handler&quot;: handler.__qualname__},</span>
<span class="gi">+                )</span>
<span class="gi">+                self.transport.stopProducing()</span>
<span class="gi">+                self.transport.loseConnection()</span>
<span class="gi">+                failure = result if result.value.fail else None</span>
<span class="gi">+                self._finish_response(flags=[&quot;download_stopped&quot;], failure=failure)</span>
<span class="gi">+</span>
<span class="gi">+        if self._maxsize and self._bytes_received &gt; self._maxsize:</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;Received (%(bytes)s) bytes larger than download &quot;</span>
<span class="gi">+                &quot;max size (%(maxsize)s) in request %(request)s.&quot;,</span>
<span class="gi">+                {</span>
<span class="gi">+                    &quot;bytes&quot;: self._bytes_received,</span>
<span class="gi">+                    &quot;maxsize&quot;: self._maxsize,</span>
<span class="gi">+                    &quot;request&quot;: self._request,</span>
<span class="gi">+                },</span>
<span class="gi">+            )</span>
<span class="gi">+            # Clear buffer earlier to avoid keeping data in memory for a long time.</span>
<span class="gi">+            self._bodybuf.truncate(0)</span>
<span class="gi">+            self._finished.cancel()</span>
<span class="gi">+</span>
<span class="gi">+        if (</span>
<span class="gi">+            self._warnsize</span>
<span class="gi">+            and self._bytes_received &gt; self._warnsize</span>
<span class="gi">+            and not self._reached_warnsize</span>
<span class="gi">+        ):</span>
<span class="gi">+            self._reached_warnsize = True</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;Received more bytes than download &quot;</span>
<span class="gi">+                &quot;warn size (%(warnsize)s) in request %(request)s.&quot;,</span>
<span class="gi">+                {&quot;warnsize&quot;: self._warnsize, &quot;request&quot;: self._request},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def connectionLost(self, reason):</span>
<span class="gi">+        if self._finished.called:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if reason.check(ResponseDone):</span>
<span class="gi">+            self._finish_response()</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if reason.check(PotentialDataLoss):</span>
<span class="gi">+            self._finish_response(flags=[&quot;partial&quot;])</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if reason.check(ResponseFailed) and any(</span>
<span class="gi">+            r.check(_DataLoss) for r in reason.value.reasons</span>
<span class="gi">+        ):</span>
<span class="gi">+            if not self._fail_on_dataloss:</span>
<span class="gi">+                self._finish_response(flags=[&quot;dataloss&quot;])</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            if not self._fail_on_dataloss_warned:</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    &quot;Got data loss in %s. If you want to process broken &quot;</span>
<span class="gi">+                    &quot;responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False&quot;</span>
<span class="gi">+                    &quot; -- This message won&#39;t be shown in further requests&quot;,</span>
<span class="gi">+                    self._txresponse.request.absoluteURI.decode(),</span>
<span class="gi">+                )</span>
<span class="gi">+                self._fail_on_dataloss_warned = True</span>
<span class="gi">+</span>
<span class="gi">+        self._finished.errback(reason)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/http2.py b/scrapy/core/downloader/handlers/http2.py</span>
<span class="gh">index 37c42a70f..b2579362c 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/http2.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/http2.py</span>
<span class="gu">@@ -1,10 +1,12 @@</span>
<span class="w"> </span>from time import time
<span class="w"> </span>from typing import Optional, Type, TypeVar
<span class="w"> </span>from urllib.parse import urldefrag
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.base import DelayedCall
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="w"> </span>from twisted.internet.error import TimeoutError
<span class="w"> </span>from twisted.web.client import URI
<span class="gi">+</span>
<span class="w"> </span>from scrapy.core.downloader.contextfactory import load_context_factory_from_settings
<span class="w"> </span>from scrapy.core.downloader.webclient import _parse
<span class="w"> </span>from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent
<span class="gu">@@ -13,29 +15,116 @@ from scrapy.http import Request, Response</span>
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.utils.python import to_bytes
<span class="gd">-H2DownloadHandlerOrSubclass = TypeVar(&#39;H2DownloadHandlerOrSubclass&#39;, bound=</span>
<span class="gd">-    &#39;H2DownloadHandler&#39;)</span>

<span class="gi">+H2DownloadHandlerOrSubclass = TypeVar(</span>
<span class="gi">+    &quot;H2DownloadHandlerOrSubclass&quot;, bound=&quot;H2DownloadHandler&quot;</span>
<span class="gi">+)</span>

<span class="gd">-class H2DownloadHandler:</span>

<span class="gd">-    def __init__(self, settings: Settings, crawler: Optional[Crawler]=None):</span>
<span class="gi">+class H2DownloadHandler:</span>
<span class="gi">+    def __init__(self, settings: Settings, crawler: Optional[Crawler] = None):</span>
<span class="w"> </span>        self._crawler = crawler
<span class="gi">+</span>
<span class="w"> </span>        from twisted.internet import reactor
<span class="gi">+</span>
<span class="w"> </span>        self._pool = H2ConnectionPool(reactor, settings)
<span class="gd">-        self._context_factory = load_context_factory_from_settings(settings,</span>
<span class="gd">-            crawler)</span>
<span class="gi">+        self._context_factory = load_context_factory_from_settings(settings, crawler)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(</span>
<span class="gi">+        cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler</span>
<span class="gi">+    ) -&gt; H2DownloadHandlerOrSubclass:</span>
<span class="gi">+        return cls(crawler.settings, crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def download_request(self, request: Request, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        agent = ScrapyH2Agent(</span>
<span class="gi">+            context_factory=self._context_factory,</span>
<span class="gi">+            pool=self._pool,</span>
<span class="gi">+            crawler=self._crawler,</span>
<span class="gi">+        )</span>
<span class="gi">+        return agent.download_request(request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; None:</span>
<span class="gi">+        self._pool.close_connections()</span>


<span class="w"> </span>class ScrapyH2Agent:
<span class="w"> </span>    _Agent = H2Agent
<span class="w"> </span>    _ProxyAgent = ScrapyProxyH2Agent

<span class="gd">-    def __init__(self, context_factory, pool: H2ConnectionPool,</span>
<span class="gd">-        connect_timeout: int=10, bind_address: Optional[bytes]=None,</span>
<span class="gd">-        crawler: Optional[Crawler]=None) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        context_factory,</span>
<span class="gi">+        pool: H2ConnectionPool,</span>
<span class="gi">+        connect_timeout: int = 10,</span>
<span class="gi">+        bind_address: Optional[bytes] = None,</span>
<span class="gi">+        crawler: Optional[Crawler] = None,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self._context_factory = context_factory
<span class="w"> </span>        self._connect_timeout = connect_timeout
<span class="w"> </span>        self._bind_address = bind_address
<span class="w"> </span>        self._pool = pool
<span class="w"> </span>        self._crawler = crawler
<span class="gi">+</span>
<span class="gi">+    def _get_agent(self, request: Request, timeout: Optional[float]) -&gt; H2Agent:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        bind_address = request.meta.get(&quot;bindaddress&quot;) or self._bind_address</span>
<span class="gi">+        proxy = request.meta.get(&quot;proxy&quot;)</span>
<span class="gi">+        if proxy:</span>
<span class="gi">+            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)</span>
<span class="gi">+            scheme = _parse(request.url)[0]</span>
<span class="gi">+</span>
<span class="gi">+            if scheme == b&quot;https&quot;:</span>
<span class="gi">+                # ToDo</span>
<span class="gi">+                raise NotImplementedError(</span>
<span class="gi">+                    &quot;Tunneling via CONNECT method using HTTP/2.0 is not yet supported&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return self._ProxyAgent(</span>
<span class="gi">+                reactor=reactor,</span>
<span class="gi">+                context_factory=self._context_factory,</span>
<span class="gi">+                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=&quot;ascii&quot;)),</span>
<span class="gi">+                connect_timeout=timeout,</span>
<span class="gi">+                bind_address=bind_address,</span>
<span class="gi">+                pool=self._pool,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        return self._Agent(</span>
<span class="gi">+            reactor=reactor,</span>
<span class="gi">+            context_factory=self._context_factory,</span>
<span class="gi">+            connect_timeout=timeout,</span>
<span class="gi">+            bind_address=bind_address,</span>
<span class="gi">+            pool=self._pool,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def download_request(self, request: Request, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        timeout = request.meta.get(&quot;download_timeout&quot;) or self._connect_timeout</span>
<span class="gi">+        agent = self._get_agent(request, timeout)</span>
<span class="gi">+</span>
<span class="gi">+        start_time = time()</span>
<span class="gi">+        d = agent.request(request, spider)</span>
<span class="gi">+        d.addCallback(self._cb_latency, request, start_time)</span>
<span class="gi">+</span>
<span class="gi">+        timeout_cl = reactor.callLater(timeout, d.cancel)</span>
<span class="gi">+        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _cb_latency(</span>
<span class="gi">+        response: Response, request: Request, start_time: float</span>
<span class="gi">+    ) -&gt; Response:</span>
<span class="gi">+        request.meta[&quot;download_latency&quot;] = time() - start_time</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _cb_timeout(</span>
<span class="gi">+        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall</span>
<span class="gi">+    ) -&gt; Response:</span>
<span class="gi">+        if timeout_cl.active():</span>
<span class="gi">+            timeout_cl.cancel()</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        url = urldefrag(request.url)[0]</span>
<span class="gi">+        raise TimeoutError(f&quot;Getting {url} took longer than {timeout} seconds.&quot;)</span>
<span class="gh">diff --git a/scrapy/core/downloader/handlers/s3.py b/scrapy/core/downloader/handlers/s3.py</span>
<span class="gh">index 6f341f7e2..81d8e8115 100644</span>
<span class="gd">--- a/scrapy/core/downloader/handlers/s3.py</span>
<span class="gi">+++ b/scrapy/core/downloader/handlers/s3.py</span>
<span class="gu">@@ -6,33 +6,78 @@ from scrapy.utils.misc import create_instance</span>


<span class="w"> </span>class S3DownloadHandler:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, settings, *, crawler=None, aws_access_key_id=None,</span>
<span class="gd">-        aws_secret_access_key=None, aws_session_token=None,</span>
<span class="gd">-        httpdownloadhandler=HTTPDownloadHandler, **kw):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        settings,</span>
<span class="gi">+        *,</span>
<span class="gi">+        crawler=None,</span>
<span class="gi">+        aws_access_key_id=None,</span>
<span class="gi">+        aws_secret_access_key=None,</span>
<span class="gi">+        aws_session_token=None,</span>
<span class="gi">+        httpdownloadhandler=HTTPDownloadHandler,</span>
<span class="gi">+        **kw,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if not is_botocore_available():
<span class="gd">-            raise NotConfigured(&#39;missing botocore library&#39;)</span>
<span class="gi">+            raise NotConfigured(&quot;missing botocore library&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>        if not aws_access_key_id:
<span class="gd">-            aws_access_key_id = settings[&#39;AWS_ACCESS_KEY_ID&#39;]</span>
<span class="gi">+            aws_access_key_id = settings[&quot;AWS_ACCESS_KEY_ID&quot;]</span>
<span class="w"> </span>        if not aws_secret_access_key:
<span class="gd">-            aws_secret_access_key = settings[&#39;AWS_SECRET_ACCESS_KEY&#39;]</span>
<span class="gi">+            aws_secret_access_key = settings[&quot;AWS_SECRET_ACCESS_KEY&quot;]</span>
<span class="w"> </span>        if not aws_session_token:
<span class="gd">-            aws_session_token = settings[&#39;AWS_SESSION_TOKEN&#39;]</span>
<span class="gd">-        anon = kw.get(&#39;anon&#39;)</span>
<span class="gd">-        if (anon is None and not aws_access_key_id and not</span>
<span class="gd">-            aws_secret_access_key):</span>
<span class="gd">-            kw[&#39;anon&#39;] = True</span>
<span class="gd">-        self.anon = kw.get(&#39;anon&#39;)</span>
<span class="gi">+            aws_session_token = settings[&quot;AWS_SESSION_TOKEN&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        # If no credentials could be found anywhere,</span>
<span class="gi">+        # consider this an anonymous connection request by default;</span>
<span class="gi">+        # unless &#39;anon&#39; was set explicitly (True/False).</span>
<span class="gi">+        anon = kw.get(&quot;anon&quot;)</span>
<span class="gi">+        if anon is None and not aws_access_key_id and not aws_secret_access_key:</span>
<span class="gi">+            kw[&quot;anon&quot;] = True</span>
<span class="gi">+        self.anon = kw.get(&quot;anon&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>        self._signer = None
<span class="w"> </span>        import botocore.auth
<span class="w"> </span>        import botocore.credentials
<span class="gd">-        kw.pop(&#39;anon&#39;, None)</span>
<span class="gi">+</span>
<span class="gi">+        kw.pop(&quot;anon&quot;, None)</span>
<span class="w"> </span>        if kw:
<span class="gd">-            raise TypeError(f&#39;Unexpected keyword arguments: {kw}&#39;)</span>
<span class="gi">+            raise TypeError(f&quot;Unexpected keyword arguments: {kw}&quot;)</span>
<span class="w"> </span>        if not self.anon:
<span class="gd">-            SignerCls = botocore.auth.AUTH_TYPE_MAPS[&#39;s3&#39;]</span>
<span class="gd">-            self._signer = SignerCls(botocore.credentials.Credentials(</span>
<span class="gd">-                aws_access_key_id, aws_secret_access_key, aws_session_token))</span>
<span class="gd">-        _http_handler = create_instance(objcls=httpdownloadhandler,</span>
<span class="gd">-            settings=settings, crawler=crawler)</span>
<span class="gi">+            SignerCls = botocore.auth.AUTH_TYPE_MAPS[&quot;s3&quot;]</span>
<span class="gi">+            self._signer = SignerCls(</span>
<span class="gi">+                botocore.credentials.Credentials(</span>
<span class="gi">+                    aws_access_key_id, aws_secret_access_key, aws_session_token</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        _http_handler = create_instance(</span>
<span class="gi">+            objcls=httpdownloadhandler,</span>
<span class="gi">+            settings=settings,</span>
<span class="gi">+            crawler=crawler,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._download_http = _http_handler.download_request
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, **kwargs):</span>
<span class="gi">+        return cls(crawler.settings, crawler=crawler, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def download_request(self, request, spider):</span>
<span class="gi">+        p = urlparse_cached(request)</span>
<span class="gi">+        scheme = &quot;https&quot; if request.meta.get(&quot;is_secure&quot;) else &quot;http&quot;</span>
<span class="gi">+        bucket = p.hostname</span>
<span class="gi">+        path = p.path + &quot;?&quot; + p.query if p.query else p.path</span>
<span class="gi">+        url = f&quot;{scheme}://{bucket}.s3.amazonaws.com{path}&quot;</span>
<span class="gi">+        if self.anon:</span>
<span class="gi">+            request = request.replace(url=url)</span>
<span class="gi">+        else:</span>
<span class="gi">+            import botocore.awsrequest</span>
<span class="gi">+</span>
<span class="gi">+            awsrequest = botocore.awsrequest.AWSRequest(</span>
<span class="gi">+                method=request.method,</span>
<span class="gi">+                url=f&quot;{scheme}://s3.amazonaws.com/{bucket}{path}&quot;,</span>
<span class="gi">+                headers=request.headers.to_unicode_dict(),</span>
<span class="gi">+                data=request.body,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._signer.add_auth(awsrequest)</span>
<span class="gi">+            request = request.replace(url=url, headers=awsrequest.headers.items())</span>
<span class="gi">+        return self._download_http(request, spider)</span>
<span class="gh">diff --git a/scrapy/core/downloader/middleware.py b/scrapy/core/downloader/middleware.py</span>
<span class="gh">index 377040b14..dca13c01e 100644</span>
<span class="gd">--- a/scrapy/core/downloader/middleware.py</span>
<span class="gi">+++ b/scrapy/core/downloader/middleware.py</span>
<span class="gu">@@ -4,8 +4,10 @@ Downloader Middleware manager</span>
<span class="w"> </span>See documentation in docs/topics/downloader-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from typing import Any, Callable, Generator, List, Union, cast
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred, inlineCallbacks
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="w"> </span>from scrapy.exceptions import _InvalidOutput
<span class="w"> </span>from scrapy.http import Request, Response
<span class="gu">@@ -16,4 +18,86 @@ from scrapy.utils.defer import deferred_from_coro, mustbe_deferred</span>


<span class="w"> </span>class DownloaderMiddlewareManager(MiddlewareManager):
<span class="gd">-    component_name = &#39;downloader middleware&#39;</span>
<span class="gi">+    component_name = &quot;downloader middleware&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -&gt; List[Any]:</span>
<span class="gi">+        return build_component_list(settings.getwithbase(&quot;DOWNLOADER_MIDDLEWARES&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def _add_middleware(self, mw: Any) -&gt; None:</span>
<span class="gi">+        if hasattr(mw, &quot;process_request&quot;):</span>
<span class="gi">+            self.methods[&quot;process_request&quot;].append(mw.process_request)</span>
<span class="gi">+        if hasattr(mw, &quot;process_response&quot;):</span>
<span class="gi">+            self.methods[&quot;process_response&quot;].appendleft(mw.process_response)</span>
<span class="gi">+        if hasattr(mw, &quot;process_exception&quot;):</span>
<span class="gi">+            self.methods[&quot;process_exception&quot;].appendleft(mw.process_exception)</span>
<span class="gi">+</span>
<span class="gi">+    def download(</span>
<span class="gi">+        self, download_func: Callable, request: Request, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        @inlineCallbacks</span>
<span class="gi">+        def process_request(request: Request) -&gt; Generator[Deferred, Any, Any]:</span>
<span class="gi">+            for method in self.methods[&quot;process_request&quot;]:</span>
<span class="gi">+                method = cast(Callable, method)</span>
<span class="gi">+                response = yield deferred_from_coro(</span>
<span class="gi">+                    method(request=request, spider=spider)</span>
<span class="gi">+                )</span>
<span class="gi">+                if response is not None and not isinstance(</span>
<span class="gi">+                    response, (Response, Request)</span>
<span class="gi">+                ):</span>
<span class="gi">+                    raise _InvalidOutput(</span>
<span class="gi">+                        f&quot;Middleware {method.__qualname__} must return None, Response or &quot;</span>
<span class="gi">+                        f&quot;Request, got {response.__class__.__name__}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if response:</span>
<span class="gi">+                    return response</span>
<span class="gi">+            return (yield download_func(request=request, spider=spider))</span>
<span class="gi">+</span>
<span class="gi">+        @inlineCallbacks</span>
<span class="gi">+        def process_response(</span>
<span class="gi">+            response: Union[Response, Request]</span>
<span class="gi">+        ) -&gt; Generator[Deferred, Any, Union[Response, Request]]:</span>
<span class="gi">+            if response is None:</span>
<span class="gi">+                raise TypeError(&quot;Received None in process_response&quot;)</span>
<span class="gi">+            elif isinstance(response, Request):</span>
<span class="gi">+                return response</span>
<span class="gi">+</span>
<span class="gi">+            for method in self.methods[&quot;process_response&quot;]:</span>
<span class="gi">+                method = cast(Callable, method)</span>
<span class="gi">+                response = yield deferred_from_coro(</span>
<span class="gi">+                    method(request=request, response=response, spider=spider)</span>
<span class="gi">+                )</span>
<span class="gi">+                if not isinstance(response, (Response, Request)):</span>
<span class="gi">+                    raise _InvalidOutput(</span>
<span class="gi">+                        f&quot;Middleware {method.__qualname__} must return Response or Request, &quot;</span>
<span class="gi">+                        f&quot;got {type(response)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if isinstance(response, Request):</span>
<span class="gi">+                    return response</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        @inlineCallbacks</span>
<span class="gi">+        def process_exception(</span>
<span class="gi">+            failure: Failure,</span>
<span class="gi">+        ) -&gt; Generator[Deferred, Any, Union[Failure, Response, Request]]:</span>
<span class="gi">+            exception = failure.value</span>
<span class="gi">+            for method in self.methods[&quot;process_exception&quot;]:</span>
<span class="gi">+                method = cast(Callable, method)</span>
<span class="gi">+                response = yield deferred_from_coro(</span>
<span class="gi">+                    method(request=request, exception=exception, spider=spider)</span>
<span class="gi">+                )</span>
<span class="gi">+                if response is not None and not isinstance(</span>
<span class="gi">+                    response, (Response, Request)</span>
<span class="gi">+                ):</span>
<span class="gi">+                    raise _InvalidOutput(</span>
<span class="gi">+                        f&quot;Middleware {method.__qualname__} must return None, Response or &quot;</span>
<span class="gi">+                        f&quot;Request, got {type(response)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if response:</span>
<span class="gi">+                    return response</span>
<span class="gi">+            return failure</span>
<span class="gi">+</span>
<span class="gi">+        deferred = mustbe_deferred(process_request, request)</span>
<span class="gi">+        deferred.addErrback(process_exception)</span>
<span class="gi">+        deferred.addCallback(process_response)</span>
<span class="gi">+        return deferred</span>
<span class="gh">diff --git a/scrapy/core/downloader/tls.py b/scrapy/core/downloader/tls.py</span>
<span class="gh">index 1295d26c4..33cea7263 100644</span>
<span class="gd">--- a/scrapy/core/downloader/tls.py</span>
<span class="gi">+++ b/scrapy/core/downloader/tls.py</span>
<span class="gu">@@ -1,18 +1,32 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>from typing import Any, Dict
<span class="gi">+</span>
<span class="w"> </span>from OpenSSL import SSL
<span class="w"> </span>from service_identity.exceptions import CertificateError
<span class="gd">-from twisted.internet._sslverify import ClientTLSOptions, VerificationError, verifyHostname</span>
<span class="gi">+from twisted.internet._sslverify import (</span>
<span class="gi">+    ClientTLSOptions,</span>
<span class="gi">+    VerificationError,</span>
<span class="gi">+    verifyHostname,</span>
<span class="gi">+)</span>
<span class="w"> </span>from twisted.internet.ssl import AcceptableCiphers
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.ssl import get_temp_key_info, x509name_to_string
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gd">-METHOD_TLS = &#39;TLS&#39;</span>
<span class="gd">-METHOD_TLSv10 = &#39;TLSv1.0&#39;</span>
<span class="gd">-METHOD_TLSv11 = &#39;TLSv1.1&#39;</span>
<span class="gd">-METHOD_TLSv12 = &#39;TLSv1.2&#39;</span>
<span class="gd">-openssl_methods: Dict[str, int] = {METHOD_TLS: SSL.SSLv23_METHOD,</span>
<span class="gd">-    METHOD_TLSv10: SSL.TLSv1_METHOD, METHOD_TLSv11: SSL.TLSv1_1_METHOD,</span>
<span class="gd">-    METHOD_TLSv12: SSL.TLSv1_2_METHOD}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+METHOD_TLS = &quot;TLS&quot;</span>
<span class="gi">+METHOD_TLSv10 = &quot;TLSv1.0&quot;</span>
<span class="gi">+METHOD_TLSv11 = &quot;TLSv1.1&quot;</span>
<span class="gi">+METHOD_TLSv12 = &quot;TLSv1.2&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+openssl_methods: Dict[str, int] = {</span>
<span class="gi">+    METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)</span>
<span class="gi">+    METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only</span>
<span class="gi">+    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only</span>
<span class="gi">+    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only</span>
<span class="gi">+}</span>


<span class="w"> </span>class ScrapyClientTLSOptions(ClientTLSOptions):
<span class="gu">@@ -26,11 +40,52 @@ class ScrapyClientTLSOptions(ClientTLSOptions):</span>
<span class="w"> </span>    logging warnings. Also, HTTPS connection parameters logging is added.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging:</span>
<span class="gd">-        bool=False):</span>
<span class="gi">+    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging: bool = False):</span>
<span class="w"> </span>        super().__init__(hostname, ctx)
<span class="w"> </span>        self.verbose_logging: bool = verbose_logging

<span class="gi">+    def _identityVerifyingInfoCallback(</span>
<span class="gi">+        self, connection: SSL.Connection, where: int, ret: Any</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        if where &amp; SSL.SSL_CB_HANDSHAKE_START:</span>
<span class="gi">+            connection.set_tlsext_host_name(self._hostnameBytes)</span>
<span class="gi">+        elif where &amp; SSL.SSL_CB_HANDSHAKE_DONE:</span>
<span class="gi">+            if self.verbose_logging:</span>
<span class="gi">+                logger.debug(</span>
<span class="gi">+                    &quot;SSL connection to %s using protocol %s, cipher %s&quot;,</span>
<span class="gi">+                    self._hostnameASCII,</span>
<span class="gi">+                    connection.get_protocol_version_name(),</span>
<span class="gi">+                    connection.get_cipher_name(),</span>
<span class="gi">+                )</span>
<span class="gi">+                server_cert = connection.get_peer_certificate()</span>
<span class="gi">+                if server_cert:</span>
<span class="gi">+                    logger.debug(</span>
<span class="gi">+                        &#39;SSL connection certificate: issuer &quot;%s&quot;, subject &quot;%s&quot;&#39;,</span>
<span class="gi">+                        x509name_to_string(server_cert.get_issuer()),</span>
<span class="gi">+                        x509name_to_string(server_cert.get_subject()),</span>
<span class="gi">+                    )</span>
<span class="gi">+                key_info = get_temp_key_info(connection._ssl)</span>
<span class="gi">+                if key_info:</span>
<span class="gi">+                    logger.debug(&quot;SSL temp key: %s&quot;, key_info)</span>
<span class="gi">+</span>
<span class="gi">+            try:</span>
<span class="gi">+                verifyHostname(connection, self._hostnameASCII)</span>
<span class="gi">+            except (CertificateError, VerificationError) as e:</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    &#39;Remote certificate is not valid for hostname &quot;%s&quot;; %s&#39;,</span>
<span class="gi">+                    self._hostnameASCII,</span>
<span class="gi">+                    e,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            except ValueError as e:</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    &quot;Ignoring error while verifying certificate &quot;</span>
<span class="gi">+                    &#39;from host &quot;%s&quot; (exception: %r)&#39;,</span>
<span class="gi">+                    self._hostnameASCII,</span>
<span class="gi">+                    e,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>

<span class="w"> </span>DEFAULT_CIPHERS: AcceptableCiphers = AcceptableCiphers.fromOpenSSLCipherString(
<span class="gd">-    &#39;DEFAULT&#39;)</span>
<span class="gi">+    &quot;DEFAULT&quot;</span>
<span class="gi">+)</span>
<span class="gh">diff --git a/scrapy/core/downloader/webclient.py b/scrapy/core/downloader/webclient.py</span>
<span class="gh">index 96778332a..bb1f73805 100644</span>
<span class="gd">--- a/scrapy/core/downloader/webclient.py</span>
<span class="gi">+++ b/scrapy/core/downloader/webclient.py</span>
<span class="gu">@@ -2,9 +2,11 @@ import re</span>
<span class="w"> </span>from time import time
<span class="w"> </span>from typing import Optional, Tuple
<span class="w"> </span>from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer
<span class="w"> </span>from twisted.internet.protocol import ClientFactory
<span class="w"> </span>from twisted.web.http import HTTPClient
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request
<span class="w"> </span>from scrapy.http import Headers
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="gu">@@ -12,48 +14,185 @@ from scrapy.utils.httpobj import urlparse_cached</span>
<span class="w"> </span>from scrapy.utils.python import to_bytes, to_unicode


<span class="gd">-def _parse(url: str) -&gt;Tuple[bytes, bytes, bytes, int, bytes]:</span>
<span class="gi">+def _parsed_url_args(parsed: ParseResult) -&gt; Tuple[bytes, bytes, bytes, int, bytes]:</span>
<span class="gi">+    # Assume parsed is urlparse-d from Request.url,</span>
<span class="gi">+    # which was passed via safe_url_string and is ascii-only.</span>
<span class="gi">+    path_str = urlunparse((&quot;&quot;, &quot;&quot;, parsed.path or &quot;/&quot;, parsed.params, parsed.query, &quot;&quot;))</span>
<span class="gi">+    path = to_bytes(path_str, encoding=&quot;ascii&quot;)</span>
<span class="gi">+    assert parsed.hostname is not None</span>
<span class="gi">+    host = to_bytes(parsed.hostname, encoding=&quot;ascii&quot;)</span>
<span class="gi">+    port = parsed.port</span>
<span class="gi">+    scheme = to_bytes(parsed.scheme, encoding=&quot;ascii&quot;)</span>
<span class="gi">+    netloc = to_bytes(parsed.netloc, encoding=&quot;ascii&quot;)</span>
<span class="gi">+    if port is None:</span>
<span class="gi">+        port = 443 if scheme == b&quot;https&quot; else 80</span>
<span class="gi">+    return scheme, netloc, host, port, path</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _parse(url: str) -&gt; Tuple[bytes, bytes, bytes, int, bytes]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return tuple of (scheme, netloc, host, port, path),
<span class="w"> </span>    all in bytes except for port which is int.
<span class="w"> </span>    Assume url is from Request.url, which was passed via safe_url_string
<span class="w"> </span>    and is ascii-only.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    url = url.strip()</span>
<span class="gi">+    if not re.match(r&quot;^\w+://&quot;, url):</span>
<span class="gi">+        url = &quot;//&quot; + url</span>
<span class="gi">+    parsed = urlparse(url)</span>
<span class="gi">+    return _parsed_url_args(parsed)</span>


<span class="w"> </span>class ScrapyHTTPPageGetter(HTTPClient):
<span class="gd">-    delimiter = b&#39;\n&#39;</span>
<span class="gi">+    delimiter = b&quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def connectionMade(self):</span>
<span class="gi">+        self.headers = Headers()  # bucket for response headers</span>
<span class="gi">+</span>
<span class="gi">+        # Method command</span>
<span class="gi">+        self.sendCommand(self.factory.method, self.factory.path)</span>
<span class="gi">+        # Headers</span>
<span class="gi">+        for key, values in self.factory.headers.items():</span>
<span class="gi">+            for value in values:</span>
<span class="gi">+                self.sendHeader(key, value)</span>
<span class="gi">+        self.endHeaders()</span>
<span class="gi">+        # Body</span>
<span class="gi">+        if self.factory.body is not None:</span>
<span class="gi">+            self.transport.write(self.factory.body)</span>
<span class="gi">+</span>
<span class="gi">+    def lineReceived(self, line):</span>
<span class="gi">+        return HTTPClient.lineReceived(self, line.rstrip())</span>
<span class="gi">+</span>
<span class="gi">+    def handleHeader(self, key, value):</span>
<span class="gi">+        self.headers.appendlist(key, value)</span>
<span class="gi">+</span>
<span class="gi">+    def handleStatus(self, version, status, message):</span>
<span class="gi">+        self.factory.gotStatus(version, status, message)</span>
<span class="gi">+</span>
<span class="gi">+    def handleEndHeaders(self):</span>
<span class="gi">+        self.factory.gotHeaders(self.headers)</span>
<span class="gi">+</span>
<span class="gi">+    def connectionLost(self, reason):</span>
<span class="gi">+        self._connection_lost_reason = reason</span>
<span class="gi">+        HTTPClient.connectionLost(self, reason)</span>
<span class="gi">+        self.factory.noPage(reason)</span>
<span class="gi">+</span>
<span class="gi">+    def handleResponse(self, response):</span>
<span class="gi">+        if self.factory.method.upper() == b&quot;HEAD&quot;:</span>
<span class="gi">+            self.factory.page(b&quot;&quot;)</span>
<span class="gi">+        elif self.length is not None and self.length &gt; 0:</span>
<span class="gi">+            self.factory.noPage(self._connection_lost_reason)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.factory.page(response)</span>
<span class="gi">+        self.transport.loseConnection()</span>
<span class="gi">+</span>
<span class="gi">+    def timeout(self):</span>
<span class="gi">+        self.transport.loseConnection()</span>
<span class="gi">+</span>
<span class="gi">+        # transport cleanup needed for HTTPS connections</span>
<span class="gi">+        if self.factory.url.startswith(b&quot;https&quot;):</span>
<span class="gi">+            self.transport.stopProducing()</span>

<span class="gi">+        self.factory.noPage(</span>
<span class="gi">+            defer.TimeoutError(</span>
<span class="gi">+                f&quot;Getting {self.factory.url} took longer &quot;</span>
<span class="gi">+                f&quot;than {self.factory.timeout} seconds.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>

<span class="gi">+</span>
<span class="gi">+# This class used to inherit from Twisted’s</span>
<span class="gi">+# twisted.web.client.HTTPClientFactory. When that class was deprecated in</span>
<span class="gi">+# Twisted (https://github.com/twisted/twisted/pull/643), we merged its</span>
<span class="gi">+# non-overridden code into this class.</span>
<span class="w"> </span>class ScrapyHTTPClientFactory(ClientFactory):
<span class="w"> </span>    protocol = ScrapyHTTPPageGetter
<span class="gi">+</span>
<span class="w"> </span>    waiting = 1
<span class="w"> </span>    noisy = False
<span class="w"> </span>    followRedirect = False
<span class="w"> </span>    afterFoundGet = False

<span class="gd">-    def __init__(self, request: Request, timeout: float=180):</span>
<span class="gi">+    def _build_response(self, body, request):</span>
<span class="gi">+        request.meta[&quot;download_latency&quot;] = self.headers_time - self.start_time</span>
<span class="gi">+        status = int(self.status)</span>
<span class="gi">+        headers = Headers(self.response_headers)</span>
<span class="gi">+        respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)</span>
<span class="gi">+        return respcls(</span>
<span class="gi">+            url=self._url,</span>
<span class="gi">+            status=status,</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            body=body,</span>
<span class="gi">+            protocol=to_unicode(self.version),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _set_connection_attributes(self, request):</span>
<span class="gi">+        parsed = urlparse_cached(request)</span>
<span class="gi">+        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(</span>
<span class="gi">+            parsed</span>
<span class="gi">+        )</span>
<span class="gi">+        proxy = request.meta.get(&quot;proxy&quot;)</span>
<span class="gi">+        if proxy:</span>
<span class="gi">+            self.scheme, _, self.host, self.port, _ = _parse(proxy)</span>
<span class="gi">+            self.path = self.url</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, request: Request, timeout: float = 180):</span>
<span class="w"> </span>        self._url: str = urldefrag(request.url)[0]
<span class="gd">-        self.url: bytes = to_bytes(self._url, encoding=&#39;ascii&#39;)</span>
<span class="gd">-        self.method: bytes = to_bytes(request.method, encoding=&#39;ascii&#39;)</span>
<span class="gi">+        # converting to bytes to comply to Twisted interface</span>
<span class="gi">+        self.url: bytes = to_bytes(self._url, encoding=&quot;ascii&quot;)</span>
<span class="gi">+        self.method: bytes = to_bytes(request.method, encoding=&quot;ascii&quot;)</span>
<span class="w"> </span>        self.body: Optional[bytes] = request.body or None
<span class="w"> </span>        self.headers: Headers = Headers(request.headers)
<span class="w"> </span>        self.response_headers: Optional[Headers] = None
<span class="gd">-        self.timeout: float = request.meta.get(&#39;download_timeout&#39;) or timeout</span>
<span class="gi">+        self.timeout: float = request.meta.get(&quot;download_timeout&quot;) or timeout</span>
<span class="w"> </span>        self.start_time: float = time()
<span class="gd">-        self.deferred: defer.Deferred = defer.Deferred().addCallback(self.</span>
<span class="gd">-            _build_response, request)</span>
<span class="gi">+        self.deferred: defer.Deferred = defer.Deferred().addCallback(</span>
<span class="gi">+            self._build_response, request</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected</span>
<span class="gi">+        # to have _disconnectedDeferred. See Twisted r32329.</span>
<span class="gi">+        # As Scrapy implements it&#39;s own logic to handle redirects is not</span>
<span class="gi">+        # needed to add the callback _waitForDisconnect.</span>
<span class="gi">+        # Specifically this avoids the AttributeError exception when</span>
<span class="gi">+        # clientConnectionFailed method is called.</span>
<span class="w"> </span>        self._disconnectedDeferred: defer.Deferred = defer.Deferred()
<span class="gi">+</span>
<span class="w"> </span>        self._set_connection_attributes(request)
<span class="gd">-        self.headers.setdefault(&#39;Host&#39;, self.netloc)</span>
<span class="gi">+</span>
<span class="gi">+        # set Host header based on url</span>
<span class="gi">+        self.headers.setdefault(&quot;Host&quot;, self.netloc)</span>
<span class="gi">+</span>
<span class="gi">+        # set Content-Length based len of body</span>
<span class="w"> </span>        if self.body is not None:
<span class="gd">-            self.headers[&#39;Content-Length&#39;] = len(self.body)</span>
<span class="gd">-            self.headers.setdefault(&#39;Connection&#39;, &#39;close&#39;)</span>
<span class="gd">-        elif self.method == b&#39;POST&#39;:</span>
<span class="gd">-            self.headers[&#39;Content-Length&#39;] = 0</span>
<span class="gi">+            self.headers[&quot;Content-Length&quot;] = len(self.body)</span>
<span class="gi">+            # just in case a broken http/1.1 decides to keep connection alive</span>
<span class="gi">+            self.headers.setdefault(&quot;Connection&quot;, &quot;close&quot;)</span>
<span class="gi">+        # Content-Length must be specified in POST method even with no body</span>
<span class="gi">+        elif self.method == b&quot;POST&quot;:</span>
<span class="gi">+            self.headers[&quot;Content-Length&quot;] = 0</span>
<span class="gi">+</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}: {self._url}&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def _cancelTimeout(self, result, timeoutCall):</span>
<span class="gi">+        if timeoutCall.active():</span>
<span class="gi">+            timeoutCall.cancel()</span>
<span class="gi">+        return result</span>
<span class="gi">+</span>
<span class="gi">+    def buildProtocol(self, addr):</span>
<span class="gi">+        p = ClientFactory.buildProtocol(self, addr)</span>
<span class="gi">+        p.followRedirect = self.followRedirect</span>
<span class="gi">+        p.afterFoundGet = self.afterFoundGet</span>
<span class="gi">+        if self.timeout:</span>
<span class="gi">+            from twisted.internet import reactor</span>

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}: {self._url}&gt;&#39;</span>
<span class="gi">+            timeoutCall = reactor.callLater(self.timeout, p.timeout)</span>
<span class="gi">+            self.deferred.addBoth(self._cancelTimeout, timeoutCall)</span>
<span class="gi">+        return p</span>
<span class="gi">+</span>
<span class="gi">+    def gotHeaders(self, headers):</span>
<span class="gi">+        self.headers_time = time()</span>
<span class="gi">+        self.response_headers = headers</span>

<span class="w"> </span>    def gotStatus(self, version, status, message):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gu">@@ -66,7 +205,17 @@ class ScrapyHTTPClientFactory(ClientFactory):</span>
<span class="w"> </span>        @param message: The HTTP status message.
<span class="w"> </span>        @type message: L{bytes}
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.version, self.status, self.message = version, status, message</span>
<span class="gi">+</span>
<span class="gi">+    def page(self, page):</span>
<span class="gi">+        if self.waiting:</span>
<span class="gi">+            self.waiting = 0</span>
<span class="gi">+            self.deferred.callback(page)</span>
<span class="gi">+</span>
<span class="gi">+    def noPage(self, reason):</span>
<span class="gi">+        if self.waiting:</span>
<span class="gi">+            self.waiting = 0</span>
<span class="gi">+            self.deferred.errback(reason)</span>

<span class="w"> </span>    def clientConnectionFailed(self, _, reason):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gu">@@ -74,4 +223,9 @@ class ScrapyHTTPClientFactory(ClientFactory):</span>
<span class="w"> </span>        result has yet been provided to the result Deferred, provide the
<span class="w"> </span>        connection failure reason as an error result.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.waiting:</span>
<span class="gi">+            self.waiting = 0</span>
<span class="gi">+            # If the connection attempt failed, there is nothing more to</span>
<span class="gi">+            # disconnect, so just fire that Deferred now.</span>
<span class="gi">+            self._disconnectedDeferred.callback(None)</span>
<span class="gi">+            self.deferred.errback(reason)</span>
<span class="gh">diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py</span>
<span class="gh">index e7403fb61..92967ffc8 100644</span>
<span class="gd">--- a/scrapy/core/engine.py</span>
<span class="gi">+++ b/scrapy/core/engine.py</span>
<span class="gu">@@ -6,10 +6,24 @@ For more information see docs/topics/architecture.rst</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import logging
<span class="w"> </span>from time import time
<span class="gd">-from typing import TYPE_CHECKING, Any, Callable, Generator, Iterable, Iterator, Optional, Set, Type, Union, cast</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    Iterator,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Set,</span>
<span class="gi">+    Type,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred, inlineCallbacks, succeed
<span class="w"> </span>from twisted.internet.task import LoopingCall
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.core.downloader import Downloader
<span class="w"> </span>from scrapy.core.scraper import Scraper
<span class="gu">@@ -23,30 +37,54 @@ from scrapy.utils.log import failure_to_exc_info, logformatter_adapter</span>
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="w"> </span>from scrapy.utils.python import global_object_name
<span class="w"> </span>from scrapy.utils.reactor import CallLaterOnce
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.core.scheduler import BaseScheduler
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class Slot:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, start_requests: Iterable[Request], close_if_idle:</span>
<span class="gd">-        bool, nextcall: CallLaterOnce, scheduler: &#39;BaseScheduler&#39;) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        start_requests: Iterable[Request],</span>
<span class="gi">+        close_if_idle: bool,</span>
<span class="gi">+        nextcall: CallLaterOnce,</span>
<span class="gi">+        scheduler: &quot;BaseScheduler&quot;,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.closing: Optional[Deferred] = None
<span class="w"> </span>        self.inprogress: Set[Request] = set()
<span class="w"> </span>        self.start_requests: Optional[Iterator[Request]] = iter(start_requests)
<span class="w"> </span>        self.close_if_idle: bool = close_if_idle
<span class="w"> </span>        self.nextcall: CallLaterOnce = nextcall
<span class="gd">-        self.scheduler: &#39;BaseScheduler&#39; = scheduler</span>
<span class="gi">+        self.scheduler: &quot;BaseScheduler&quot; = scheduler</span>
<span class="w"> </span>        self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)

<span class="gi">+    def add_request(self, request: Request) -&gt; None:</span>
<span class="gi">+        self.inprogress.add(request)</span>

<span class="gd">-class ExecutionEngine:</span>
<span class="gi">+    def remove_request(self, request: Request) -&gt; None:</span>
<span class="gi">+        self.inprogress.remove(request)</span>
<span class="gi">+        self._maybe_fire_closing()</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; Deferred:</span>
<span class="gi">+        self.closing = Deferred()</span>
<span class="gi">+        self._maybe_fire_closing()</span>
<span class="gi">+        return self.closing</span>
<span class="gi">+</span>
<span class="gi">+    def _maybe_fire_closing(self) -&gt; None:</span>
<span class="gi">+        if self.closing is not None and not self.inprogress:</span>
<span class="gi">+            if self.nextcall:</span>
<span class="gi">+                self.nextcall.cancel()</span>
<span class="gi">+                if self.heartbeat.running:</span>
<span class="gi">+                    self.heartbeat.stop()</span>
<span class="gi">+            self.closing.callback(None)</span>

<span class="gd">-    def __init__(self, crawler: &#39;Crawler&#39;, spider_closed_callback: Callable</span>
<span class="gd">-        ) -&gt;None:</span>
<span class="gd">-        self.crawler: &#39;Crawler&#39; = crawler</span>
<span class="gi">+</span>
<span class="gi">+class ExecutionEngine:</span>
<span class="gi">+    def __init__(self, crawler: &quot;Crawler&quot;, spider_closed_callback: Callable) -&gt; None:</span>
<span class="gi">+        self.crawler: &quot;Crawler&quot; = crawler</span>
<span class="w"> </span>        self.settings: Settings = crawler.settings
<span class="w"> </span>        self.signals: SignalManager = crawler.signals
<span class="w"> </span>        assert crawler.logformatter
<span class="gu">@@ -55,43 +93,386 @@ class ExecutionEngine:</span>
<span class="w"> </span>        self.spider: Optional[Spider] = None
<span class="w"> </span>        self.running: bool = False
<span class="w"> </span>        self.paused: bool = False
<span class="gd">-        self.scheduler_cls: Type[&#39;BaseScheduler&#39;] = self._get_scheduler_class(</span>
<span class="gd">-            crawler.settings)</span>
<span class="gd">-        downloader_cls: Type[Downloader] = load_object(self.settings[</span>
<span class="gd">-            &#39;DOWNLOADER&#39;])</span>
<span class="gi">+        self.scheduler_cls: Type[&quot;BaseScheduler&quot;] = self._get_scheduler_class(</span>
<span class="gi">+            crawler.settings</span>
<span class="gi">+        )</span>
<span class="gi">+        downloader_cls: Type[Downloader] = load_object(self.settings[&quot;DOWNLOADER&quot;])</span>
<span class="w"> </span>        self.downloader: Downloader = downloader_cls(crawler)
<span class="w"> </span>        self.scraper = Scraper(crawler)
<span class="w"> </span>        self._spider_closed_callback: Callable = spider_closed_callback
<span class="w"> </span>        self.start_time: Optional[float] = None

<span class="gd">-    def stop(self) -&gt;Deferred:</span>
<span class="gi">+    def _get_scheduler_class(self, settings: BaseSettings) -&gt; Type[&quot;BaseScheduler&quot;]:</span>
<span class="gi">+        from scrapy.core.scheduler import BaseScheduler</span>
<span class="gi">+</span>
<span class="gi">+        scheduler_cls: Type = load_object(settings[&quot;SCHEDULER&quot;])</span>
<span class="gi">+        if not issubclass(scheduler_cls, BaseScheduler):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;The provided scheduler class ({settings[&#39;SCHEDULER&#39;]})&quot;</span>
<span class="gi">+                &quot; does not fully implement the scheduler interface&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        return scheduler_cls</span>
<span class="gi">+</span>
<span class="gi">+    @inlineCallbacks</span>
<span class="gi">+    def start(self) -&gt; Generator[Deferred, Any, None]:</span>
<span class="gi">+        if self.running:</span>
<span class="gi">+            raise RuntimeError(&quot;Engine already running&quot;)</span>
<span class="gi">+        self.start_time = time()</span>
<span class="gi">+        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)</span>
<span class="gi">+        self.running = True</span>
<span class="gi">+        self._closewait: Deferred = Deferred()</span>
<span class="gi">+        yield self._closewait</span>
<span class="gi">+</span>
<span class="gi">+    def stop(self) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;Gracefully stop the execution engine&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="gd">-    def close(self) -&gt;Deferred:</span>
<span class="gi">+        @inlineCallbacks</span>
<span class="gi">+        def _finish_stopping_engine(_: Any) -&gt; Generator[Deferred, Any, None]:</span>
<span class="gi">+            yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)</span>
<span class="gi">+            self._closewait.callback(None)</span>
<span class="gi">+</span>
<span class="gi">+        if not self.running:</span>
<span class="gi">+            raise RuntimeError(&quot;Engine not running&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        self.running = False</span>
<span class="gi">+        dfd = (</span>
<span class="gi">+            self.close_spider(self.spider, reason=&quot;shutdown&quot;)</span>
<span class="gi">+            if self.spider is not None</span>
<span class="gi">+            else succeed(None)</span>
<span class="gi">+        )</span>
<span class="gi">+        return dfd.addBoth(_finish_stopping_engine)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Gracefully close the execution engine.
<span class="w"> </span>        If it has already been started, stop it. In all cases, close the spider and the downloader.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.running:</span>
<span class="gi">+            return self.stop()  # will also close spider and downloader</span>
<span class="gi">+        if self.spider is not None:</span>
<span class="gi">+            return self.close_spider(</span>
<span class="gi">+                self.spider, reason=&quot;shutdown&quot;</span>
<span class="gi">+            )  # will also close downloader</span>
<span class="gi">+        self.downloader.close()</span>
<span class="gi">+        return succeed(None)</span>
<span class="gi">+</span>
<span class="gi">+    def pause(self) -&gt; None:</span>
<span class="gi">+        self.paused = True</span>
<span class="gi">+</span>
<span class="gi">+    def unpause(self) -&gt; None:</span>
<span class="gi">+        self.paused = False</span>
<span class="gi">+</span>
<span class="gi">+    def _next_request(self) -&gt; None:</span>
<span class="gi">+        if self.slot is None:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        assert self.spider is not None  # typing</span>
<span class="gi">+</span>
<span class="gi">+        if self.paused:</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        while (</span>
<span class="gi">+            not self._needs_backout()</span>
<span class="gi">+            and self._next_request_from_scheduler() is not None</span>
<span class="gi">+        ):</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+        if self.slot.start_requests is not None and not self._needs_backout():</span>
<span class="gi">+            try:</span>
<span class="gi">+                request = next(self.slot.start_requests)</span>
<span class="gi">+            except StopIteration:</span>
<span class="gi">+                self.slot.start_requests = None</span>
<span class="gi">+            except Exception:</span>
<span class="gi">+                self.slot.start_requests = None</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    &quot;Error while obtaining start requests&quot;,</span>
<span class="gi">+                    exc_info=True,</span>
<span class="gi">+                    extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                self.crawl(request)</span>
<span class="gi">+</span>
<span class="gi">+        if self.spider_is_idle() and self.slot.close_if_idle:</span>
<span class="gi">+            self._spider_idle()</span>
<span class="gi">+</span>
<span class="gi">+    def _needs_backout(self) -&gt; bool:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        assert self.scraper.slot is not None  # typing</span>
<span class="gi">+        return (</span>
<span class="gi">+            not self.running</span>
<span class="gi">+            or bool(self.slot.closing)</span>
<span class="gi">+            or self.downloader.needs_backout()</span>
<span class="gi">+            or self.scraper.slot.needs_backout()</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _next_request_from_scheduler(self) -&gt; Optional[Deferred]:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        assert self.spider is not None  # typing</span>
<span class="gi">+</span>
<span class="gi">+        request = self.slot.scheduler.next_request()</span>
<span class="gi">+        if request is None:</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        d = self._download(request)</span>
<span class="gi">+        d.addBoth(self._handle_downloader_output, request)</span>
<span class="gi">+        d.addErrback(</span>
<span class="gi">+            lambda f: logger.info(</span>
<span class="gi">+                &quot;Error while handling downloader output&quot;,</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        def _remove_request(_: Any) -&gt; None:</span>
<span class="gi">+            assert self.slot</span>
<span class="gi">+            self.slot.remove_request(request)</span>
<span class="gi">+</span>
<span class="gi">+        d.addBoth(_remove_request)</span>
<span class="gi">+        d.addErrback(</span>
<span class="gi">+            lambda f: logger.info(</span>
<span class="gi">+                &quot;Error while removing request from slot&quot;,</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        slot = self.slot</span>
<span class="gi">+        d.addBoth(lambda _: slot.nextcall.schedule())</span>
<span class="gi">+        d.addErrback(</span>
<span class="gi">+            lambda f: logger.info(</span>
<span class="gi">+                &quot;Error while scheduling new request&quot;,</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        return d</span>

<span class="gd">-    def crawl(self, request: Request) -&gt;None:</span>
<span class="gi">+    def _handle_downloader_output(</span>
<span class="gi">+        self, result: Union[Request, Response, Failure], request: Request</span>
<span class="gi">+    ) -&gt; Optional[Deferred]:</span>
<span class="gi">+        assert self.spider is not None  # typing</span>
<span class="gi">+</span>
<span class="gi">+        if not isinstance(result, (Request, Response, Failure)):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # downloader middleware can return requests (for example, redirects)</span>
<span class="gi">+        if isinstance(result, Request):</span>
<span class="gi">+            self.crawl(result)</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        d = self.scraper.enqueue_scrape(result, request, self.spider)</span>
<span class="gi">+        d.addErrback(</span>
<span class="gi">+            lambda f: logger.error(</span>
<span class="gi">+                &quot;Error while enqueuing downloader output&quot;,</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def spider_is_idle(self) -&gt; bool:</span>
<span class="gi">+        if self.slot is None:</span>
<span class="gi">+            raise RuntimeError(&quot;Engine slot not assigned&quot;)</span>
<span class="gi">+        if not self.scraper.slot.is_idle():  # type: ignore[union-attr]</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.downloader.active:  # downloader has pending requests</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.slot.start_requests is not None:  # not all start requests are handled</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.slot.scheduler.has_pending_requests():</span>
<span class="gi">+            return False</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def crawl(self, request: Request) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Inject the request into the spider &lt;-&gt; downloader pipeline&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.spider is None:</span>
<span class="gi">+            raise RuntimeError(f&quot;No open spider to crawl: {request}&quot;)</span>
<span class="gi">+        self._schedule_request(request, self.spider)</span>
<span class="gi">+        self.slot.nextcall.schedule()  # type: ignore[union-attr]</span>
<span class="gi">+</span>
<span class="gi">+    def _schedule_request(self, request: Request, spider: Spider) -&gt; None:</span>
<span class="gi">+        request_scheduled_result = self.signals.send_catch_log(</span>
<span class="gi">+            signals.request_scheduled,</span>
<span class="gi">+            request=request,</span>
<span class="gi">+            spider=spider,</span>
<span class="gi">+            dont_log=IgnoreRequest,</span>
<span class="gi">+        )</span>
<span class="gi">+        for handler, result in request_scheduled_result:</span>
<span class="gi">+            if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):</span>
<span class="gi">+                logger.debug(</span>
<span class="gi">+                    f&quot;Signal handler {global_object_name(handler)} dropped &quot;</span>
<span class="gi">+                    f&quot;request {request} before it reached the scheduler.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                return</span>
<span class="gi">+        if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]</span>
<span class="gi">+            self.signals.send_catch_log(</span>
<span class="gi">+                signals.request_dropped, request=request, spider=spider</span>
<span class="gi">+            )</span>

<span class="gd">-    def download(self, request: Request) -&gt;Deferred:</span>
<span class="gi">+    def download(self, request: Request) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return a Deferred which fires with a Response as result, only downloader middlewares are applied&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.spider is None:</span>
<span class="gi">+            raise RuntimeError(f&quot;No open spider to crawl: {request}&quot;)</span>
<span class="gi">+        return self._download(request).addBoth(self._downloaded, request)</span>
<span class="gi">+</span>
<span class="gi">+    def _downloaded(</span>
<span class="gi">+        self, result: Union[Response, Request, Failure], request: Request</span>
<span class="gi">+    ) -&gt; Union[Deferred, Response, Failure]:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        self.slot.remove_request(request)</span>
<span class="gi">+        return self.download(result) if isinstance(result, Request) else result</span>
<span class="gi">+</span>
<span class="gi">+    def _download(self, request: Request) -&gt; Deferred:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>

<span class="gd">-    def _spider_idle(self) -&gt;None:</span>
<span class="gi">+        self.slot.add_request(request)</span>
<span class="gi">+</span>
<span class="gi">+        def _on_success(result: Union[Response, Request]) -&gt; Union[Response, Request]:</span>
<span class="gi">+            if not isinstance(result, (Response, Request)):</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    f&quot;Incorrect type: expected Response or Request, got {type(result)}: {result!r}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            if isinstance(result, Response):</span>
<span class="gi">+                if result.request is None:</span>
<span class="gi">+                    result.request = request</span>
<span class="gi">+                assert self.spider is not None</span>
<span class="gi">+                logkws = self.logformatter.crawled(result.request, result, self.spider)</span>
<span class="gi">+                if logkws is not None:</span>
<span class="gi">+                    logger.log(</span>
<span class="gi">+                        *logformatter_adapter(logkws), extra={&quot;spider&quot;: self.spider}</span>
<span class="gi">+                    )</span>
<span class="gi">+                self.signals.send_catch_log(</span>
<span class="gi">+                    signal=signals.response_received,</span>
<span class="gi">+                    response=result,</span>
<span class="gi">+                    request=result.request,</span>
<span class="gi">+                    spider=self.spider,</span>
<span class="gi">+                )</span>
<span class="gi">+            return result</span>
<span class="gi">+</span>
<span class="gi">+        def _on_complete(_: Any) -&gt; Any:</span>
<span class="gi">+            assert self.slot is not None</span>
<span class="gi">+            self.slot.nextcall.schedule()</span>
<span class="gi">+            return _</span>
<span class="gi">+</span>
<span class="gi">+        assert self.spider is not None</span>
<span class="gi">+        dwld = self.downloader.fetch(request, self.spider)</span>
<span class="gi">+        dwld.addCallbacks(_on_success)</span>
<span class="gi">+        dwld.addBoth(_on_complete)</span>
<span class="gi">+        return dwld</span>
<span class="gi">+</span>
<span class="gi">+    @inlineCallbacks</span>
<span class="gi">+    def open_spider(</span>
<span class="gi">+        self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True</span>
<span class="gi">+    ) -&gt; Generator[Deferred, Any, None]:</span>
<span class="gi">+        if self.slot is not None:</span>
<span class="gi">+            raise RuntimeError(f&quot;No free spider slot when opening {spider.name!r}&quot;)</span>
<span class="gi">+        logger.info(&quot;Spider opened&quot;, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+        nextcall = CallLaterOnce(self._next_request)</span>
<span class="gi">+        scheduler = create_instance(</span>
<span class="gi">+            self.scheduler_cls, settings=None, crawler=self.crawler</span>
<span class="gi">+        )</span>
<span class="gi">+        start_requests = yield self.scraper.spidermw.process_start_requests(</span>
<span class="gi">+            start_requests, spider</span>
<span class="gi">+        )</span>
<span class="gi">+        self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)</span>
<span class="gi">+        self.spider = spider</span>
<span class="gi">+        if hasattr(scheduler, &quot;open&quot;):</span>
<span class="gi">+            yield scheduler.open(spider)</span>
<span class="gi">+        yield self.scraper.open_spider(spider)</span>
<span class="gi">+        assert self.crawler.stats</span>
<span class="gi">+        self.crawler.stats.open_spider(spider)</span>
<span class="gi">+        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)</span>
<span class="gi">+        self.slot.nextcall.schedule()</span>
<span class="gi">+        self.slot.heartbeat.start(5)</span>
<span class="gi">+</span>
<span class="gi">+    def _spider_idle(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.
<span class="w"> </span>        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider
<span class="w"> </span>        exception, the spider is not closed until the next loop and this function is guaranteed to be called
<span class="w"> </span>        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.spider is not None  # typing</span>
<span class="gi">+        expected_ex = (DontCloseSpider, CloseSpider)</span>
<span class="gi">+        res = self.signals.send_catch_log(</span>
<span class="gi">+            signals.spider_idle, spider=self.spider, dont_log=expected_ex</span>
<span class="gi">+        )</span>
<span class="gi">+        detected_ex = {</span>
<span class="gi">+            ex: x.value</span>
<span class="gi">+            for _, x in res</span>
<span class="gi">+            for ex in expected_ex</span>
<span class="gi">+            if isinstance(x, Failure) and isinstance(x.value, ex)</span>
<span class="gi">+        }</span>
<span class="gi">+        if DontCloseSpider in detected_ex:</span>
<span class="gi">+            return None</span>
<span class="gi">+        if self.spider_is_idle():</span>
<span class="gi">+            ex = detected_ex.get(CloseSpider, CloseSpider(reason=&quot;finished&quot;))</span>
<span class="gi">+            assert isinstance(ex, CloseSpider)  # typing</span>
<span class="gi">+            self.close_spider(self.spider, reason=ex.reason)</span>

<span class="gd">-    def close_spider(self, spider: Spider, reason: str=&#39;cancelled&#39;) -&gt;Deferred:</span>
<span class="gi">+    def close_spider(self, spider: Spider, reason: str = &quot;cancelled&quot;) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;Close (cancel) spider and clear all its outstanding requests&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.slot is None:</span>
<span class="gi">+            raise RuntimeError(&quot;Engine slot not assigned&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if self.slot.closing is not None:</span>
<span class="gi">+            return self.slot.closing</span>
<span class="gi">+</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Closing spider (%(reason)s)&quot;, {&quot;reason&quot;: reason}, extra={&quot;spider&quot;: spider}</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        dfd = self.slot.close()</span>
<span class="gi">+</span>
<span class="gi">+        def log_failure(msg: str) -&gt; Callable:</span>
<span class="gi">+            def errback(failure: Failure) -&gt; None:</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    msg, exc_info=failure_to_exc_info(failure), extra={&quot;spider&quot;: spider}</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            return errback</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(lambda _: self.downloader.close())</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Downloader close failure&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(lambda _: self.scraper.close_spider(spider))</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Scraper close failure&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        if hasattr(self.slot.scheduler, &quot;close&quot;):</span>
<span class="gi">+            dfd.addBoth(lambda _: cast(Slot, self.slot).scheduler.close(reason))</span>
<span class="gi">+            dfd.addErrback(log_failure(&quot;Scheduler close failure&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(</span>
<span class="gi">+            lambda _: self.signals.send_catch_log_deferred(</span>
<span class="gi">+                signal=signals.spider_closed,</span>
<span class="gi">+                spider=spider,</span>
<span class="gi">+                reason=reason,</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Error while sending spider_close signal&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        def close_stats(_: Any) -&gt; None:</span>
<span class="gi">+            assert self.crawler.stats</span>
<span class="gi">+            self.crawler.stats.close_spider(spider, reason=reason)</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(close_stats)</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Stats close failure&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(</span>
<span class="gi">+            lambda _: logger.info(</span>
<span class="gi">+                &quot;Spider closed (%(reason)s)&quot;,</span>
<span class="gi">+                {&quot;reason&quot;: reason},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(lambda _: setattr(self, &quot;slot&quot;, None))</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Error while unassigning slot&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(lambda _: setattr(self, &quot;spider&quot;, None))</span>
<span class="gi">+        dfd.addErrback(log_failure(&quot;Error while unassigning spider&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(lambda _: self._spider_closed_callback(spider))</span>
<span class="gi">+</span>
<span class="gi">+        return dfd</span>
<span class="gh">diff --git a/scrapy/core/http2/agent.py b/scrapy/core/http2/agent.py</span>
<span class="gh">index caf067cd1..215ea9716 100644</span>
<span class="gd">--- a/scrapy/core/http2/agent.py</span>
<span class="gi">+++ b/scrapy/core/http2/agent.py</span>
<span class="gu">@@ -1,12 +1,19 @@</span>
<span class="w"> </span>from collections import deque
<span class="w"> </span>from typing import Deque, Dict, List, Optional, Tuple
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer
<span class="w"> </span>from twisted.internet.base import ReactorBase
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="w"> </span>from twisted.internet.endpoints import HostnameEndpoint
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gd">-from twisted.web.client import URI, BrowserLikePolicyForHTTPS, ResponseFailed, _StandardEndpointFactory</span>
<span class="gi">+from twisted.web.client import (</span>
<span class="gi">+    URI,</span>
<span class="gi">+    BrowserLikePolicyForHTTPS,</span>
<span class="gi">+    ResponseFailed,</span>
<span class="gi">+    _StandardEndpointFactory,</span>
<span class="gi">+)</span>
<span class="w"> </span>from twisted.web.error import SchemeNotSupported
<span class="gi">+</span>
<span class="w"> </span>from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory
<span class="w"> </span>from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol
<span class="w"> </span>from scrapy.http.request import Request
<span class="gu">@@ -15,54 +22,148 @@ from scrapy.spiders import Spider</span>


<span class="w"> </span>class H2ConnectionPool:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, reactor: ReactorBase, settings: Settings) -&gt;None:</span>
<span class="gi">+    def __init__(self, reactor: ReactorBase, settings: Settings) -&gt; None:</span>
<span class="w"> </span>        self._reactor = reactor
<span class="w"> </span>        self.settings = settings
<span class="gi">+</span>
<span class="gi">+        # Store a dictionary which is used to get the respective</span>
<span class="gi">+        # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)</span>
<span class="w"> </span>        self._connections: Dict[Tuple, H2ClientProtocol] = {}
<span class="gi">+</span>
<span class="gi">+        # Save all requests that arrive before the connection is established</span>
<span class="w"> </span>        self._pending_requests: Dict[Tuple, Deque[Deferred]] = {}

<span class="gd">-    def close_connections(self) -&gt;None:</span>
<span class="gi">+    def get_connection(</span>
<span class="gi">+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        if key in self._pending_requests:</span>
<span class="gi">+            # Received a request while connecting to remote</span>
<span class="gi">+            # Create a deferred which will fire with the H2ClientProtocol</span>
<span class="gi">+            # instance</span>
<span class="gi">+            d: Deferred = Deferred()</span>
<span class="gi">+            self._pending_requests[key].append(d)</span>
<span class="gi">+            return d</span>
<span class="gi">+</span>
<span class="gi">+        # Check if we already have a connection to the remote</span>
<span class="gi">+        conn = self._connections.get(key, None)</span>
<span class="gi">+        if conn:</span>
<span class="gi">+            # Return this connection instance wrapped inside a deferred</span>
<span class="gi">+            return defer.succeed(conn)</span>
<span class="gi">+</span>
<span class="gi">+        # No connection is established for the given URI</span>
<span class="gi">+        return self._new_connection(key, uri, endpoint)</span>
<span class="gi">+</span>
<span class="gi">+    def _new_connection(</span>
<span class="gi">+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        self._pending_requests[key] = deque()</span>
<span class="gi">+</span>
<span class="gi">+        conn_lost_deferred: Deferred = Deferred()</span>
<span class="gi">+        conn_lost_deferred.addCallback(self._remove_connection, key)</span>
<span class="gi">+</span>
<span class="gi">+        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)</span>
<span class="gi">+        conn_d = endpoint.connect(factory)</span>
<span class="gi">+        conn_d.addCallback(self.put_connection, key)</span>
<span class="gi">+</span>
<span class="gi">+        d: Deferred = Deferred()</span>
<span class="gi">+        self._pending_requests[key].append(d)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def put_connection(self, conn: H2ClientProtocol, key: Tuple) -&gt; H2ClientProtocol:</span>
<span class="gi">+        self._connections[key] = conn</span>
<span class="gi">+</span>
<span class="gi">+        # Now as we have established a proper HTTP/2 connection</span>
<span class="gi">+        # we fire all the deferred&#39;s with the connection instance</span>
<span class="gi">+        pending_requests = self._pending_requests.pop(key, None)</span>
<span class="gi">+        while pending_requests:</span>
<span class="gi">+            d = pending_requests.popleft()</span>
<span class="gi">+            d.callback(conn)</span>
<span class="gi">+</span>
<span class="gi">+        return conn</span>
<span class="gi">+</span>
<span class="gi">+    def _remove_connection(self, errors: List[BaseException], key: Tuple) -&gt; None:</span>
<span class="gi">+        self._connections.pop(key)</span>
<span class="gi">+</span>
<span class="gi">+        # Call the errback of all the pending requests for this connection</span>
<span class="gi">+        pending_requests = self._pending_requests.pop(key, None)</span>
<span class="gi">+        while pending_requests:</span>
<span class="gi">+            d = pending_requests.popleft()</span>
<span class="gi">+            d.errback(ResponseFailed(errors))</span>
<span class="gi">+</span>
<span class="gi">+    def close_connections(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Close all the HTTP/2 connections and remove them from pool

<span class="w"> </span>        Returns:
<span class="w"> </span>            Deferred that fires when all connections have been closed
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for conn in self._connections.values():</span>
<span class="gi">+            assert conn.transport is not None  # typing</span>
<span class="gi">+            conn.transport.abortConnection()</span>


<span class="w"> </span>class H2Agent:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, reactor: ReactorBase, pool: H2ConnectionPool,</span>
<span class="gd">-        context_factory: BrowserLikePolicyForHTTPS=</span>
<span class="gd">-        BrowserLikePolicyForHTTPS(), connect_timeout: Optional[float]=None,</span>
<span class="gd">-        bind_address: Optional[bytes]=None) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        reactor: ReactorBase,</span>
<span class="gi">+        pool: H2ConnectionPool,</span>
<span class="gi">+        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),</span>
<span class="gi">+        connect_timeout: Optional[float] = None,</span>
<span class="gi">+        bind_address: Optional[bytes] = None,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self._reactor = reactor
<span class="w"> </span>        self._pool = pool
<span class="w"> </span>        self._context_factory = AcceptableProtocolsContextFactory(
<span class="gd">-            context_factory, acceptable_protocols=[b&#39;h2&#39;])</span>
<span class="gd">-        self.endpoint_factory = _StandardEndpointFactory(self._reactor,</span>
<span class="gd">-            self._context_factory, connect_timeout, bind_address)</span>
<span class="gi">+            context_factory, acceptable_protocols=[b&quot;h2&quot;]</span>
<span class="gi">+        )</span>
<span class="gi">+        self.endpoint_factory = _StandardEndpointFactory(</span>
<span class="gi">+            self._reactor, self._context_factory, connect_timeout, bind_address</span>
<span class="gi">+        )</span>

<span class="gd">-    def get_key(self, uri: URI) -&gt;Tuple:</span>
<span class="gi">+    def get_endpoint(self, uri: URI):</span>
<span class="gi">+        return self.endpoint_factory.endpointForURI(uri)</span>
<span class="gi">+</span>
<span class="gi">+    def get_key(self, uri: URI) -&gt; Tuple:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Arguments:
<span class="w"> </span>            uri - URI obtained directly from request URL
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return uri.scheme, uri.host, uri.port</span>

<span class="gi">+    def request(self, request: Request, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        uri = URI.fromBytes(bytes(request.url, encoding=&quot;utf-8&quot;))</span>
<span class="gi">+        try:</span>
<span class="gi">+            endpoint = self.get_endpoint(uri)</span>
<span class="gi">+        except SchemeNotSupported:</span>
<span class="gi">+            return defer.fail(Failure())</span>
<span class="gi">+</span>
<span class="gi">+        key = self.get_key(uri)</span>
<span class="gi">+        d = self._pool.get_connection(key, uri, endpoint)</span>
<span class="gi">+        d.addCallback(lambda conn: conn.request(request, spider))</span>
<span class="gi">+        return d</span>

<span class="gd">-class ScrapyProxyH2Agent(H2Agent):</span>

<span class="gd">-    def __init__(self, reactor: ReactorBase, proxy_uri: URI, pool:</span>
<span class="gd">-        H2ConnectionPool, context_factory: BrowserLikePolicyForHTTPS=</span>
<span class="gd">-        BrowserLikePolicyForHTTPS(), connect_timeout: Optional[float]=None,</span>
<span class="gd">-        bind_address: Optional[bytes]=None) -&gt;None:</span>
<span class="gd">-        super().__init__(reactor=reactor, pool=pool, context_factory=</span>
<span class="gd">-            context_factory, connect_timeout=connect_timeout, bind_address=</span>
<span class="gd">-            bind_address)</span>
<span class="gi">+class ScrapyProxyH2Agent(H2Agent):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        reactor: ReactorBase,</span>
<span class="gi">+        proxy_uri: URI,</span>
<span class="gi">+        pool: H2ConnectionPool,</span>
<span class="gi">+        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),</span>
<span class="gi">+        connect_timeout: Optional[float] = None,</span>
<span class="gi">+        bind_address: Optional[bytes] = None,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            reactor=reactor,</span>
<span class="gi">+            pool=pool,</span>
<span class="gi">+            context_factory=context_factory,</span>
<span class="gi">+            connect_timeout=connect_timeout,</span>
<span class="gi">+            bind_address=bind_address,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._proxy_uri = proxy_uri

<span class="gd">-    def get_key(self, uri: URI) -&gt;Tuple:</span>
<span class="gi">+    def get_endpoint(self, uri: URI):</span>
<span class="gi">+        return self.endpoint_factory.endpointForURI(self._proxy_uri)</span>
<span class="gi">+</span>
<span class="gi">+    def get_key(self, uri: URI) -&gt; Tuple:</span>
<span class="w"> </span>        &quot;&quot;&quot;We use the proxy uri instead of uri obtained from request url&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return &quot;http-proxy&quot;, self._proxy_uri.host, self._proxy_uri.port</span>
<span class="gh">diff --git a/scrapy/core/http2/protocol.py b/scrapy/core/http2/protocol.py</span>
<span class="gh">index 7f0ed1e12..bc8da50d7 100644</span>
<span class="gd">--- a/scrapy/core/http2/protocol.py</span>
<span class="gi">+++ b/scrapy/core/http2/protocol.py</span>
<span class="gu">@@ -4,10 +4,21 @@ import logging</span>
<span class="w"> </span>from collections import deque
<span class="w"> </span>from ipaddress import IPv4Address, IPv6Address
<span class="w"> </span>from typing import Dict, List, Optional, Union
<span class="gi">+</span>
<span class="w"> </span>from h2.config import H2Configuration
<span class="w"> </span>from h2.connection import H2Connection
<span class="w"> </span>from h2.errors import ErrorCodes
<span class="gd">-from h2.events import ConnectionTerminated, DataReceived, Event, ResponseReceived, SettingsAcknowledged, StreamEnded, StreamReset, UnknownFrameReceived, WindowUpdated</span>
<span class="gi">+from h2.events import (</span>
<span class="gi">+    ConnectionTerminated,</span>
<span class="gi">+    DataReceived,</span>
<span class="gi">+    Event,</span>
<span class="gi">+    ResponseReceived,</span>
<span class="gi">+    SettingsAcknowledged,</span>
<span class="gi">+    StreamEnded,</span>
<span class="gi">+    StreamReset,</span>
<span class="gi">+    UnknownFrameReceived,</span>
<span class="gi">+    WindowUpdated,</span>
<span class="gi">+)</span>
<span class="w"> </span>from h2.exceptions import FrameTooLargeError, H2Error
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="w"> </span>from twisted.internet.error import TimeoutError
<span class="gu">@@ -18,54 +29,56 @@ from twisted.protocols.policies import TimeoutMixin</span>
<span class="w"> </span>from twisted.python.failure import Failure
<span class="w"> </span>from twisted.web.client import URI
<span class="w"> </span>from zope.interface import implementer
<span class="gi">+</span>
<span class="w"> </span>from scrapy.core.http2.stream import Stream, StreamCloseReason
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.spiders import Spider
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gd">-PROTOCOL_NAME = b&#39;h2&#39;</span>


<span class="gd">-class InvalidNegotiatedProtocol(H2Error):</span>
<span class="gi">+PROTOCOL_NAME = b&quot;h2&quot;</span>
<span class="gi">+</span>

<span class="gd">-    def __init__(self, negotiated_protocol: bytes) -&gt;None:</span>
<span class="gi">+class InvalidNegotiatedProtocol(H2Error):</span>
<span class="gi">+    def __init__(self, negotiated_protocol: bytes) -&gt; None:</span>
<span class="w"> </span>        self.negotiated_protocol = negotiated_protocol

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gd">-        return (</span>
<span class="gd">-            f&#39;Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}&quot;</span>


<span class="w"> </span>class RemoteTerminatedConnection(H2Error):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address,</span>
<span class="gd">-        IPv6Address]], event: ConnectionTerminated) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],</span>
<span class="gi">+        event: ConnectionTerminated,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.remote_ip_address = remote_ip_address
<span class="w"> </span>        self.terminate_event = event

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gd">-        return f&#39;Received GOAWAY frame from {self.remote_ip_address!r}&#39;</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;Received GOAWAY frame from {self.remote_ip_address!r}&quot;</span>


<span class="w"> </span>class MethodNotAllowed405(H2Error):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address,</span>
<span class="gd">-        IPv6Address]]) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.remote_ip_address = remote_ip_address

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gd">-        return (</span>
<span class="gd">-            f&quot;Received &#39;HTTP/2.0 405 Method Not Allowed&#39; from {self.remote_ip_address!r}&quot;</span>
<span class="gd">-            )</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;Received &#39;HTTP/2.0 405 Method Not Allowed&#39; from {self.remote_ip_address!r}&quot;</span>


<span class="w"> </span>@implementer(IHandshakeListener)
<span class="w"> </span>class H2ClientProtocol(Protocol, TimeoutMixin):
<span class="w"> </span>    IDLE_TIMEOUT = 240

<span class="gd">-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred:</span>
<span class="gd">-        Deferred) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Arguments:
<span class="w"> </span>            uri -- URI of the base url to which HTTP/2 Connection will be made.
<span class="gu">@@ -76,108 +89,350 @@ class H2ClientProtocol(Protocol, TimeoutMixin):</span>
<span class="w"> </span>                that connection was lost
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        self._conn_lost_deferred = conn_lost_deferred
<span class="gd">-        config = H2Configuration(client_side=True, header_encoding=&#39;utf-8&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        config = H2Configuration(client_side=True, header_encoding=&quot;utf-8&quot;)</span>
<span class="w"> </span>        self.conn = H2Connection(config=config)
<span class="gi">+</span>
<span class="gi">+        # ID of the next request stream</span>
<span class="gi">+        # Following the convention - &#39;Streams initiated by a client MUST</span>
<span class="gi">+        # use odd-numbered stream identifiers&#39; (RFC 7540 - Section 5.1.1)</span>
<span class="w"> </span>        self._stream_id_generator = itertools.count(start=1, step=2)
<span class="gi">+</span>
<span class="gi">+        # Streams are stored in a dictionary keyed off their stream IDs</span>
<span class="w"> </span>        self.streams: Dict[int, Stream] = {}
<span class="gi">+</span>
<span class="gi">+        # If requests are received before connection is made we keep</span>
<span class="gi">+        # all requests in a pool and send them as the connection is made</span>
<span class="w"> </span>        self._pending_request_stream_pool: deque = deque()
<span class="gi">+</span>
<span class="gi">+        # Save an instance of errors raised which lead to losing the connection</span>
<span class="gi">+        # We pass these instances to the streams ResponseFailed() failure</span>
<span class="w"> </span>        self._conn_lost_errors: List[BaseException] = []
<span class="gd">-        self.metadata: Dict = {&#39;certificate&#39;: None, &#39;ip_address&#39;: None,</span>
<span class="gd">-            &#39;uri&#39;: uri, &#39;default_download_maxsize&#39;: settings.getint(</span>
<span class="gd">-            &#39;DOWNLOAD_MAXSIZE&#39;), &#39;default_download_warnsize&#39;: settings.</span>
<span class="gd">-            getint(&#39;DOWNLOAD_WARNSIZE&#39;), &#39;active_streams&#39;: 0,</span>
<span class="gd">-            &#39;settings_acknowledged&#39;: False}</span>
<span class="gi">+</span>
<span class="gi">+        # Some meta data of this connection</span>
<span class="gi">+        # initialized when connection is successfully made</span>
<span class="gi">+        self.metadata: Dict = {</span>
<span class="gi">+            # Peer certificate instance</span>
<span class="gi">+            &quot;certificate&quot;: None,</span>
<span class="gi">+            # Address of the server we are connected to which</span>
<span class="gi">+            # is updated when HTTP/2 connection is  made successfully</span>
<span class="gi">+            &quot;ip_address&quot;: None,</span>
<span class="gi">+            # URI of the peer HTTP/2 connection is made</span>
<span class="gi">+            &quot;uri&quot;: uri,</span>
<span class="gi">+            # Both ip_address and uri are used by the Stream before</span>
<span class="gi">+            # initiating the request to verify that the base address</span>
<span class="gi">+            # Variables taken from Project Settings</span>
<span class="gi">+            &quot;default_download_maxsize&quot;: settings.getint(&quot;DOWNLOAD_MAXSIZE&quot;),</span>
<span class="gi">+            &quot;default_download_warnsize&quot;: settings.getint(&quot;DOWNLOAD_WARNSIZE&quot;),</span>
<span class="gi">+            # Counter to keep track of opened streams. This counter</span>
<span class="gi">+            # is used to make sure that not more than MAX_CONCURRENT_STREAMS</span>
<span class="gi">+            # streams are opened which leads to ProtocolError</span>
<span class="gi">+            # We use simple FIFO policy to handle pending requests</span>
<span class="gi">+            &quot;active_streams&quot;: 0,</span>
<span class="gi">+            # Flag to keep track if settings were acknowledged by the remote</span>
<span class="gi">+            # This ensures that we have established a HTTP/2 connection</span>
<span class="gi">+            &quot;settings_acknowledged&quot;: False,</span>
<span class="gi">+        }</span>

<span class="w"> </span>    @property
<span class="gd">-    def h2_connected(self) -&gt;bool:</span>
<span class="gi">+    def h2_connected(self) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;Boolean to keep track of the connection status.
<span class="w"> </span>        This is used while initiating pending streams to make sure
<span class="w"> </span>        that we initiate stream only during active HTTP/2 Connection
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        return bool(self.transport.connected) and self.metadata[&quot;settings_acknowledged&quot;]</span>

<span class="w"> </span>    @property
<span class="gd">-    def allowed_max_concurrent_streams(self) -&gt;int:</span>
<span class="gi">+    def allowed_max_concurrent_streams(self) -&gt; int:</span>
<span class="w"> </span>        &quot;&quot;&quot;We keep total two streams for client (sending data) and
<span class="w"> </span>        server side (receiving data) for a single request. To be safe
<span class="w"> </span>        we choose the minimum. Since this value can change in event
<span class="w"> </span>        RemoteSettingsChanged we make variable a property.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return min(</span>
<span class="gi">+            self.conn.local_settings.max_concurrent_streams,</span>
<span class="gi">+            self.conn.remote_settings.max_concurrent_streams,</span>
<span class="gi">+        )</span>

<span class="gd">-    def _send_pending_requests(self) -&gt;None:</span>
<span class="gi">+    def _send_pending_requests(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Initiate all pending requests from the deque following FIFO
<span class="w"> </span>        We make sure that at any time {allowed_max_concurrent_streams}
<span class="w"> </span>        streams are active.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def pop_stream(self, stream_id: int) -&gt;Stream:</span>
<span class="gi">+        while (</span>
<span class="gi">+            self._pending_request_stream_pool</span>
<span class="gi">+            and self.metadata[&quot;active_streams&quot;] &lt; self.allowed_max_concurrent_streams</span>
<span class="gi">+            and self.h2_connected</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.metadata[&quot;active_streams&quot;] += 1</span>
<span class="gi">+            stream = self._pending_request_stream_pool.popleft()</span>
<span class="gi">+            stream.initiate_request()</span>
<span class="gi">+            self._write_to_transport()</span>
<span class="gi">+</span>
<span class="gi">+    def pop_stream(self, stream_id: int) -&gt; Stream:</span>
<span class="w"> </span>        &quot;&quot;&quot;Perform cleanup when a stream is closed&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        stream = self.streams.pop(stream_id)</span>
<span class="gi">+        self.metadata[&quot;active_streams&quot;] -= 1</span>
<span class="gi">+        self._send_pending_requests()</span>
<span class="gi">+        return stream</span>

<span class="gd">-    def _new_stream(self, request: Request, spider: Spider) -&gt;Stream:</span>
<span class="gi">+    def _new_stream(self, request: Request, spider: Spider) -&gt; Stream:</span>
<span class="w"> </span>        &quot;&quot;&quot;Instantiates a new Stream object&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def _write_to_transport(self) -&gt;None:</span>
<span class="gi">+        stream = Stream(</span>
<span class="gi">+            stream_id=next(self._stream_id_generator),</span>
<span class="gi">+            request=request,</span>
<span class="gi">+            protocol=self,</span>
<span class="gi">+            download_maxsize=getattr(</span>
<span class="gi">+                spider, &quot;download_maxsize&quot;, self.metadata[&quot;default_download_maxsize&quot;]</span>
<span class="gi">+            ),</span>
<span class="gi">+            download_warnsize=getattr(</span>
<span class="gi">+                spider, &quot;download_warnsize&quot;, self.metadata[&quot;default_download_warnsize&quot;]</span>
<span class="gi">+            ),</span>
<span class="gi">+        )</span>
<span class="gi">+        self.streams[stream.stream_id] = stream</span>
<span class="gi">+        return stream</span>
<span class="gi">+</span>
<span class="gi">+    def _write_to_transport(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Write data to the underlying transport connection
<span class="w"> </span>        from the HTTP2 connection instance if any
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        # Reset the idle timeout as connection is still actively sending data</span>
<span class="gi">+        self.resetTimeout()</span>

<span class="gd">-    def connectionMade(self) -&gt;None:</span>
<span class="gi">+        data = self.conn.data_to_send()</span>
<span class="gi">+        self.transport.write(data)</span>
<span class="gi">+</span>
<span class="gi">+    def request(self, request: Request, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        if not isinstance(request, Request):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;Expected scrapy.http.Request, received {request.__class__.__qualname__}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        stream = self._new_stream(request, spider)</span>
<span class="gi">+        d = stream.get_response()</span>
<span class="gi">+</span>
<span class="gi">+        # Add the stream to the request pool</span>
<span class="gi">+        self._pending_request_stream_pool.append(stream)</span>
<span class="gi">+</span>
<span class="gi">+        # If we receive a request when connection is idle</span>
<span class="gi">+        # We need to initiate pending requests</span>
<span class="gi">+        self._send_pending_requests()</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def connectionMade(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Called by Twisted when the connection is established. We can start
<span class="w"> </span>        sending some data now: we should open with the connection preamble.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Initialize the timeout</span>
<span class="gi">+        self.setTimeout(self.IDLE_TIMEOUT)</span>
<span class="gi">+</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        destination = self.transport.getPeer()</span>
<span class="gi">+        self.metadata[&quot;ip_address&quot;] = ipaddress.ip_address(destination.host)</span>

<span class="gd">-    def _lose_connection_with_error(self, errors: List[BaseException]) -&gt;None:</span>
<span class="gi">+        # Initiate H2 Connection</span>
<span class="gi">+        self.conn.initiate_connection()</span>
<span class="gi">+        self._write_to_transport()</span>
<span class="gi">+</span>
<span class="gi">+    def _lose_connection_with_error(self, errors: List[BaseException]) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Helper function to lose the connection with the error sent as a
<span class="w"> </span>        reason&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._conn_lost_errors += errors</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        self.transport.loseConnection()</span>

<span class="gd">-    def handshakeCompleted(self) -&gt;None:</span>
<span class="gi">+    def handshakeCompleted(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Close the connection if it&#39;s not made via the expected protocol
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.transport.negotiatedProtocol is not None</span>
<span class="gi">+            and self.transport.negotiatedProtocol != PROTOCOL_NAME</span>
<span class="gi">+        ):</span>
<span class="gi">+            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer</span>
<span class="gi">+            self._lose_connection_with_error(</span>
<span class="gi">+                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]</span>
<span class="gi">+            )</span>

<span class="gd">-    def _check_received_data(self, data: bytes) -&gt;None:</span>
<span class="gi">+    def _check_received_data(self, data: bytes) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Checks for edge cases where the connection to remote fails
<span class="w"> </span>        without raising an appropriate H2Error

<span class="w"> </span>        Arguments:
<span class="w"> </span>            data -- Data received from the remote
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def timeoutConnection(self) -&gt;None:</span>
<span class="gi">+        if data.startswith(b&quot;HTTP/2.0 405 Method Not Allowed&quot;):</span>
<span class="gi">+            raise MethodNotAllowed405(self.metadata[&quot;ip_address&quot;])</span>
<span class="gi">+</span>
<span class="gi">+    def dataReceived(self, data: bytes) -&gt; None:</span>
<span class="gi">+        # Reset the idle timeout as connection is still actively receiving data</span>
<span class="gi">+        self.resetTimeout()</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            self._check_received_data(data)</span>
<span class="gi">+            events = self.conn.receive_data(data)</span>
<span class="gi">+            self._handle_events(events)</span>
<span class="gi">+        except H2Error as e:</span>
<span class="gi">+            if isinstance(e, FrameTooLargeError):</span>
<span class="gi">+                # hyper-h2 does not drop the connection in this scenario, we</span>
<span class="gi">+                # need to abort the connection manually.</span>
<span class="gi">+                self._conn_lost_errors += [e]</span>
<span class="gi">+                assert self.transport is not None  # typing</span>
<span class="gi">+                self.transport.abortConnection()</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            # Save this error as ultimately the connection will be dropped</span>
<span class="gi">+            # internally by hyper-h2. Saved error will be passed to all the streams</span>
<span class="gi">+            # closed with the connection.</span>
<span class="gi">+            self._lose_connection_with_error([e])</span>
<span class="gi">+        finally:</span>
<span class="gi">+            self._write_to_transport()</span>
<span class="gi">+</span>
<span class="gi">+    def timeoutConnection(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Called when the connection times out.
<span class="w"> </span>        We lose the connection with TimeoutError&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="gd">-    def connectionLost(self, reason: Failure=connectionDone) -&gt;None:</span>
<span class="gi">+        # Check whether there are open streams. If there are, we&#39;re going to</span>
<span class="gi">+        # want to use the error code PROTOCOL_ERROR. If there aren&#39;t, use</span>
<span class="gi">+        # NO_ERROR.</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.conn.open_outbound_streams &gt; 0</span>
<span class="gi">+            or self.conn.open_inbound_streams &gt; 0</span>
<span class="gi">+            or self.metadata[&quot;active_streams&quot;] &gt; 0</span>
<span class="gi">+        ):</span>
<span class="gi">+            error_code = ErrorCodes.PROTOCOL_ERROR</span>
<span class="gi">+        else:</span>
<span class="gi">+            error_code = ErrorCodes.NO_ERROR</span>
<span class="gi">+        self.conn.close_connection(error_code=error_code)</span>
<span class="gi">+        self._write_to_transport()</span>
<span class="gi">+</span>
<span class="gi">+        self._lose_connection_with_error(</span>
<span class="gi">+            [TimeoutError(f&quot;Connection was IDLE for more than {self.IDLE_TIMEOUT}s&quot;)]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def connectionLost(self, reason: Failure = connectionDone) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Called by Twisted when the transport connection is lost.
<span class="w"> </span>        No need to write anything to transport here.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Cancel the timeout if not done yet</span>
<span class="gi">+        self.setTimeout(None)</span>
<span class="gi">+</span>
<span class="gi">+        # Notify the connection pool instance such that no new requests are</span>
<span class="gi">+        # sent over current connection</span>
<span class="gi">+        if not reason.check(connectionDone):</span>
<span class="gi">+            self._conn_lost_errors.append(reason)</span>
<span class="gi">+</span>
<span class="gi">+        self._conn_lost_deferred.callback(self._conn_lost_errors)</span>
<span class="gi">+</span>
<span class="gi">+        for stream in self.streams.values():</span>
<span class="gi">+            if stream.metadata[&quot;request_sent&quot;]:</span>
<span class="gi">+                close_reason = StreamCloseReason.CONNECTION_LOST</span>
<span class="gi">+            else:</span>
<span class="gi">+                close_reason = StreamCloseReason.INACTIVE</span>
<span class="gi">+            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)</span>

<span class="gd">-    def _handle_events(self, events: List[Event]) -&gt;None:</span>
<span class="gi">+        self.metadata[&quot;active_streams&quot;] -= len(self.streams)</span>
<span class="gi">+        self.streams.clear()</span>
<span class="gi">+        self._pending_request_stream_pool.clear()</span>
<span class="gi">+        self.conn.close_connection()</span>
<span class="gi">+</span>
<span class="gi">+    def _handle_events(self, events: List[Event]) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Private method which acts as a bridge between the events
<span class="w"> </span>        received from the HTTP/2 data and IH2EventsHandler

<span class="w"> </span>        Arguments:
<span class="w"> </span>            events -- A list of events that the remote peer triggered by sending data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for event in events:</span>
<span class="gi">+            if isinstance(event, ConnectionTerminated):</span>
<span class="gi">+                self.connection_terminated(event)</span>
<span class="gi">+            elif isinstance(event, DataReceived):</span>
<span class="gi">+                self.data_received(event)</span>
<span class="gi">+            elif isinstance(event, ResponseReceived):</span>
<span class="gi">+                self.response_received(event)</span>
<span class="gi">+            elif isinstance(event, StreamEnded):</span>
<span class="gi">+                self.stream_ended(event)</span>
<span class="gi">+            elif isinstance(event, StreamReset):</span>
<span class="gi">+                self.stream_reset(event)</span>
<span class="gi">+            elif isinstance(event, WindowUpdated):</span>
<span class="gi">+                self.window_updated(event)</span>
<span class="gi">+            elif isinstance(event, SettingsAcknowledged):</span>
<span class="gi">+                self.settings_acknowledged(event)</span>
<span class="gi">+            elif isinstance(event, UnknownFrameReceived):</span>
<span class="gi">+                logger.warning(&quot;Unknown frame received: %s&quot;, event.frame)</span>
<span class="gi">+</span>
<span class="gi">+    # Event handler functions starts here</span>
<span class="gi">+    def connection_terminated(self, event: ConnectionTerminated) -&gt; None:</span>
<span class="gi">+        self._lose_connection_with_error(</span>
<span class="gi">+            [RemoteTerminatedConnection(self.metadata[&quot;ip_address&quot;], event)]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def data_received(self, event: DataReceived) -&gt; None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            stream = self.streams[event.stream_id]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass  # We ignore server-initiated events</span>
<span class="gi">+        else:</span>
<span class="gi">+            stream.receive_data(event.data, event.flow_controlled_length)</span>
<span class="gi">+</span>
<span class="gi">+    def response_received(self, event: ResponseReceived) -&gt; None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            stream = self.streams[event.stream_id]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass  # We ignore server-initiated events</span>
<span class="gi">+        else:</span>
<span class="gi">+            stream.receive_headers(event.headers)</span>
<span class="gi">+</span>
<span class="gi">+    def settings_acknowledged(self, event: SettingsAcknowledged) -&gt; None:</span>
<span class="gi">+        self.metadata[&quot;settings_acknowledged&quot;] = True</span>
<span class="gi">+</span>
<span class="gi">+        # Send off all the pending requests as now we have</span>
<span class="gi">+        # established a proper HTTP/2 connection</span>
<span class="gi">+        self._send_pending_requests()</span>
<span class="gi">+</span>
<span class="gi">+        # Update certificate when our HTTP/2 connection is established</span>
<span class="gi">+        assert self.transport is not None  # typing</span>
<span class="gi">+        self.metadata[&quot;certificate&quot;] = Certificate(self.transport.getPeerCertificate())</span>
<span class="gi">+</span>
<span class="gi">+    def stream_ended(self, event: StreamEnded) -&gt; None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            stream = self.pop_stream(event.stream_id)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass  # We ignore server-initiated events</span>
<span class="gi">+        else:</span>
<span class="gi">+            stream.close(StreamCloseReason.ENDED, from_protocol=True)</span>
<span class="gi">+</span>
<span class="gi">+    def stream_reset(self, event: StreamReset) -&gt; None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            stream = self.pop_stream(event.stream_id)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass  # We ignore server-initiated events</span>
<span class="gi">+        else:</span>
<span class="gi">+            stream.close(StreamCloseReason.RESET, from_protocol=True)</span>
<span class="gi">+</span>
<span class="gi">+    def window_updated(self, event: WindowUpdated) -&gt; None:</span>
<span class="gi">+        if event.stream_id != 0:</span>
<span class="gi">+            self.streams[event.stream_id].receive_window_update()</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Send leftover data for all the streams</span>
<span class="gi">+            for stream in self.streams.values():</span>
<span class="gi">+                stream.receive_window_update()</span>


<span class="w"> </span>@implementer(IProtocolNegotiationFactory)
<span class="w"> </span>class H2ClientFactory(Factory):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred:</span>
<span class="gd">-        Deferred) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.uri = uri
<span class="w"> </span>        self.settings = settings
<span class="w"> </span>        self.conn_lost_deferred = conn_lost_deferred
<span class="gi">+</span>
<span class="gi">+    def buildProtocol(self, addr) -&gt; H2ClientProtocol:</span>
<span class="gi">+        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)</span>
<span class="gi">+</span>
<span class="gi">+    def acceptableProtocols(self) -&gt; List[bytes]:</span>
<span class="gi">+        return [PROTOCOL_NAME]</span>
<span class="gh">diff --git a/scrapy/core/http2/stream.py b/scrapy/core/http2/stream.py</span>
<span class="gh">index dcbe8e224..6c6ed6f9b 100644</span>
<span class="gd">--- a/scrapy/core/http2/stream.py</span>
<span class="gi">+++ b/scrapy/core/http2/stream.py</span>
<span class="gu">@@ -3,6 +3,7 @@ from enum import Enum</span>
<span class="w"> </span>from io import BytesIO
<span class="w"> </span>from typing import TYPE_CHECKING, Dict, List, Optional, Tuple
<span class="w"> </span>from urllib.parse import urlparse
<span class="gi">+</span>
<span class="w"> </span>from h2.errors import ErrorCodes
<span class="w"> </span>from h2.exceptions import H2Error, ProtocolError, StreamClosedError
<span class="w"> </span>from hpack import HeaderTuple
<span class="gu">@@ -10,11 +11,15 @@ from twisted.internet.defer import CancelledError, Deferred</span>
<span class="w"> </span>from twisted.internet.error import ConnectionClosed
<span class="w"> </span>from twisted.python.failure import Failure
<span class="w"> </span>from twisted.web.client import ResponseFailed
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.http.headers import Headers
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.core.http2.protocol import H2ClientProtocol
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -23,36 +28,47 @@ class InactiveStreamClosed(ConnectionClosed):</span>
<span class="w"> </span>    of the stream. This happens when a stream is waiting for other
<span class="w"> </span>    streams to close and connection is lost.&quot;&quot;&quot;

<span class="gd">-    def __init__(self, request: Request) -&gt;None:</span>
<span class="gi">+    def __init__(self, request: Request) -&gt; None:</span>
<span class="w"> </span>        self.request = request

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gd">-        return (</span>
<span class="gd">-            f&#39;InactiveStreamClosed: Connection was closed without sending the request {self.request!r}&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;InactiveStreamClosed: Connection was closed without sending the request {self.request!r}&quot;</span>


<span class="w"> </span>class InvalidHostname(H2Error):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, request: Request, expected_hostname: str,</span>
<span class="gd">-        expected_netloc: str) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, request: Request, expected_hostname: str, expected_netloc: str</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.request = request
<span class="w"> </span>        self.expected_hostname = expected_hostname
<span class="w"> </span>        self.expected_netloc = expected_netloc

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gd">-        return (</span>
<span class="gd">-            f&#39;InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}&quot;</span>


<span class="w"> </span>class StreamCloseReason(Enum):
<span class="gi">+    # Received a StreamEnded event from the remote</span>
<span class="w"> </span>    ENDED = 1
<span class="gi">+</span>
<span class="gi">+    # Received a StreamReset event -- ended abruptly</span>
<span class="w"> </span>    RESET = 2
<span class="gi">+</span>
<span class="gi">+    # Transport connection was lost</span>
<span class="w"> </span>    CONNECTION_LOST = 3
<span class="gi">+</span>
<span class="gi">+    # Expected response body size is more than allowed limit</span>
<span class="w"> </span>    MAXSIZE_EXCEEDED = 4
<span class="gi">+</span>
<span class="gi">+    # Response deferred is cancelled by the client</span>
<span class="gi">+    # (happens when client called response_deferred.cancel())</span>
<span class="w"> </span>    CANCELLED = 5
<span class="gi">+</span>
<span class="gi">+    # Connection lost and the stream was not initiated</span>
<span class="w"> </span>    INACTIVE = 6
<span class="gi">+</span>
<span class="gi">+    # The hostname of the request is not same as of connected peer hostname</span>
<span class="gi">+    # As a result sending this request will the end the connection</span>
<span class="w"> </span>    INVALID_HOSTNAME = 7


<span class="gu">@@ -67,9 +83,14 @@ class Stream:</span>
<span class="w"> </span>    1. Combine all the data frames
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, stream_id: int, request: Request, protocol:</span>
<span class="gd">-        &#39;H2ClientProtocol&#39;, download_maxsize: int=0, download_warnsize: int=0</span>
<span class="gd">-        ) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        stream_id: int,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        protocol: &quot;H2ClientProtocol&quot;,</span>
<span class="gi">+        download_maxsize: int = 0,</span>
<span class="gi">+        download_warnsize: int = 0,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Arguments:
<span class="w"> </span>            stream_id -- Unique identifier for the stream within a single HTTP/2 connection
<span class="gu">@@ -78,31 +99,65 @@ class Stream:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        self.stream_id: int = stream_id
<span class="w"> </span>        self._request: Request = request
<span class="gd">-        self._protocol: &#39;H2ClientProtocol&#39; = protocol</span>
<span class="gd">-        self._download_maxsize = self._request.meta.get(&#39;download_maxsize&#39;,</span>
<span class="gd">-            download_maxsize)</span>
<span class="gd">-        self._download_warnsize = self._request.meta.get(&#39;download_warnsize&#39;,</span>
<span class="gd">-            download_warnsize)</span>
<span class="gd">-        self.metadata: Dict = {&#39;request_content_length&#39;: 0 if self._request</span>
<span class="gd">-            .body is None else len(self._request.body), &#39;request_sent&#39;: </span>
<span class="gd">-            False, &#39;reached_warnsize&#39;: False, &#39;remaining_content_length&#39;: 0 if</span>
<span class="gd">-            self._request.body is None else len(self._request.body),</span>
<span class="gd">-            &#39;stream_closed_local&#39;: False, &#39;stream_closed_server&#39;: False}</span>
<span class="gd">-        self._response: Dict = {&#39;body&#39;: BytesIO(), &#39;flow_controlled_size&#39;: </span>
<span class="gd">-            0, &#39;headers&#39;: Headers({})}</span>
<span class="gd">-</span>
<span class="gd">-        def _cancel(_) -&gt;None:</span>
<span class="gd">-            if self.metadata[&#39;request_sent&#39;]:</span>
<span class="gi">+        self._protocol: &quot;H2ClientProtocol&quot; = protocol</span>
<span class="gi">+</span>
<span class="gi">+        self._download_maxsize = self._request.meta.get(</span>
<span class="gi">+            &quot;download_maxsize&quot;, download_maxsize</span>
<span class="gi">+        )</span>
<span class="gi">+        self._download_warnsize = self._request.meta.get(</span>
<span class="gi">+            &quot;download_warnsize&quot;, download_warnsize</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Metadata of an HTTP/2 connection stream</span>
<span class="gi">+        # initialized when stream is instantiated</span>
<span class="gi">+        self.metadata: Dict = {</span>
<span class="gi">+            &quot;request_content_length&quot;: 0</span>
<span class="gi">+            if self._request.body is None</span>
<span class="gi">+            else len(self._request.body),</span>
<span class="gi">+            # Flag to keep track whether the stream has initiated the request</span>
<span class="gi">+            &quot;request_sent&quot;: False,</span>
<span class="gi">+            # Flag to track whether we have logged about exceeding download warnsize</span>
<span class="gi">+            &quot;reached_warnsize&quot;: False,</span>
<span class="gi">+            # Each time we send a data frame, we will decrease value by the amount send.</span>
<span class="gi">+            &quot;remaining_content_length&quot;: 0</span>
<span class="gi">+            if self._request.body is None</span>
<span class="gi">+            else len(self._request.body),</span>
<span class="gi">+            # Flag to keep track whether client (self) have closed this stream</span>
<span class="gi">+            &quot;stream_closed_local&quot;: False,</span>
<span class="gi">+            # Flag to keep track whether the server has closed the stream</span>
<span class="gi">+            &quot;stream_closed_server&quot;: False,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+        # Private variable used to build the response</span>
<span class="gi">+        # this response is then converted to appropriate Response class</span>
<span class="gi">+        # passed to the response deferred callback</span>
<span class="gi">+        self._response: Dict = {</span>
<span class="gi">+            # Data received frame by frame from the server is appended</span>
<span class="gi">+            # and passed to the response Deferred when completely received.</span>
<span class="gi">+            &quot;body&quot;: BytesIO(),</span>
<span class="gi">+            # The amount of data received that counts against the</span>
<span class="gi">+            # flow control window</span>
<span class="gi">+            &quot;flow_controlled_size&quot;: 0,</span>
<span class="gi">+            # Headers received after sending the request</span>
<span class="gi">+            &quot;headers&quot;: Headers({}),</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+        def _cancel(_) -&gt; None:</span>
<span class="gi">+            # Close this stream as gracefully as possible</span>
<span class="gi">+            # If the associated request is initiated we reset this stream</span>
<span class="gi">+            # else we directly call close() method</span>
<span class="gi">+            if self.metadata[&quot;request_sent&quot;]:</span>
<span class="w"> </span>                self.reset_stream(StreamCloseReason.CANCELLED)
<span class="w"> </span>            else:
<span class="w"> </span>                self.close(StreamCloseReason.CANCELLED)
<span class="gi">+</span>
<span class="w"> </span>        self._deferred_response: Deferred = Deferred(_cancel)

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gd">-        return f&#39;Stream(id={self.stream_id!r})&#39;</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;Stream(id={self.stream_id!r})&quot;</span>

<span class="w"> </span>    @property
<span class="gd">-    def _log_warnsize(self) -&gt;bool:</span>
<span class="gi">+    def _log_warnsize(self) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;Checks if we have received data which exceeds the download warnsize
<span class="w"> </span>        and whether we have not already logged about it.

<span class="gu">@@ -110,15 +165,97 @@ class Stream:</span>
<span class="w"> </span>            True if both the above conditions hold true
<span class="w"> </span>            False if any of the conditions is false
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        content_length_header = int(</span>
<span class="gi">+            self._response[&quot;headers&quot;].get(b&quot;Content-Length&quot;, -1)</span>
<span class="gi">+        )</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._download_warnsize</span>
<span class="gi">+            and (</span>
<span class="gi">+                self._response[&quot;flow_controlled_size&quot;] &gt; self._download_warnsize</span>
<span class="gi">+                or content_length_header &gt; self._download_warnsize</span>
<span class="gi">+            )</span>
<span class="gi">+            and not self.metadata[&quot;reached_warnsize&quot;]</span>
<span class="gi">+        )</span>

<span class="gd">-    def get_response(self) -&gt;Deferred:</span>
<span class="gi">+    def get_response(self) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;Simply return a Deferred which fires when response
<span class="w"> </span>        from the asynchronous request is available
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._deferred_response</span>
<span class="gi">+</span>
<span class="gi">+    def check_request_url(self) -&gt; bool:</span>
<span class="gi">+        # Make sure that we are sending the request to the correct URL</span>
<span class="gi">+        url = urlparse(self._request.url)</span>
<span class="gi">+        return (</span>
<span class="gi">+            url.netloc == str(self._protocol.metadata[&quot;uri&quot;].host, &quot;utf-8&quot;)</span>
<span class="gi">+            or url.netloc == str(self._protocol.metadata[&quot;uri&quot;].netloc, &quot;utf-8&quot;)</span>
<span class="gi">+            or url.netloc</span>
<span class="gi">+            == f&#39;{self._protocol.metadata[&quot;ip_address&quot;]}:{self._protocol.metadata[&quot;uri&quot;].port}&#39;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _get_request_headers(self) -&gt; List[Tuple[str, str]]:</span>
<span class="gi">+        url = urlparse(self._request.url)</span>
<span class="gi">+</span>
<span class="gi">+        path = url.path</span>
<span class="gi">+        if url.query:</span>
<span class="gi">+            path += &quot;?&quot; + url.query</span>
<span class="gi">+</span>
<span class="gi">+        # This pseudo-header field MUST NOT be empty for &quot;http&quot; or &quot;https&quot;</span>
<span class="gi">+        # URIs; &quot;http&quot; or &quot;https&quot; URIs that do not contain a path component</span>
<span class="gi">+        # MUST include a value of &#39;/&#39;. The exception to this rule is an</span>
<span class="gi">+        # OPTIONS request for an &quot;http&quot; or &quot;https&quot; URI that does not include</span>
<span class="gi">+        # a path component; these MUST include a &quot;:path&quot; pseudo-header field</span>
<span class="gi">+        # with a value of &#39;*&#39; (refer RFC 7540 - Section 8.1.2.3)</span>
<span class="gi">+        if not path:</span>
<span class="gi">+            path = &quot;*&quot; if self._request.method == &quot;OPTIONS&quot; else &quot;/&quot;</span>
<span class="gi">+</span>
<span class="gi">+        # Make sure pseudo-headers comes before all the other headers</span>
<span class="gi">+        headers = [</span>
<span class="gi">+            (&quot;:method&quot;, self._request.method),</span>
<span class="gi">+            (&quot;:authority&quot;, url.netloc),</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+        # The &quot;:scheme&quot; and &quot;:path&quot; pseudo-header fields MUST</span>
<span class="gi">+        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)</span>
<span class="gi">+        if self._request.method != &quot;CONNECT&quot;:</span>
<span class="gi">+            headers += [</span>
<span class="gi">+                (&quot;:scheme&quot;, self._protocol.metadata[&quot;uri&quot;].scheme),</span>
<span class="gi">+                (&quot;:path&quot;, path),</span>
<span class="gi">+            ]</span>

<span class="gd">-    def send_data(self) -&gt;None:</span>
<span class="gi">+        content_length = str(len(self._request.body))</span>
<span class="gi">+        headers.append((&quot;Content-Length&quot;, content_length))</span>
<span class="gi">+</span>
<span class="gi">+        content_length_name = self._request.headers.normkey(b&quot;Content-Length&quot;)</span>
<span class="gi">+        for name, values in self._request.headers.items():</span>
<span class="gi">+            for value in values:</span>
<span class="gi">+                value = str(value, &quot;utf-8&quot;)</span>
<span class="gi">+                if name == content_length_name:</span>
<span class="gi">+                    if value != content_length:</span>
<span class="gi">+                        logger.warning(</span>
<span class="gi">+                            &quot;Ignoring bad Content-Length header %r of request %r, &quot;</span>
<span class="gi">+                            &quot;sending %r instead&quot;,</span>
<span class="gi">+                            value,</span>
<span class="gi">+                            self._request,</span>
<span class="gi">+                            content_length,</span>
<span class="gi">+                        )</span>
<span class="gi">+                    continue</span>
<span class="gi">+                headers.append((str(name, &quot;utf-8&quot;), value))</span>
<span class="gi">+</span>
<span class="gi">+        return headers</span>
<span class="gi">+</span>
<span class="gi">+    def initiate_request(self) -&gt; None:</span>
<span class="gi">+        if self.check_request_url():</span>
<span class="gi">+            headers = self._get_request_headers()</span>
<span class="gi">+            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)</span>
<span class="gi">+            self.metadata[&quot;request_sent&quot;] = True</span>
<span class="gi">+            self.send_data()</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Close this stream calling the response errback</span>
<span class="gi">+            # Note that we have not sent any headers</span>
<span class="gi">+            self.close(StreamCloseReason.INVALID_HOSTNAME)</span>
<span class="gi">+</span>
<span class="gi">+    def send_data(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Called immediately after the headers are sent. Here we send all the
<span class="w"> </span>        data as part of the request.

<span class="gu">@@ -129,27 +266,227 @@ class Stream:</span>
<span class="w"> </span>           and has initiated request already by sending HEADER frame. If not then
<span class="w"> </span>           stream will raise ProtocolError (raise by h2 state machine).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.metadata[&quot;stream_closed_local&quot;]:</span>
<span class="gi">+            raise StreamClosedError(self.stream_id)</span>
<span class="gi">+</span>
<span class="gi">+        # Firstly, check what the flow control window is for current stream.</span>
<span class="gi">+        window_size = self._protocol.conn.local_flow_control_window(</span>
<span class="gi">+            stream_id=self.stream_id</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Next, check what the maximum frame size is.</span>
<span class="gi">+        max_frame_size = self._protocol.conn.max_outbound_frame_size</span>
<span class="gi">+</span>
<span class="gi">+        # We will send no more than the window size or the remaining file size</span>
<span class="gi">+        # of data in this call, whichever is smaller.</span>
<span class="gi">+        bytes_to_send_size = min(window_size, self.metadata[&quot;remaining_content_length&quot;])</span>
<span class="gi">+</span>
<span class="gi">+        # We now need to send a number of data frames.</span>
<span class="gi">+        while bytes_to_send_size &gt; 0:</span>
<span class="gi">+            chunk_size = min(bytes_to_send_size, max_frame_size)</span>
<span class="gi">+</span>
<span class="gi">+            data_chunk_start_id = (</span>
<span class="gi">+                self.metadata[&quot;request_content_length&quot;]</span>
<span class="gi">+                - self.metadata[&quot;remaining_content_length&quot;]</span>
<span class="gi">+            )</span>
<span class="gi">+            data_chunk = self._request.body[</span>
<span class="gi">+                data_chunk_start_id : data_chunk_start_id + chunk_size</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)</span>
<span class="gi">+</span>
<span class="gi">+            bytes_to_send_size -= chunk_size</span>
<span class="gi">+            self.metadata[&quot;remaining_content_length&quot;] -= chunk_size</span>
<span class="gi">+</span>
<span class="gi">+        self.metadata[&quot;remaining_content_length&quot;] = max(</span>
<span class="gi">+            0, self.metadata[&quot;remaining_content_length&quot;]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # End the stream if no more data needs to be send</span>
<span class="gi">+        if self.metadata[&quot;remaining_content_length&quot;] == 0:</span>
<span class="gi">+            self._protocol.conn.end_stream(self.stream_id)</span>

<span class="gd">-    def receive_window_update(self) -&gt;None:</span>
<span class="gi">+        # Q. What about the rest of the data?</span>
<span class="gi">+        # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame</span>
<span class="gi">+</span>
<span class="gi">+    def receive_window_update(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Flow control window size was changed.
<span class="w"> </span>        Send data that earlier could not be sent as we were
<span class="w"> </span>        blocked behind the flow control.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.metadata[&quot;remaining_content_length&quot;]</span>
<span class="gi">+            and not self.metadata[&quot;stream_closed_server&quot;]</span>
<span class="gi">+            and self.metadata[&quot;request_sent&quot;]</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.send_data()</span>
<span class="gi">+</span>
<span class="gi">+    def receive_data(self, data: bytes, flow_controlled_length: int) -&gt; None:</span>
<span class="gi">+        self._response[&quot;body&quot;].write(data)</span>
<span class="gi">+        self._response[&quot;flow_controlled_size&quot;] += flow_controlled_length</span>
<span class="gi">+</span>
<span class="gi">+        # We check maxsize here in case the Content-Length header was not received</span>
<span class="gi">+        if (</span>
<span class="gi">+            self._download_maxsize</span>
<span class="gi">+            and self._response[&quot;flow_controlled_size&quot;] &gt; self._download_maxsize</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if self._log_warnsize:</span>
<span class="gi">+            self.metadata[&quot;reached_warnsize&quot;] = True</span>
<span class="gi">+            warning_msg = (</span>
<span class="gi">+                f&#39;Received more ({self._response[&quot;flow_controlled_size&quot;]}) bytes than download &#39;</span>
<span class="gi">+                f&quot;warn size ({self._download_warnsize}) in request {self._request}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            logger.warning(warning_msg)</span>

<span class="gd">-    def reset_stream(self, reason: StreamCloseReason=StreamCloseReason.RESET</span>
<span class="gd">-        ) -&gt;None:</span>
<span class="gi">+        # Acknowledge the data received</span>
<span class="gi">+        self._protocol.conn.acknowledge_received_data(</span>
<span class="gi">+            self._response[&quot;flow_controlled_size&quot;], self.stream_id</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def receive_headers(self, headers: List[HeaderTuple]) -&gt; None:</span>
<span class="gi">+        for name, value in headers:</span>
<span class="gi">+            self._response[&quot;headers&quot;].appendlist(name, value)</span>
<span class="gi">+</span>
<span class="gi">+        # Check if we exceed the allowed max data size which can be received</span>
<span class="gi">+        expected_size = int(self._response[&quot;headers&quot;].get(b&quot;Content-Length&quot;, -1))</span>
<span class="gi">+        if self._download_maxsize and expected_size &gt; self._download_maxsize:</span>
<span class="gi">+            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if self._log_warnsize:</span>
<span class="gi">+            self.metadata[&quot;reached_warnsize&quot;] = True</span>
<span class="gi">+            warning_msg = (</span>
<span class="gi">+                f&quot;Expected response size ({expected_size}) larger than &quot;</span>
<span class="gi">+                f&quot;download warn size ({self._download_warnsize}) in request {self._request}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            logger.warning(warning_msg)</span>
<span class="gi">+</span>
<span class="gi">+    def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Close this stream by sending a RST_FRAME to the remote peer&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.metadata[&quot;stream_closed_local&quot;]:</span>
<span class="gi">+            raise StreamClosedError(self.stream_id)</span>

<span class="gd">-    def close(self, reason: StreamCloseReason, errors: Optional[List[</span>
<span class="gd">-        BaseException]]=None, from_protocol: bool=False) -&gt;None:</span>
<span class="gi">+        # Clear buffer earlier to avoid keeping data in memory for a long time</span>
<span class="gi">+        self._response[&quot;body&quot;].truncate(0)</span>
<span class="gi">+</span>
<span class="gi">+        self.metadata[&quot;stream_closed_local&quot;] = True</span>
<span class="gi">+        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)</span>
<span class="gi">+        self.close(reason)</span>
<span class="gi">+</span>
<span class="gi">+    def close(</span>
<span class="gi">+        self,</span>
<span class="gi">+        reason: StreamCloseReason,</span>
<span class="gi">+        errors: Optional[List[BaseException]] = None,</span>
<span class="gi">+        from_protocol: bool = False,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Based on the reason sent we will handle each case.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.metadata[&quot;stream_closed_server&quot;]:</span>
<span class="gi">+            raise StreamClosedError(self.stream_id)</span>
<span class="gi">+</span>
<span class="gi">+        if not isinstance(reason, StreamCloseReason):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;Expected StreamCloseReason, received {reason.__class__.__qualname__}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Have default value of errors as an empty list as</span>
<span class="gi">+        # some cases can add a list of exceptions</span>
<span class="gi">+        errors = errors or []</span>

<span class="gd">-    def _fire_response_deferred(self) -&gt;None:</span>
<span class="gi">+        if not from_protocol:</span>
<span class="gi">+            self._protocol.pop_stream(self.stream_id)</span>
<span class="gi">+</span>
<span class="gi">+        self.metadata[&quot;stream_closed_server&quot;] = True</span>
<span class="gi">+</span>
<span class="gi">+        # We do not check for Content-Length or Transfer-Encoding in response headers</span>
<span class="gi">+        # and add `partial` flag as in HTTP/1.1 as &#39;A request or response that includes</span>
<span class="gi">+        # a payload body can include a content-length header field&#39; (RFC 7540 - Section 8.1.2.6)</span>
<span class="gi">+</span>
<span class="gi">+        # NOTE: Order of handling the events is important here</span>
<span class="gi">+        # As we immediately cancel the request when maxsize is exceeded while</span>
<span class="gi">+        # receiving DATA_FRAME&#39;s when we have received the headers (not</span>
<span class="gi">+        # having Content-Length)</span>
<span class="gi">+        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:</span>
<span class="gi">+            expected_size = int(</span>
<span class="gi">+                self._response[&quot;headers&quot;].get(</span>
<span class="gi">+                    b&quot;Content-Length&quot;, self._response[&quot;flow_controlled_size&quot;]</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+            error_msg = (</span>
<span class="gi">+                f&quot;Cancelling download of {self._request.url}: received response &quot;</span>
<span class="gi">+                f&quot;size ({expected_size}) larger than download max size ({self._download_maxsize})&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            logger.error(error_msg)</span>
<span class="gi">+            self._deferred_response.errback(CancelledError(error_msg))</span>
<span class="gi">+</span>
<span class="gi">+        elif reason is StreamCloseReason.ENDED:</span>
<span class="gi">+            self._fire_response_deferred()</span>
<span class="gi">+</span>
<span class="gi">+        # Stream was abruptly ended here</span>
<span class="gi">+        elif reason is StreamCloseReason.CANCELLED:</span>
<span class="gi">+            # Client has cancelled the request. Remove all the data</span>
<span class="gi">+            # received and fire the response deferred with no flags set</span>
<span class="gi">+</span>
<span class="gi">+            # NOTE: The data is already flushed in Stream.reset_stream() called</span>
<span class="gi">+            # immediately when the stream needs to be cancelled</span>
<span class="gi">+</span>
<span class="gi">+            # There maybe no :status in headers, we make</span>
<span class="gi">+            # HTTP Status Code: 499 - Client Closed Request</span>
<span class="gi">+            self._response[&quot;headers&quot;][&quot;:status&quot;] = &quot;499&quot;</span>
<span class="gi">+            self._fire_response_deferred()</span>
<span class="gi">+</span>
<span class="gi">+        elif reason is StreamCloseReason.RESET:</span>
<span class="gi">+            self._deferred_response.errback(</span>
<span class="gi">+                ResponseFailed(</span>
<span class="gi">+                    [</span>
<span class="gi">+                        Failure(</span>
<span class="gi">+                            f&#39;Remote peer {self._protocol.metadata[&quot;ip_address&quot;]} sent RST_STREAM&#39;,</span>
<span class="gi">+                            ProtocolError,</span>
<span class="gi">+                        )</span>
<span class="gi">+                    ]</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        elif reason is StreamCloseReason.CONNECTION_LOST:</span>
<span class="gi">+            self._deferred_response.errback(ResponseFailed(errors))</span>
<span class="gi">+</span>
<span class="gi">+        elif reason is StreamCloseReason.INACTIVE:</span>
<span class="gi">+            errors.insert(0, InactiveStreamClosed(self._request))</span>
<span class="gi">+            self._deferred_response.errback(ResponseFailed(errors))</span>
<span class="gi">+</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert reason is StreamCloseReason.INVALID_HOSTNAME</span>
<span class="gi">+            self._deferred_response.errback(</span>
<span class="gi">+                InvalidHostname(</span>
<span class="gi">+                    self._request,</span>
<span class="gi">+                    str(self._protocol.metadata[&quot;uri&quot;].host, &quot;utf-8&quot;),</span>
<span class="gi">+                    f&#39;{self._protocol.metadata[&quot;ip_address&quot;]}:{self._protocol.metadata[&quot;uri&quot;].port}&#39;,</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _fire_response_deferred(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Builds response from the self._response dict
<span class="w"> </span>        and fires the response deferred callback with the
<span class="w"> </span>        generated response instance&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        body = self._response[&quot;body&quot;].getvalue()</span>
<span class="gi">+        response_cls = responsetypes.from_args(</span>
<span class="gi">+            headers=self._response[&quot;headers&quot;],</span>
<span class="gi">+            url=self._request.url,</span>
<span class="gi">+            body=body,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        response = response_cls(</span>
<span class="gi">+            url=self._request.url,</span>
<span class="gi">+            status=int(self._response[&quot;headers&quot;][&quot;:status&quot;]),</span>
<span class="gi">+            headers=self._response[&quot;headers&quot;],</span>
<span class="gi">+            body=body,</span>
<span class="gi">+            request=self._request,</span>
<span class="gi">+            certificate=self._protocol.metadata[&quot;certificate&quot;],</span>
<span class="gi">+            ip_address=self._protocol.metadata[&quot;ip_address&quot;],</span>
<span class="gi">+            protocol=&quot;h2&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        self._deferred_response.callback(response)</span>
<span class="gh">diff --git a/scrapy/core/scheduler.py b/scrapy/core/scheduler.py</span>
<span class="gh">index 39a4bb6a3..17c95f1ea 100644</span>
<span class="gd">--- a/scrapy/core/scheduler.py</span>
<span class="gi">+++ b/scrapy/core/scheduler.py</span>
<span class="gu">@@ -1,10 +1,13 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import json
<span class="w"> </span>import logging
<span class="w"> </span>from abc import abstractmethod
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import TYPE_CHECKING, Any, Optional, Type, TypeVar, cast
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy.crawler import Crawler
<span class="w"> </span>from scrapy.dupefilters import BaseDupeFilter
<span class="w"> </span>from scrapy.http.request import Request
<span class="gu">@@ -12,8 +15,12 @@ from scrapy.spiders import Spider</span>
<span class="w"> </span>from scrapy.statscollectors import StatsCollector
<span class="w"> </span>from scrapy.utils.job import job_dir
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -22,14 +29,18 @@ class BaseSchedulerMeta(type):</span>
<span class="w"> </span>    Metaclass to check scheduler classes against the necessary interface
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __instancecheck__(cls, instance: Any) -&gt;bool:</span>
<span class="gi">+    def __instancecheck__(cls, instance: Any) -&gt; bool:</span>
<span class="w"> </span>        return cls.__subclasscheck__(type(instance))

<span class="gd">-    def __subclasscheck__(cls, subclass: type) -&gt;bool:</span>
<span class="gd">-        return hasattr(subclass, &#39;has_pending_requests&#39;) and callable(subclass</span>
<span class="gd">-            .has_pending_requests) and hasattr(subclass, &#39;enqueue_request&#39;</span>
<span class="gd">-            ) and callable(subclass.enqueue_request) and hasattr(subclass,</span>
<span class="gd">-            &#39;next_request&#39;) and callable(subclass.next_request)</span>
<span class="gi">+    def __subclasscheck__(cls, subclass: type) -&gt; bool:</span>
<span class="gi">+        return (</span>
<span class="gi">+            hasattr(subclass, &quot;has_pending_requests&quot;)</span>
<span class="gi">+            and callable(subclass.has_pending_requests)</span>
<span class="gi">+            and hasattr(subclass, &quot;enqueue_request&quot;)</span>
<span class="gi">+            and callable(subclass.enqueue_request)</span>
<span class="gi">+            and hasattr(subclass, &quot;next_request&quot;)</span>
<span class="gi">+            and callable(subclass.next_request)</span>
<span class="gi">+        )</span>


<span class="w"> </span>class BaseScheduler(metaclass=BaseSchedulerMeta):
<span class="gu">@@ -50,13 +61,13 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    @classmethod
<span class="gd">-    def from_crawler(cls, crawler: Crawler) -&gt;Self:</span>
<span class="gi">+    def from_crawler(cls, crawler: Crawler) -&gt; Self:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return cls()</span>

<span class="gd">-    def open(self, spider: Spider) -&gt;Optional[Deferred]:</span>
<span class="gi">+    def open(self, spider: Spider) -&gt; Optional[Deferred]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Called when the spider is opened by the engine. It receives the spider
<span class="w"> </span>        instance as argument and it&#39;s useful to execute initialization code.
<span class="gu">@@ -66,7 +77,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        pass

<span class="gd">-    def close(self, reason: str) -&gt;Optional[Deferred]:</span>
<span class="gi">+    def close(self, reason: str) -&gt; Optional[Deferred]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Called when the spider is closed by the engine. It receives the reason why the crawl
<span class="w"> </span>        finished as argument and it&#39;s useful to execute cleaning code.
<span class="gu">@@ -77,14 +88,14 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):</span>
<span class="w"> </span>        pass

<span class="w"> </span>    @abstractmethod
<span class="gd">-    def has_pending_requests(self) -&gt;bool:</span>
<span class="gi">+    def has_pending_requests(self) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        ``True`` if the scheduler has enqueued requests, ``False`` otherwise
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        raise NotImplementedError()</span>

<span class="w"> </span>    @abstractmethod
<span class="gd">-    def enqueue_request(self, request: Request) -&gt;bool:</span>
<span class="gi">+    def enqueue_request(self, request: Request) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Process a request received by the engine.

<span class="gu">@@ -95,10 +106,10 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):</span>
<span class="w"> </span>        For reference, the default Scrapy scheduler returns ``False`` when the
<span class="w"> </span>        request is rejected by the dupefilter.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        raise NotImplementedError()</span>

<span class="w"> </span>    @abstractmethod
<span class="gd">-    def next_request(self) -&gt;Optional[Request]:</span>
<span class="gi">+    def next_request(self) -&gt; Optional[Request]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``
<span class="w"> </span>        to indicate that there are no requests to be considered ready at the moment.
<span class="gu">@@ -107,10 +118,10 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):</span>
<span class="w"> </span>        to the downloader in the current reactor cycle. The engine will continue
<span class="w"> </span>        calling ``next_request`` until ``has_pending_requests`` is ``False``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        raise NotImplementedError()</span>


<span class="gd">-SchedulerTV = TypeVar(&#39;SchedulerTV&#39;, bound=&#39;Scheduler&#39;)</span>
<span class="gi">+SchedulerTV = TypeVar(&quot;SchedulerTV&quot;, bound=&quot;Scheduler&quot;)</span>


<span class="w"> </span>class Scheduler(BaseScheduler):
<span class="gu">@@ -164,10 +175,17 @@ class Scheduler(BaseScheduler):</span>
<span class="w"> </span>    :type crawler: :class:`scrapy.crawler.Crawler`
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, dupefilter: BaseDupeFilter, jobdir: Optional[str]=</span>
<span class="gd">-        None, dqclass=None, mqclass=None, logunser: bool=False, stats:</span>
<span class="gd">-        Optional[StatsCollector]=None, pqclass=None, crawler: Optional[</span>
<span class="gd">-        Crawler]=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        dupefilter: BaseDupeFilter,</span>
<span class="gi">+        jobdir: Optional[str] = None,</span>
<span class="gi">+        dqclass=None,</span>
<span class="gi">+        mqclass=None,</span>
<span class="gi">+        logunser: bool = False,</span>
<span class="gi">+        stats: Optional[StatsCollector] = None,</span>
<span class="gi">+        pqclass=None,</span>
<span class="gi">+        crawler: Optional[Crawler] = None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.df: BaseDupeFilter = dupefilter
<span class="w"> </span>        self.dqdir: Optional[str] = self._dqdir(jobdir)
<span class="w"> </span>        self.pqclass = pqclass
<span class="gu">@@ -178,28 +196,48 @@ class Scheduler(BaseScheduler):</span>
<span class="w"> </span>        self.crawler: Optional[Crawler] = crawler

<span class="w"> </span>    @classmethod
<span class="gd">-    def from_crawler(cls: Type[SchedulerTV], crawler: Crawler) -&gt;SchedulerTV:</span>
<span class="gi">+    def from_crawler(cls: Type[SchedulerTV], crawler: Crawler) -&gt; SchedulerTV:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Factory method, initializes the scheduler with arguments taken from the crawl settings
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def open(self, spider: Spider) -&gt;Optional[Deferred]:</span>
<span class="gi">+        dupefilter_cls = load_object(crawler.settings[&quot;DUPEFILTER_CLASS&quot;])</span>
<span class="gi">+        return cls(</span>
<span class="gi">+            dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),</span>
<span class="gi">+            jobdir=job_dir(crawler.settings),</span>
<span class="gi">+            dqclass=load_object(crawler.settings[&quot;SCHEDULER_DISK_QUEUE&quot;]),</span>
<span class="gi">+            mqclass=load_object(crawler.settings[&quot;SCHEDULER_MEMORY_QUEUE&quot;]),</span>
<span class="gi">+            logunser=crawler.settings.getbool(&quot;SCHEDULER_DEBUG&quot;),</span>
<span class="gi">+            stats=crawler.stats,</span>
<span class="gi">+            pqclass=load_object(crawler.settings[&quot;SCHEDULER_PRIORITY_QUEUE&quot;]),</span>
<span class="gi">+            crawler=crawler,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def has_pending_requests(self) -&gt; bool:</span>
<span class="gi">+        return len(self) &gt; 0</span>
<span class="gi">+</span>
<span class="gi">+    def open(self, spider: Spider) -&gt; Optional[Deferred]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        (1) initialize the memory queue
<span class="w"> </span>        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory
<span class="w"> </span>        (3) return the result of the dupefilter&#39;s ``open`` method
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.spider = spider</span>
<span class="gi">+        self.mqs = self._mq()</span>
<span class="gi">+        self.dqs = self._dq() if self.dqdir else None</span>
<span class="gi">+        return self.df.open()</span>

<span class="gd">-    def close(self, reason: str) -&gt;Optional[Deferred]:</span>
<span class="gi">+    def close(self, reason: str) -&gt; Optional[Deferred]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        (1) dump pending requests to disk if there is a disk queue
<span class="w"> </span>        (2) return the result of the dupefilter&#39;s ``close`` method
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.dqs is not None:</span>
<span class="gi">+            state = self.dqs.close()</span>
<span class="gi">+            assert isinstance(self.dqdir, str)</span>
<span class="gi">+            self._write_dqs_state(self.dqdir, state)</span>
<span class="gi">+        return self.df.close(reason)</span>

<span class="gd">-    def enqueue_request(self, request: Request) -&gt;bool:</span>
<span class="gi">+    def enqueue_request(self, request: Request) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Unless the received request is filtered out by the Dupefilter, attempt to push
<span class="w"> </span>        it into the disk queue, falling back to pushing it into the memory queue.
<span class="gu">@@ -209,9 +247,20 @@ class Scheduler(BaseScheduler):</span>

<span class="w"> </span>        Return ``True`` if the request was stored successfully, ``False`` otherwise.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def next_request(self) -&gt;Optional[Request]:</span>
<span class="gi">+        if not request.dont_filter and self.df.request_seen(request):</span>
<span class="gi">+            self.df.log(request, self.spider)</span>
<span class="gi">+            return False</span>
<span class="gi">+        dqok = self._dqpush(request)</span>
<span class="gi">+        assert self.stats is not None</span>
<span class="gi">+        if dqok:</span>
<span class="gi">+            self.stats.inc_value(&quot;scheduler/enqueued/disk&quot;, spider=self.spider)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._mqpush(request)</span>
<span class="gi">+            self.stats.inc_value(&quot;scheduler/enqueued/memory&quot;, spider=self.spider)</span>
<span class="gi">+        self.stats.inc_value(&quot;scheduler/enqueued&quot;, spider=self.spider)</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def next_request(self) -&gt; Optional[Request]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return a :class:`~scrapy.http.Request` object from the memory queue,
<span class="w"> </span>        falling back to the disk queue if the memory queue is empty.
<span class="gu">@@ -220,23 +269,103 @@ class Scheduler(BaseScheduler):</span>
<span class="w"> </span>        Increment the appropriate stats, such as: ``scheduler/dequeued``,
<span class="w"> </span>        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def __len__(self) -&gt;int:</span>
<span class="gi">+        request: Optional[Request] = self.mqs.pop()</span>
<span class="gi">+        assert self.stats is not None</span>
<span class="gi">+        if request is not None:</span>
<span class="gi">+            self.stats.inc_value(&quot;scheduler/dequeued/memory&quot;, spider=self.spider)</span>
<span class="gi">+        else:</span>
<span class="gi">+            request = self._dqpop()</span>
<span class="gi">+            if request is not None:</span>
<span class="gi">+                self.stats.inc_value(&quot;scheduler/dequeued/disk&quot;, spider=self.spider)</span>
<span class="gi">+        if request is not None:</span>
<span class="gi">+            self.stats.inc_value(&quot;scheduler/dequeued&quot;, spider=self.spider)</span>
<span class="gi">+        return request</span>
<span class="gi">+</span>
<span class="gi">+    def __len__(self) -&gt; int:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return the total amount of enqueued requests
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(</span>
<span class="gd">-            self.mqs)</span>
<span class="gi">+        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)</span>
<span class="gi">+</span>
<span class="gi">+    def _dqpush(self, request: Request) -&gt; bool:</span>
<span class="gi">+        if self.dqs is None:</span>
<span class="gi">+            return False</span>
<span class="gi">+        try:</span>
<span class="gi">+            self.dqs.push(request)</span>
<span class="gi">+        except ValueError as e:  # non serializable request</span>
<span class="gi">+            if self.logunser:</span>
<span class="gi">+                msg = (</span>
<span class="gi">+                    &quot;Unable to serialize request: %(request)s - reason:&quot;</span>
<span class="gi">+                    &quot; %(reason)s - no more unserializable requests will be&quot;</span>
<span class="gi">+                    &quot; logged (stats being collected)&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    msg,</span>
<span class="gi">+                    {&quot;request&quot;: request, &quot;reason&quot;: e},</span>
<span class="gi">+                    exc_info=True,</span>
<span class="gi">+                    extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+                )</span>
<span class="gi">+                self.logunser = False</span>
<span class="gi">+            assert self.stats is not None</span>
<span class="gi">+            self.stats.inc_value(&quot;scheduler/unserializable&quot;, spider=self.spider)</span>
<span class="gi">+            return False</span>
<span class="gi">+        else:</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+    def _mqpush(self, request: Request) -&gt; None:</span>
<span class="gi">+        self.mqs.push(request)</span>
<span class="gi">+</span>
<span class="gi">+    def _dqpop(self) -&gt; Optional[Request]:</span>
<span class="gi">+        if self.dqs is not None:</span>
<span class="gi">+            return self.dqs.pop()</span>
<span class="gi">+        return None</span>

<span class="w"> </span>    def _mq(self):
<span class="w"> </span>        &quot;&quot;&quot;Create a new priority queue instance, with in-memory storage&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return create_instance(</span>
<span class="gi">+            self.pqclass,</span>
<span class="gi">+            settings=None,</span>
<span class="gi">+            crawler=self.crawler,</span>
<span class="gi">+            downstream_queue_cls=self.mqclass,</span>
<span class="gi">+            key=&quot;&quot;,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _dq(self):
<span class="w"> </span>        &quot;&quot;&quot;Create a new priority queue instance, with disk storage&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def _dqdir(self, jobdir: Optional[str]) -&gt;Optional[str]:</span>
<span class="gi">+        assert self.dqdir</span>
<span class="gi">+        state = self._read_dqs_state(self.dqdir)</span>
<span class="gi">+        q = create_instance(</span>
<span class="gi">+            self.pqclass,</span>
<span class="gi">+            settings=None,</span>
<span class="gi">+            crawler=self.crawler,</span>
<span class="gi">+            downstream_queue_cls=self.dqclass,</span>
<span class="gi">+            key=self.dqdir,</span>
<span class="gi">+            startprios=state,</span>
<span class="gi">+        )</span>
<span class="gi">+        if q:</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;Resuming crawl (%(queuesize)d requests scheduled)&quot;,</span>
<span class="gi">+                {&quot;queuesize&quot;: len(q)},</span>
<span class="gi">+                extra={&quot;spider&quot;: self.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        return q</span>
<span class="gi">+</span>
<span class="gi">+    def _dqdir(self, jobdir: Optional[str]) -&gt; Optional[str]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return a folder name to keep disk queue state at&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if jobdir:</span>
<span class="gi">+            dqdir = Path(jobdir, &quot;requests.queue&quot;)</span>
<span class="gi">+            if not dqdir.exists():</span>
<span class="gi">+                dqdir.mkdir(parents=True)</span>
<span class="gi">+            return str(dqdir)</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    def _read_dqs_state(self, dqdir: str) -&gt; list:</span>
<span class="gi">+        path = Path(dqdir, &quot;active.json&quot;)</span>
<span class="gi">+        if not path.exists():</span>
<span class="gi">+            return []</span>
<span class="gi">+        with path.open(encoding=&quot;utf-8&quot;) as f:</span>
<span class="gi">+            return cast(list, json.load(f))</span>
<span class="gi">+</span>
<span class="gi">+    def _write_dqs_state(self, dqdir: str, state: list) -&gt; None:</span>
<span class="gi">+        with Path(dqdir, &quot;active.json&quot;).open(&quot;w&quot;, encoding=&quot;utf-8&quot;) as f:</span>
<span class="gi">+            json.dump(state, f)</span>
<span class="gh">diff --git a/scrapy/core/scraper.py b/scrapy/core/scraper.py</span>
<span class="gh">index d5cedad9e..b2c26507c 100644</span>
<span class="gd">--- a/scrapy/core/scraper.py</span>
<span class="gi">+++ b/scrapy/core/scraper.py</span>
<span class="gu">@@ -1,12 +1,28 @@</span>
<span class="w"> </span>&quot;&quot;&quot;This module implements the Scraper component which parses responses and
<span class="w"> </span>extracts information from them&quot;&quot;&quot;
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>from collections import deque
<span class="gd">-from typing import TYPE_CHECKING, Any, AsyncGenerator, AsyncIterable, Deque, Generator, Iterable, Optional, Set, Tuple, Type, Union</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    AsyncGenerator,</span>
<span class="gi">+    AsyncIterable,</span>
<span class="gi">+    Deque,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Set,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Type,</span>
<span class="gi">+    Union,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import is_item
<span class="w"> </span>from twisted.internet.defer import Deferred, inlineCallbacks
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider, signals
<span class="w"> </span>from scrapy.core.spidermw import SpiderMiddlewareManager
<span class="w"> </span>from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
<span class="gu">@@ -14,21 +30,34 @@ from scrapy.http import Request, Response</span>
<span class="w"> </span>from scrapy.logformatter import LogFormatter
<span class="w"> </span>from scrapy.pipelines import ItemPipelineManager
<span class="w"> </span>from scrapy.signalmanager import SignalManager
<span class="gd">-from scrapy.utils.defer import aiter_errback, defer_fail, defer_succeed, iter_errback, parallel, parallel_async</span>
<span class="gi">+from scrapy.utils.defer import (</span>
<span class="gi">+    aiter_errback,</span>
<span class="gi">+    defer_fail,</span>
<span class="gi">+    defer_succeed,</span>
<span class="gi">+    iter_errback,</span>
<span class="gi">+    parallel,</span>
<span class="gi">+    parallel_async,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info, logformatter_adapter
<span class="w"> </span>from scrapy.utils.misc import load_object, warn_on_generator_with_return_value
<span class="w"> </span>from scrapy.utils.spider import iterate_spider_output
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class Slot:
<span class="w"> </span>    &quot;&quot;&quot;Scraper slot (one per running spider)&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    MIN_RESPONSE_SIZE = 1024

<span class="gd">-    def __init__(self, max_active_size: int=5000000):</span>
<span class="gi">+    def __init__(self, max_active_size: int = 5000000):</span>
<span class="w"> </span>        self.max_active_size = max_active_size
<span class="w"> </span>        self.queue: Deque[QueueTuple] = deque()
<span class="w"> </span>        self.active: Set[Request] = set()
<span class="gu">@@ -36,60 +65,260 @@ class Slot:</span>
<span class="w"> </span>        self.itemproc_size: int = 0
<span class="w"> </span>        self.closing: Optional[Deferred] = None

<span class="gi">+    def add_response_request(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        deferred: Deferred = Deferred()</span>
<span class="gi">+        self.queue.append((result, request, deferred))</span>
<span class="gi">+        if isinstance(result, Response):</span>
<span class="gi">+            self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.active_size += self.MIN_RESPONSE_SIZE</span>
<span class="gi">+        return deferred</span>
<span class="gi">+</span>
<span class="gi">+    def next_response_request_deferred(self) -&gt; QueueTuple:</span>
<span class="gi">+        response, request, deferred = self.queue.popleft()</span>
<span class="gi">+        self.active.add(request)</span>
<span class="gi">+        return response, request, deferred</span>
<span class="gi">+</span>
<span class="gi">+    def finish_response(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        self.active.remove(request)</span>
<span class="gi">+        if isinstance(result, Response):</span>
<span class="gi">+            self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.active_size -= self.MIN_RESPONSE_SIZE</span>
<span class="gi">+</span>
<span class="gi">+    def is_idle(self) -&gt; bool:</span>
<span class="gi">+        return not (self.queue or self.active)</span>
<span class="gi">+</span>
<span class="gi">+    def needs_backout(self) -&gt; bool:</span>
<span class="gi">+        return self.active_size &gt; self.max_active_size</span>

<span class="gd">-class Scraper:</span>

<span class="gd">-    def __init__(self, crawler: Crawler) -&gt;None:</span>
<span class="gi">+class Scraper:</span>
<span class="gi">+    def __init__(self, crawler: Crawler) -&gt; None:</span>
<span class="w"> </span>        self.slot: Optional[Slot] = None
<span class="gd">-        self.spidermw: SpiderMiddlewareManager = (SpiderMiddlewareManager.</span>
<span class="gd">-            from_crawler(crawler))</span>
<span class="gd">-        itemproc_cls: Type[ItemPipelineManager] = load_object(crawler.</span>
<span class="gd">-            settings[&#39;ITEM_PROCESSOR&#39;])</span>
<span class="gi">+        self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(</span>
<span class="gi">+            crawler</span>
<span class="gi">+        )</span>
<span class="gi">+        itemproc_cls: Type[ItemPipelineManager] = load_object(</span>
<span class="gi">+            crawler.settings[&quot;ITEM_PROCESSOR&quot;]</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
<span class="gd">-        self.concurrent_items: int = crawler.settings.getint(&#39;CONCURRENT_ITEMS&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+        self.concurrent_items: int = crawler.settings.getint(&quot;CONCURRENT_ITEMS&quot;)</span>
<span class="w"> </span>        self.crawler: Crawler = crawler
<span class="w"> </span>        self.signals: SignalManager = crawler.signals
<span class="w"> </span>        assert crawler.logformatter
<span class="w"> </span>        self.logformatter: LogFormatter = crawler.logformatter

<span class="w"> </span>    @inlineCallbacks
<span class="gd">-    def open_spider(self, spider: Spider) -&gt;Generator[Deferred, Any, None]:</span>
<span class="gi">+    def open_spider(self, spider: Spider) -&gt; Generator[Deferred, Any, None]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Open the given spider for scraping and allocate resources for it&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.slot = Slot(self.crawler.settings.getint(&quot;SCRAPER_SLOT_MAX_ACTIVE_SIZE&quot;))</span>
<span class="gi">+        yield self.itemproc.open_spider(spider)</span>

<span class="gd">-    def close_spider(self, spider: Spider) -&gt;Deferred:</span>
<span class="gi">+    def close_spider(self, spider: Spider) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;Close a spider being scraped and release its resources&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.slot is None:</span>
<span class="gi">+            raise RuntimeError(&quot;Scraper slot not assigned&quot;)</span>
<span class="gi">+        self.slot.closing = Deferred()</span>
<span class="gi">+        self.slot.closing.addCallback(self.itemproc.close_spider)</span>
<span class="gi">+        self._check_if_closing(spider)</span>
<span class="gi">+        return self.slot.closing</span>

<span class="gd">-    def is_idle(self) -&gt;bool:</span>
<span class="gi">+    def is_idle(self) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return True if there isn&#39;t any more spiders to process&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return not self.slot</span>
<span class="gi">+</span>
<span class="gi">+    def _check_if_closing(self, spider: Spider) -&gt; None:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        if self.slot.closing and self.slot.is_idle():</span>
<span class="gi">+            self.slot.closing.callback(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def enqueue_scrape(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        if self.slot is None:</span>
<span class="gi">+            raise RuntimeError(&quot;Scraper slot not assigned&quot;)</span>
<span class="gi">+        dfd = self.slot.add_response_request(result, request)</span>

<span class="gd">-    def _scrape(self, result: Union[Response, Failure], request: Request,</span>
<span class="gd">-        spider: Spider) -&gt;Deferred:</span>
<span class="gi">+        def finish_scraping(_: Any) -&gt; Any:</span>
<span class="gi">+            assert self.slot is not None</span>
<span class="gi">+            self.slot.finish_response(result, request)</span>
<span class="gi">+            self._check_if_closing(spider)</span>
<span class="gi">+            self._scrape_next(spider)</span>
<span class="gi">+            return _</span>
<span class="gi">+</span>
<span class="gi">+        dfd.addBoth(finish_scraping)</span>
<span class="gi">+        dfd.addErrback(</span>
<span class="gi">+            lambda f: logger.error(</span>
<span class="gi">+                &quot;Scraper bug processing %(request)s&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request},</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        self._scrape_next(spider)</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def _scrape_next(self, spider: Spider) -&gt; None:</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        while self.slot.queue:</span>
<span class="gi">+            response, request, deferred = self.slot.next_response_request_deferred()</span>
<span class="gi">+            self._scrape(response, request, spider).chainDeferred(deferred)</span>
<span class="gi">+</span>
<span class="gi">+    def _scrape(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Handle the downloaded response or failure through the spider callback/errback
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not isinstance(result, (Response, Failure)):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;Incorrect type: expected Response or Failure, got {type(result)}: {result!r}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        dfd = self._scrape2(</span>
<span class="gi">+            result, request, spider</span>
<span class="gi">+        )  # returns spider&#39;s processed output</span>
<span class="gi">+        dfd.addErrback(self.handle_spider_error, request, result, spider)</span>
<span class="gi">+        dfd.addCallback(self.handle_spider_output, request, result, spider)</span>
<span class="gi">+        return dfd</span>

<span class="gd">-    def _scrape2(self, result: Union[Response, Failure], request: Request,</span>
<span class="gd">-        spider: Spider) -&gt;Deferred:</span>
<span class="gi">+    def _scrape2(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Handle the different cases of request&#39;s result been a Response or a Failure
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(result, Response):</span>
<span class="gi">+            return self.spidermw.scrape_response(</span>
<span class="gi">+                self.call_spider, result, request, spider</span>
<span class="gi">+            )</span>
<span class="gi">+        # else result is a Failure</span>
<span class="gi">+        dfd = self.call_spider(result, request, spider)</span>
<span class="gi">+        return dfd.addErrback(self._log_download_errors, result, request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def call_spider(</span>
<span class="gi">+        self, result: Union[Response, Failure], request: Request, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        if isinstance(result, Response):</span>
<span class="gi">+            if getattr(result, &quot;request&quot;, None) is None:</span>
<span class="gi">+                result.request = request</span>
<span class="gi">+            callback = result.request.callback or spider._parse</span>
<span class="gi">+            warn_on_generator_with_return_value(spider, callback)</span>
<span class="gi">+            dfd = defer_succeed(result)</span>
<span class="gi">+            dfd.addCallbacks(</span>
<span class="gi">+                callback=callback, callbackKeywords=result.request.cb_kwargs</span>
<span class="gi">+            )</span>
<span class="gi">+        else:  # result is a Failure</span>
<span class="gi">+            # TODO: properly type adding this attribute to a Failure</span>
<span class="gi">+            result.request = request  # type: ignore[attr-defined]</span>
<span class="gi">+            dfd = defer_fail(result)</span>
<span class="gi">+            if request.errback:</span>
<span class="gi">+                warn_on_generator_with_return_value(spider, request.errback)</span>
<span class="gi">+                dfd.addErrback(request.errback)</span>
<span class="gi">+        return dfd.addCallback(iterate_spider_output)</span>
<span class="gi">+</span>
<span class="gi">+    def handle_spider_error(</span>
<span class="gi">+        self,</span>
<span class="gi">+        _failure: Failure,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        response: Union[Response, Failure],</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        exc = _failure.value</span>
<span class="gi">+        if isinstance(exc, CloseSpider):</span>
<span class="gi">+            assert self.crawler.engine is not None  # typing</span>
<span class="gi">+            self.crawler.engine.close_spider(spider, exc.reason or &quot;cancelled&quot;)</span>
<span class="gi">+            return</span>
<span class="gi">+        logkws = self.logformatter.spider_error(_failure, request, response, spider)</span>
<span class="gi">+        logger.log(</span>
<span class="gi">+            *logformatter_adapter(logkws),</span>
<span class="gi">+            exc_info=failure_to_exc_info(_failure),</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        self.signals.send_catch_log(</span>
<span class="gi">+            signal=signals.spider_error,</span>
<span class="gi">+            failure=_failure,</span>
<span class="gi">+            response=response,</span>
<span class="gi">+            spider=spider,</span>
<span class="gi">+        )</span>
<span class="gi">+        assert self.crawler.stats</span>
<span class="gi">+        self.crawler.stats.inc_value(</span>
<span class="gi">+            f&quot;spider_exceptions/{_failure.value.__class__.__name__}&quot;, spider=spider</span>
<span class="gi">+        )</span>

<span class="gd">-    def _process_spidermw_output(self, output: Any, request: Request,</span>
<span class="gd">-        response: Response, spider: Spider) -&gt;Optional[Deferred]:</span>
<span class="gi">+    def handle_spider_output(</span>
<span class="gi">+        self,</span>
<span class="gi">+        result: Union[Iterable, AsyncIterable],</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        response: Union[Response, Failure],</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        if not result:</span>
<span class="gi">+            return defer_succeed(None)</span>
<span class="gi">+        it: Union[Generator, AsyncGenerator]</span>
<span class="gi">+        if isinstance(result, AsyncIterable):</span>
<span class="gi">+            it = aiter_errback(</span>
<span class="gi">+                result, self.handle_spider_error, request, response, spider</span>
<span class="gi">+            )</span>
<span class="gi">+            dfd = parallel_async(</span>
<span class="gi">+                it,</span>
<span class="gi">+                self.concurrent_items,</span>
<span class="gi">+                self._process_spidermw_output,</span>
<span class="gi">+                request,</span>
<span class="gi">+                response,</span>
<span class="gi">+                spider,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            it = iter_errback(</span>
<span class="gi">+                result, self.handle_spider_error, request, response, spider</span>
<span class="gi">+            )</span>
<span class="gi">+            dfd = parallel(</span>
<span class="gi">+                it,</span>
<span class="gi">+                self.concurrent_items,</span>
<span class="gi">+                self._process_spidermw_output,</span>
<span class="gi">+                request,</span>
<span class="gi">+                response,</span>
<span class="gi">+                spider,</span>
<span class="gi">+            )</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def _process_spidermw_output(</span>
<span class="gi">+        self, output: Any, request: Request, response: Response, spider: Spider</span>
<span class="gi">+    ) -&gt; Optional[Deferred]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Process each Request/Item (given in the output parameter) returned
<span class="w"> </span>        from the given spider
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        if isinstance(output, Request):</span>
<span class="gi">+            assert self.crawler.engine is not None  # typing</span>
<span class="gi">+            self.crawler.engine.crawl(request=output)</span>
<span class="gi">+        elif is_item(output):</span>
<span class="gi">+            self.slot.itemproc_size += 1</span>
<span class="gi">+            dfd = self.itemproc.process_item(output, spider)</span>
<span class="gi">+            dfd.addBoth(self._itemproc_finished, output, response, spider)</span>
<span class="gi">+            return dfd</span>
<span class="gi">+        elif output is None:</span>
<span class="gi">+            pass</span>
<span class="gi">+        else:</span>
<span class="gi">+            typename = type(output).__name__</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;Spider must return request, item, or None, got %(typename)r in %(request)s&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request, &quot;typename&quot;: typename},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        return None</span>

<span class="gd">-    def _log_download_errors(self, spider_failure: Failure,</span>
<span class="gd">-        download_failure: Failure, request: Request, spider: Spider) -&gt;Union[</span>
<span class="gd">-        Failure, None]:</span>
<span class="gi">+    def _log_download_errors(</span>
<span class="gi">+        self,</span>
<span class="gi">+        spider_failure: Failure,</span>
<span class="gi">+        download_failure: Failure,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; Union[Failure, None]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Log and silence errors that come from the engine (typically download
<span class="w"> </span>        errors that got propagated thru here).

<span class="gu">@@ -97,9 +326,67 @@ class Scraper:</span>
<span class="w"> </span>        download_failure: the value passed into _scrape2() from
<span class="w"> </span>        ExecutionEngine._handle_downloader_output() as &quot;result&quot;
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not download_failure.check(IgnoreRequest):</span>
<span class="gi">+            if download_failure.frames:</span>
<span class="gi">+                logkws = self.logformatter.download_error(</span>
<span class="gi">+                    download_failure, request, spider</span>
<span class="gi">+                )</span>
<span class="gi">+                logger.log(</span>
<span class="gi">+                    *logformatter_adapter(logkws),</span>
<span class="gi">+                    extra={&quot;spider&quot;: spider},</span>
<span class="gi">+                    exc_info=failure_to_exc_info(download_failure),</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                errmsg = download_failure.getErrorMessage()</span>
<span class="gi">+                if errmsg:</span>
<span class="gi">+                    logkws = self.logformatter.download_error(</span>
<span class="gi">+                        download_failure, request, spider, errmsg</span>
<span class="gi">+                    )</span>
<span class="gi">+                    logger.log(</span>
<span class="gi">+                        *logformatter_adapter(logkws),</span>
<span class="gi">+                        extra={&quot;spider&quot;: spider},</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+        if spider_failure is not download_failure:</span>
<span class="gi">+            return spider_failure</span>
<span class="gi">+        return None</span>

<span class="gd">-    def _itemproc_finished(self, output: Any, item: Any, response: Response,</span>
<span class="gd">-        spider: Spider) -&gt;Deferred:</span>
<span class="gi">+    def _itemproc_finished(</span>
<span class="gi">+        self, output: Any, item: Any, response: Response, spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;ItemProcessor finished for the given ``item`` and returned ``output``&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.slot is not None  # typing</span>
<span class="gi">+        self.slot.itemproc_size -= 1</span>
<span class="gi">+        if isinstance(output, Failure):</span>
<span class="gi">+            ex = output.value</span>
<span class="gi">+            if isinstance(ex, DropItem):</span>
<span class="gi">+                logkws = self.logformatter.dropped(item, ex, response, spider)</span>
<span class="gi">+                if logkws is not None:</span>
<span class="gi">+                    logger.log(*logformatter_adapter(logkws), extra={&quot;spider&quot;: spider})</span>
<span class="gi">+                return self.signals.send_catch_log_deferred(</span>
<span class="gi">+                    signal=signals.item_dropped,</span>
<span class="gi">+                    item=item,</span>
<span class="gi">+                    response=response,</span>
<span class="gi">+                    spider=spider,</span>
<span class="gi">+                    exception=output.value,</span>
<span class="gi">+                )</span>
<span class="gi">+            assert ex</span>
<span class="gi">+            logkws = self.logformatter.item_error(item, ex, response, spider)</span>
<span class="gi">+            logger.log(</span>
<span class="gi">+                *logformatter_adapter(logkws),</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+                exc_info=failure_to_exc_info(output),</span>
<span class="gi">+            )</span>
<span class="gi">+            return self.signals.send_catch_log_deferred(</span>
<span class="gi">+                signal=signals.item_error,</span>
<span class="gi">+                item=item,</span>
<span class="gi">+                response=response,</span>
<span class="gi">+                spider=spider,</span>
<span class="gi">+                failure=output,</span>
<span class="gi">+            )</span>
<span class="gi">+        logkws = self.logformatter.scraped(output, response, spider)</span>
<span class="gi">+        if logkws is not None:</span>
<span class="gi">+            logger.log(*logformatter_adapter(logkws), extra={&quot;spider&quot;: spider})</span>
<span class="gi">+        return self.signals.send_catch_log_deferred(</span>
<span class="gi">+            signal=signals.item_scraped, item=output, response=response, spider=spider</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/core/spidermw.py b/scrapy/core/spidermw.py</span>
<span class="gh">index 9922755bf..dcf1a6dbc 100644</span>
<span class="gd">--- a/scrapy/core/spidermw.py</span>
<span class="gi">+++ b/scrapy/core/spidermw.py</span>
<span class="gu">@@ -6,9 +6,23 @@ See documentation in docs/topics/spider-middleware.rst</span>
<span class="w"> </span>import logging
<span class="w"> </span>from inspect import isasyncgenfunction, iscoroutine
<span class="w"> </span>from itertools import islice
<span class="gd">-from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, List, Optional, Tuple, Union, cast</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    Any,</span>
<span class="gi">+    AsyncGenerator,</span>
<span class="gi">+    AsyncIterable,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred, inlineCallbacks
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request, Spider
<span class="w"> </span>from scrapy.exceptions import _InvalidOutput
<span class="w"> </span>from scrapy.http import Response
<span class="gu">@@ -16,15 +30,317 @@ from scrapy.middleware import MiddlewareManager</span>
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="w"> </span>from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen
<span class="w"> </span>from scrapy.utils.conf import build_component_list
<span class="gd">-from scrapy.utils.defer import deferred_f_from_coro_f, deferred_from_coro, maybe_deferred_to_future, mustbe_deferred</span>
<span class="gi">+from scrapy.utils.defer import (</span>
<span class="gi">+    deferred_f_from_coro_f,</span>
<span class="gi">+    deferred_from_coro,</span>
<span class="gi">+    maybe_deferred_to_future,</span>
<span class="gi">+    mustbe_deferred,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.utils.python import MutableAsyncChain, MutableChain
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]


<span class="gi">+def _isiterable(o: Any) -&gt; bool:</span>
<span class="gi">+    return isinstance(o, (Iterable, AsyncIterable))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class SpiderMiddlewareManager(MiddlewareManager):
<span class="gd">-    component_name = &#39;spider middleware&#39;</span>
<span class="gi">+    component_name = &quot;spider middleware&quot;</span>

<span class="w"> </span>    def __init__(self, *middlewares: Any):
<span class="w"> </span>        super().__init__(*middlewares)
<span class="w"> </span>        self.downgrade_warning_done = False
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -&gt; List[Any]:</span>
<span class="gi">+        return build_component_list(settings.getwithbase(&quot;SPIDER_MIDDLEWARES&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def _add_middleware(self, mw: Any) -&gt; None:</span>
<span class="gi">+        super()._add_middleware(mw)</span>
<span class="gi">+        if hasattr(mw, &quot;process_spider_input&quot;):</span>
<span class="gi">+            self.methods[&quot;process_spider_input&quot;].append(mw.process_spider_input)</span>
<span class="gi">+        if hasattr(mw, &quot;process_start_requests&quot;):</span>
<span class="gi">+            self.methods[&quot;process_start_requests&quot;].appendleft(mw.process_start_requests)</span>
<span class="gi">+        process_spider_output = self._get_async_method_pair(mw, &quot;process_spider_output&quot;)</span>
<span class="gi">+        self.methods[&quot;process_spider_output&quot;].appendleft(process_spider_output)</span>
<span class="gi">+        process_spider_exception = getattr(mw, &quot;process_spider_exception&quot;, None)</span>
<span class="gi">+        self.methods[&quot;process_spider_exception&quot;].appendleft(process_spider_exception)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_spider_input(</span>
<span class="gi">+        self,</span>
<span class="gi">+        scrape_func: ScrapeFunc,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; Any:</span>
<span class="gi">+        for method in self.methods[&quot;process_spider_input&quot;]:</span>
<span class="gi">+            method = cast(Callable, method)</span>
<span class="gi">+            try:</span>
<span class="gi">+                result = method(response=response, spider=spider)</span>
<span class="gi">+                if result is not None:</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        f&quot;{method.__qualname__} must return None &quot;</span>
<span class="gi">+                        f&quot;or raise an exception, got {type(result)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                    raise _InvalidOutput(msg)</span>
<span class="gi">+            except _InvalidOutput:</span>
<span class="gi">+                raise</span>
<span class="gi">+            except Exception:</span>
<span class="gi">+                return scrape_func(Failure(), request, spider)</span>
<span class="gi">+        return scrape_func(response, request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _evaluate_iterable(</span>
<span class="gi">+        self,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        iterable: Union[Iterable, AsyncIterable],</span>
<span class="gi">+        exception_processor_index: int,</span>
<span class="gi">+        recover_to: Union[MutableChain, MutableAsyncChain],</span>
<span class="gi">+    ) -&gt; Union[Generator, AsyncGenerator]:</span>
<span class="gi">+        def process_sync(iterable: Iterable) -&gt; Generator:</span>
<span class="gi">+            try:</span>
<span class="gi">+                for r in iterable:</span>
<span class="gi">+                    yield r</span>
<span class="gi">+            except Exception as ex:</span>
<span class="gi">+                exception_result = self._process_spider_exception(</span>
<span class="gi">+                    response, spider, Failure(ex), exception_processor_index</span>
<span class="gi">+                )</span>
<span class="gi">+                if isinstance(exception_result, Failure):</span>
<span class="gi">+                    raise</span>
<span class="gi">+                recover_to.extend(exception_result)</span>
<span class="gi">+</span>
<span class="gi">+        async def process_async(iterable: AsyncIterable) -&gt; AsyncGenerator:</span>
<span class="gi">+            try:</span>
<span class="gi">+                async for r in iterable:</span>
<span class="gi">+                    yield r</span>
<span class="gi">+            except Exception as ex:</span>
<span class="gi">+                exception_result = self._process_spider_exception(</span>
<span class="gi">+                    response, spider, Failure(ex), exception_processor_index</span>
<span class="gi">+                )</span>
<span class="gi">+                if isinstance(exception_result, Failure):</span>
<span class="gi">+                    raise</span>
<span class="gi">+                recover_to.extend(exception_result)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(iterable, AsyncIterable):</span>
<span class="gi">+            return process_async(iterable)</span>
<span class="gi">+        return process_sync(iterable)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_spider_exception(</span>
<span class="gi">+        self,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        _failure: Failure,</span>
<span class="gi">+        start_index: int = 0,</span>
<span class="gi">+    ) -&gt; Union[Failure, MutableChain]:</span>
<span class="gi">+        exception = _failure.value</span>
<span class="gi">+        # don&#39;t handle _InvalidOutput exception</span>
<span class="gi">+        if isinstance(exception, _InvalidOutput):</span>
<span class="gi">+            return _failure</span>
<span class="gi">+        method_list = islice(</span>
<span class="gi">+            self.methods[&quot;process_spider_exception&quot;], start_index, None</span>
<span class="gi">+        )</span>
<span class="gi">+        for method_index, method in enumerate(method_list, start=start_index):</span>
<span class="gi">+            if method is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            method = cast(Callable, method)</span>
<span class="gi">+            result = method(response=response, exception=exception, spider=spider)</span>
<span class="gi">+            if _isiterable(result):</span>
<span class="gi">+                # stop exception handling by handing control over to the</span>
<span class="gi">+                # process_spider_output chain if an iterable has been returned</span>
<span class="gi">+                dfd: Deferred = self._process_spider_output(</span>
<span class="gi">+                    response, spider, result, method_index + 1</span>
<span class="gi">+                )</span>
<span class="gi">+                # _process_spider_output() returns a Deferred only because of downgrading so this can be</span>
<span class="gi">+                # simplified when downgrading is removed.</span>
<span class="gi">+                if dfd.called:</span>
<span class="gi">+                    # the result is available immediately if _process_spider_output didn&#39;t do downgrading</span>
<span class="gi">+                    return cast(MutableChain, dfd.result)</span>
<span class="gi">+                # we forbid waiting here because otherwise we would need to return a deferred from</span>
<span class="gi">+                # _process_spider_exception too, which complicates the architecture</span>
<span class="gi">+                msg = f&quot;Async iterable returned from {method.__qualname__} cannot be downgraded&quot;</span>
<span class="gi">+                raise _InvalidOutput(msg)</span>
<span class="gi">+            elif result is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            else:</span>
<span class="gi">+                msg = (</span>
<span class="gi">+                    f&quot;{method.__qualname__} must return None &quot;</span>
<span class="gi">+                    f&quot;or an iterable, got {type(result)}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                raise _InvalidOutput(msg)</span>
<span class="gi">+        return _failure</span>
<span class="gi">+</span>
<span class="gi">+    # This method cannot be made async def, as _process_spider_exception relies on the Deferred result</span>
<span class="gi">+    # being available immediately which doesn&#39;t work when it&#39;s a wrapped coroutine.</span>
<span class="gi">+    # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.</span>
<span class="gi">+    @inlineCallbacks</span>
<span class="gi">+    def _process_spider_output(</span>
<span class="gi">+        self,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        result: Union[Iterable, AsyncIterable],</span>
<span class="gi">+        start_index: int = 0,</span>
<span class="gi">+    ) -&gt; Generator[Deferred, Any, Union[MutableChain, MutableAsyncChain]]:</span>
<span class="gi">+        # items in this iterable do not need to go through the process_spider_output</span>
<span class="gi">+        # chain, they went through it already from the process_spider_exception method</span>
<span class="gi">+        recovered: Union[MutableChain, MutableAsyncChain]</span>
<span class="gi">+        last_result_is_async = isinstance(result, AsyncIterable)</span>
<span class="gi">+        if last_result_is_async:</span>
<span class="gi">+            recovered = MutableAsyncChain()</span>
<span class="gi">+        else:</span>
<span class="gi">+            recovered = MutableChain()</span>
<span class="gi">+</span>
<span class="gi">+        # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.</span>
<span class="gi">+        # 1. def foo. Sync iterables are passed as is, async ones are downgraded.</span>
<span class="gi">+        # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.</span>
<span class="gi">+        # 3. def foo + async def foo_async. Iterables are passed to the respective method.</span>
<span class="gi">+        # Storing methods and method tuples in the same list is weird but we should be able to roll this back</span>
<span class="gi">+        # when we drop this compatibility feature.</span>
<span class="gi">+</span>
<span class="gi">+        method_list = islice(self.methods[&quot;process_spider_output&quot;], start_index, None)</span>
<span class="gi">+        for method_index, method_pair in enumerate(method_list, start=start_index):</span>
<span class="gi">+            if method_pair is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            need_upgrade = need_downgrade = False</span>
<span class="gi">+            if isinstance(method_pair, tuple):</span>
<span class="gi">+                # This tuple handling is only needed until _async compatibility methods are removed.</span>
<span class="gi">+                method_sync, method_async = method_pair</span>
<span class="gi">+                method = method_async if last_result_is_async else method_sync</span>
<span class="gi">+            else:</span>
<span class="gi">+                method = method_pair</span>
<span class="gi">+                if not last_result_is_async and isasyncgenfunction(method):</span>
<span class="gi">+                    need_upgrade = True</span>
<span class="gi">+                elif last_result_is_async and not isasyncgenfunction(method):</span>
<span class="gi">+                    need_downgrade = True</span>
<span class="gi">+            try:</span>
<span class="gi">+                if need_upgrade:</span>
<span class="gi">+                    # Iterable -&gt; AsyncIterable</span>
<span class="gi">+                    result = as_async_generator(result)</span>
<span class="gi">+                elif need_downgrade:</span>
<span class="gi">+                    if not self.downgrade_warning_done:</span>
<span class="gi">+                        logger.warning(</span>
<span class="gi">+                            f&quot;Async iterable passed to {method.__qualname__} &quot;</span>
<span class="gi">+                            f&quot;was downgraded to a non-async one&quot;</span>
<span class="gi">+                        )</span>
<span class="gi">+                        self.downgrade_warning_done = True</span>
<span class="gi">+                    assert isinstance(result, AsyncIterable)</span>
<span class="gi">+                    # AsyncIterable -&gt; Iterable</span>
<span class="gi">+                    result = yield deferred_from_coro(collect_asyncgen(result))</span>
<span class="gi">+                    if isinstance(recovered, AsyncIterable):</span>
<span class="gi">+                        recovered_collected = yield deferred_from_coro(</span>
<span class="gi">+                            collect_asyncgen(recovered)</span>
<span class="gi">+                        )</span>
<span class="gi">+                        recovered = MutableChain(recovered_collected)</span>
<span class="gi">+                # might fail directly if the output value is not a generator</span>
<span class="gi">+                result = method(response=response, result=result, spider=spider)</span>
<span class="gi">+            except Exception as ex:</span>
<span class="gi">+                exception_result = self._process_spider_exception(</span>
<span class="gi">+                    response, spider, Failure(ex), method_index + 1</span>
<span class="gi">+                )</span>
<span class="gi">+                if isinstance(exception_result, Failure):</span>
<span class="gi">+                    raise</span>
<span class="gi">+                return exception_result</span>
<span class="gi">+            if _isiterable(result):</span>
<span class="gi">+                result = self._evaluate_iterable(</span>
<span class="gi">+                    response, spider, result, method_index + 1, recovered</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                if iscoroutine(result):</span>
<span class="gi">+                    result.close()  # Silence warning about not awaiting</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        f&quot;{method.__qualname__} must be an asynchronous &quot;</span>
<span class="gi">+                        f&quot;generator (i.e. use yield)&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        f&quot;{method.__qualname__} must return an iterable, got &quot;</span>
<span class="gi">+                        f&quot;{type(result)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                raise _InvalidOutput(msg)</span>
<span class="gi">+            last_result_is_async = isinstance(result, AsyncIterable)</span>
<span class="gi">+</span>
<span class="gi">+        if last_result_is_async:</span>
<span class="gi">+            return MutableAsyncChain(result, recovered)</span>
<span class="gi">+        return MutableChain(result, recovered)  # type: ignore[arg-type]</span>
<span class="gi">+</span>
<span class="gi">+    async def _process_callback_output(</span>
<span class="gi">+        self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]</span>
<span class="gi">+    ) -&gt; Union[MutableChain, MutableAsyncChain]:</span>
<span class="gi">+        recovered: Union[MutableChain, MutableAsyncChain]</span>
<span class="gi">+        if isinstance(result, AsyncIterable):</span>
<span class="gi">+            recovered = MutableAsyncChain()</span>
<span class="gi">+        else:</span>
<span class="gi">+            recovered = MutableChain()</span>
<span class="gi">+        result = self._evaluate_iterable(response, spider, result, 0, recovered)</span>
<span class="gi">+        result = await maybe_deferred_to_future(</span>
<span class="gi">+            self._process_spider_output(response, spider, result)</span>
<span class="gi">+        )</span>
<span class="gi">+        if isinstance(result, AsyncIterable):</span>
<span class="gi">+            return MutableAsyncChain(result, recovered)</span>
<span class="gi">+        if isinstance(recovered, AsyncIterable):</span>
<span class="gi">+            recovered_collected = await collect_asyncgen(recovered)</span>
<span class="gi">+            recovered = MutableChain(recovered_collected)</span>
<span class="gi">+        return MutableChain(result, recovered)</span>
<span class="gi">+</span>
<span class="gi">+    def scrape_response(</span>
<span class="gi">+        self,</span>
<span class="gi">+        scrape_func: ScrapeFunc,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        async def process_callback_output(</span>
<span class="gi">+            result: Union[Iterable, AsyncIterable]</span>
<span class="gi">+        ) -&gt; Union[MutableChain, MutableAsyncChain]:</span>
<span class="gi">+            return await self._process_callback_output(response, spider, result)</span>
<span class="gi">+</span>
<span class="gi">+        def process_spider_exception(_failure: Failure) -&gt; Union[Failure, MutableChain]:</span>
<span class="gi">+            return self._process_spider_exception(response, spider, _failure)</span>
<span class="gi">+</span>
<span class="gi">+        dfd = mustbe_deferred(</span>
<span class="gi">+            self._process_spider_input, scrape_func, response, request, spider</span>
<span class="gi">+        )</span>
<span class="gi">+        dfd.addCallbacks(</span>
<span class="gi">+            callback=deferred_f_from_coro_f(process_callback_output),</span>
<span class="gi">+            errback=process_spider_exception,</span>
<span class="gi">+        )</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def process_start_requests(</span>
<span class="gi">+        self, start_requests: Iterable[Request], spider: Spider</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        return self._process_chain(&quot;process_start_requests&quot;, start_requests, spider)</span>
<span class="gi">+</span>
<span class="gi">+    # This method is only needed until _async compatibility methods are removed.</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _get_async_method_pair(</span>
<span class="gi">+        mw: Any, methodname: str</span>
<span class="gi">+    ) -&gt; Union[None, Callable, Tuple[Callable, Callable]]:</span>
<span class="gi">+        normal_method: Optional[Callable] = getattr(mw, methodname, None)</span>
<span class="gi">+        methodname_async = methodname + &quot;_async&quot;</span>
<span class="gi">+        async_method: Optional[Callable] = getattr(mw, methodname_async, None)</span>
<span class="gi">+        if not async_method:</span>
<span class="gi">+            return normal_method</span>
<span class="gi">+        if not normal_method:</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                f&quot;Middleware {mw.__qualname__} has {methodname_async} &quot;</span>
<span class="gi">+                f&quot;without {methodname}, skipping this method.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            return None</span>
<span class="gi">+        if not isasyncgenfunction(async_method):</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                f&quot;{async_method.__qualname__} is not &quot;</span>
<span class="gi">+                f&quot;an async generator function, skipping this method.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            return normal_method</span>
<span class="gi">+        if isasyncgenfunction(normal_method):</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                f&quot;{normal_method.__qualname__} is an async &quot;</span>
<span class="gi">+                f&quot;generator function while {methodname_async} exists, &quot;</span>
<span class="gi">+                f&quot;skipping both methods.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            return None</span>
<span class="gi">+        return normal_method, async_method</span>
<span class="gh">diff --git a/scrapy/crawler.py b/scrapy/crawler.py</span>
<span class="gh">index 4271118c2..6f54e62e9 100644</span>
<span class="gd">--- a/scrapy/crawler.py</span>
<span class="gi">+++ b/scrapy/crawler.py</span>
<span class="gu">@@ -1,16 +1,27 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import pprint
<span class="w"> </span>import signal
<span class="w"> </span>import warnings
<span class="w"> </span>from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union, cast
<span class="gd">-from twisted.internet.defer import Deferred, DeferredList, inlineCallbacks, maybeDeferred</span>
<span class="gi">+</span>
<span class="gi">+from twisted.internet.defer import (</span>
<span class="gi">+    Deferred,</span>
<span class="gi">+    DeferredList,</span>
<span class="gi">+    inlineCallbacks,</span>
<span class="gi">+    maybeDeferred,</span>
<span class="gi">+)</span>
<span class="w"> </span>from zope.interface.exceptions import DoesNotImplement
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # zope &gt;= 5.0 only supports MultipleInvalid</span>
<span class="w"> </span>    from zope.interface.exceptions import MultipleInvalid
<span class="w"> </span>except ImportError:
<span class="w"> </span>    MultipleInvalid = None
<span class="gi">+</span>
<span class="w"> </span>from zope.interface.verify import verifyClass
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider, signals
<span class="w"> </span>from scrapy.addons import AddonManager
<span class="w"> </span>from scrapy.core.engine import ExecutionEngine
<span class="gu">@@ -21,33 +32,55 @@ from scrapy.logformatter import LogFormatter</span>
<span class="w"> </span>from scrapy.settings import BaseSettings, Settings, overridden_settings
<span class="w"> </span>from scrapy.signalmanager import SignalManager
<span class="w"> </span>from scrapy.statscollectors import StatsCollector
<span class="gd">-from scrapy.utils.log import LogCounterHandler, configure_logging, get_scrapy_root_handler, install_scrapy_root_handler, log_reactor_info, log_scrapy_info</span>
<span class="gi">+from scrapy.utils.log import (</span>
<span class="gi">+    LogCounterHandler,</span>
<span class="gi">+    configure_logging,</span>
<span class="gi">+    get_scrapy_root_handler,</span>
<span class="gi">+    install_scrapy_root_handler,</span>
<span class="gi">+    log_reactor_info,</span>
<span class="gi">+    log_scrapy_info,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="w"> </span>from scrapy.utils.ossignal import install_shutdown_handlers, signal_names
<span class="gd">-from scrapy.utils.reactor import install_reactor, is_asyncio_reactor_installed, verify_installed_asyncio_event_loop, verify_installed_reactor</span>
<span class="gi">+from scrapy.utils.reactor import (</span>
<span class="gi">+    install_reactor,</span>
<span class="gi">+    is_asyncio_reactor_installed,</span>
<span class="gi">+    verify_installed_asyncio_event_loop,</span>
<span class="gi">+    verify_installed_reactor,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.utils.request import RequestFingerprinter
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class Crawler:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, spidercls: Type[Spider], settings: Union[None, Dict[</span>
<span class="gd">-        str, Any], Settings]=None, init_reactor: bool=False):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        spidercls: Type[Spider],</span>
<span class="gi">+        settings: Union[None, Dict[str, Any], Settings] = None,</span>
<span class="gi">+        init_reactor: bool = False,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if isinstance(spidercls, Spider):
<span class="gd">-            raise ValueError(</span>
<span class="gd">-                &#39;The spidercls argument must be a class, not an object&#39;)</span>
<span class="gi">+            raise ValueError(&quot;The spidercls argument must be a class, not an object&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(settings, dict) or settings is None:
<span class="w"> </span>            settings = Settings(settings)
<span class="gi">+</span>
<span class="w"> </span>        self.spidercls: Type[Spider] = spidercls
<span class="w"> </span>        self.settings: Settings = settings.copy()
<span class="w"> </span>        self.spidercls.update_settings(self.settings)
<span class="w"> </span>        self._update_root_log_handler()
<span class="gi">+</span>
<span class="w"> </span>        self.addons: AddonManager = AddonManager(self)
<span class="w"> </span>        self.signals: SignalManager = SignalManager(self)
<span class="gi">+</span>
<span class="w"> </span>        self._init_reactor: bool = init_reactor
<span class="w"> </span>        self.crawling: bool = False
<span class="w"> </span>        self._started: bool = False
<span class="gi">+</span>
<span class="w"> </span>        self.extensions: Optional[ExtensionManager] = None
<span class="w"> </span>        self.stats: Optional[StatsCollector] = None
<span class="w"> </span>        self.logformatter: Optional[LogFormatter] = None
<span class="gu">@@ -55,11 +88,97 @@ class Crawler:</span>
<span class="w"> </span>        self.spider: Optional[Spider] = None
<span class="w"> </span>        self.engine: Optional[ExecutionEngine] = None

<span class="gi">+    def _update_root_log_handler(self) -&gt; None:</span>
<span class="gi">+        if get_scrapy_root_handler() is not None:</span>
<span class="gi">+            # scrapy root handler already installed: update it with new settings</span>
<span class="gi">+            install_scrapy_root_handler(self.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def _apply_settings(self) -&gt; None:</span>
<span class="gi">+        if self.settings.frozen:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        self.addons.load_settings(self.settings)</span>
<span class="gi">+        self.stats = load_object(self.settings[&quot;STATS_CLASS&quot;])(self)</span>
<span class="gi">+</span>
<span class="gi">+        handler = LogCounterHandler(self, level=self.settings.get(&quot;LOG_LEVEL&quot;))</span>
<span class="gi">+        logging.root.addHandler(handler)</span>
<span class="gi">+        # lambda is assigned to Crawler attribute because this way it is not</span>
<span class="gi">+        # garbage collected after leaving the scope</span>
<span class="gi">+        self.__remove_handler = lambda: logging.root.removeHandler(handler)</span>
<span class="gi">+        self.signals.connect(self.__remove_handler, signals.engine_stopped)</span>
<span class="gi">+</span>
<span class="gi">+        lf_cls: Type[LogFormatter] = load_object(self.settings[&quot;LOG_FORMATTER&quot;])</span>
<span class="gi">+        self.logformatter = lf_cls.from_crawler(self)</span>
<span class="gi">+</span>
<span class="gi">+        self.request_fingerprinter = create_instance(</span>
<span class="gi">+            load_object(self.settings[&quot;REQUEST_FINGERPRINTER_CLASS&quot;]),</span>
<span class="gi">+            settings=self.settings,</span>
<span class="gi">+            crawler=self,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        reactor_class: str = self.settings[&quot;TWISTED_REACTOR&quot;]</span>
<span class="gi">+        event_loop: str = self.settings[&quot;ASYNCIO_EVENT_LOOP&quot;]</span>
<span class="gi">+        if self._init_reactor:</span>
<span class="gi">+            # this needs to be done after the spider settings are merged,</span>
<span class="gi">+            # but before something imports twisted.internet.reactor</span>
<span class="gi">+            if reactor_class:</span>
<span class="gi">+                install_reactor(reactor_class, event_loop)</span>
<span class="gi">+            else:</span>
<span class="gi">+                from twisted.internet import reactor  # noqa: F401</span>
<span class="gi">+            log_reactor_info()</span>
<span class="gi">+        if reactor_class:</span>
<span class="gi">+            verify_installed_reactor(reactor_class)</span>
<span class="gi">+            if is_asyncio_reactor_installed() and event_loop:</span>
<span class="gi">+                verify_installed_asyncio_event_loop(event_loop)</span>
<span class="gi">+</span>
<span class="gi">+        self.extensions = ExtensionManager.from_crawler(self)</span>
<span class="gi">+        self.settings.freeze()</span>
<span class="gi">+</span>
<span class="gi">+        d = dict(overridden_settings(self.settings))</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Overridden settings:\n%(settings)s&quot;, {&quot;settings&quot;: pprint.pformat(d)}</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @inlineCallbacks</span>
<span class="gi">+    def crawl(self, *args: Any, **kwargs: Any) -&gt; Generator[Deferred, Any, None]:</span>
<span class="gi">+        if self.crawling:</span>
<span class="gi">+            raise RuntimeError(&quot;Crawling already taking place&quot;)</span>
<span class="gi">+        if self._started:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;Running Crawler.crawl() more than once is deprecated.&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+        self.crawling = self._started = True</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            self.spider = self._create_spider(*args, **kwargs)</span>
<span class="gi">+            self._apply_settings()</span>
<span class="gi">+            self._update_root_log_handler()</span>
<span class="gi">+            self.engine = self._create_engine()</span>
<span class="gi">+            start_requests = iter(self.spider.start_requests())</span>
<span class="gi">+            yield self.engine.open_spider(self.spider, start_requests)</span>
<span class="gi">+            yield maybeDeferred(self.engine.start)</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            self.crawling = False</span>
<span class="gi">+            if self.engine is not None:</span>
<span class="gi">+                yield self.engine.close()</span>
<span class="gi">+            raise</span>
<span class="gi">+</span>
<span class="gi">+    def _create_spider(self, *args: Any, **kwargs: Any) -&gt; Spider:</span>
<span class="gi">+        return self.spidercls.from_crawler(self, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def _create_engine(self) -&gt; ExecutionEngine:</span>
<span class="gi">+        return ExecutionEngine(self, lambda _: self.stop())</span>
<span class="gi">+</span>
<span class="w"> </span>    @inlineCallbacks
<span class="gd">-    def stop(self) -&gt;Generator[Deferred, Any, None]:</span>
<span class="gi">+    def stop(self) -&gt; Generator[Deferred, Any, None]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Starts a graceful stop of the crawler and returns a deferred that is
<span class="w"> </span>        fired when the crawler is stopped.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.crawling:</span>
<span class="gi">+            self.crawling = False</span>
<span class="gi">+            assert self.engine</span>
<span class="gi">+            yield maybeDeferred(self.engine.stop)</span>


<span class="w"> </span>class CrawlerRunner:
<span class="gu">@@ -74,16 +193,34 @@ class CrawlerRunner:</span>
<span class="w"> </span>    accordingly) unless writing scripts that manually handle the crawling
<span class="w"> </span>    process. See :ref:`run-from-script` for an example.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    crawlers = property(lambda self: self._crawlers, doc=</span>
<span class="gd">-        &#39;Set of :class:`crawlers &lt;scrapy.crawler.Crawler&gt;` started by :meth:`crawl` and managed by this class.&#39;</span>
<span class="gd">-        )</span>
<span class="gi">+</span>
<span class="gi">+    crawlers = property(</span>
<span class="gi">+        lambda self: self._crawlers,</span>
<span class="gi">+        doc=&quot;Set of :class:`crawlers &lt;scrapy.crawler.Crawler&gt;` started by &quot;</span>
<span class="gi">+        &quot;:meth:`crawl` and managed by this class.&quot;,</span>
<span class="gi">+    )</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def _get_spider_loader(settings: BaseSettings):
<span class="w"> </span>        &quot;&quot;&quot;Get SpiderLoader instance from settings&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, settings: Union[Dict[str, Any], Settings, None]=None):</span>
<span class="gi">+        cls_path = settings.get(&quot;SPIDER_LOADER_CLASS&quot;)</span>
<span class="gi">+        loader_cls = load_object(cls_path)</span>
<span class="gi">+        excs = (</span>
<span class="gi">+            (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement</span>
<span class="gi">+        )</span>
<span class="gi">+        try:</span>
<span class="gi">+            verifyClass(ISpiderLoader, loader_cls)</span>
<span class="gi">+        except excs:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does &quot;</span>
<span class="gi">+                &quot;not fully implement scrapy.interfaces.ISpiderLoader interface. &quot;</span>
<span class="gi">+                &quot;Please add all missing methods to avoid unexpected runtime errors.&quot;,</span>
<span class="gi">+                category=ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+        return loader_cls.from_settings(settings.frozencopy())</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):</span>
<span class="w"> </span>        if isinstance(settings, dict) or settings is None:
<span class="w"> </span>            settings = Settings(settings)
<span class="w"> </span>        self.settings = settings
<span class="gu">@@ -92,8 +229,12 @@ class CrawlerRunner:</span>
<span class="w"> </span>        self._active: Set[Deferred] = set()
<span class="w"> </span>        self.bootstrap_failed = False

<span class="gd">-    def crawl(self, crawler_or_spidercls: Union[Type[Spider], str, Crawler],</span>
<span class="gd">-        *args: Any, **kwargs: Any) -&gt;Deferred:</span>
<span class="gi">+    def crawl(</span>
<span class="gi">+        self,</span>
<span class="gi">+        crawler_or_spidercls: Union[Type[Spider], str, Crawler],</span>
<span class="gi">+        *args: Any,</span>
<span class="gi">+        **kwargs: Any,</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Run a crawler with the provided arguments.

<span class="gu">@@ -115,10 +256,30 @@ class CrawlerRunner:</span>

<span class="w"> </span>        :param kwargs: keyword arguments to initialize the spider
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def create_crawler(self, crawler_or_spidercls: Union[Type[Spider], str,</span>
<span class="gd">-        Crawler]) -&gt;Crawler:</span>
<span class="gi">+        if isinstance(crawler_or_spidercls, Spider):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;The crawler_or_spidercls argument cannot be a spider object, &quot;</span>
<span class="gi">+                &quot;it must be a spider class (or a Crawler object)&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        crawler = self.create_crawler(crawler_or_spidercls)</span>
<span class="gi">+        return self._crawl(crawler, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -&gt; Deferred:</span>
<span class="gi">+        self.crawlers.add(crawler)</span>
<span class="gi">+        d = crawler.crawl(*args, **kwargs)</span>
<span class="gi">+        self._active.add(d)</span>
<span class="gi">+</span>
<span class="gi">+        def _done(result: Any) -&gt; Any:</span>
<span class="gi">+            self.crawlers.discard(crawler)</span>
<span class="gi">+            self._active.discard(d)</span>
<span class="gi">+            self.bootstrap_failed |= not getattr(crawler, &quot;spider&quot;, None)</span>
<span class="gi">+            return result</span>
<span class="gi">+</span>
<span class="gi">+        return d.addBoth(_done)</span>
<span class="gi">+</span>
<span class="gi">+    def create_crawler(</span>
<span class="gi">+        self, crawler_or_spidercls: Union[Type[Spider], str, Crawler]</span>
<span class="gi">+    ) -&gt; Crawler:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return a :class:`~scrapy.crawler.Crawler` object.

<span class="gu">@@ -129,25 +290,39 @@ class CrawlerRunner:</span>
<span class="w"> </span>          a spider with this name in a Scrapy project (using spider loader),
<span class="w"> </span>          then creates a Crawler instance for it.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def stop(self) -&gt;Deferred:</span>
<span class="gi">+        if isinstance(crawler_or_spidercls, Spider):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;The crawler_or_spidercls argument cannot be a spider object, &quot;</span>
<span class="gi">+                &quot;it must be a spider class (or a Crawler object)&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        if isinstance(crawler_or_spidercls, Crawler):</span>
<span class="gi">+            return crawler_or_spidercls</span>
<span class="gi">+        return self._create_crawler(crawler_or_spidercls)</span>
<span class="gi">+</span>
<span class="gi">+    def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -&gt; Crawler:</span>
<span class="gi">+        if isinstance(spidercls, str):</span>
<span class="gi">+            spidercls = self.spider_loader.load(spidercls)</span>
<span class="gi">+        # temporary cast until self.spider_loader is typed</span>
<span class="gi">+        return Crawler(cast(Type[Spider], spidercls), self.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def stop(self) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Stops simultaneously all the crawling jobs taking place.

<span class="w"> </span>        Returns a deferred that is fired when they all have ended.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return DeferredList([c.stop() for c in list(self.crawlers)])</span>

<span class="w"> </span>    @inlineCallbacks
<span class="gd">-    def join(self) -&gt;Generator[Deferred, Any, None]:</span>
<span class="gi">+    def join(self) -&gt; Generator[Deferred, Any, None]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        join()

<span class="w"> </span>        Returns a deferred that is fired when all managed :attr:`crawlers` have
<span class="w"> </span>        completed their executions.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        while self._active:</span>
<span class="gi">+            yield DeferredList(self._active)</span>


<span class="w"> </span>class CrawlerProcess(CrawlerRunner):
<span class="gu">@@ -174,15 +349,50 @@ class CrawlerProcess(CrawlerRunner):</span>
<span class="w"> </span>    process. See :ref:`run-from-script` for an example.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, settings: Union[Dict[str, Any], Settings, None]=None,</span>
<span class="gd">-        install_root_handler: bool=True):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        settings: Union[Dict[str, Any], Settings, None] = None,</span>
<span class="gi">+        install_root_handler: bool = True,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(settings)
<span class="w"> </span>        configure_logging(self.settings, install_root_handler)
<span class="w"> </span>        log_scrapy_info(self.settings)
<span class="w"> </span>        self._initialized_reactor = False

<span class="gd">-    def start(self, stop_after_crawl: bool=True, install_signal_handlers:</span>
<span class="gd">-        bool=True) -&gt;None:</span>
<span class="gi">+    def _signal_shutdown(self, signum: int, _: Any) -&gt; None:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        install_shutdown_handlers(self._signal_kill)</span>
<span class="gi">+        signame = signal_names[signum]</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Received %(signame)s, shutting down gracefully. Send again to force &quot;,</span>
<span class="gi">+            {&quot;signame&quot;: signame},</span>
<span class="gi">+        )</span>
<span class="gi">+        reactor.callFromThread(self._graceful_stop_reactor)</span>
<span class="gi">+</span>
<span class="gi">+    def _signal_kill(self, signum: int, _: Any) -&gt; None:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        install_shutdown_handlers(signal.SIG_IGN)</span>
<span class="gi">+        signame = signal_names[signum]</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Received %(signame)s twice, forcing unclean shutdown&quot;, {&quot;signame&quot;: signame}</span>
<span class="gi">+        )</span>
<span class="gi">+        reactor.callFromThread(self._stop_reactor)</span>
<span class="gi">+</span>
<span class="gi">+    def _create_crawler(self, spidercls: Union[Type[Spider], str]) -&gt; Crawler:</span>
<span class="gi">+        if isinstance(spidercls, str):</span>
<span class="gi">+            spidercls = self.spider_loader.load(spidercls)</span>
<span class="gi">+        init_reactor = not self._initialized_reactor</span>
<span class="gi">+        self._initialized_reactor = True</span>
<span class="gi">+        # temporary cast until self.spider_loader is typed</span>
<span class="gi">+        return Crawler(</span>
<span class="gi">+            cast(Type[Spider], spidercls), self.settings, init_reactor=init_reactor</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def start(</span>
<span class="gi">+        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool
<span class="w"> </span>        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache
<span class="gu">@@ -197,4 +407,36 @@ class CrawlerProcess(CrawlerRunner):</span>
<span class="w"> </span>        :param bool install_signal_handlers: whether to install the OS signal
<span class="w"> </span>            handlers from Twisted and Scrapy (default: True)
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        if stop_after_crawl:</span>
<span class="gi">+            d = self.join()</span>
<span class="gi">+            # Don&#39;t start the reactor if the deferreds are already fired</span>
<span class="gi">+            if d.called:</span>
<span class="gi">+                return</span>
<span class="gi">+            d.addBoth(self._stop_reactor)</span>
<span class="gi">+</span>
<span class="gi">+        resolver_class = load_object(self.settings[&quot;DNS_RESOLVER&quot;])</span>
<span class="gi">+        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)</span>
<span class="gi">+        resolver.install_on_reactor()</span>
<span class="gi">+        tp = reactor.getThreadPool()</span>
<span class="gi">+        tp.adjustPoolsize(maxthreads=self.settings.getint(&quot;REACTOR_THREADPOOL_MAXSIZE&quot;))</span>
<span class="gi">+        reactor.addSystemEventTrigger(&quot;before&quot;, &quot;shutdown&quot;, self.stop)</span>
<span class="gi">+        if install_signal_handlers:</span>
<span class="gi">+            reactor.addSystemEventTrigger(</span>
<span class="gi">+                &quot;after&quot;, &quot;startup&quot;, install_shutdown_handlers, self._signal_shutdown</span>
<span class="gi">+            )</span>
<span class="gi">+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call</span>
<span class="gi">+</span>
<span class="gi">+    def _graceful_stop_reactor(self) -&gt; Deferred:</span>
<span class="gi">+        d = self.stop()</span>
<span class="gi">+        d.addBoth(self._stop_reactor)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _stop_reactor(self, _: Any = None) -&gt; None:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            reactor.stop()</span>
<span class="gi">+        except RuntimeError:  # raised if already stopped or in shutdown stage</span>
<span class="gi">+            pass</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/ajaxcrawl.py b/scrapy/downloadermiddlewares/ajaxcrawl.py</span>
<span class="gh">index c8df05f69..04ae719de 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/ajaxcrawl.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/ajaxcrawl.py</span>
<span class="gu">@@ -1,8 +1,11 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="gi">+</span>
<span class="w"> </span>from w3lib import html
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.http import HtmlResponse
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -13,20 +16,57 @@ class AjaxCrawlMiddleware:</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        if not settings.getbool(&#39;AJAXCRAWL_ENABLED&#39;):</span>
<span class="gi">+        if not settings.getbool(&quot;AJAXCRAWL_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self.lookup_bytes = settings.getint(&#39;AJAXCRAWL_MAXSIZE&#39;, 32768)</span>
<span class="gi">+</span>
<span class="gi">+        # XXX: Google parses at least first 100k bytes; scrapy&#39;s redirect</span>
<span class="gi">+        # middleware parses first 4k. 4k turns out to be insufficient</span>
<span class="gi">+        # for this middleware, and parsing 100k could be slow.</span>
<span class="gi">+        # We use something in between (32K) by default.</span>
<span class="gi">+        self.lookup_bytes = settings.getint(&quot;AJAXCRAWL_MAXSIZE&quot;, 32768)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if not isinstance(response, HtmlResponse) or response.status != 200:</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        if request.method != &quot;GET&quot;:</span>
<span class="gi">+            # other HTTP methods are either not safe or don&#39;t have a body</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        if &quot;ajax_crawlable&quot; in request.meta:  # prevent loops</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        if not self._has_ajax_crawlable_variant(response):</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        # scrapy already handles #! links properly</span>
<span class="gi">+        ajax_crawl_request = request.replace(url=request.url + &quot;#!&quot;)</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s&quot;,</span>
<span class="gi">+            {&quot;ajax_crawl_request&quot;: ajax_crawl_request, &quot;request&quot;: request},</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        ajax_crawl_request.meta[&quot;ajax_crawlable&quot;] = True</span>
<span class="gi">+        return ajax_crawl_request</span>

<span class="w"> </span>    def _has_ajax_crawlable_variant(self, response):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return True if a page without hash fragment could be &quot;AJAX crawlable&quot;
<span class="w"> </span>        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        body = response.text[: self.lookup_bytes]</span>
<span class="gi">+        return _has_ajaxcrawlable_meta(body)</span>


<span class="gi">+# XXX: move it to w3lib?</span>
<span class="w"> </span>_ajax_crawlable_re = re.compile(
<span class="gd">-    &#39;&lt;meta\\s+name=[&quot;\\\&#39;]fragment[&quot;\\\&#39;]\\s+content=[&quot;\\\&#39;]![&quot;\\\&#39;]/?&gt;&#39;)</span>
<span class="gi">+    r&#39;&lt;meta\s+name=[&quot;\&#39;]fragment[&quot;\&#39;]\s+content=[&quot;\&#39;]![&quot;\&#39;]/?&gt;&#39;</span>
<span class="gi">+)</span>


<span class="w"> </span>def _has_ajaxcrawlable_meta(text):
<span class="gu">@@ -40,4 +80,16 @@ def _has_ajaxcrawlable_meta(text):</span>
<span class="w"> </span>    &gt;&gt;&gt; _has_ajaxcrawlable_meta(&#39;&lt;html&gt;&lt;/html&gt;&#39;)
<span class="w"> </span>    False
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    # Stripping scripts and comments is slow (about 20x slower than</span>
<span class="gi">+    # just checking if a string is in text); this is a quick fail-fast</span>
<span class="gi">+    # path that should work for most pages.</span>
<span class="gi">+    if &quot;fragment&quot; not in text:</span>
<span class="gi">+        return False</span>
<span class="gi">+    if &quot;content&quot; not in text:</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    text = html.remove_tags_with_content(text, (&quot;script&quot;, &quot;noscript&quot;))</span>
<span class="gi">+    text = html.replace_entities(text)</span>
<span class="gi">+    text = html.remove_comments(text)</span>
<span class="gi">+    return _ajax_crawlable_re.search(text) is not None</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/cookies.py b/scrapy/downloadermiddlewares/cookies.py</span>
<span class="gh">index 3b97e0653..6495157d7 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/cookies.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/cookies.py</span>
<span class="gu">@@ -1,15 +1,25 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>from collections import defaultdict
<span class="gi">+</span>
<span class="w"> </span>from tldextract import TLDExtract
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.http import Response
<span class="w"> </span>from scrapy.http.cookies import CookieJar
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>_split_domain = TLDExtract(include_psl_private_domains=True)


<span class="gi">+def _is_public_domain(domain):</span>
<span class="gi">+    parts = _split_domain(domain)</span>
<span class="gi">+    return not parts.domain</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class CookiesMiddleware:
<span class="w"> </span>    &quot;&quot;&quot;This middleware enables working with sites that need cookies&quot;&quot;&quot;

<span class="gu">@@ -17,15 +27,118 @@ class CookiesMiddleware:</span>
<span class="w"> </span>        self.jars = defaultdict(CookieJar)
<span class="w"> </span>        self.debug = debug

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;COOKIES_ENABLED&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        return cls(crawler.settings.getbool(&quot;COOKIES_DEBUG&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def _process_cookies(self, cookies, *, jar, request):</span>
<span class="gi">+        for cookie in cookies:</span>
<span class="gi">+            cookie_domain = cookie.domain</span>
<span class="gi">+            if cookie_domain.startswith(&quot;.&quot;):</span>
<span class="gi">+                cookie_domain = cookie_domain[1:]</span>
<span class="gi">+</span>
<span class="gi">+            request_domain = urlparse_cached(request).hostname.lower()</span>
<span class="gi">+</span>
<span class="gi">+            if cookie_domain and _is_public_domain(cookie_domain):</span>
<span class="gi">+                if cookie_domain != request_domain:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                cookie.domain = request_domain</span>
<span class="gi">+</span>
<span class="gi">+            jar.set_cookie_if_ok(cookie, request)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        if request.meta.get(&quot;dont_merge_cookies&quot;, False):</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        cookiejarkey = request.meta.get(&quot;cookiejar&quot;)</span>
<span class="gi">+        jar = self.jars[cookiejarkey]</span>
<span class="gi">+        cookies = self._get_request_cookies(jar, request)</span>
<span class="gi">+        self._process_cookies(cookies, jar=jar, request=request)</span>
<span class="gi">+</span>
<span class="gi">+        # set Cookie header</span>
<span class="gi">+        request.headers.pop(&quot;Cookie&quot;, None)</span>
<span class="gi">+        jar.add_cookie_header(request)</span>
<span class="gi">+        self._debug_cookie(request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if request.meta.get(&quot;dont_merge_cookies&quot;, False):</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        # extract cookies from Set-Cookie and drop invalid/expired cookies</span>
<span class="gi">+        cookiejarkey = request.meta.get(&quot;cookiejar&quot;)</span>
<span class="gi">+        jar = self.jars[cookiejarkey]</span>
<span class="gi">+        cookies = jar.make_cookies(response, request)</span>
<span class="gi">+        self._process_cookies(cookies, jar=jar, request=request)</span>
<span class="gi">+</span>
<span class="gi">+        self._debug_set_cookie(response, spider)</span>
<span class="gi">+</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def _debug_cookie(self, request, spider):</span>
<span class="gi">+        if self.debug:</span>
<span class="gi">+            cl = [</span>
<span class="gi">+                to_unicode(c, errors=&quot;replace&quot;)</span>
<span class="gi">+                for c in request.headers.getlist(&quot;Cookie&quot;)</span>
<span class="gi">+            ]</span>
<span class="gi">+            if cl:</span>
<span class="gi">+                cookies = &quot;\n&quot;.join(f&quot;Cookie: {c}\n&quot; for c in cl)</span>
<span class="gi">+                msg = f&quot;Sending cookies to: {request}\n{cookies}&quot;</span>
<span class="gi">+                logger.debug(msg, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+</span>
<span class="gi">+    def _debug_set_cookie(self, response, spider):</span>
<span class="gi">+        if self.debug:</span>
<span class="gi">+            cl = [</span>
<span class="gi">+                to_unicode(c, errors=&quot;replace&quot;)</span>
<span class="gi">+                for c in response.headers.getlist(&quot;Set-Cookie&quot;)</span>
<span class="gi">+            ]</span>
<span class="gi">+            if cl:</span>
<span class="gi">+                cookies = &quot;\n&quot;.join(f&quot;Set-Cookie: {c}\n&quot; for c in cl)</span>
<span class="gi">+                msg = f&quot;Received cookies from: {response}\n{cookies}&quot;</span>
<span class="gi">+                logger.debug(msg, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+</span>
<span class="w"> </span>    def _format_cookie(self, cookie, request):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Given a dict consisting of cookie components, return its string representation.
<span class="w"> </span>        Decode from bytes if necessary.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        decoded = {}</span>
<span class="gi">+        for key in (&quot;name&quot;, &quot;value&quot;, &quot;path&quot;, &quot;domain&quot;):</span>
<span class="gi">+            if cookie.get(key) is None:</span>
<span class="gi">+                if key in (&quot;name&quot;, &quot;value&quot;):</span>
<span class="gi">+                    msg = f&quot;Invalid cookie found in request {request}: {cookie} (&#39;{key}&#39; is missing)&quot;</span>
<span class="gi">+                    logger.warning(msg)</span>
<span class="gi">+                    return</span>
<span class="gi">+                continue</span>
<span class="gi">+            if isinstance(cookie[key], (bool, float, int, str)):</span>
<span class="gi">+                decoded[key] = str(cookie[key])</span>
<span class="gi">+            else:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    decoded[key] = cookie[key].decode(&quot;utf8&quot;)</span>
<span class="gi">+                except UnicodeDecodeError:</span>
<span class="gi">+                    logger.warning(</span>
<span class="gi">+                        &quot;Non UTF-8 encoded cookie found in request %s: %s&quot;,</span>
<span class="gi">+                        request,</span>
<span class="gi">+                        cookie,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    decoded[key] = cookie[key].decode(&quot;latin1&quot;, errors=&quot;replace&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        cookie_str = f&quot;{decoded.pop(&#39;name&#39;)}={decoded.pop(&#39;value&#39;)}&quot;</span>
<span class="gi">+        for key, value in decoded.items():  # path, domain</span>
<span class="gi">+            cookie_str += f&quot;; {key.capitalize()}={value}&quot;</span>
<span class="gi">+        return cookie_str</span>

<span class="w"> </span>    def _get_request_cookies(self, jar, request):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Extract cookies from the Request.cookies attribute
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not request.cookies:</span>
<span class="gi">+            return []</span>
<span class="gi">+        if isinstance(request.cookies, dict):</span>
<span class="gi">+            cookies = ({&quot;name&quot;: k, &quot;value&quot;: v} for k, v in request.cookies.items())</span>
<span class="gi">+        else:</span>
<span class="gi">+            cookies = request.cookies</span>
<span class="gi">+        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))</span>
<span class="gi">+        response = Response(request.url, headers={&quot;Set-Cookie&quot;: formatted})</span>
<span class="gi">+        return jar.make_cookies(response, request)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/defaultheaders.py b/scrapy/downloadermiddlewares/defaultheaders.py</span>
<span class="gh">index d104ee821..cdacc7368 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/defaultheaders.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/defaultheaders.py</span>
<span class="gu">@@ -3,10 +3,19 @@ DefaultHeaders downloader middleware</span>

<span class="w"> </span>See documentation in docs/topics/downloader-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.python import without_none_values


<span class="w"> </span>class DefaultHeadersMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, headers):
<span class="w"> </span>        self._headers = headers
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        headers = without_none_values(crawler.settings[&quot;DEFAULT_REQUEST_HEADERS&quot;])</span>
<span class="gi">+        return cls(headers.items())</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        for k, v in self._headers:</span>
<span class="gi">+            request.headers.setdefault(k, v)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/downloadtimeout.py b/scrapy/downloadermiddlewares/downloadtimeout.py</span>
<span class="gh">index 222bc5eb8..a926ecf56 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/downloadtimeout.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/downloadtimeout.py</span>
<span class="gu">@@ -3,10 +3,23 @@ Download timeout middleware</span>

<span class="w"> </span>See documentation in docs/topics/downloader-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals


<span class="w"> </span>class DownloadTimeoutMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, timeout=180):
<span class="w"> </span>        self._timeout = timeout
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls(crawler.settings.getfloat(&quot;DOWNLOAD_TIMEOUT&quot;))</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self._timeout = getattr(spider, &quot;download_timeout&quot;, self._timeout)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        if self._timeout:</span>
<span class="gi">+            request.meta.setdefault(&quot;download_timeout&quot;, self._timeout)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/httpauth.py b/scrapy/downloadermiddlewares/httpauth.py</span>
<span class="gh">index ec0f5cc32..de5a81388 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/httpauth.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/httpauth.py</span>
<span class="gu">@@ -4,7 +4,9 @@ HTTP basic auth downloader middleware</span>
<span class="w"> </span>See documentation in docs/topics/downloader-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>from w3lib.http import basic_auth_header
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="gu">@@ -14,3 +16,37 @@ from scrapy.utils.url import url_is_from_any_domain</span>
<span class="w"> </span>class HttpAuthMiddleware:
<span class="w"> </span>    &quot;&quot;&quot;Set Basic HTTP Authorization header
<span class="w"> </span>    (http_user and http_pass spider class attributes)&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls()</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        usr = getattr(spider, &quot;http_user&quot;, &quot;&quot;)</span>
<span class="gi">+        pwd = getattr(spider, &quot;http_pass&quot;, &quot;&quot;)</span>
<span class="gi">+        if usr or pwd:</span>
<span class="gi">+            self.auth = basic_auth_header(usr, pwd)</span>
<span class="gi">+            if not hasattr(spider, &quot;http_auth_domain&quot;):</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &quot;Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security &quot;</span>
<span class="gi">+                    &quot;problems if the spider makes requests to several different domains. http_auth_domain &quot;</span>
<span class="gi">+                    &quot;will be set to the domain of the first request, please set it to the correct value &quot;</span>
<span class="gi">+                    &quot;explicitly.&quot;,</span>
<span class="gi">+                    category=ScrapyDeprecationWarning,</span>
<span class="gi">+                )</span>
<span class="gi">+                self.domain_unset = True</span>
<span class="gi">+            else:</span>
<span class="gi">+                self.domain = spider.http_auth_domain</span>
<span class="gi">+                self.domain_unset = False</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        auth = getattr(self, &quot;auth&quot;, None)</span>
<span class="gi">+        if auth and b&quot;Authorization&quot; not in request.headers:</span>
<span class="gi">+            domain = urlparse_cached(request).hostname</span>
<span class="gi">+            if self.domain_unset:</span>
<span class="gi">+                self.domain = domain</span>
<span class="gi">+                self.domain_unset = False</span>
<span class="gi">+            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):</span>
<span class="gi">+                request.headers[b&quot;Authorization&quot;] = auth</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/httpcache.py b/scrapy/downloadermiddlewares/httpcache.py</span>
<span class="gh">index 2ffedebca..a521cde7a 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/httpcache.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/httpcache.py</span>
<span class="gu">@@ -1,8 +1,18 @@</span>
<span class="w"> </span>from email.utils import formatdate
<span class="w"> </span>from typing import Optional, Type, TypeVar
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer
<span class="gd">-from twisted.internet.error import ConnectError, ConnectionDone, ConnectionLost, ConnectionRefusedError, DNSLookupError, TCPTimedOutError, TimeoutError</span>
<span class="gi">+from twisted.internet.error import (</span>
<span class="gi">+    ConnectError,</span>
<span class="gi">+    ConnectionDone,</span>
<span class="gi">+    ConnectionLost,</span>
<span class="gi">+    ConnectionRefusedError,</span>
<span class="gi">+    DNSLookupError,</span>
<span class="gi">+    TCPTimedOutError,</span>
<span class="gi">+    TimeoutError,</span>
<span class="gi">+)</span>
<span class="w"> </span>from twisted.web.client import ResponseFailed
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.crawler import Crawler
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest, NotConfigured
<span class="gu">@@ -12,19 +22,129 @@ from scrapy.settings import Settings</span>
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.statscollectors import StatsCollector
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="gd">-HttpCacheMiddlewareTV = TypeVar(&#39;HttpCacheMiddlewareTV&#39;, bound=</span>
<span class="gd">-    &#39;HttpCacheMiddleware&#39;)</span>
<span class="gi">+</span>
<span class="gi">+HttpCacheMiddlewareTV = TypeVar(&quot;HttpCacheMiddlewareTV&quot;, bound=&quot;HttpCacheMiddleware&quot;)</span>


<span class="w"> </span>class HttpCacheMiddleware:
<span class="gd">-    DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,</span>
<span class="gd">-        ConnectionRefusedError, ConnectionDone, ConnectError,</span>
<span class="gd">-        ConnectionLost, TCPTimedOutError, ResponseFailed, OSError)</span>
<span class="gi">+    DOWNLOAD_EXCEPTIONS = (</span>
<span class="gi">+        defer.TimeoutError,</span>
<span class="gi">+        TimeoutError,</span>
<span class="gi">+        DNSLookupError,</span>
<span class="gi">+        ConnectionRefusedError,</span>
<span class="gi">+        ConnectionDone,</span>
<span class="gi">+        ConnectError,</span>
<span class="gi">+        ConnectionLost,</span>
<span class="gi">+        TCPTimedOutError,</span>
<span class="gi">+        ResponseFailed,</span>
<span class="gi">+        OSError,</span>
<span class="gi">+    )</span>

<span class="gd">-    def __init__(self, settings: Settings, stats: StatsCollector) -&gt;None:</span>
<span class="gd">-        if not settings.getbool(&#39;HTTPCACHE_ENABLED&#39;):</span>
<span class="gi">+    def __init__(self, settings: Settings, stats: StatsCollector) -&gt; None:</span>
<span class="gi">+        if not settings.getbool(&quot;HTTPCACHE_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self.policy = load_object(settings[&#39;HTTPCACHE_POLICY&#39;])(settings)</span>
<span class="gd">-        self.storage = load_object(settings[&#39;HTTPCACHE_STORAGE&#39;])(settings)</span>
<span class="gd">-        self.ignore_missing = settings.getbool(&#39;HTTPCACHE_IGNORE_MISSING&#39;)</span>
<span class="gi">+        self.policy = load_object(settings[&quot;HTTPCACHE_POLICY&quot;])(settings)</span>
<span class="gi">+        self.storage = load_object(settings[&quot;HTTPCACHE_STORAGE&quot;])(settings)</span>
<span class="gi">+        self.ignore_missing = settings.getbool(&quot;HTTPCACHE_IGNORE_MISSING&quot;)</span>
<span class="w"> </span>        self.stats = stats
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(</span>
<span class="gi">+        cls: Type[HttpCacheMiddlewareTV], crawler: Crawler</span>
<span class="gi">+    ) -&gt; HttpCacheMiddlewareTV:</span>
<span class="gi">+        assert crawler.stats</span>
<span class="gi">+        o = cls(crawler.settings, crawler.stats)</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider: Spider) -&gt; None:</span>
<span class="gi">+        self.storage.open_spider(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider: Spider) -&gt; None:</span>
<span class="gi">+        self.storage.close_spider(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request: Request, spider: Spider) -&gt; Optional[Response]:</span>
<span class="gi">+        if request.meta.get(&quot;dont_cache&quot;, False):</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        # Skip uncacheable requests</span>
<span class="gi">+        if not self.policy.should_cache_request(request):</span>
<span class="gi">+            request.meta[&quot;_dont_cache&quot;] = True  # flag as uncacheable</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        # Look for cached response and check if expired</span>
<span class="gi">+        cachedresponse = self.storage.retrieve_response(spider, request)</span>
<span class="gi">+        if cachedresponse is None:</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/miss&quot;, spider=spider)</span>
<span class="gi">+            if self.ignore_missing:</span>
<span class="gi">+                self.stats.inc_value(&quot;httpcache/ignore&quot;, spider=spider)</span>
<span class="gi">+                raise IgnoreRequest(f&quot;Ignored request not in cache: {request}&quot;)</span>
<span class="gi">+            return None  # first time request</span>
<span class="gi">+</span>
<span class="gi">+        # Return cached response only if not expired</span>
<span class="gi">+        cachedresponse.flags.append(&quot;cached&quot;)</span>
<span class="gi">+        if self.policy.is_cached_response_fresh(cachedresponse, request):</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/hit&quot;, spider=spider)</span>
<span class="gi">+            return cachedresponse</span>
<span class="gi">+</span>
<span class="gi">+        # Keep a reference to cached response to avoid a second cache lookup on</span>
<span class="gi">+        # process_response hook</span>
<span class="gi">+        request.meta[&quot;cached_response&quot;] = cachedresponse</span>
<span class="gi">+</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(</span>
<span class="gi">+        self, request: Request, response: Response, spider: Spider</span>
<span class="gi">+    ) -&gt; Response:</span>
<span class="gi">+        if request.meta.get(&quot;dont_cache&quot;, False):</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        # Skip cached responses and uncacheable requests</span>
<span class="gi">+        if &quot;cached&quot; in response.flags or &quot;_dont_cache&quot; in request.meta:</span>
<span class="gi">+            request.meta.pop(&quot;_dont_cache&quot;, None)</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        # RFC2616 requires origin server to set Date header,</span>
<span class="gi">+        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18</span>
<span class="gi">+        if &quot;Date&quot; not in response.headers:</span>
<span class="gi">+            response.headers[&quot;Date&quot;] = formatdate(usegmt=True)</span>
<span class="gi">+</span>
<span class="gi">+        # Do not validate first-hand responses</span>
<span class="gi">+        cachedresponse = request.meta.pop(&quot;cached_response&quot;, None)</span>
<span class="gi">+        if cachedresponse is None:</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/firsthand&quot;, spider=spider)</span>
<span class="gi">+            self._cache_response(spider, response, request, cachedresponse)</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        if self.policy.is_cached_response_valid(cachedresponse, response, request):</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/revalidate&quot;, spider=spider)</span>
<span class="gi">+            return cachedresponse</span>
<span class="gi">+</span>
<span class="gi">+        self.stats.inc_value(&quot;httpcache/invalidate&quot;, spider=spider)</span>
<span class="gi">+        self._cache_response(spider, response, request, cachedresponse)</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def process_exception(</span>
<span class="gi">+        self, request: Request, exception: Exception, spider: Spider</span>
<span class="gi">+    ) -&gt; Optional[Response]:</span>
<span class="gi">+        cachedresponse = request.meta.pop(&quot;cached_response&quot;, None)</span>
<span class="gi">+        if cachedresponse is not None and isinstance(</span>
<span class="gi">+            exception, self.DOWNLOAD_EXCEPTIONS</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/errorrecovery&quot;, spider=spider)</span>
<span class="gi">+            return cachedresponse</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    def _cache_response(</span>
<span class="gi">+        self,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        response: Response,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        cachedresponse: Optional[Response],</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        if self.policy.should_cache_response(response, request):</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/store&quot;, spider=spider)</span>
<span class="gi">+            self.storage.store_response(spider, request, response)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.stats.inc_value(&quot;httpcache/uncacheable&quot;, spider=spider)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/httpcompression.py b/scrapy/downloadermiddlewares/httpcompression.py</span>
<span class="gh">index cc3614d22..816be25a1 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/httpcompression.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/httpcompression.py</span>
<span class="gu">@@ -1,26 +1,36 @@</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from logging import getLogger
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest, NotConfigured
<span class="w"> </span>from scrapy.http import Response, TextResponse
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="gd">-from scrapy.utils._compression import _DecompressionMaxSizeExceeded, _inflate, _unbrotli, _unzstd</span>
<span class="gi">+from scrapy.utils._compression import (</span>
<span class="gi">+    _DecompressionMaxSizeExceeded,</span>
<span class="gi">+    _inflate,</span>
<span class="gi">+    _unbrotli,</span>
<span class="gi">+    _unzstd,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.utils.deprecate import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.gz import gunzip
<span class="gi">+</span>
<span class="w"> </span>logger = getLogger(__name__)
<span class="gd">-ACCEPTED_ENCODINGS = [b&#39;gzip&#39;, b&#39;deflate&#39;]</span>
<span class="gi">+</span>
<span class="gi">+ACCEPTED_ENCODINGS = [b&quot;gzip&quot;, b&quot;deflate&quot;]</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gd">-    import brotli</span>
<span class="gi">+    import brotli  # noqa: F401</span>
<span class="w"> </span>except ImportError:
<span class="w"> </span>    pass
<span class="w"> </span>else:
<span class="gd">-    ACCEPTED_ENCODINGS.append(b&#39;br&#39;)</span>
<span class="gi">+    ACCEPTED_ENCODINGS.append(b&quot;br&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gd">-    import zstandard</span>
<span class="gi">+    import zstandard  # noqa: F401</span>
<span class="w"> </span>except ImportError:
<span class="w"> </span>    pass
<span class="w"> </span>else:
<span class="gd">-    ACCEPTED_ENCODINGS.append(b&#39;zstd&#39;)</span>
<span class="gi">+    ACCEPTED_ENCODINGS.append(b&quot;zstd&quot;)</span>


<span class="w"> </span>class HttpCompressionMiddleware:
<span class="gu">@@ -34,6 +44,94 @@ class HttpCompressionMiddleware:</span>
<span class="w"> </span>            self._warn_size = 33554432
<span class="w"> </span>            return
<span class="w"> </span>        self.stats = crawler.stats
<span class="gd">-        self._max_size = crawler.settings.getint(&#39;DOWNLOAD_MAXSIZE&#39;)</span>
<span class="gd">-        self._warn_size = crawler.settings.getint(&#39;DOWNLOAD_WARNSIZE&#39;)</span>
<span class="gi">+        self._max_size = crawler.settings.getint(&quot;DOWNLOAD_MAXSIZE&quot;)</span>
<span class="gi">+        self._warn_size = crawler.settings.getint(&quot;DOWNLOAD_WARNSIZE&quot;)</span>
<span class="w"> </span>        crawler.signals.connect(self.open_spider, signals.spider_opened)
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;COMPRESSION_ENABLED&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        try:</span>
<span class="gi">+            return cls(crawler=crawler)</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;HttpCompressionMiddleware subclasses must either modify &quot;</span>
<span class="gi">+                &quot;their &#39;__init__&#39; method to support a &#39;crawler&#39; parameter or &quot;</span>
<span class="gi">+                &quot;reimplement their &#39;from_crawler&#39; method.&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            mw = cls()</span>
<span class="gi">+            mw.stats = crawler.stats</span>
<span class="gi">+            mw._max_size = crawler.settings.getint(&quot;DOWNLOAD_MAXSIZE&quot;)</span>
<span class="gi">+            mw._warn_size = crawler.settings.getint(&quot;DOWNLOAD_WARNSIZE&quot;)</span>
<span class="gi">+            crawler.signals.connect(mw.open_spider, signals.spider_opened)</span>
<span class="gi">+            return mw</span>
<span class="gi">+</span>
<span class="gi">+    def open_spider(self, spider):</span>
<span class="gi">+        if hasattr(spider, &quot;download_maxsize&quot;):</span>
<span class="gi">+            self._max_size = spider.download_maxsize</span>
<span class="gi">+        if hasattr(spider, &quot;download_warnsize&quot;):</span>
<span class="gi">+            self._warn_size = spider.download_warnsize</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        request.headers.setdefault(&quot;Accept-Encoding&quot;, b&quot;, &quot;.join(ACCEPTED_ENCODINGS))</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if request.method == &quot;HEAD&quot;:</span>
<span class="gi">+            return response</span>
<span class="gi">+        if isinstance(response, Response):</span>
<span class="gi">+            content_encoding = response.headers.getlist(&quot;Content-Encoding&quot;)</span>
<span class="gi">+            if content_encoding:</span>
<span class="gi">+                encoding = content_encoding.pop()</span>
<span class="gi">+                max_size = request.meta.get(&quot;download_maxsize&quot;, self._max_size)</span>
<span class="gi">+                warn_size = request.meta.get(&quot;download_warnsize&quot;, self._warn_size)</span>
<span class="gi">+                try:</span>
<span class="gi">+                    decoded_body = self._decode(</span>
<span class="gi">+                        response.body, encoding.lower(), max_size</span>
<span class="gi">+                    )</span>
<span class="gi">+                except _DecompressionMaxSizeExceeded:</span>
<span class="gi">+                    raise IgnoreRequest(</span>
<span class="gi">+                        f&quot;Ignored response {response} because its body &quot;</span>
<span class="gi">+                        f&quot;({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE &quot;</span>
<span class="gi">+                        f&quot;({max_size} B) during decompression.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if len(response.body) &lt; warn_size &lt;= len(decoded_body):</span>
<span class="gi">+                    logger.warning(</span>
<span class="gi">+                        f&quot;{response} body size after decompression &quot;</span>
<span class="gi">+                        f&quot;({len(decoded_body)} B) is larger than the &quot;</span>
<span class="gi">+                        f&quot;download warning size ({warn_size} B).&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if self.stats:</span>
<span class="gi">+                    self.stats.inc_value(</span>
<span class="gi">+                        &quot;httpcompression/response_bytes&quot;,</span>
<span class="gi">+                        len(decoded_body),</span>
<span class="gi">+                        spider=spider,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    self.stats.inc_value(</span>
<span class="gi">+                        &quot;httpcompression/response_count&quot;, spider=spider</span>
<span class="gi">+                    )</span>
<span class="gi">+                respcls = responsetypes.from_args(</span>
<span class="gi">+                    headers=response.headers, url=response.url, body=decoded_body</span>
<span class="gi">+                )</span>
<span class="gi">+                kwargs = dict(cls=respcls, body=decoded_body)</span>
<span class="gi">+                if issubclass(respcls, TextResponse):</span>
<span class="gi">+                    # force recalculating the encoding until we make sure the</span>
<span class="gi">+                    # responsetypes guessing is reliable</span>
<span class="gi">+                    kwargs[&quot;encoding&quot;] = None</span>
<span class="gi">+                response = response.replace(**kwargs)</span>
<span class="gi">+                if not content_encoding:</span>
<span class="gi">+                    del response.headers[&quot;Content-Encoding&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def _decode(self, body, encoding, max_size):</span>
<span class="gi">+        if encoding == b&quot;gzip&quot; or encoding == b&quot;x-gzip&quot;:</span>
<span class="gi">+            return gunzip(body, max_size=max_size)</span>
<span class="gi">+        if encoding == b&quot;deflate&quot;:</span>
<span class="gi">+            return _inflate(body, max_size=max_size)</span>
<span class="gi">+        if encoding == b&quot;br&quot; and b&quot;br&quot; in ACCEPTED_ENCODINGS:</span>
<span class="gi">+            return _unbrotli(body, max_size=max_size)</span>
<span class="gi">+        if encoding == b&quot;zstd&quot; and b&quot;zstd&quot; in ACCEPTED_ENCODINGS:</span>
<span class="gi">+            return _unzstd(body, max_size=max_size)</span>
<span class="gi">+        return body</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/httpproxy.py b/scrapy/downloadermiddlewares/httpproxy.py</span>
<span class="gh">index 3fb8b5cbc..522237674 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/httpproxy.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/httpproxy.py</span>
<span class="gu">@@ -1,18 +1,83 @@</span>
<span class="w"> </span>import base64
<span class="w"> </span>from urllib.parse import unquote, urlunparse
<span class="w"> </span>from urllib.request import _parse_proxy, getproxies, proxy_bypass
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.python import to_bytes


<span class="w"> </span>class HttpProxyMiddleware:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, auth_encoding=&#39;latin-1&#39;):</span>
<span class="gi">+    def __init__(self, auth_encoding=&quot;latin-1&quot;):</span>
<span class="w"> </span>        self.auth_encoding = auth_encoding
<span class="w"> </span>        self.proxies = {}
<span class="w"> </span>        for type_, url in getproxies().items():
<span class="w"> </span>            try:
<span class="w"> </span>                self.proxies[type_] = self._get_proxy(url, type_)
<span class="gi">+            # some values such as &#39;/var/run/docker.sock&#39; can&#39;t be parsed</span>
<span class="gi">+            # by _parse_proxy and as such should be skipped</span>
<span class="w"> </span>            except ValueError:
<span class="w"> </span>                continue
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;HTTPPROXY_ENABLED&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;)</span>
<span class="gi">+        return cls(auth_encoding)</span>
<span class="gi">+</span>
<span class="gi">+    def _basic_auth_header(self, username, password):</span>
<span class="gi">+        user_pass = to_bytes(</span>
<span class="gi">+            f&quot;{unquote(username)}:{unquote(password)}&quot;, encoding=self.auth_encoding</span>
<span class="gi">+        )</span>
<span class="gi">+        return base64.b64encode(user_pass)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_proxy(self, url, orig_type):</span>
<span class="gi">+        proxy_type, user, password, hostport = _parse_proxy(url)</span>
<span class="gi">+        proxy_url = urlunparse((proxy_type or orig_type, hostport, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        if user:</span>
<span class="gi">+            creds = self._basic_auth_header(user, password)</span>
<span class="gi">+        else:</span>
<span class="gi">+            creds = None</span>
<span class="gi">+</span>
<span class="gi">+        return creds, proxy_url</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        creds, proxy_url, scheme = None, None, None</span>
<span class="gi">+        if &quot;proxy&quot; in request.meta:</span>
<span class="gi">+            if request.meta[&quot;proxy&quot;] is not None:</span>
<span class="gi">+                creds, proxy_url = self._get_proxy(request.meta[&quot;proxy&quot;], &quot;&quot;)</span>
<span class="gi">+        elif self.proxies:</span>
<span class="gi">+            parsed = urlparse_cached(request)</span>
<span class="gi">+            _scheme = parsed.scheme</span>
<span class="gi">+            if (</span>
<span class="gi">+                # &#39;no_proxy&#39; is only supported by http schemes</span>
<span class="gi">+                _scheme not in (&quot;http&quot;, &quot;https&quot;)</span>
<span class="gi">+                or not proxy_bypass(parsed.hostname)</span>
<span class="gi">+            ) and _scheme in self.proxies:</span>
<span class="gi">+                scheme = _scheme</span>
<span class="gi">+                creds, proxy_url = self.proxies[scheme]</span>
<span class="gi">+</span>
<span class="gi">+        self._set_proxy_and_creds(request, proxy_url, creds, scheme)</span>
<span class="gi">+</span>
<span class="gi">+    def _set_proxy_and_creds(self, request, proxy_url, creds, scheme):</span>
<span class="gi">+        if scheme:</span>
<span class="gi">+            request.meta[&quot;_scheme_proxy&quot;] = True</span>
<span class="gi">+        if proxy_url:</span>
<span class="gi">+            request.meta[&quot;proxy&quot;] = proxy_url</span>
<span class="gi">+        elif request.meta.get(&quot;proxy&quot;) is not None:</span>
<span class="gi">+            request.meta[&quot;proxy&quot;] = None</span>
<span class="gi">+        if creds:</span>
<span class="gi">+            request.headers[b&quot;Proxy-Authorization&quot;] = b&quot;Basic &quot; + creds</span>
<span class="gi">+            request.meta[&quot;_auth_proxy&quot;] = proxy_url</span>
<span class="gi">+        elif &quot;_auth_proxy&quot; in request.meta:</span>
<span class="gi">+            if proxy_url != request.meta[&quot;_auth_proxy&quot;]:</span>
<span class="gi">+                if b&quot;Proxy-Authorization&quot; in request.headers:</span>
<span class="gi">+                    del request.headers[b&quot;Proxy-Authorization&quot;]</span>
<span class="gi">+                del request.meta[&quot;_auth_proxy&quot;]</span>
<span class="gi">+        elif b&quot;Proxy-Authorization&quot; in request.headers:</span>
<span class="gi">+            if proxy_url:</span>
<span class="gi">+                request.meta[&quot;_auth_proxy&quot;] = proxy_url</span>
<span class="gi">+            else:</span>
<span class="gi">+                del request.headers[b&quot;Proxy-Authorization&quot;]</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/offsite.py b/scrapy/downloadermiddlewares/offsite.py</span>
<span class="gh">index d5ebe2f64..1e5026925 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/offsite.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/offsite.py</span>
<span class="gu">@@ -1,18 +1,77 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class OffsiteMiddleware:
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls(crawler.stats)</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)</span>
<span class="gi">+        return o</span>

<span class="w"> </span>    def __init__(self, stats):
<span class="w"> </span>        self.stats = stats
<span class="w"> </span>        self.domains_seen = set()

<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.host_regex = self.get_host_regex(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def request_scheduled(self, request, spider):</span>
<span class="gi">+        self.process_request(request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        if request.dont_filter or self.should_follow(request, spider):</span>
<span class="gi">+            return None</span>
<span class="gi">+        domain = urlparse_cached(request).hostname</span>
<span class="gi">+        if domain and domain not in self.domains_seen:</span>
<span class="gi">+            self.domains_seen.add(domain)</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Filtered offsite request to %(domain)r: %(request)s&quot;,</span>
<span class="gi">+                {&quot;domain&quot;: domain, &quot;request&quot;: request},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            self.stats.inc_value(&quot;offsite/domains&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(&quot;offsite/filtered&quot;, spider=spider)</span>
<span class="gi">+        raise IgnoreRequest</span>
<span class="gi">+</span>
<span class="gi">+    def should_follow(self, request, spider):</span>
<span class="gi">+        regex = self.host_regex</span>
<span class="gi">+        # hostname can be None for wrong urls (like javascript links)</span>
<span class="gi">+        host = urlparse_cached(request).hostname or &quot;&quot;</span>
<span class="gi">+        return bool(regex.search(host))</span>
<span class="gi">+</span>
<span class="w"> </span>    def get_host_regex(self, spider):
<span class="w"> </span>        &quot;&quot;&quot;Override this method to implement a different offsite policy&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        allowed_domains = getattr(spider, &quot;allowed_domains&quot;, None)</span>
<span class="gi">+        if not allowed_domains:</span>
<span class="gi">+            return re.compile(&quot;&quot;)  # allow all by default</span>
<span class="gi">+        url_pattern = re.compile(r&quot;^https?://.*$&quot;)</span>
<span class="gi">+        port_pattern = re.compile(r&quot;:\d+$&quot;)</span>
<span class="gi">+        domains = []</span>
<span class="gi">+        for domain in allowed_domains:</span>
<span class="gi">+            if domain is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            if url_pattern.match(domain):</span>
<span class="gi">+                message = (</span>
<span class="gi">+                    &quot;allowed_domains accepts only domains, not URLs. &quot;</span>
<span class="gi">+                    f&quot;Ignoring URL entry {domain} in allowed_domains.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                warnings.warn(message)</span>
<span class="gi">+            elif port_pattern.search(domain):</span>
<span class="gi">+                message = (</span>
<span class="gi">+                    &quot;allowed_domains accepts only domains without ports. &quot;</span>
<span class="gi">+                    f&quot;Ignoring entry {domain} in allowed_domains.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                warnings.warn(message)</span>
<span class="gi">+            else:</span>
<span class="gi">+                domains.append(re.escape(domain))</span>
<span class="gi">+        regex = rf&#39;^(.*\.)?({&quot;|&quot;.join(domains)})$&#39;</span>
<span class="gi">+        return re.compile(regex)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py</span>
<span class="gh">index dce283f2f..63be1d0ca 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/redirect.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/redirect.py</span>
<span class="gu">@@ -1,21 +1,120 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>from urllib.parse import urljoin, urlparse
<span class="gi">+</span>
<span class="w"> </span>from w3lib.url import safe_url_string
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest, NotConfigured
<span class="w"> </span>from scrapy.http import HtmlResponse
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.response import get_meta_refresh
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gi">+def _build_redirect_request(source_request, *, url, **kwargs):</span>
<span class="gi">+    redirect_request = source_request.replace(</span>
<span class="gi">+        url=url,</span>
<span class="gi">+        **kwargs,</span>
<span class="gi">+        cookies=None,</span>
<span class="gi">+    )</span>
<span class="gi">+    if &quot;_scheme_proxy&quot; in redirect_request.meta:</span>
<span class="gi">+        source_request_scheme = urlparse_cached(source_request).scheme</span>
<span class="gi">+        redirect_request_scheme = urlparse_cached(redirect_request).scheme</span>
<span class="gi">+        if source_request_scheme != redirect_request_scheme:</span>
<span class="gi">+            redirect_request.meta.pop(&quot;_scheme_proxy&quot;)</span>
<span class="gi">+            redirect_request.meta.pop(&quot;proxy&quot;, None)</span>
<span class="gi">+            redirect_request.meta.pop(&quot;_auth_proxy&quot;, None)</span>
<span class="gi">+            redirect_request.headers.pop(b&quot;Proxy-Authorization&quot;, None)</span>
<span class="gi">+    has_cookie_header = &quot;Cookie&quot; in redirect_request.headers</span>
<span class="gi">+    has_authorization_header = &quot;Authorization&quot; in redirect_request.headers</span>
<span class="gi">+    if has_cookie_header or has_authorization_header:</span>
<span class="gi">+        default_ports = {&quot;http&quot;: 80, &quot;https&quot;: 443}</span>
<span class="gi">+</span>
<span class="gi">+        parsed_source_request = urlparse_cached(source_request)</span>
<span class="gi">+        source_scheme, source_host, source_port = (</span>
<span class="gi">+            parsed_source_request.scheme,</span>
<span class="gi">+            parsed_source_request.hostname,</span>
<span class="gi">+            parsed_source_request.port</span>
<span class="gi">+            or default_ports.get(parsed_source_request.scheme),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        parsed_redirect_request = urlparse_cached(redirect_request)</span>
<span class="gi">+        redirect_scheme, redirect_host, redirect_port = (</span>
<span class="gi">+            parsed_redirect_request.scheme,</span>
<span class="gi">+            parsed_redirect_request.hostname,</span>
<span class="gi">+            parsed_redirect_request.port</span>
<span class="gi">+            or default_ports.get(parsed_redirect_request.scheme),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        if has_cookie_header and (</span>
<span class="gi">+            (source_scheme != redirect_scheme and redirect_scheme != &quot;https&quot;)</span>
<span class="gi">+            or source_host != redirect_host</span>
<span class="gi">+        ):</span>
<span class="gi">+            del redirect_request.headers[&quot;Cookie&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name</span>
<span class="gi">+        if has_authorization_header and (</span>
<span class="gi">+            source_scheme != redirect_scheme</span>
<span class="gi">+            or source_host != redirect_host</span>
<span class="gi">+            or source_port != redirect_port</span>
<span class="gi">+        ):</span>
<span class="gi">+            del redirect_request.headers[&quot;Authorization&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    return redirect_request</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class BaseRedirectMiddleware:
<span class="gd">-    enabled_setting = &#39;REDIRECT_ENABLED&#39;</span>
<span class="gi">+    enabled_setting = &quot;REDIRECT_ENABLED&quot;</span>

<span class="w"> </span>    def __init__(self, settings):
<span class="w"> </span>        if not settings.getbool(self.enabled_setting):
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self.max_redirect_times = settings.getint(&#39;REDIRECT_MAX_TIMES&#39;)</span>
<span class="gd">-        self.priority_adjust = settings.getint(&#39;REDIRECT_PRIORITY_ADJUST&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        self.max_redirect_times = settings.getint(&quot;REDIRECT_MAX_TIMES&quot;)</span>
<span class="gi">+        self.priority_adjust = settings.getint(&quot;REDIRECT_PRIORITY_ADJUST&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def _redirect(self, redirected, request, spider, reason):</span>
<span class="gi">+        ttl = request.meta.setdefault(&quot;redirect_ttl&quot;, self.max_redirect_times)</span>
<span class="gi">+        redirects = request.meta.get(&quot;redirect_times&quot;, 0) + 1</span>
<span class="gi">+</span>
<span class="gi">+        if ttl and redirects &lt;= self.max_redirect_times:</span>
<span class="gi">+            redirected.meta[&quot;redirect_times&quot;] = redirects</span>
<span class="gi">+            redirected.meta[&quot;redirect_ttl&quot;] = ttl - 1</span>
<span class="gi">+            redirected.meta[&quot;redirect_urls&quot;] = request.meta.get(&quot;redirect_urls&quot;, []) + [</span>
<span class="gi">+                request.url</span>
<span class="gi">+            ]</span>
<span class="gi">+            redirected.meta[&quot;redirect_reasons&quot;] = request.meta.get(</span>
<span class="gi">+                &quot;redirect_reasons&quot;, []</span>
<span class="gi">+            ) + [reason]</span>
<span class="gi">+            redirected.dont_filter = request.dont_filter</span>
<span class="gi">+            redirected.priority = request.priority + self.priority_adjust</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Redirecting (%(reason)s) to %(redirected)s from %(request)s&quot;,</span>
<span class="gi">+                {&quot;reason&quot;: reason, &quot;redirected&quot;: redirected, &quot;request&quot;: request},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            return redirected</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Discarding %(request)s: max redirections reached&quot;,</span>
<span class="gi">+            {&quot;request&quot;: request},</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        raise IgnoreRequest(&quot;max redirections reached&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _redirect_request_using_get(self, request, redirect_url):</span>
<span class="gi">+        redirect_request = _build_redirect_request(</span>
<span class="gi">+            request,</span>
<span class="gi">+            url=redirect_url,</span>
<span class="gi">+            method=&quot;GET&quot;,</span>
<span class="gi">+            body=&quot;&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        redirect_request.headers.pop(&quot;Content-Type&quot;, None)</span>
<span class="gi">+        redirect_request.headers.pop(&quot;Content-Length&quot;, None)</span>
<span class="gi">+        return redirect_request</span>


<span class="w"> </span>class RedirectMiddleware(BaseRedirectMiddleware):
<span class="gu">@@ -24,11 +123,59 @@ class RedirectMiddleware(BaseRedirectMiddleware):</span>
<span class="w"> </span>    and meta-refresh html tag.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if (</span>
<span class="gi">+            request.meta.get(&quot;dont_redirect&quot;, False)</span>
<span class="gi">+            or response.status in getattr(spider, &quot;handle_httpstatus_list&quot;, [])</span>
<span class="gi">+            or response.status in request.meta.get(&quot;handle_httpstatus_list&quot;, [])</span>
<span class="gi">+            or request.meta.get(&quot;handle_httpstatus_all&quot;, False)</span>
<span class="gi">+        ):</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        allowed_status = (301, 302, 303, 307, 308)</span>
<span class="gi">+        if &quot;Location&quot; not in response.headers or response.status not in allowed_status:</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        location = safe_url_string(response.headers[&quot;Location&quot;])</span>
<span class="gi">+        if response.headers[&quot;Location&quot;].startswith(b&quot;//&quot;):</span>
<span class="gi">+            request_scheme = urlparse(request.url).scheme</span>
<span class="gi">+            location = request_scheme + &quot;://&quot; + location.lstrip(&quot;/&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        redirected_url = urljoin(request.url, location)</span>
<span class="gi">+        if urlparse(redirected_url).scheme not in {&quot;http&quot;, &quot;https&quot;}:</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        if response.status in (301, 307, 308) or request.method == &quot;HEAD&quot;:</span>
<span class="gi">+            redirected = _build_redirect_request(request, url=redirected_url)</span>
<span class="gi">+            return self._redirect(redirected, request, spider, response.status)</span>
<span class="gi">+</span>
<span class="gi">+        redirected = self._redirect_request_using_get(request, redirected_url)</span>
<span class="gi">+        return self._redirect(redirected, request, spider, response.status)</span>
<span class="gi">+</span>

<span class="w"> </span>class MetaRefreshMiddleware(BaseRedirectMiddleware):
<span class="gd">-    enabled_setting = &#39;METAREFRESH_ENABLED&#39;</span>
<span class="gi">+    enabled_setting = &quot;METAREFRESH_ENABLED&quot;</span>

<span class="w"> </span>    def __init__(self, settings):
<span class="w"> </span>        super().__init__(settings)
<span class="gd">-        self._ignore_tags = settings.getlist(&#39;METAREFRESH_IGNORE_TAGS&#39;)</span>
<span class="gd">-        self._maxdelay = settings.getint(&#39;METAREFRESH_MAXDELAY&#39;)</span>
<span class="gi">+        self._ignore_tags = settings.getlist(&quot;METAREFRESH_IGNORE_TAGS&quot;)</span>
<span class="gi">+        self._maxdelay = settings.getint(&quot;METAREFRESH_MAXDELAY&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if (</span>
<span class="gi">+            request.meta.get(&quot;dont_redirect&quot;, False)</span>
<span class="gi">+            or request.method == &quot;HEAD&quot;</span>
<span class="gi">+            or not isinstance(response, HtmlResponse)</span>
<span class="gi">+            or urlparse_cached(request).scheme not in {&quot;http&quot;, &quot;https&quot;}</span>
<span class="gi">+        ):</span>
<span class="gi">+            return response</span>
<span class="gi">+</span>
<span class="gi">+        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)</span>
<span class="gi">+        if not url:</span>
<span class="gi">+            return response</span>
<span class="gi">+        if urlparse(url).scheme not in {&quot;http&quot;, &quot;https&quot;}:</span>
<span class="gi">+            return response</span>
<span class="gi">+        if interval &lt; self._maxdelay:</span>
<span class="gi">+            redirected = self._redirect_request_using_get(request, url)</span>
<span class="gi">+            return self._redirect(redirected, request, spider, &quot;meta refresh&quot;)</span>
<span class="gi">+        return response</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/retry.py b/scrapy/downloadermiddlewares/retry.py</span>
<span class="gh">index 38df481a3..380623cea 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/retry.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/retry.py</span>
<span class="gu">@@ -12,6 +12,7 @@ once the spider has finished crawling all regular (non failed) pages.</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from logging import Logger, getLogger
<span class="w"> </span>from typing import Optional, Type, Union
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.settings import Settings
<span class="gu">@@ -19,17 +20,41 @@ from scrapy.spiders import Spider</span>
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="w"> </span>from scrapy.utils.python import global_object_name
<span class="w"> </span>from scrapy.utils.response import response_status_message
<span class="gi">+</span>
<span class="w"> </span>retry_logger = getLogger(__name__)


<span class="gi">+def backwards_compatibility_getattr(self, name):</span>
<span class="gi">+    if name == &quot;EXCEPTIONS_TO_RETRY&quot;:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. &quot;</span>
<span class="gi">+            &quot;Use the RETRY_EXCEPTIONS setting instead.&quot;,</span>
<span class="gi">+            ScrapyDeprecationWarning,</span>
<span class="gi">+            stacklevel=2,</span>
<span class="gi">+        )</span>
<span class="gi">+        return tuple(</span>
<span class="gi">+            load_object(x) if isinstance(x, str) else x</span>
<span class="gi">+            for x in Settings().getlist(&quot;RETRY_EXCEPTIONS&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+    raise AttributeError(</span>
<span class="gi">+        f&quot;{self.__class__.__name__!r} object has no attribute {name!r}&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class BackwardsCompatibilityMetaclass(type):
<span class="w"> </span>    __getattr__ = backwards_compatibility_getattr


<span class="gd">-def get_retry_request(request: Request, *, spider: Spider, reason: Union[</span>
<span class="gd">-    str, Exception, Type[Exception]]=&#39;unspecified&#39;, max_retry_times:</span>
<span class="gd">-    Optional[int]=None, priority_adjust: Optional[int]=None, logger: Logger</span>
<span class="gd">-    =retry_logger, stats_base_key: str=&#39;retry&#39;):</span>
<span class="gi">+def get_retry_request(</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    *,</span>
<span class="gi">+    spider: Spider,</span>
<span class="gi">+    reason: Union[str, Exception, Type[Exception]] = &quot;unspecified&quot;,</span>
<span class="gi">+    max_retry_times: Optional[int] = None,</span>
<span class="gi">+    priority_adjust: Optional[int] = None,</span>
<span class="gi">+    logger: Logger = retry_logger,</span>
<span class="gi">+    stats_base_key: str = &quot;retry&quot;,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Returns a new :class:`~scrapy.Request` object to retry the specified
<span class="w"> </span>    request, or ``None`` if retries of the specified request have been
<span class="gu">@@ -70,22 +95,90 @@ def get_retry_request(request: Request, *, spider: Spider, reason: Union[</span>
<span class="w"> </span>    *stats_base_key* is a string to be used as the base key for the
<span class="w"> </span>    retry-related job stats
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    settings = spider.crawler.settings</span>
<span class="gi">+    assert spider.crawler.stats</span>
<span class="gi">+    stats = spider.crawler.stats</span>
<span class="gi">+    retry_times = request.meta.get(&quot;retry_times&quot;, 0) + 1</span>
<span class="gi">+    if max_retry_times is None:</span>
<span class="gi">+        max_retry_times = request.meta.get(&quot;max_retry_times&quot;)</span>
<span class="gi">+        if max_retry_times is None:</span>
<span class="gi">+            max_retry_times = settings.getint(&quot;RETRY_TIMES&quot;)</span>
<span class="gi">+    if retry_times &lt;= max_retry_times:</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Retrying %(request)s (failed %(retry_times)d times): %(reason)s&quot;,</span>
<span class="gi">+            {&quot;request&quot;: request, &quot;retry_times&quot;: retry_times, &quot;reason&quot;: reason},</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        new_request: Request = request.copy()</span>
<span class="gi">+        new_request.meta[&quot;retry_times&quot;] = retry_times</span>
<span class="gi">+        new_request.dont_filter = True</span>
<span class="gi">+        if priority_adjust is None:</span>
<span class="gi">+            priority_adjust = settings.getint(&quot;RETRY_PRIORITY_ADJUST&quot;)</span>
<span class="gi">+        new_request.priority = request.priority + priority_adjust</span>
<span class="gi">+</span>
<span class="gi">+        if callable(reason):</span>
<span class="gi">+            reason = reason()</span>
<span class="gi">+        if isinstance(reason, Exception):</span>
<span class="gi">+            reason = global_object_name(reason.__class__)</span>
<span class="gi">+</span>
<span class="gi">+        stats.inc_value(f&quot;{stats_base_key}/count&quot;)</span>
<span class="gi">+        stats.inc_value(f&quot;{stats_base_key}/reason_count/{reason}&quot;)</span>
<span class="gi">+        return new_request</span>
<span class="gi">+    stats.inc_value(f&quot;{stats_base_key}/max_reached&quot;)</span>
<span class="gi">+    logger.error(</span>
<span class="gi">+        &quot;Gave up retrying %(request)s (failed %(retry_times)d times): &quot; &quot;%(reason)s&quot;,</span>
<span class="gi">+        {&quot;request&quot;: request, &quot;retry_times&quot;: retry_times, &quot;reason&quot;: reason},</span>
<span class="gi">+        extra={&quot;spider&quot;: spider},</span>
<span class="gi">+    )</span>
<span class="gi">+    return None</span>


<span class="w"> </span>class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        if not settings.getbool(&#39;RETRY_ENABLED&#39;):</span>
<span class="gi">+        if not settings.getbool(&quot;RETRY_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self.max_retry_times = settings.getint(&#39;RETRY_TIMES&#39;)</span>
<span class="gd">-        self.retry_http_codes = set(int(x) for x in settings.getlist(</span>
<span class="gd">-            &#39;RETRY_HTTP_CODES&#39;))</span>
<span class="gd">-        self.priority_adjust = settings.getint(&#39;RETRY_PRIORITY_ADJUST&#39;)</span>
<span class="gi">+        self.max_retry_times = settings.getint(&quot;RETRY_TIMES&quot;)</span>
<span class="gi">+        self.retry_http_codes = set(</span>
<span class="gi">+            int(x) for x in settings.getlist(&quot;RETRY_HTTP_CODES&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+        self.priority_adjust = settings.getint(&quot;RETRY_PRIORITY_ADJUST&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>        try:
<span class="gd">-            self.exceptions_to_retry = self.__getattribute__(</span>
<span class="gd">-                &#39;EXCEPTIONS_TO_RETRY&#39;)</span>
<span class="gi">+            self.exceptions_to_retry = self.__getattribute__(&quot;EXCEPTIONS_TO_RETRY&quot;)</span>
<span class="w"> </span>        except AttributeError:
<span class="gd">-            self.exceptions_to_retry = tuple(load_object(x) if isinstance(x,</span>
<span class="gd">-                str) else x for x in settings.getlist(&#39;RETRY_EXCEPTIONS&#39;))</span>
<span class="gi">+            # If EXCEPTIONS_TO_RETRY is not &quot;overridden&quot;</span>
<span class="gi">+            self.exceptions_to_retry = tuple(</span>
<span class="gi">+                load_object(x) if isinstance(x, str) else x</span>
<span class="gi">+                for x in settings.getlist(&quot;RETRY_EXCEPTIONS&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings)</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        if request.meta.get(&quot;dont_retry&quot;, False):</span>
<span class="gi">+            return response</span>
<span class="gi">+        if response.status in self.retry_http_codes:</span>
<span class="gi">+            reason = response_status_message(response.status)</span>
<span class="gi">+            return self._retry(request, reason, spider) or response</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def process_exception(self, request, exception, spider):</span>
<span class="gi">+        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(</span>
<span class="gi">+            &quot;dont_retry&quot;, False</span>
<span class="gi">+        ):</span>
<span class="gi">+            return self._retry(request, exception, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _retry(self, request, reason, spider):</span>
<span class="gi">+        max_retry_times = request.meta.get(&quot;max_retry_times&quot;, self.max_retry_times)</span>
<span class="gi">+        priority_adjust = request.meta.get(&quot;priority_adjust&quot;, self.priority_adjust)</span>
<span class="gi">+        return get_retry_request(</span>
<span class="gi">+            request,</span>
<span class="gi">+            reason=reason,</span>
<span class="gi">+            spider=spider,</span>
<span class="gi">+            max_retry_times=max_retry_times,</span>
<span class="gi">+            priority_adjust=priority_adjust,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>    __getattr__ = backwards_compatibility_getattr
<span class="gh">diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py</span>
<span class="gh">index e1699b10f..6cab27c5a 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/robotstxt.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/robotstxt.py</span>
<span class="gu">@@ -3,14 +3,18 @@ This is a middleware to respect robots.txt policies. To activate it you must</span>
<span class="w"> </span>enable this middleware and enable the ROBOTSTXT_OBEY setting.

<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred, maybeDeferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest, NotConfigured
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.http.request import NO_CALLBACK
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -18,13 +22,100 @@ class RobotsTxtMiddleware:</span>
<span class="w"> </span>    DOWNLOAD_PRIORITY = 1000

<span class="w"> </span>    def __init__(self, crawler):
<span class="gd">-        if not crawler.settings.getbool(&#39;ROBOTSTXT_OBEY&#39;):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;ROBOTSTXT_OBEY&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self._default_useragent = crawler.settings.get(&#39;USER_AGENT&#39;, &#39;Scrapy&#39;)</span>
<span class="gd">-        self._robotstxt_useragent = crawler.settings.get(&#39;ROBOTSTXT_USER_AGENT&#39;</span>
<span class="gd">-            , None)</span>
<span class="gi">+        self._default_useragent = crawler.settings.get(&quot;USER_AGENT&quot;, &quot;Scrapy&quot;)</span>
<span class="gi">+        self._robotstxt_useragent = crawler.settings.get(&quot;ROBOTSTXT_USER_AGENT&quot;, None)</span>
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self._parsers = {}
<span class="gd">-        self._parserimpl = load_object(crawler.settings.get(&#39;ROBOTSTXT_PARSER&#39;)</span>
<span class="gi">+        self._parserimpl = load_object(crawler.settings.get(&quot;ROBOTSTXT_PARSER&quot;))</span>
<span class="gi">+</span>
<span class="gi">+        # check if parser dependencies are met, this should throw an error otherwise.</span>
<span class="gi">+        self._parserimpl.from_crawler(self.crawler, b&quot;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        if request.meta.get(&quot;dont_obey_robotstxt&quot;):</span>
<span class="gi">+            return</span>
<span class="gi">+        if request.url.startswith(&quot;data:&quot;) or request.url.startswith(&quot;file:&quot;):</span>
<span class="gi">+            return</span>
<span class="gi">+        d = maybeDeferred(self.robot_parser, request, spider)</span>
<span class="gi">+        d.addCallback(self.process_request_2, request, spider)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def process_request_2(self, rp, request, spider):</span>
<span class="gi">+        if rp is None:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        useragent = self._robotstxt_useragent</span>
<span class="gi">+        if not useragent:</span>
<span class="gi">+            useragent = request.headers.get(b&quot;User-Agent&quot;, self._default_useragent)</span>
<span class="gi">+        if not rp.allowed(request.url, useragent):</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Forbidden by robots.txt: %(request)s&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="w"> </span>            )
<span class="gd">-        self._parserimpl.from_crawler(self.crawler, b&#39;&#39;)</span>
<span class="gi">+            self.crawler.stats.inc_value(&quot;robotstxt/forbidden&quot;)</span>
<span class="gi">+            raise IgnoreRequest(&quot;Forbidden by robots.txt&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def robot_parser(self, request, spider):</span>
<span class="gi">+        url = urlparse_cached(request)</span>
<span class="gi">+        netloc = url.netloc</span>
<span class="gi">+</span>
<span class="gi">+        if netloc not in self._parsers:</span>
<span class="gi">+            self._parsers[netloc] = Deferred()</span>
<span class="gi">+            robotsurl = f&quot;{url.scheme}://{url.netloc}/robots.txt&quot;</span>
<span class="gi">+            robotsreq = Request(</span>
<span class="gi">+                robotsurl,</span>
<span class="gi">+                priority=self.DOWNLOAD_PRIORITY,</span>
<span class="gi">+                meta={&quot;dont_obey_robotstxt&quot;: True},</span>
<span class="gi">+                callback=NO_CALLBACK,</span>
<span class="gi">+            )</span>
<span class="gi">+            dfd = self.crawler.engine.download(robotsreq)</span>
<span class="gi">+            dfd.addCallback(self._parse_robots, netloc, spider)</span>
<span class="gi">+            dfd.addErrback(self._logerror, robotsreq, spider)</span>
<span class="gi">+            dfd.addErrback(self._robots_error, netloc)</span>
<span class="gi">+            self.crawler.stats.inc_value(&quot;robotstxt/request_count&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self._parsers[netloc], Deferred):</span>
<span class="gi">+            d = Deferred()</span>
<span class="gi">+</span>
<span class="gi">+            def cb(result):</span>
<span class="gi">+                d.callback(result)</span>
<span class="gi">+                return result</span>
<span class="gi">+</span>
<span class="gi">+            self._parsers[netloc].addCallback(cb)</span>
<span class="gi">+            return d</span>
<span class="gi">+        return self._parsers[netloc]</span>
<span class="gi">+</span>
<span class="gi">+    def _logerror(self, failure, request, spider):</span>
<span class="gi">+        if failure.type is not IgnoreRequest:</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;Error downloading %(request)s: %(f_exception)s&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request, &quot;f_exception&quot;: failure.value},</span>
<span class="gi">+                exc_info=failure_to_exc_info(failure),</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        return failure</span>
<span class="gi">+</span>
<span class="gi">+    def _parse_robots(self, response, netloc, spider):</span>
<span class="gi">+        self.crawler.stats.inc_value(&quot;robotstxt/response_count&quot;)</span>
<span class="gi">+        self.crawler.stats.inc_value(</span>
<span class="gi">+            f&quot;robotstxt/response_status_count/{response.status}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        rp = self._parserimpl.from_crawler(self.crawler, response.body)</span>
<span class="gi">+        rp_dfd = self._parsers[netloc]</span>
<span class="gi">+        self._parsers[netloc] = rp</span>
<span class="gi">+        rp_dfd.callback(rp)</span>
<span class="gi">+</span>
<span class="gi">+    def _robots_error(self, failure, netloc):</span>
<span class="gi">+        if failure.type is not IgnoreRequest:</span>
<span class="gi">+            key = f&quot;robotstxt/exception_count/{failure.type}&quot;</span>
<span class="gi">+            self.crawler.stats.inc_value(key)</span>
<span class="gi">+        rp_dfd = self._parsers[netloc]</span>
<span class="gi">+        self._parsers[netloc] = None</span>
<span class="gi">+        rp_dfd.callback(None)</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/stats.py b/scrapy/downloadermiddlewares/stats.py</span>
<span class="gh">index 571687317..a0f62e262 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/stats.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/stats.py</span>
<span class="gu">@@ -1,10 +1,60 @@</span>
<span class="w"> </span>from twisted.web import http
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.python import global_object_name, to_bytes
<span class="w"> </span>from scrapy.utils.request import request_httprepr


<span class="gd">-class DownloaderStats:</span>
<span class="gi">+def get_header_size(headers):</span>
<span class="gi">+    size = 0</span>
<span class="gi">+    for key, value in headers.items():</span>
<span class="gi">+        if isinstance(value, (list, tuple)):</span>
<span class="gi">+            for v in value:</span>
<span class="gi">+                size += len(b&quot;: &quot;) + len(key) + len(v)</span>
<span class="gi">+    return size + len(b&quot;\r\n&quot;) * (len(headers.keys()) - 1)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_status_size(response_status):</span>
<span class="gi">+    return len(to_bytes(http.RESPONSES.get(response_status, b&quot;&quot;))) + 15</span>
<span class="gi">+    # resp.status + b&quot;\r\n&quot; + b&quot;HTTP/1.1 &lt;100-599&gt; &quot;</span>
<span class="gi">+</span>

<span class="gi">+class DownloaderStats:</span>
<span class="w"> </span>    def __init__(self, stats):
<span class="w"> </span>        self.stats = stats
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;DOWNLOADER_STATS&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        return cls(crawler.stats)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        self.stats.inc_value(&quot;downloader/request_count&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(</span>
<span class="gi">+            f&quot;downloader/request_method_count/{request.method}&quot;, spider=spider</span>
<span class="gi">+        )</span>
<span class="gi">+        reqlen = len(request_httprepr(request))</span>
<span class="gi">+        self.stats.inc_value(&quot;downloader/request_bytes&quot;, reqlen, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def process_response(self, request, response, spider):</span>
<span class="gi">+        self.stats.inc_value(&quot;downloader/response_count&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(</span>
<span class="gi">+            f&quot;downloader/response_status_count/{response.status}&quot;, spider=spider</span>
<span class="gi">+        )</span>
<span class="gi">+        reslen = (</span>
<span class="gi">+            len(response.body)</span>
<span class="gi">+            + get_header_size(response.headers)</span>
<span class="gi">+            + get_status_size(response.status)</span>
<span class="gi">+            + 4</span>
<span class="gi">+        )</span>
<span class="gi">+        # response.body + b&quot;\r\n&quot;+ response.header + b&quot;\r\n&quot; + response.status</span>
<span class="gi">+        self.stats.inc_value(&quot;downloader/response_bytes&quot;, reslen, spider=spider)</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def process_exception(self, request, exception, spider):</span>
<span class="gi">+        ex_class = global_object_name(exception.__class__)</span>
<span class="gi">+        self.stats.inc_value(&quot;downloader/exception_count&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(</span>
<span class="gi">+            f&quot;downloader/exception_type_count/{ex_class}&quot;, spider=spider</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/downloadermiddlewares/useragent.py b/scrapy/downloadermiddlewares/useragent.py</span>
<span class="gh">index 5199b0472..856a275ab 100644</span>
<span class="gd">--- a/scrapy/downloadermiddlewares/useragent.py</span>
<span class="gi">+++ b/scrapy/downloadermiddlewares/useragent.py</span>
<span class="gu">@@ -1,9 +1,23 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Set User-Agent header per spider or use a default value from settings&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals


<span class="w"> </span>class UserAgentMiddleware:
<span class="w"> </span>    &quot;&quot;&quot;This middleware allows spiders to override the user_agent&quot;&quot;&quot;

<span class="gd">-    def __init__(self, user_agent=&#39;Scrapy&#39;):</span>
<span class="gi">+    def __init__(self, user_agent=&quot;Scrapy&quot;):</span>
<span class="w"> </span>        self.user_agent = user_agent
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls(crawler.settings[&quot;USER_AGENT&quot;])</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.user_agent = getattr(spider, &quot;user_agent&quot;, self.user_agent)</span>
<span class="gi">+</span>
<span class="gi">+    def process_request(self, request, spider):</span>
<span class="gi">+        if self.user_agent:</span>
<span class="gi">+            request.headers.setdefault(b&quot;User-Agent&quot;, self.user_agent)</span>
<span class="gh">diff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py</span>
<span class="gh">index 684ffcbe6..0b20f53b9 100644</span>
<span class="gd">--- a/scrapy/dupefilters.py</span>
<span class="gi">+++ b/scrapy/dupefilters.py</span>
<span class="gu">@@ -1,23 +1,45 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import TYPE_CHECKING, Optional, Set
<span class="w"> </span>from warnings import warn
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.utils.deprecate import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.job import job_dir
<span class="gd">-from scrapy.utils.request import RequestFingerprinter, RequestFingerprinterProtocol, referer_str</span>
<span class="gi">+from scrapy.utils.request import (</span>
<span class="gi">+    RequestFingerprinter,</span>
<span class="gi">+    RequestFingerprinterProtocol,</span>
<span class="gi">+    referer_str,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="w"> </span>    from scrapy.crawler import Crawler


<span class="w"> </span>class BaseDupeFilter:
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings: BaseSettings) -&gt; Self:</span>
<span class="gi">+        return cls()</span>

<span class="gd">-    def log(self, request: Request, spider: Spider) -&gt;None:</span>
<span class="gi">+    def request_seen(self, request: Request) -&gt; bool:</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def open(self) -&gt; Optional[Deferred]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def close(self, reason: str) -&gt; Optional[Deferred]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def log(self, request: Request, spider: Spider) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;Log that a request has been filtered&quot;&quot;&quot;
<span class="w"> </span>        pass

<span class="gu">@@ -25,17 +47,96 @@ class BaseDupeFilter:</span>
<span class="w"> </span>class RFPDupeFilter(BaseDupeFilter):
<span class="w"> </span>    &quot;&quot;&quot;Request Fingerprint duplicates filter&quot;&quot;&quot;

<span class="gd">-    def __init__(self, path: Optional[str]=None, debug: bool=False, *,</span>
<span class="gd">-        fingerprinter: Optional[RequestFingerprinterProtocol]=None) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        path: Optional[str] = None,</span>
<span class="gi">+        debug: bool = False,</span>
<span class="gi">+        *,</span>
<span class="gi">+        fingerprinter: Optional[RequestFingerprinterProtocol] = None,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.file = None
<span class="gd">-        self.fingerprinter: RequestFingerprinterProtocol = (fingerprinter or</span>
<span class="gd">-            RequestFingerprinter())</span>
<span class="gi">+        self.fingerprinter: RequestFingerprinterProtocol = (</span>
<span class="gi">+            fingerprinter or RequestFingerprinter()</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.fingerprints: Set[str] = set()
<span class="w"> </span>        self.logdupes = True
<span class="w"> </span>        self.debug = debug
<span class="w"> </span>        self.logger = logging.getLogger(__name__)
<span class="w"> </span>        if path:
<span class="gd">-            self.file = Path(path, &#39;requests.seen&#39;).open(&#39;a+&#39;, encoding=&#39;utf-8&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+            self.file = Path(path, &quot;requests.seen&quot;).open(&quot;a+&quot;, encoding=&quot;utf-8&quot;)</span>
<span class="w"> </span>            self.file.seek(0)
<span class="w"> </span>            self.fingerprints.update(x.rstrip() for x in self.file)
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(</span>
<span class="gi">+        cls,</span>
<span class="gi">+        settings: BaseSettings,</span>
<span class="gi">+        *,</span>
<span class="gi">+        fingerprinter: Optional[RequestFingerprinterProtocol] = None,</span>
<span class="gi">+    ) -&gt; Self:</span>
<span class="gi">+        debug = settings.getbool(&quot;DUPEFILTER_DEBUG&quot;)</span>
<span class="gi">+        try:</span>
<span class="gi">+            return cls(job_dir(settings), debug, fingerprinter=fingerprinter)</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;RFPDupeFilter subclasses must either modify their &#39;__init__&#39; &quot;</span>
<span class="gi">+                &quot;method to support a &#39;fingerprinter&#39; parameter or reimplement &quot;</span>
<span class="gi">+                &quot;the &#39;from_settings&#39; class method.&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            result = cls(job_dir(settings), debug)</span>
<span class="gi">+            result.fingerprinter = fingerprinter or RequestFingerprinter()</span>
<span class="gi">+            return result</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler: Crawler) -&gt; Self:</span>
<span class="gi">+        assert crawler.request_fingerprinter</span>
<span class="gi">+        try:</span>
<span class="gi">+            return cls.from_settings(</span>
<span class="gi">+                crawler.settings,</span>
<span class="gi">+                fingerprinter=crawler.request_fingerprinter,</span>
<span class="gi">+            )</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;RFPDupeFilter subclasses must either modify their overridden &quot;</span>
<span class="gi">+                &quot;&#39;__init__&#39; method and &#39;from_settings&#39; class method to &quot;</span>
<span class="gi">+                &quot;support a &#39;fingerprinter&#39; parameter, or reimplement the &quot;</span>
<span class="gi">+                &quot;&#39;from_crawler&#39; class method.&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            result = cls.from_settings(crawler.settings)</span>
<span class="gi">+            result.fingerprinter = crawler.request_fingerprinter</span>
<span class="gi">+            return result</span>
<span class="gi">+</span>
<span class="gi">+    def request_seen(self, request: Request) -&gt; bool:</span>
<span class="gi">+        fp = self.request_fingerprint(request)</span>
<span class="gi">+        if fp in self.fingerprints:</span>
<span class="gi">+            return True</span>
<span class="gi">+        self.fingerprints.add(fp)</span>
<span class="gi">+        if self.file:</span>
<span class="gi">+            self.file.write(fp + &quot;\n&quot;)</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def request_fingerprint(self, request: Request) -&gt; str:</span>
<span class="gi">+        return self.fingerprinter.fingerprint(request).hex()</span>
<span class="gi">+</span>
<span class="gi">+    def close(self, reason: str) -&gt; None:</span>
<span class="gi">+        if self.file:</span>
<span class="gi">+            self.file.close()</span>
<span class="gi">+</span>
<span class="gi">+    def log(self, request: Request, spider: Spider) -&gt; None:</span>
<span class="gi">+        if self.debug:</span>
<span class="gi">+            msg = &quot;Filtered duplicate request: %(request)s (referer: %(referer)s)&quot;</span>
<span class="gi">+            args = {&quot;request&quot;: request, &quot;referer&quot;: referer_str(request)}</span>
<span class="gi">+            self.logger.debug(msg, args, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+        elif self.logdupes:</span>
<span class="gi">+            msg = (</span>
<span class="gi">+                &quot;Filtered duplicate request: %(request)s&quot;</span>
<span class="gi">+                &quot; - no more duplicates will be shown&quot;</span>
<span class="gi">+                &quot; (see DUPEFILTER_DEBUG to show all duplicates)&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            self.logger.debug(msg, {&quot;request&quot;: request}, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+            self.logdupes = False</span>
<span class="gi">+</span>
<span class="gi">+        assert spider.crawler.stats</span>
<span class="gi">+        spider.crawler.stats.inc_value(&quot;dupefilter/filtered&quot;, spider=spider)</span>
<span class="gh">diff --git a/scrapy/exceptions.py b/scrapy/exceptions.py</span>
<span class="gh">index 5dee6b7da..6d188c489 100644</span>
<span class="gd">--- a/scrapy/exceptions.py</span>
<span class="gi">+++ b/scrapy/exceptions.py</span>
<span class="gu">@@ -6,9 +6,12 @@ new exceptions here without documenting them there.</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from typing import Any

<span class="gi">+# Internal</span>
<span class="gi">+</span>

<span class="w"> </span>class NotConfigured(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Indicates a missing configuration situation&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="gu">@@ -17,22 +20,27 @@ class _InvalidOutput(TypeError):</span>
<span class="w"> </span>    Indicates an invalid value has been returned by a middleware&#39;s processing method.
<span class="w"> </span>    Internal and undocumented, it should not be raised or caught by user code.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="gi">+# HTTP and crawling</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class IgnoreRequest(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Indicates a decision was made not to process a request&quot;&quot;&quot;


<span class="w"> </span>class DontCloseSpider(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Request the spider not to be closed yet&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="w"> </span>class CloseSpider(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Raise this from callbacks to request the spider to be closed&quot;&quot;&quot;

<span class="gd">-    def __init__(self, reason: str=&#39;cancelled&#39;):</span>
<span class="gi">+    def __init__(self, reason: str = &quot;cancelled&quot;):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.reason = reason

<span class="gu">@@ -44,26 +52,34 @@ class StopDownload(Exception):</span>
<span class="w"> </span>    should be handled by the request errback. Note that &#39;fail&#39; is a keyword-only argument.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, *, fail: bool=True):</span>
<span class="gi">+    def __init__(self, *, fail: bool = True):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.fail = fail


<span class="gi">+# Items</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class DropItem(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Drop item from the item pipeline&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="w"> </span>class NotSupported(Exception):
<span class="w"> </span>    &quot;&quot;&quot;Indicates a feature or method is not supported&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="gi">+# Commands</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class UsageError(Exception):
<span class="w"> </span>    &quot;&quot;&quot;To indicate a command-line usage error&quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, *a: Any, **kw: Any):
<span class="gd">-        self.print_help = kw.pop(&#39;print_help&#39;, True)</span>
<span class="gi">+        self.print_help = kw.pop(&quot;print_help&quot;, True)</span>
<span class="w"> </span>        super().__init__(*a, **kw)


<span class="gu">@@ -71,9 +87,11 @@ class ScrapyDeprecationWarning(Warning):</span>
<span class="w"> </span>    &quot;&quot;&quot;Warning category for deprecated features, since the default
<span class="w"> </span>    DeprecationWarning is silenced on Python 2.7+
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass


<span class="w"> </span>class ContractFail(AssertionError):
<span class="w"> </span>    &quot;&quot;&quot;Error raised in case of a failing contract&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    pass
<span class="gh">diff --git a/scrapy/exporters.py b/scrapy/exporters.py</span>
<span class="gh">index d22653341..f85f1dad8 100644</span>
<span class="gd">--- a/scrapy/exporters.py</span>
<span class="gi">+++ b/scrapy/exporters.py</span>
<span class="gu">@@ -1,6 +1,7 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Item Exporters are used to export/serialize items into different formats.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import csv
<span class="w"> </span>import io
<span class="w"> </span>import marshal
<span class="gu">@@ -8,17 +9,26 @@ import pickle</span>
<span class="w"> </span>import pprint
<span class="w"> </span>from collections.abc import Mapping
<span class="w"> </span>from xml.sax.saxutils import XMLGenerator
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter, is_item
<span class="gi">+</span>
<span class="w"> </span>from scrapy.item import Item
<span class="w"> </span>from scrapy.utils.python import is_listlike, to_bytes, to_unicode
<span class="w"> </span>from scrapy.utils.serialize import ScrapyJSONEncoder
<span class="gd">-__all__ = [&#39;BaseItemExporter&#39;, &#39;PprintItemExporter&#39;, &#39;PickleItemExporter&#39;,</span>
<span class="gd">-    &#39;CsvItemExporter&#39;, &#39;XmlItemExporter&#39;, &#39;JsonLinesItemExporter&#39;,</span>
<span class="gd">-    &#39;JsonItemExporter&#39;, &#39;MarshalItemExporter&#39;]</span>

<span class="gi">+__all__ = [</span>
<span class="gi">+    &quot;BaseItemExporter&quot;,</span>
<span class="gi">+    &quot;PprintItemExporter&quot;,</span>
<span class="gi">+    &quot;PickleItemExporter&quot;,</span>
<span class="gi">+    &quot;CsvItemExporter&quot;,</span>
<span class="gi">+    &quot;XmlItemExporter&quot;,</span>
<span class="gi">+    &quot;JsonLinesItemExporter&quot;,</span>
<span class="gi">+    &quot;JsonItemExporter&quot;,</span>
<span class="gi">+    &quot;MarshalItemExporter&quot;,</span>
<span class="gi">+]</span>

<span class="gd">-class BaseItemExporter:</span>

<span class="gi">+class BaseItemExporter:</span>
<span class="w"> </span>    def __init__(self, *, dont_fail=False, **kwargs):
<span class="w"> </span>        self._kwargs = kwargs
<span class="w"> </span>        self._configure(kwargs, dont_fail=dont_fail)
<span class="gu">@@ -28,72 +38,257 @@ class BaseItemExporter:</span>
<span class="w"> </span>        If dont_fail is set, it won&#39;t raise an exception on unexpected options
<span class="w"> </span>        (useful for using with keyword arguments in subclasses ``__init__`` methods)
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gi">+        self.encoding = options.pop(&quot;encoding&quot;, None)</span>
<span class="gi">+        self.fields_to_export = options.pop(&quot;fields_to_export&quot;, None)</span>
<span class="gi">+        self.export_empty_fields = options.pop(&quot;export_empty_fields&quot;, False)</span>
<span class="gi">+        self.indent = options.pop(&quot;indent&quot;, None)</span>
<span class="gi">+        if not dont_fail and options:</span>
<span class="gi">+            raise TypeError(f&quot;Unexpected options: {&#39;, &#39;.join(options.keys())}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    def serialize_field(self, field, name, value):</span>
<span class="gi">+        serializer = field.get(&quot;serializer&quot;, lambda x: x)</span>
<span class="gi">+        return serializer(value)</span>
<span class="gi">+</span>
<span class="gi">+    def start_exporting(self):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def finish_exporting(self):</span>
<span class="w"> </span>        pass

<span class="gd">-    def _get_serialized_fields(self, item, default_value=None,</span>
<span class="gd">-        include_empty=None):</span>
<span class="gi">+    def _get_serialized_fields(self, item, default_value=None, include_empty=None):</span>
<span class="w"> </span>        &quot;&quot;&quot;Return the fields to export as an iterable of tuples
<span class="w"> </span>        (name, serialized_value)
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        item = ItemAdapter(item)</span>

<span class="gi">+        if include_empty is None:</span>
<span class="gi">+            include_empty = self.export_empty_fields</span>

<span class="gd">-class JsonLinesItemExporter(BaseItemExporter):</span>
<span class="gi">+        if self.fields_to_export is None:</span>
<span class="gi">+            if include_empty:</span>
<span class="gi">+                field_iter = item.field_names()</span>
<span class="gi">+            else:</span>
<span class="gi">+                field_iter = item.keys()</span>
<span class="gi">+        elif isinstance(self.fields_to_export, Mapping):</span>
<span class="gi">+            if include_empty:</span>
<span class="gi">+                field_iter = self.fields_to_export.items()</span>
<span class="gi">+            else:</span>
<span class="gi">+                field_iter = (</span>
<span class="gi">+                    (x, y) for x, y in self.fields_to_export.items() if x in item</span>
<span class="gi">+                )</span>
<span class="gi">+        else:</span>
<span class="gi">+            if include_empty:</span>
<span class="gi">+                field_iter = self.fields_to_export</span>
<span class="gi">+            else:</span>
<span class="gi">+                field_iter = (x for x in self.fields_to_export if x in item)</span>
<span class="gi">+</span>
<span class="gi">+        for field_name in field_iter:</span>
<span class="gi">+            if isinstance(field_name, str):</span>
<span class="gi">+                item_field, output_field = field_name, field_name</span>
<span class="gi">+            else:</span>
<span class="gi">+                item_field, output_field = field_name</span>
<span class="gi">+            if item_field in item:</span>
<span class="gi">+                field_meta = item.get_field_meta(item_field)</span>
<span class="gi">+                value = self.serialize_field(field_meta, output_field, item[item_field])</span>
<span class="gi">+            else:</span>
<span class="gi">+                value = default_value</span>

<span class="gi">+            yield output_field, value</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class JsonLinesItemExporter(BaseItemExporter):</span>
<span class="w"> </span>    def __init__(self, file, **kwargs):
<span class="w"> </span>        super().__init__(dont_fail=True, **kwargs)
<span class="w"> </span>        self.file = file
<span class="gd">-        self._kwargs.setdefault(&#39;ensure_ascii&#39;, not self.encoding)</span>
<span class="gi">+        self._kwargs.setdefault(&quot;ensure_ascii&quot;, not self.encoding)</span>
<span class="w"> </span>        self.encoder = ScrapyJSONEncoder(**self._kwargs)

<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        itemdict = dict(self._get_serialized_fields(item))</span>
<span class="gi">+        data = self.encoder.encode(itemdict) + &quot;\n&quot;</span>
<span class="gi">+        self.file.write(to_bytes(data, self.encoding))</span>

<span class="gd">-class JsonItemExporter(BaseItemExporter):</span>

<span class="gi">+class JsonItemExporter(BaseItemExporter):</span>
<span class="w"> </span>    def __init__(self, file, **kwargs):
<span class="w"> </span>        super().__init__(dont_fail=True, **kwargs)
<span class="w"> </span>        self.file = file
<span class="gd">-        json_indent = (self.indent if self.indent is not None and self.</span>
<span class="gd">-            indent &gt; 0 else None)</span>
<span class="gd">-        self._kwargs.setdefault(&#39;indent&#39;, json_indent)</span>
<span class="gd">-        self._kwargs.setdefault(&#39;ensure_ascii&#39;, not self.encoding)</span>
<span class="gi">+        # there is a small difference between the behaviour or JsonItemExporter.indent</span>
<span class="gi">+        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent</span>
<span class="gi">+        # the addition of newlines everywhere</span>
<span class="gi">+        json_indent = (</span>
<span class="gi">+            self.indent if self.indent is not None and self.indent &gt; 0 else None</span>
<span class="gi">+        )</span>
<span class="gi">+        self._kwargs.setdefault(&quot;indent&quot;, json_indent)</span>
<span class="gi">+        self._kwargs.setdefault(&quot;ensure_ascii&quot;, not self.encoding)</span>
<span class="w"> </span>        self.encoder = ScrapyJSONEncoder(**self._kwargs)
<span class="w"> </span>        self.first_item = True

<span class="gi">+    def _beautify_newline(self):</span>
<span class="gi">+        if self.indent is not None:</span>
<span class="gi">+            self.file.write(b&quot;\n&quot;)</span>

<span class="gd">-class XmlItemExporter(BaseItemExporter):</span>
<span class="gi">+    def _add_comma_after_first(self):</span>
<span class="gi">+        if self.first_item:</span>
<span class="gi">+            self.first_item = False</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.file.write(b&quot;,&quot;)</span>
<span class="gi">+            self._beautify_newline()</span>

<span class="gi">+    def start_exporting(self):</span>
<span class="gi">+        self.file.write(b&quot;[&quot;)</span>
<span class="gi">+        self._beautify_newline()</span>
<span class="gi">+</span>
<span class="gi">+    def finish_exporting(self):</span>
<span class="gi">+        self._beautify_newline()</span>
<span class="gi">+        self.file.write(b&quot;]&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        itemdict = dict(self._get_serialized_fields(item))</span>
<span class="gi">+        data = to_bytes(self.encoder.encode(itemdict), self.encoding)</span>
<span class="gi">+        self._add_comma_after_first()</span>
<span class="gi">+        self.file.write(data)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class XmlItemExporter(BaseItemExporter):</span>
<span class="w"> </span>    def __init__(self, file, **kwargs):
<span class="gd">-        self.item_element = kwargs.pop(&#39;item_element&#39;, &#39;item&#39;)</span>
<span class="gd">-        self.root_element = kwargs.pop(&#39;root_element&#39;, &#39;items&#39;)</span>
<span class="gi">+        self.item_element = kwargs.pop(&quot;item_element&quot;, &quot;item&quot;)</span>
<span class="gi">+        self.root_element = kwargs.pop(&quot;root_element&quot;, &quot;items&quot;)</span>
<span class="w"> </span>        super().__init__(**kwargs)
<span class="w"> </span>        if not self.encoding:
<span class="gd">-            self.encoding = &#39;utf-8&#39;</span>
<span class="gi">+            self.encoding = &quot;utf-8&quot;</span>
<span class="w"> </span>        self.xg = XMLGenerator(file, encoding=self.encoding)

<span class="gi">+    def _beautify_newline(self, new_item=False):</span>
<span class="gi">+        if self.indent is not None and (self.indent &gt; 0 or new_item):</span>
<span class="gi">+            self.xg.characters(&quot;\n&quot;)</span>

<span class="gd">-class CsvItemExporter(BaseItemExporter):</span>
<span class="gi">+    def _beautify_indent(self, depth=1):</span>
<span class="gi">+        if self.indent:</span>
<span class="gi">+            self.xg.characters(&quot; &quot; * self.indent * depth)</span>

<span class="gd">-    def __init__(self, file, include_headers_line=True, join_multivalued=</span>
<span class="gd">-        &#39;,&#39;, errors=None, **kwargs):</span>
<span class="gi">+    def start_exporting(self):</span>
<span class="gi">+        self.xg.startDocument()</span>
<span class="gi">+        self.xg.startElement(self.root_element, {})</span>
<span class="gi">+        self._beautify_newline(new_item=True)</span>
<span class="gi">+</span>
<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        self._beautify_indent(depth=1)</span>
<span class="gi">+        self.xg.startElement(self.item_element, {})</span>
<span class="gi">+        self._beautify_newline()</span>
<span class="gi">+        for name, value in self._get_serialized_fields(item, default_value=&quot;&quot;):</span>
<span class="gi">+            self._export_xml_field(name, value, depth=2)</span>
<span class="gi">+        self._beautify_indent(depth=1)</span>
<span class="gi">+        self.xg.endElement(self.item_element)</span>
<span class="gi">+        self._beautify_newline(new_item=True)</span>
<span class="gi">+</span>
<span class="gi">+    def finish_exporting(self):</span>
<span class="gi">+        self.xg.endElement(self.root_element)</span>
<span class="gi">+        self.xg.endDocument()</span>
<span class="gi">+</span>
<span class="gi">+    def _export_xml_field(self, name, serialized_value, depth):</span>
<span class="gi">+        self._beautify_indent(depth=depth)</span>
<span class="gi">+        self.xg.startElement(name, {})</span>
<span class="gi">+        if hasattr(serialized_value, &quot;items&quot;):</span>
<span class="gi">+            self._beautify_newline()</span>
<span class="gi">+            for subname, value in serialized_value.items():</span>
<span class="gi">+                self._export_xml_field(subname, value, depth=depth + 1)</span>
<span class="gi">+            self._beautify_indent(depth=depth)</span>
<span class="gi">+        elif is_listlike(serialized_value):</span>
<span class="gi">+            self._beautify_newline()</span>
<span class="gi">+            for value in serialized_value:</span>
<span class="gi">+                self._export_xml_field(&quot;value&quot;, value, depth=depth + 1)</span>
<span class="gi">+            self._beautify_indent(depth=depth)</span>
<span class="gi">+        elif isinstance(serialized_value, str):</span>
<span class="gi">+            self.xg.characters(serialized_value)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.xg.characters(str(serialized_value))</span>
<span class="gi">+        self.xg.endElement(name)</span>
<span class="gi">+        self._beautify_newline()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class CsvItemExporter(BaseItemExporter):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        file,</span>
<span class="gi">+        include_headers_line=True,</span>
<span class="gi">+        join_multivalued=&quot;,&quot;,</span>
<span class="gi">+        errors=None,</span>
<span class="gi">+        **kwargs,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(dont_fail=True, **kwargs)
<span class="w"> </span>        if not self.encoding:
<span class="gd">-            self.encoding = &#39;utf-8&#39;</span>
<span class="gi">+            self.encoding = &quot;utf-8&quot;</span>
<span class="w"> </span>        self.include_headers_line = include_headers_line
<span class="gd">-        self.stream = io.TextIOWrapper(file, line_buffering=False,</span>
<span class="gd">-            write_through=True, encoding=self.encoding, newline=&#39;&#39;, errors=</span>
<span class="gd">-            errors)</span>
<span class="gi">+        self.stream = io.TextIOWrapper(</span>
<span class="gi">+            file,</span>
<span class="gi">+            line_buffering=False,</span>
<span class="gi">+            write_through=True,</span>
<span class="gi">+            encoding=self.encoding,</span>
<span class="gi">+            newline=&quot;&quot;,  # Windows needs this https://github.com/scrapy/scrapy/issues/3034</span>
<span class="gi">+            errors=errors,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.csv_writer = csv.writer(self.stream, **self._kwargs)
<span class="w"> </span>        self._headers_not_written = True
<span class="w"> </span>        self._join_multivalued = join_multivalued

<span class="gi">+    def serialize_field(self, field, name, value):</span>
<span class="gi">+        serializer = field.get(&quot;serializer&quot;, self._join_if_needed)</span>
<span class="gi">+        return serializer(value)</span>
<span class="gi">+</span>
<span class="gi">+    def _join_if_needed(self, value):</span>
<span class="gi">+        if isinstance(value, (list, tuple)):</span>
<span class="gi">+            try:</span>
<span class="gi">+                return self._join_multivalued.join(value)</span>
<span class="gi">+            except TypeError:  # list in value may not contain strings</span>
<span class="gi">+                pass</span>
<span class="gi">+        return value</span>
<span class="gi">+</span>
<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        if self._headers_not_written:</span>
<span class="gi">+            self._headers_not_written = False</span>
<span class="gi">+            self._write_headers_and_set_fields_to_export(item)</span>
<span class="gi">+</span>
<span class="gi">+        fields = self._get_serialized_fields(item, default_value=&quot;&quot;, include_empty=True)</span>
<span class="gi">+        values = list(self._build_row(x for _, x in fields))</span>
<span class="gi">+        self.csv_writer.writerow(values)</span>
<span class="gi">+</span>
<span class="gi">+    def finish_exporting(self):</span>
<span class="gi">+        self.stream.detach()  # Avoid closing the wrapped file.</span>
<span class="gi">+</span>
<span class="gi">+    def _build_row(self, values):</span>
<span class="gi">+        for s in values:</span>
<span class="gi">+            try:</span>
<span class="gi">+                yield to_unicode(s, self.encoding)</span>
<span class="gi">+            except TypeError:</span>
<span class="gi">+                yield s</span>
<span class="gi">+</span>
<span class="gi">+    def _write_headers_and_set_fields_to_export(self, item):</span>
<span class="gi">+        if self.include_headers_line:</span>
<span class="gi">+            if not self.fields_to_export:</span>
<span class="gi">+                # use declared field names, or keys if the item is a dict</span>
<span class="gi">+                self.fields_to_export = ItemAdapter(item).field_names()</span>
<span class="gi">+            if isinstance(self.fields_to_export, Mapping):</span>
<span class="gi">+                fields = self.fields_to_export.values()</span>
<span class="gi">+            else:</span>
<span class="gi">+                fields = self.fields_to_export</span>
<span class="gi">+            row = list(self._build_row(fields))</span>
<span class="gi">+            self.csv_writer.writerow(row)</span>

<span class="gd">-class PickleItemExporter(BaseItemExporter):</span>

<span class="gi">+class PickleItemExporter(BaseItemExporter):</span>
<span class="w"> </span>    def __init__(self, file, protocol=4, **kwargs):
<span class="w"> </span>        super().__init__(**kwargs)
<span class="w"> </span>        self.file = file
<span class="w"> </span>        self.protocol = protocol

<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        d = dict(self._get_serialized_fields(item))</span>
<span class="gi">+        pickle.dump(d, self.file, self.protocol)</span>
<span class="gi">+</span>

<span class="w"> </span>class MarshalItemExporter(BaseItemExporter):
<span class="w"> </span>    &quot;&quot;&quot;Exports items in a Python-specific binary format (see
<span class="gu">@@ -108,13 +303,19 @@ class MarshalItemExporter(BaseItemExporter):</span>
<span class="w"> </span>        super().__init__(**kwargs)
<span class="w"> </span>        self.file = file

<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        marshal.dump(dict(self._get_serialized_fields(item)), self.file)</span>

<span class="gd">-class PprintItemExporter(BaseItemExporter):</span>

<span class="gi">+class PprintItemExporter(BaseItemExporter):</span>
<span class="w"> </span>    def __init__(self, file, **kwargs):
<span class="w"> </span>        super().__init__(**kwargs)
<span class="w"> </span>        self.file = file

<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        itemdict = dict(self._get_serialized_fields(item))</span>
<span class="gi">+        self.file.write(to_bytes(pprint.pformat(itemdict) + &quot;\n&quot;))</span>
<span class="gi">+</span>

<span class="w"> </span>class PythonItemExporter(BaseItemExporter):
<span class="w"> </span>    &quot;&quot;&quot;This is a base class for item exporters that extends
<span class="gu">@@ -125,3 +326,31 @@ class PythonItemExporter(BaseItemExporter):</span>

<span class="w"> </span>    .. _msgpack: https://pypi.org/project/msgpack/
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+    def _configure(self, options, dont_fail=False):</span>
<span class="gi">+        super()._configure(options, dont_fail)</span>
<span class="gi">+        if not self.encoding:</span>
<span class="gi">+            self.encoding = &quot;utf-8&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def serialize_field(self, field, name, value):</span>
<span class="gi">+        serializer = field.get(&quot;serializer&quot;, self._serialize_value)</span>
<span class="gi">+        return serializer(value)</span>
<span class="gi">+</span>
<span class="gi">+    def _serialize_value(self, value):</span>
<span class="gi">+        if isinstance(value, Item):</span>
<span class="gi">+            return self.export_item(value)</span>
<span class="gi">+        if is_item(value):</span>
<span class="gi">+            return dict(self._serialize_item(value))</span>
<span class="gi">+        if is_listlike(value):</span>
<span class="gi">+            return [self._serialize_value(v) for v in value]</span>
<span class="gi">+        if isinstance(value, (str, bytes)):</span>
<span class="gi">+            return to_unicode(value, encoding=self.encoding)</span>
<span class="gi">+        return value</span>
<span class="gi">+</span>
<span class="gi">+    def _serialize_item(self, item):</span>
<span class="gi">+        for key, value in ItemAdapter(item).items():</span>
<span class="gi">+            yield key, self._serialize_value(value)</span>
<span class="gi">+</span>
<span class="gi">+    def export_item(self, item):</span>
<span class="gi">+        result = dict(self._get_serialized_fields(item))</span>
<span class="gi">+        return result</span>
<span class="gh">diff --git a/scrapy/extension.py b/scrapy/extension.py</span>
<span class="gh">index 27464cc17..4e365cfa1 100644</span>
<span class="gd">--- a/scrapy/extension.py</span>
<span class="gi">+++ b/scrapy/extension.py</span>
<span class="gu">@@ -8,4 +8,8 @@ from scrapy.utils.conf import build_component_list</span>


<span class="w"> </span>class ExtensionManager(MiddlewareManager):
<span class="gd">-    component_name = &#39;extension&#39;</span>
<span class="gi">+    component_name = &quot;extension&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _get_mwlist_from_settings(cls, settings):</span>
<span class="gi">+        return build_component_list(settings.getwithbase(&quot;EXTENSIONS&quot;))</span>
<span class="gh">diff --git a/scrapy/extensions/closespider.py b/scrapy/extensions/closespider.py</span>
<span class="gh">index a01d48a8e..4307b4170 100644</span>
<span class="gd">--- a/scrapy/extensions/closespider.py</span>
<span class="gi">+++ b/scrapy/extensions/closespider.py</span>
<span class="gu">@@ -3,44 +3,110 @@ conditions are met.</span>

<span class="w"> </span>See documentation in docs/topics/extensions.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>from collections import defaultdict
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class CloseSpider:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="w"> </span>        self.crawler = crawler
<span class="gd">-        self.close_on = {&#39;timeout&#39;: crawler.settings.getfloat(</span>
<span class="gd">-            &#39;CLOSESPIDER_TIMEOUT&#39;), &#39;itemcount&#39;: crawler.settings.getint(</span>
<span class="gd">-            &#39;CLOSESPIDER_ITEMCOUNT&#39;), &#39;pagecount&#39;: crawler.settings.getint(</span>
<span class="gd">-            &#39;CLOSESPIDER_PAGECOUNT&#39;), &#39;errorcount&#39;: crawler.settings.getint</span>
<span class="gd">-            (&#39;CLOSESPIDER_ERRORCOUNT&#39;), &#39;timeout_no_item&#39;: crawler.settings</span>
<span class="gd">-            .getint(&#39;CLOSESPIDER_TIMEOUT_NO_ITEM&#39;)}</span>
<span class="gi">+</span>
<span class="gi">+        self.close_on = {</span>
<span class="gi">+            &quot;timeout&quot;: crawler.settings.getfloat(&quot;CLOSESPIDER_TIMEOUT&quot;),</span>
<span class="gi">+            &quot;itemcount&quot;: crawler.settings.getint(&quot;CLOSESPIDER_ITEMCOUNT&quot;),</span>
<span class="gi">+            &quot;pagecount&quot;: crawler.settings.getint(&quot;CLOSESPIDER_PAGECOUNT&quot;),</span>
<span class="gi">+            &quot;errorcount&quot;: crawler.settings.getint(&quot;CLOSESPIDER_ERRORCOUNT&quot;),</span>
<span class="gi">+            &quot;timeout_no_item&quot;: crawler.settings.getint(&quot;CLOSESPIDER_TIMEOUT_NO_ITEM&quot;),</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="w"> </span>        if not any(self.close_on.values()):
<span class="w"> </span>            raise NotConfigured
<span class="gi">+</span>
<span class="w"> </span>        self.counter = defaultdict(int)
<span class="gd">-        if self.close_on.get(&#39;errorcount&#39;):</span>
<span class="gd">-            crawler.signals.connect(self.error_count, signal=signals.</span>
<span class="gd">-                spider_error)</span>
<span class="gd">-        if self.close_on.get(&#39;pagecount&#39;):</span>
<span class="gd">-            crawler.signals.connect(self.page_count, signal=signals.</span>
<span class="gd">-                response_received)</span>
<span class="gd">-        if self.close_on.get(&#39;timeout&#39;):</span>
<span class="gd">-            crawler.signals.connect(self.spider_opened, signal=signals.</span>
<span class="gd">-                spider_opened)</span>
<span class="gd">-        if self.close_on.get(&#39;itemcount&#39;):</span>
<span class="gd">-            crawler.signals.connect(self.item_scraped, signal=signals.</span>
<span class="gd">-                item_scraped)</span>
<span class="gd">-        if self.close_on.get(&#39;timeout_no_item&#39;):</span>
<span class="gd">-            self.timeout_no_item = self.close_on[&#39;timeout_no_item&#39;]</span>
<span class="gi">+</span>
<span class="gi">+        if self.close_on.get(&quot;errorcount&quot;):</span>
<span class="gi">+            crawler.signals.connect(self.error_count, signal=signals.spider_error)</span>
<span class="gi">+        if self.close_on.get(&quot;pagecount&quot;):</span>
<span class="gi">+            crawler.signals.connect(self.page_count, signal=signals.response_received)</span>
<span class="gi">+        if self.close_on.get(&quot;timeout&quot;):</span>
<span class="gi">+            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        if self.close_on.get(&quot;itemcount&quot;):</span>
<span class="gi">+            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)</span>
<span class="gi">+        if self.close_on.get(&quot;timeout_no_item&quot;):</span>
<span class="gi">+            self.timeout_no_item = self.close_on[&quot;timeout_no_item&quot;]</span>
<span class="gi">+            self.items_in_period = 0</span>
<span class="gi">+            crawler.signals.connect(</span>
<span class="gi">+                self.spider_opened_no_item, signal=signals.spider_opened</span>
<span class="gi">+            )</span>
<span class="gi">+            crawler.signals.connect(</span>
<span class="gi">+                self.item_scraped_no_item, signal=signals.item_scraped</span>
<span class="gi">+            )</span>
<span class="gi">+        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def error_count(self, failure, response, spider):</span>
<span class="gi">+        self.counter[&quot;errorcount&quot;] += 1</span>
<span class="gi">+        if self.counter[&quot;errorcount&quot;] == self.close_on[&quot;errorcount&quot;]:</span>
<span class="gi">+            self.crawler.engine.close_spider(spider, &quot;closespider_errorcount&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def page_count(self, response, request, spider):</span>
<span class="gi">+        self.counter[&quot;pagecount&quot;] += 1</span>
<span class="gi">+        if self.counter[&quot;pagecount&quot;] == self.close_on[&quot;pagecount&quot;]:</span>
<span class="gi">+            self.crawler.engine.close_spider(spider, &quot;closespider_pagecount&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        self.task = reactor.callLater(</span>
<span class="gi">+            self.close_on[&quot;timeout&quot;],</span>
<span class="gi">+            self.crawler.engine.close_spider,</span>
<span class="gi">+            spider,</span>
<span class="gi">+            reason=&quot;closespider_timeout&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def item_scraped(self, item, spider):</span>
<span class="gi">+        self.counter[&quot;itemcount&quot;] += 1</span>
<span class="gi">+        if self.counter[&quot;itemcount&quot;] == self.close_on[&quot;itemcount&quot;]:</span>
<span class="gi">+            self.crawler.engine.close_spider(spider, &quot;closespider_itemcount&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider):</span>
<span class="gi">+        task = getattr(self, &quot;task&quot;, False)</span>
<span class="gi">+        if task and task.active():</span>
<span class="gi">+            task.cancel()</span>
<span class="gi">+</span>
<span class="gi">+        task_no_item = getattr(self, &quot;task_no_item&quot;, False)</span>
<span class="gi">+        if task_no_item and task_no_item.running:</span>
<span class="gi">+            task_no_item.stop()</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened_no_item(self, spider):</span>
<span class="gi">+        from twisted.internet import task</span>
<span class="gi">+</span>
<span class="gi">+        self.task_no_item = task.LoopingCall(self._count_items_produced, spider)</span>
<span class="gi">+        self.task_no_item.start(self.timeout_no_item, now=False)</span>
<span class="gi">+</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            f&quot;Spider will stop when no items are produced after &quot;</span>
<span class="gi">+            f&quot;{self.timeout_no_item} seconds.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def item_scraped_no_item(self, item, spider):</span>
<span class="gi">+        self.items_in_period += 1</span>
<span class="gi">+</span>
<span class="gi">+    def _count_items_produced(self, spider):</span>
<span class="gi">+        if self.items_in_period &gt;= 1:</span>
<span class="w"> </span>            self.items_in_period = 0
<span class="gd">-            crawler.signals.connect(self.spider_opened_no_item, signal=</span>
<span class="gd">-                signals.spider_opened)</span>
<span class="gd">-            crawler.signals.connect(self.item_scraped_no_item, signal=</span>
<span class="gd">-                signals.item_scraped)</span>
<span class="gd">-        crawler.signals.connect(self.spider_closed, signal=signals.</span>
<span class="gd">-            spider_closed)</span>
<span class="gi">+        else:</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                f&quot;Closing spider since no items were produced in the last &quot;</span>
<span class="gi">+                f&quot;{self.timeout_no_item} seconds.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            self.crawler.engine.close_spider(spider, &quot;closespider_timeout_no_item&quot;)</span>
<span class="gh">diff --git a/scrapy/extensions/corestats.py b/scrapy/extensions/corestats.py</span>
<span class="gh">index c8451087e..302a615f2 100644</span>
<span class="gd">--- a/scrapy/extensions/corestats.py</span>
<span class="gi">+++ b/scrapy/extensions/corestats.py</span>
<span class="gu">@@ -2,11 +2,46 @@</span>
<span class="w"> </span>Extension for collecting core stats like items scraped and start/finish times
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from datetime import datetime, timezone
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals


<span class="w"> </span>class CoreStats:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, stats):
<span class="w"> </span>        self.stats = stats
<span class="w"> </span>        self.start_time = None
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls(crawler.stats)</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)</span>
<span class="gi">+        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)</span>
<span class="gi">+        crawler.signals.connect(o.response_received, signal=signals.response_received)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.start_time = datetime.now(tz=timezone.utc)</span>
<span class="gi">+        self.stats.set_value(&quot;start_time&quot;, self.start_time, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider, reason):</span>
<span class="gi">+        finish_time = datetime.now(tz=timezone.utc)</span>
<span class="gi">+        elapsed_time = finish_time - self.start_time</span>
<span class="gi">+        elapsed_time_seconds = elapsed_time.total_seconds()</span>
<span class="gi">+        self.stats.set_value(</span>
<span class="gi">+            &quot;elapsed_time_seconds&quot;, elapsed_time_seconds, spider=spider</span>
<span class="gi">+        )</span>
<span class="gi">+        self.stats.set_value(&quot;finish_time&quot;, finish_time, spider=spider)</span>
<span class="gi">+        self.stats.set_value(&quot;finish_reason&quot;, reason, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def item_scraped(self, item, spider):</span>
<span class="gi">+        self.stats.inc_value(&quot;item_scraped_count&quot;, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def response_received(self, spider):</span>
<span class="gi">+        self.stats.inc_value(&quot;response_received_count&quot;, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def item_dropped(self, item, spider, exception):</span>
<span class="gi">+        reason = exception.__class__.__name__</span>
<span class="gi">+        self.stats.inc_value(&quot;item_dropped_count&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(f&quot;item_dropped_reasons_count/{reason}&quot;, spider=spider)</span>
<span class="gh">diff --git a/scrapy/extensions/debug.py b/scrapy/extensions/debug.py</span>
<span class="gh">index cac078bc7..1b6c7777f 100644</span>
<span class="gd">--- a/scrapy/extensions/debug.py</span>
<span class="gi">+++ b/scrapy/extensions/debug.py</span>
<span class="gu">@@ -3,32 +3,64 @@ Extensions for debugging Scrapy</span>

<span class="w"> </span>See documentation in docs/topics/extensions.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import signal
<span class="w"> </span>import sys
<span class="w"> </span>import threading
<span class="w"> </span>import traceback
<span class="w"> </span>from pdb import Pdb
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.engine import format_engine_status
<span class="w"> </span>from scrapy.utils.trackref import format_live_refs
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class StackTraceDump:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler=None):
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        try:
<span class="w"> </span>            signal.signal(signal.SIGUSR2, self.dump_stacktrace)
<span class="w"> </span>            signal.signal(signal.SIGQUIT, self.dump_stacktrace)
<span class="w"> </span>        except AttributeError:
<span class="gi">+            # win32 platforms don&#39;t support SIGUSR signals</span>
<span class="w"> </span>            pass

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>

<span class="gd">-class Debugger:</span>
<span class="gi">+    def dump_stacktrace(self, signum, frame):</span>
<span class="gi">+        log_args = {</span>
<span class="gi">+            &quot;stackdumps&quot;: self._thread_stacks(),</span>
<span class="gi">+            &quot;enginestatus&quot;: format_engine_status(self.crawler.engine),</span>
<span class="gi">+            &quot;liverefs&quot;: format_live_refs(),</span>
<span class="gi">+        }</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Dumping stack trace and engine status\n&quot;</span>
<span class="gi">+            &quot;%(enginestatus)s\n%(liverefs)s\n%(stackdumps)s&quot;,</span>
<span class="gi">+            log_args,</span>
<span class="gi">+            extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+        )</span>

<span class="gi">+    def _thread_stacks(self):</span>
<span class="gi">+        id2name = dict((th.ident, th.name) for th in threading.enumerate())</span>
<span class="gi">+        dumps = &quot;&quot;</span>
<span class="gi">+        for id_, frame in sys._current_frames().items():</span>
<span class="gi">+            name = id2name.get(id_, &quot;&quot;)</span>
<span class="gi">+            dump = &quot;&quot;.join(traceback.format_stack(frame))</span>
<span class="gi">+            dumps += f&quot;# Thread: {name}({id_})\n{dump}\n&quot;</span>
<span class="gi">+        return dumps</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class Debugger:</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        try:
<span class="w"> </span>            signal.signal(signal.SIGUSR2, self._enter_debugger)
<span class="w"> </span>        except AttributeError:
<span class="gi">+            # win32 platforms don&#39;t support SIGUSR signals</span>
<span class="w"> </span>            pass
<span class="gi">+</span>
<span class="gi">+    def _enter_debugger(self, signum, frame):</span>
<span class="gi">+        Pdb().set_trace(frame.f_back)</span>
<span class="gh">diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py</span>
<span class="gh">index a30ae2dbd..4e846d1bd 100644</span>
<span class="gd">--- a/scrapy/extensions/feedexport.py</span>
<span class="gi">+++ b/scrapy/extensions/feedexport.py</span>
<span class="gu">@@ -3,6 +3,7 @@ Feed Exports extension</span>

<span class="w"> </span>See documentation in docs/topics/feed-exports.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="w"> </span>import sys
<span class="gu">@@ -12,10 +13,12 @@ from pathlib import Path, PureWindowsPath</span>
<span class="w"> </span>from tempfile import NamedTemporaryFile
<span class="w"> </span>from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Union
<span class="w"> </span>from urllib.parse import unquote, urlparse
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer, threads
<span class="w"> </span>from twisted.internet.defer import DeferredList
<span class="w"> </span>from w3lib.url import file_uri_to_path
<span class="w"> </span>from zope.interface import Interface, implementer
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider, signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.extensions.postprocessing import PostProcessingManager
<span class="gu">@@ -27,14 +30,32 @@ from scrapy.utils.ftp import ftp_store_file</span>
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="w"> </span>from scrapy.utils.python import get_func_args, without_none_values
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gd">-    import boto3</span>
<span class="gi">+    import boto3  # noqa: F401</span>
<span class="gi">+</span>
<span class="w"> </span>    IS_BOTO3_AVAILABLE = True
<span class="w"> </span>except ImportError:
<span class="w"> </span>    IS_BOTO3_AVAILABLE = False


<span class="gi">+def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):</span>
<span class="gi">+    argument_names = get_func_args(builder)</span>
<span class="gi">+    if &quot;feed_options&quot; in argument_names:</span>
<span class="gi">+        kwargs[&quot;feed_options&quot;] = feed_options</span>
<span class="gi">+    else:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            f&quot;{builder.__qualname__} does not support the &#39;feed_options&#39; keyword argument. Add a &quot;</span>
<span class="gi">+            &quot;&#39;feed_options&#39; parameter to its signature to remove this &quot;</span>
<span class="gi">+            &quot;warning. This parameter will become mandatory in a future &quot;</span>
<span class="gi">+            &quot;version of Scrapy.&quot;,</span>
<span class="gi">+            category=ScrapyDeprecationWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+    return builder(*preargs, uri, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class ItemFilter:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    This will be used by FeedExporter to decide if an item should be allowed
<span class="gu">@@ -43,18 +64,21 @@ class ItemFilter:</span>
<span class="w"> </span>    :param feed_options: feed specific options passed from FeedExporter
<span class="w"> </span>    :type feed_options: dict
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    feed_options: Optional[dict]
<span class="w"> </span>    item_classes: Tuple

<span class="gd">-    def __init__(self, feed_options: Optional[dict]) -&gt;None:</span>
<span class="gi">+    def __init__(self, feed_options: Optional[dict]) -&gt; None:</span>
<span class="w"> </span>        self.feed_options = feed_options
<span class="w"> </span>        if feed_options is not None:
<span class="gd">-            self.item_classes = tuple(load_object(item_class) for</span>
<span class="gd">-                item_class in feed_options.get(&#39;item_classes&#39;) or ())</span>
<span class="gi">+            self.item_classes = tuple(</span>
<span class="gi">+                load_object(item_class)</span>
<span class="gi">+                for item_class in feed_options.get(&quot;item_classes&quot;) or ()</span>
<span class="gi">+            )</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.item_classes = tuple()

<span class="gd">-    def accepts(self, item: Any) -&gt;bool:</span>
<span class="gi">+    def accepts(self, item: Any) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return ``True`` if `item` should be exported or ``False`` otherwise.

<span class="gu">@@ -63,7 +87,9 @@ class ItemFilter:</span>
<span class="w"> </span>        :return: `True` if accepted, `False` otherwise
<span class="w"> </span>        :rtype: bool
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.item_classes:</span>
<span class="gi">+            return isinstance(item, self.item_classes)</span>
<span class="gi">+        return True  # accept all items by default</span>


<span class="w"> </span>class IFeedStorage(Interface):
<span class="gu">@@ -76,175 +102,437 @@ class IFeedStorage(Interface):</span>
<span class="w"> </span>    def open(spider):
<span class="w"> </span>        &quot;&quot;&quot;Open the storage for the given spider. It must return a file-like
<span class="w"> </span>        object that will be used for the exporters&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def store(file):
<span class="w"> </span>        &quot;&quot;&quot;Store the given file stream&quot;&quot;&quot;
<span class="gd">-        pass</span>


<span class="w"> </span>@implementer(IFeedStorage)
<span class="w"> </span>class BlockingFeedStorage:
<span class="gd">-    pass</span>
<span class="gi">+    def open(self, spider):</span>
<span class="gi">+        path = spider.crawler.settings[&quot;FEED_TEMPDIR&quot;]</span>
<span class="gi">+        if path and not Path(path).is_dir():</span>
<span class="gi">+            raise OSError(&quot;Not a Directory: &quot; + str(path))</span>
<span class="gi">+</span>
<span class="gi">+        return NamedTemporaryFile(prefix=&quot;feed-&quot;, dir=path)</span>
<span class="gi">+</span>
<span class="gi">+    def store(self, file):</span>
<span class="gi">+        return threads.deferToThread(self._store_in_thread, file)</span>
<span class="gi">+</span>
<span class="gi">+    def _store_in_thread(self, file):</span>
<span class="gi">+        raise NotImplementedError</span>


<span class="w"> </span>@implementer(IFeedStorage)
<span class="w"> </span>class StdoutFeedStorage:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, uri, _stdout=None, *, feed_options=None):
<span class="w"> </span>        if not _stdout:
<span class="w"> </span>            _stdout = sys.stdout.buffer
<span class="w"> </span>        self._stdout = _stdout
<span class="gd">-        if feed_options and feed_options.get(&#39;overwrite&#39;, False) is True:</span>
<span class="gi">+        if feed_options and feed_options.get(&quot;overwrite&quot;, False) is True:</span>
<span class="w"> </span>            logger.warning(
<span class="gd">-                &#39;Standard output (stdout) storage does not support overwriting. To suppress this warning, remove the overwrite option from your FEEDS setting, or set it to False.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                &quot;Standard output (stdout) storage does not support &quot;</span>
<span class="gi">+                &quot;overwriting. To suppress this warning, remove the &quot;</span>
<span class="gi">+                &quot;overwrite option from your FEEDS setting, or set &quot;</span>
<span class="gi">+                &quot;it to False.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def open(self, spider):</span>
<span class="gi">+        return self._stdout</span>
<span class="gi">+</span>
<span class="gi">+    def store(self, file):</span>
<span class="gi">+        pass</span>


<span class="w"> </span>@implementer(IFeedStorage)
<span class="w"> </span>class FileFeedStorage:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, uri, *, feed_options=None):
<span class="w"> </span>        self.path = file_uri_to_path(uri)
<span class="w"> </span>        feed_options = feed_options or {}
<span class="gd">-        self.write_mode = &#39;wb&#39; if feed_options.get(&#39;overwrite&#39;, False</span>
<span class="gd">-            ) else &#39;ab&#39;</span>
<span class="gi">+        self.write_mode = &quot;wb&quot; if feed_options.get(&quot;overwrite&quot;, False) else &quot;ab&quot;</span>

<span class="gi">+    def open(self, spider) -&gt; IO[Any]:</span>
<span class="gi">+        dirname = Path(self.path).parent</span>
<span class="gi">+        if dirname and not dirname.exists():</span>
<span class="gi">+            dirname.mkdir(parents=True)</span>
<span class="gi">+        return Path(self.path).open(self.write_mode)</span>

<span class="gd">-class S3FeedStorage(BlockingFeedStorage):</span>
<span class="gi">+    def store(self, file):</span>
<span class="gi">+        file.close()</span>

<span class="gd">-    def __init__(self, uri, access_key=None, secret_key=None, acl=None,</span>
<span class="gd">-        endpoint_url=None, *, feed_options=None, session_token=None,</span>
<span class="gd">-        region_name=None):</span>
<span class="gi">+</span>
<span class="gi">+class S3FeedStorage(BlockingFeedStorage):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        uri,</span>
<span class="gi">+        access_key=None,</span>
<span class="gi">+        secret_key=None,</span>
<span class="gi">+        acl=None,</span>
<span class="gi">+        endpoint_url=None,</span>
<span class="gi">+        *,</span>
<span class="gi">+        feed_options=None,</span>
<span class="gi">+        session_token=None,</span>
<span class="gi">+        region_name=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if not is_botocore_available():
<span class="gd">-            raise NotConfigured(&#39;missing botocore library&#39;)</span>
<span class="gi">+            raise NotConfigured(&quot;missing botocore library&quot;)</span>
<span class="w"> </span>        u = urlparse(uri)
<span class="w"> </span>        self.bucketname = u.hostname
<span class="w"> </span>        self.access_key = u.username or access_key
<span class="w"> </span>        self.secret_key = u.password or secret_key
<span class="w"> </span>        self.session_token = session_token
<span class="gd">-        self.keyname = u.path[1:]</span>
<span class="gi">+        self.keyname = u.path[1:]  # remove first &quot;/&quot;</span>
<span class="w"> </span>        self.acl = acl
<span class="w"> </span>        self.endpoint_url = endpoint_url
<span class="w"> </span>        self.region_name = region_name
<span class="gi">+</span>
<span class="w"> </span>        if IS_BOTO3_AVAILABLE:
<span class="w"> </span>            import boto3.session
<span class="gi">+</span>
<span class="w"> </span>            session = boto3.session.Session()
<span class="gd">-            self.s3_client = session.client(&#39;s3&#39;, aws_access_key_id=self.</span>
<span class="gd">-                access_key, aws_secret_access_key=self.secret_key,</span>
<span class="gd">-                aws_session_token=self.session_token, endpoint_url=self.</span>
<span class="gd">-                endpoint_url, region_name=self.region_name)</span>
<span class="gi">+</span>
<span class="gi">+            self.s3_client = session.client(</span>
<span class="gi">+                &quot;s3&quot;,</span>
<span class="gi">+                aws_access_key_id=self.access_key,</span>
<span class="gi">+                aws_secret_access_key=self.secret_key,</span>
<span class="gi">+                aws_session_token=self.session_token,</span>
<span class="gi">+                endpoint_url=self.endpoint_url,</span>
<span class="gi">+                region_name=self.region_name,</span>
<span class="gi">+            )</span>
<span class="w"> </span>        else:
<span class="w"> </span>            warnings.warn(
<span class="gd">-                &#39;`botocore` usage has been deprecated for S3 feed export, please use `boto3` to avoid problems&#39;</span>
<span class="gd">-                , category=ScrapyDeprecationWarning)</span>
<span class="gi">+                &quot;`botocore` usage has been deprecated for S3 feed &quot;</span>
<span class="gi">+                &quot;export, please use `boto3` to avoid problems&quot;,</span>
<span class="gi">+                category=ScrapyDeprecationWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="w"> </span>            import botocore.session
<span class="gi">+</span>
<span class="w"> </span>            session = botocore.session.get_session()
<span class="gd">-            self.s3_client = session.create_client(&#39;s3&#39;, aws_access_key_id=</span>
<span class="gd">-                self.access_key, aws_secret_access_key=self.secret_key,</span>
<span class="gd">-                aws_session_token=self.session_token, endpoint_url=self.</span>
<span class="gd">-                endpoint_url, region_name=self.region_name)</span>
<span class="gd">-        if feed_options and feed_options.get(&#39;overwrite&#39;, True) is False:</span>
<span class="gi">+</span>
<span class="gi">+            self.s3_client = session.create_client(</span>
<span class="gi">+                &quot;s3&quot;,</span>
<span class="gi">+                aws_access_key_id=self.access_key,</span>
<span class="gi">+                aws_secret_access_key=self.secret_key,</span>
<span class="gi">+                aws_session_token=self.session_token,</span>
<span class="gi">+                endpoint_url=self.endpoint_url,</span>
<span class="gi">+                region_name=self.region_name,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if feed_options and feed_options.get(&quot;overwrite&quot;, True) is False:</span>
<span class="w"> </span>            logger.warning(
<span class="gd">-                &#39;S3 does not support appending to files. To suppress this warning, remove the overwrite option from your FEEDS setting or set it to True.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                &quot;S3 does not support appending to files. To &quot;</span>
<span class="gi">+                &quot;suppress this warning, remove the overwrite &quot;</span>
<span class="gi">+                &quot;option from your FEEDS setting or set it to True.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, uri, *, feed_options=None):</span>
<span class="gi">+        return build_storage(</span>
<span class="gi">+            cls,</span>
<span class="gi">+            uri,</span>
<span class="gi">+            access_key=crawler.settings[&quot;AWS_ACCESS_KEY_ID&quot;],</span>
<span class="gi">+            secret_key=crawler.settings[&quot;AWS_SECRET_ACCESS_KEY&quot;],</span>
<span class="gi">+            session_token=crawler.settings[&quot;AWS_SESSION_TOKEN&quot;],</span>
<span class="gi">+            acl=crawler.settings[&quot;FEED_STORAGE_S3_ACL&quot;] or None,</span>
<span class="gi">+            endpoint_url=crawler.settings[&quot;AWS_ENDPOINT_URL&quot;] or None,</span>
<span class="gi">+            region_name=crawler.settings[&quot;AWS_REGION_NAME&quot;] or None,</span>
<span class="gi">+            feed_options=feed_options,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _store_in_thread(self, file):</span>
<span class="gi">+        file.seek(0)</span>
<span class="gi">+        if IS_BOTO3_AVAILABLE:</span>
<span class="gi">+            kwargs = {&quot;ExtraArgs&quot;: {&quot;ACL&quot;: self.acl}} if self.acl else {}</span>
<span class="gi">+            self.s3_client.upload_fileobj(</span>
<span class="gi">+                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            kwargs = {&quot;ACL&quot;: self.acl} if self.acl else {}</span>
<span class="gi">+            self.s3_client.put_object(</span>
<span class="gi">+                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs</span>
<span class="gi">+            )</span>
<span class="gi">+        file.close()</span>


<span class="w"> </span>class GCSFeedStorage(BlockingFeedStorage):
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, uri, project_id, acl):
<span class="w"> </span>        self.project_id = project_id
<span class="w"> </span>        self.acl = acl
<span class="w"> </span>        u = urlparse(uri)
<span class="w"> </span>        self.bucket_name = u.hostname
<span class="gd">-        self.blob_name = u.path[1:]</span>
<span class="gi">+        self.blob_name = u.path[1:]  # remove first &quot;/&quot;</span>

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, uri):</span>
<span class="gi">+        return cls(</span>
<span class="gi">+            uri,</span>
<span class="gi">+            crawler.settings[&quot;GCS_PROJECT_ID&quot;],</span>
<span class="gi">+            crawler.settings[&quot;FEED_STORAGE_GCS_ACL&quot;] or None,</span>
<span class="gi">+        )</span>

<span class="gd">-class FTPFeedStorage(BlockingFeedStorage):</span>
<span class="gi">+    def _store_in_thread(self, file):</span>
<span class="gi">+        file.seek(0)</span>
<span class="gi">+        from google.cloud.storage import Client</span>

<span class="gd">-    def __init__(self, uri: str, use_active_mode: bool=False, *,</span>
<span class="gd">-        feed_options: Optional[Dict[str, Any]]=None):</span>
<span class="gi">+        client = Client(project=self.project_id)</span>
<span class="gi">+        bucket = client.get_bucket(self.bucket_name)</span>
<span class="gi">+        blob = bucket.blob(self.blob_name)</span>
<span class="gi">+        blob.upload_from_file(file, predefined_acl=self.acl)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class FTPFeedStorage(BlockingFeedStorage):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        uri: str,</span>
<span class="gi">+        use_active_mode: bool = False,</span>
<span class="gi">+        *,</span>
<span class="gi">+        feed_options: Optional[Dict[str, Any]] = None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        u = urlparse(uri)
<span class="w"> </span>        if not u.hostname:
<span class="gd">-            raise ValueError(f&#39;Got a storage URI without a hostname: {uri}&#39;)</span>
<span class="gi">+            raise ValueError(f&quot;Got a storage URI without a hostname: {uri}&quot;)</span>
<span class="w"> </span>        self.host: str = u.hostname
<span class="gd">-        self.port: int = int(u.port or &#39;21&#39;)</span>
<span class="gd">-        self.username: str = u.username or &#39;&#39;</span>
<span class="gd">-        self.password: str = unquote(u.password or &#39;&#39;)</span>
<span class="gi">+        self.port: int = int(u.port or &quot;21&quot;)</span>
<span class="gi">+        self.username: str = u.username or &quot;&quot;</span>
<span class="gi">+        self.password: str = unquote(u.password or &quot;&quot;)</span>
<span class="w"> </span>        self.path: str = u.path
<span class="w"> </span>        self.use_active_mode: bool = use_active_mode
<span class="gd">-        self.overwrite: bool = not feed_options or feed_options.get(&#39;overwrite&#39;</span>
<span class="gd">-            , True)</span>
<span class="gi">+        self.overwrite: bool = not feed_options or feed_options.get(&quot;overwrite&quot;, True)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, uri, *, feed_options=None):</span>
<span class="gi">+        return build_storage(</span>
<span class="gi">+            cls,</span>
<span class="gi">+            uri,</span>
<span class="gi">+            crawler.settings.getbool(&quot;FEED_STORAGE_FTP_ACTIVE&quot;),</span>
<span class="gi">+            feed_options=feed_options,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _store_in_thread(self, file):</span>
<span class="gi">+        ftp_store_file(</span>
<span class="gi">+            path=self.path,</span>
<span class="gi">+            file=file,</span>
<span class="gi">+            host=self.host,</span>
<span class="gi">+            port=self.port,</span>
<span class="gi">+            username=self.username,</span>
<span class="gi">+            password=self.password,</span>
<span class="gi">+            use_active_mode=self.use_active_mode,</span>
<span class="gi">+            overwrite=self.overwrite,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class FeedSlot:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, storage, uri, format, store_empty, batch_id,</span>
<span class="gd">-        uri_template, filter, feed_options, spider, exporters, settings,</span>
<span class="gd">-        crawler):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        storage,</span>
<span class="gi">+        uri,</span>
<span class="gi">+        format,</span>
<span class="gi">+        store_empty,</span>
<span class="gi">+        batch_id,</span>
<span class="gi">+        uri_template,</span>
<span class="gi">+        filter,</span>
<span class="gi">+        feed_options,</span>
<span class="gi">+        spider,</span>
<span class="gi">+        exporters,</span>
<span class="gi">+        settings,</span>
<span class="gi">+        crawler,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.file = None
<span class="w"> </span>        self.exporter = None
<span class="w"> </span>        self.storage = storage
<span class="gi">+        # feed params</span>
<span class="w"> </span>        self.batch_id = batch_id
<span class="w"> </span>        self.format = format
<span class="w"> </span>        self.store_empty = store_empty
<span class="w"> </span>        self.uri_template = uri_template
<span class="w"> </span>        self.uri = uri
<span class="w"> </span>        self.filter = filter
<span class="gi">+        # exporter params</span>
<span class="w"> </span>        self.feed_options = feed_options
<span class="w"> </span>        self.spider = spider
<span class="w"> </span>        self.exporters = exporters
<span class="w"> </span>        self.settings = settings
<span class="w"> </span>        self.crawler = crawler
<span class="gi">+        # flags</span>
<span class="w"> </span>        self.itemcount = 0
<span class="w"> </span>        self._exporting = False
<span class="w"> </span>        self._fileloaded = False

<span class="gi">+    def start_exporting(self):</span>
<span class="gi">+        if not self._fileloaded:</span>
<span class="gi">+            self.file = self.storage.open(self.spider)</span>
<span class="gi">+            if &quot;postprocessing&quot; in self.feed_options:</span>
<span class="gi">+                self.file = PostProcessingManager(</span>
<span class="gi">+                    self.feed_options[&quot;postprocessing&quot;], self.file, self.feed_options</span>
<span class="gi">+                )</span>
<span class="gi">+            self.exporter = self._get_exporter(</span>
<span class="gi">+                file=self.file,</span>
<span class="gi">+                format=self.feed_options[&quot;format&quot;],</span>
<span class="gi">+                fields_to_export=self.feed_options[&quot;fields&quot;],</span>
<span class="gi">+                encoding=self.feed_options[&quot;encoding&quot;],</span>
<span class="gi">+                indent=self.feed_options[&quot;indent&quot;],</span>
<span class="gi">+                **self.feed_options[&quot;item_export_kwargs&quot;],</span>
<span class="gi">+            )</span>
<span class="gi">+            self._fileloaded = True</span>
<span class="gi">+</span>
<span class="gi">+        if not self._exporting:</span>
<span class="gi">+            self.exporter.start_exporting()</span>
<span class="gi">+            self._exporting = True</span>
<span class="gi">+</span>
<span class="gi">+    def _get_instance(self, objcls, *args, **kwargs):</span>
<span class="gi">+        return create_instance(objcls, self.settings, self.crawler, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_exporter(self, file, format, *args, **kwargs):</span>
<span class="gi">+        return self._get_instance(self.exporters[format], file, *args, **kwargs)</span>

<span class="gd">-_FeedSlot = create_deprecated_class(name=&#39;_FeedSlot&#39;, new_class=FeedSlot)</span>
<span class="gi">+    def finish_exporting(self):</span>
<span class="gi">+        if self._exporting:</span>
<span class="gi">+            self.exporter.finish_exporting()</span>
<span class="gi">+            self._exporting = False</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_FeedSlot = create_deprecated_class(</span>
<span class="gi">+    name=&quot;_FeedSlot&quot;,</span>
<span class="gi">+    new_class=FeedSlot,</span>
<span class="gi">+)</span>


<span class="w"> </span>class FeedExporter:
<span class="w"> </span>    _pending_deferreds: List[defer.Deferred] = []

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        exporter = cls(crawler)</span>
<span class="gi">+        crawler.signals.connect(exporter.open_spider, signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(exporter.close_spider, signals.spider_closed)</span>
<span class="gi">+        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)</span>
<span class="gi">+        return exporter</span>
<span class="gi">+</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self.settings = crawler.settings
<span class="w"> </span>        self.feeds = {}
<span class="w"> </span>        self.slots = []
<span class="w"> </span>        self.filters = {}
<span class="gd">-        if not self.settings[&#39;FEEDS&#39;] and not self.settings[&#39;FEED_URI&#39;]:</span>
<span class="gi">+</span>
<span class="gi">+        if not self.settings[&quot;FEEDS&quot;] and not self.settings[&quot;FEED_URI&quot;]:</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        if self.settings[&#39;FEED_URI&#39;]:</span>
<span class="gi">+</span>
<span class="gi">+        # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings</span>
<span class="gi">+        if self.settings[&quot;FEED_URI&quot;]:</span>
<span class="w"> </span>            warnings.warn(
<span class="gd">-                &#39;The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details&#39;</span>
<span class="gd">-                , category=ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gd">-            uri = self.settings[&#39;FEED_URI&#39;]</span>
<span class="gd">-            uri = str(uri) if not isinstance(uri, Path) else uri.absolute(</span>
<span class="gd">-                ).as_uri()</span>
<span class="gd">-            feed_options = {&#39;format&#39;: self.settings.get(&#39;FEED_FORMAT&#39;,</span>
<span class="gd">-                &#39;jsonlines&#39;)}</span>
<span class="gi">+                &quot;The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of &quot;</span>
<span class="gi">+                &quot;the `FEEDS` setting. Please see the `FEEDS` setting docs for more details&quot;,</span>
<span class="gi">+                category=ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+            uri = self.settings[&quot;FEED_URI&quot;]</span>
<span class="gi">+            # handle pathlib.Path objects</span>
<span class="gi">+            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()</span>
<span class="gi">+            feed_options = {&quot;format&quot;: self.settings.get(&quot;FEED_FORMAT&quot;, &quot;jsonlines&quot;)}</span>
<span class="w"> </span>            self.feeds[uri] = feed_complete_default_values_from_settings(
<span class="gd">-                feed_options, self.settings)</span>
<span class="gi">+                feed_options, self.settings</span>
<span class="gi">+            )</span>
<span class="w"> </span>            self.filters[uri] = self._load_filter(feed_options)
<span class="gd">-        for uri, feed_options in self.settings.getdict(&#39;FEEDS&#39;).items():</span>
<span class="gd">-            uri = str(uri) if not isinstance(uri, Path) else uri.absolute(</span>
<span class="gd">-                ).as_uri()</span>
<span class="gi">+        # End: Backward compatibility for FEED_URI and FEED_FORMAT settings</span>
<span class="gi">+</span>
<span class="gi">+        # &#39;FEEDS&#39; setting takes precedence over &#39;FEED_URI&#39;</span>
<span class="gi">+        for uri, feed_options in self.settings.getdict(&quot;FEEDS&quot;).items():</span>
<span class="gi">+            # handle pathlib.Path objects</span>
<span class="gi">+            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()</span>
<span class="w"> </span>            self.feeds[uri] = feed_complete_default_values_from_settings(
<span class="gd">-                feed_options, self.settings)</span>
<span class="gi">+                feed_options, self.settings</span>
<span class="gi">+            )</span>
<span class="w"> </span>            self.filters[uri] = self._load_filter(feed_options)
<span class="gd">-        self.storages = self._load_components(&#39;FEED_STORAGES&#39;)</span>
<span class="gd">-        self.exporters = self._load_components(&#39;FEED_EXPORTERS&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        self.storages = self._load_components(&quot;FEED_STORAGES&quot;)</span>
<span class="gi">+        self.exporters = self._load_components(&quot;FEED_EXPORTERS&quot;)</span>
<span class="w"> </span>        for uri, feed_options in self.feeds.items():
<span class="w"> </span>            if not self._storage_supported(uri, feed_options):
<span class="w"> </span>                raise NotConfigured
<span class="w"> </span>            if not self._settings_are_valid():
<span class="w"> </span>                raise NotConfigured
<span class="gd">-            if not self._exporter_supported(feed_options[&#39;format&#39;]):</span>
<span class="gi">+            if not self._exporter_supported(feed_options[&quot;format&quot;]):</span>
<span class="w"> </span>                raise NotConfigured

<span class="gd">-    def _start_new_batch(self, batch_id, uri, feed_options, spider,</span>
<span class="gd">-        uri_template):</span>
<span class="gi">+    def open_spider(self, spider):</span>
<span class="gi">+        for uri, feed_options in self.feeds.items():</span>
<span class="gi">+            uri_params = self._get_uri_params(spider, feed_options[&quot;uri_params&quot;])</span>
<span class="gi">+            self.slots.append(</span>
<span class="gi">+                self._start_new_batch(</span>
<span class="gi">+                    batch_id=1,</span>
<span class="gi">+                    uri=uri % uri_params,</span>
<span class="gi">+                    feed_options=feed_options,</span>
<span class="gi">+                    spider=spider,</span>
<span class="gi">+                    uri_template=uri,</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    async def close_spider(self, spider):</span>
<span class="gi">+        for slot in self.slots:</span>
<span class="gi">+            self._close_slot(slot, spider)</span>
<span class="gi">+</span>
<span class="gi">+        # Await all deferreds</span>
<span class="gi">+        if self._pending_deferreds:</span>
<span class="gi">+            await maybe_deferred_to_future(DeferredList(self._pending_deferreds))</span>
<span class="gi">+</span>
<span class="gi">+        # Send FEED_EXPORTER_CLOSED signal</span>
<span class="gi">+        await maybe_deferred_to_future(</span>
<span class="gi">+            self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _close_slot(self, slot, spider):</span>
<span class="gi">+        def get_file(slot_):</span>
<span class="gi">+            if isinstance(slot_.file, PostProcessingManager):</span>
<span class="gi">+                slot_.file.close()</span>
<span class="gi">+                return slot_.file.file</span>
<span class="gi">+            return slot_.file</span>
<span class="gi">+</span>
<span class="gi">+        if slot.itemcount:</span>
<span class="gi">+            # Normal case</span>
<span class="gi">+            slot.finish_exporting()</span>
<span class="gi">+        elif slot.store_empty and slot.batch_id == 1:</span>
<span class="gi">+            # Need to store the empty file</span>
<span class="gi">+            slot.start_exporting()</span>
<span class="gi">+            slot.finish_exporting()</span>
<span class="gi">+        else:</span>
<span class="gi">+            # In this case, the file is not stored, so no processing is required.</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        logmsg = f&quot;{slot.format} feed ({slot.itemcount} items) in: {slot.uri}&quot;</span>
<span class="gi">+        d = defer.maybeDeferred(slot.storage.store, get_file(slot))</span>
<span class="gi">+</span>
<span class="gi">+        d.addCallback(</span>
<span class="gi">+            self._handle_store_success, logmsg, spider, type(slot.storage).__name__</span>
<span class="gi">+        )</span>
<span class="gi">+        d.addErrback(</span>
<span class="gi">+            self._handle_store_error, logmsg, spider, type(slot.storage).__name__</span>
<span class="gi">+        )</span>
<span class="gi">+        self._pending_deferreds.append(d)</span>
<span class="gi">+        d.addCallback(</span>
<span class="gi">+            lambda _: self.crawler.signals.send_catch_log_deferred(</span>
<span class="gi">+                signals.feed_slot_closed, slot=slot</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        d.addBoth(lambda _: self._pending_deferreds.remove(d))</span>
<span class="gi">+</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _handle_store_error(self, f, logmsg, spider, slot_type):</span>
<span class="gi">+        logger.error(</span>
<span class="gi">+            &quot;Error storing %s&quot;,</span>
<span class="gi">+            logmsg,</span>
<span class="gi">+            exc_info=failure_to_exc_info(f),</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        self.crawler.stats.inc_value(f&quot;feedexport/failed_count/{slot_type}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _handle_store_success(self, f, logmsg, spider, slot_type):</span>
<span class="gi">+        logger.info(&quot;Stored %s&quot;, logmsg, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+        self.crawler.stats.inc_value(f&quot;feedexport/success_count/{slot_type}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Redirect the output data stream to a new file.
<span class="w"> </span>        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified
<span class="gu">@@ -254,14 +542,103 @@ class FeedExporter:</span>
<span class="w"> </span>        :param spider: user spider
<span class="w"> </span>        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        storage = self._get_storage(uri, feed_options)</span>
<span class="gi">+        slot = FeedSlot(</span>
<span class="gi">+            storage=storage,</span>
<span class="gi">+            uri=uri,</span>
<span class="gi">+            format=feed_options[&quot;format&quot;],</span>
<span class="gi">+            store_empty=feed_options[&quot;store_empty&quot;],</span>
<span class="gi">+            batch_id=batch_id,</span>
<span class="gi">+            uri_template=uri_template,</span>
<span class="gi">+            filter=self.filters[uri_template],</span>
<span class="gi">+            feed_options=feed_options,</span>
<span class="gi">+            spider=spider,</span>
<span class="gi">+            exporters=self.exporters,</span>
<span class="gi">+            settings=self.settings,</span>
<span class="gi">+            crawler=getattr(self, &quot;crawler&quot;, None),</span>
<span class="gi">+        )</span>
<span class="gi">+        return slot</span>
<span class="gi">+</span>
<span class="gi">+    def item_scraped(self, item, spider):</span>
<span class="gi">+        slots = []</span>
<span class="gi">+        for slot in self.slots:</span>
<span class="gi">+            if not slot.filter.accepts(item):</span>
<span class="gi">+                slots.append(</span>
<span class="gi">+                    slot</span>
<span class="gi">+                )  # if slot doesn&#39;t accept item, continue with next slot</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            slot.start_exporting()</span>
<span class="gi">+            slot.exporter.export_item(item)</span>
<span class="gi">+            slot.itemcount += 1</span>
<span class="gi">+            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one</span>
<span class="gi">+            if (</span>
<span class="gi">+                self.feeds[slot.uri_template][&quot;batch_item_count&quot;]</span>
<span class="gi">+                and slot.itemcount &gt;= self.feeds[slot.uri_template][&quot;batch_item_count&quot;]</span>
<span class="gi">+            ):</span>
<span class="gi">+                uri_params = self._get_uri_params(</span>
<span class="gi">+                    spider, self.feeds[slot.uri_template][&quot;uri_params&quot;], slot</span>
<span class="gi">+                )</span>
<span class="gi">+                self._close_slot(slot, spider)</span>
<span class="gi">+                slots.append(</span>
<span class="gi">+                    self._start_new_batch(</span>
<span class="gi">+                        batch_id=slot.batch_id + 1,</span>
<span class="gi">+                        uri=slot.uri_template % uri_params,</span>
<span class="gi">+                        feed_options=self.feeds[slot.uri_template],</span>
<span class="gi">+                        spider=spider,</span>
<span class="gi">+                        uri_template=slot.uri_template,</span>
<span class="gi">+                    )</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                slots.append(slot)</span>
<span class="gi">+        self.slots = slots</span>
<span class="gi">+</span>
<span class="gi">+    def _load_components(self, setting_prefix):</span>
<span class="gi">+        conf = without_none_values(self.settings.getwithbase(setting_prefix))</span>
<span class="gi">+        d = {}</span>
<span class="gi">+        for k, v in conf.items():</span>
<span class="gi">+            try:</span>
<span class="gi">+                d[k] = load_object(v)</span>
<span class="gi">+            except NotConfigured:</span>
<span class="gi">+                pass</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _exporter_supported(self, format):</span>
<span class="gi">+        if format in self.exporters:</span>
<span class="gi">+            return True</span>
<span class="gi">+        logger.error(&quot;Unknown feed format: %(format)s&quot;, {&quot;format&quot;: format})</span>

<span class="w"> </span>    def _settings_are_valid(self):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain
<span class="w"> </span>        %(batch_time)s or %(batch_id)d to distinguish different files of partial output
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for uri_template, values in self.feeds.items():</span>
<span class="gi">+            if values[&quot;batch_item_count&quot;] and not re.search(</span>
<span class="gi">+                r&quot;%\(batch_time\)s|%\(batch_id\)&quot;, uri_template</span>
<span class="gi">+            ):</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    &quot;%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT &quot;</span>
<span class="gi">+                    &quot;setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: &quot;</span>
<span class="gi">+                    &quot;https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count&quot;,</span>
<span class="gi">+                    uri_template,</span>
<span class="gi">+                )</span>
<span class="gi">+                return False</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def _storage_supported(self, uri, feed_options):</span>
<span class="gi">+        scheme = urlparse(uri).scheme</span>
<span class="gi">+        if scheme in self.storages or PureWindowsPath(uri).drive:</span>
<span class="gi">+            try:</span>
<span class="gi">+                self._get_storage(uri, feed_options)</span>
<span class="gi">+                return True</span>
<span class="gi">+            except NotConfigured as e:</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    &quot;Disabled feed storage scheme: %(scheme)s. &quot; &quot;Reason: %(reason)s&quot;,</span>
<span class="gi">+                    {&quot;scheme&quot;: scheme, &quot;reason&quot;: str(e)},</span>
<span class="gi">+                )</span>
<span class="gi">+        else:</span>
<span class="gi">+            logger.error(&quot;Unknown feed storage scheme: %(scheme)s&quot;, {&quot;scheme&quot;: scheme})</span>

<span class="w"> </span>    def _get_storage(self, uri, feed_options):
<span class="w"> </span>        &quot;&quot;&quot;Fork of create_instance specific to feed storage classes
<span class="gu">@@ -269,4 +646,49 @@ class FeedExporter:</span>
<span class="w"> </span>        It supports not passing the *feed_options* parameters to classes that
<span class="w"> </span>        do not support it, and issuing a deprecation warning instead.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        feedcls = self.storages.get(urlparse(uri).scheme, self.storages[&quot;file&quot;])</span>
<span class="gi">+        crawler = getattr(self, &quot;crawler&quot;, None)</span>
<span class="gi">+</span>
<span class="gi">+        def build_instance(builder, *preargs):</span>
<span class="gi">+            return build_storage(</span>
<span class="gi">+                builder, uri, feed_options=feed_options, preargs=preargs</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if crawler and hasattr(feedcls, &quot;from_crawler&quot;):</span>
<span class="gi">+            instance = build_instance(feedcls.from_crawler, crawler)</span>
<span class="gi">+            method_name = &quot;from_crawler&quot;</span>
<span class="gi">+        elif hasattr(feedcls, &quot;from_settings&quot;):</span>
<span class="gi">+            instance = build_instance(feedcls.from_settings, self.settings)</span>
<span class="gi">+            method_name = &quot;from_settings&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            instance = build_instance(feedcls)</span>
<span class="gi">+            method_name = &quot;__new__&quot;</span>
<span class="gi">+        if instance is None:</span>
<span class="gi">+            raise TypeError(f&quot;{feedcls.__qualname__}.{method_name} returned None&quot;)</span>
<span class="gi">+        return instance</span>
<span class="gi">+</span>
<span class="gi">+    def _get_uri_params(</span>
<span class="gi">+        self,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        uri_params_function: Optional[Union[str, Callable[[dict, Spider], dict]]],</span>
<span class="gi">+        slot: Optional[FeedSlot] = None,</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="gi">+        params = {}</span>
<span class="gi">+        for k in dir(spider):</span>
<span class="gi">+            params[k] = getattr(spider, k)</span>
<span class="gi">+        utc_now = datetime.now(tz=timezone.utc)</span>
<span class="gi">+        params[&quot;time&quot;] = utc_now.replace(microsecond=0).isoformat().replace(&quot;:&quot;, &quot;-&quot;)</span>
<span class="gi">+        params[&quot;batch_time&quot;] = utc_now.isoformat().replace(&quot;:&quot;, &quot;-&quot;)</span>
<span class="gi">+        params[&quot;batch_id&quot;] = slot.batch_id + 1 if slot is not None else 1</span>
<span class="gi">+        uripar_function = (</span>
<span class="gi">+            load_object(uri_params_function)</span>
<span class="gi">+            if uri_params_function</span>
<span class="gi">+            else lambda params, _: params</span>
<span class="gi">+        )</span>
<span class="gi">+        new_params = uripar_function(params, spider)</span>
<span class="gi">+        return new_params if new_params is not None else params</span>
<span class="gi">+</span>
<span class="gi">+    def _load_filter(self, feed_options):</span>
<span class="gi">+        # load the item filter if declared else load the default filter class</span>
<span class="gi">+        item_filter_class = load_object(feed_options.get(&quot;item_filter&quot;, ItemFilter))</span>
<span class="gi">+        return item_filter_class(feed_options)</span>
<span class="gh">diff --git a/scrapy/extensions/httpcache.py b/scrapy/extensions/httpcache.py</span>
<span class="gh">index b70d7ec39..7e4f047a8 100644</span>
<span class="gd">--- a/scrapy/extensions/httpcache.py</span>
<span class="gi">+++ b/scrapy/extensions/httpcache.py</span>
<span class="gu">@@ -6,7 +6,9 @@ from importlib import import_module</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from time import time
<span class="w"> </span>from weakref import WeakKeyDictionary
<span class="gi">+</span>
<span class="w"> </span>from w3lib.http import headers_dict_to_raw, headers_raw_to_dict
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Headers, Response
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.responsetypes import responsetypes
<span class="gu">@@ -14,52 +16,343 @@ from scrapy.spiders import Spider</span>
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.project import data_path
<span class="w"> </span>from scrapy.utils.python import to_bytes, to_unicode
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class DummyPolicy:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.ignore_schemes = settings.getlist(&#39;HTTPCACHE_IGNORE_SCHEMES&#39;)</span>
<span class="gd">-        self.ignore_http_codes = [int(x) for x in settings.getlist(</span>
<span class="gd">-            &#39;HTTPCACHE_IGNORE_HTTP_CODES&#39;)]</span>
<span class="gi">+        self.ignore_schemes = settings.getlist(&quot;HTTPCACHE_IGNORE_SCHEMES&quot;)</span>
<span class="gi">+        self.ignore_http_codes = [</span>
<span class="gi">+            int(x) for x in settings.getlist(&quot;HTTPCACHE_IGNORE_HTTP_CODES&quot;)</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+    def should_cache_request(self, request):</span>
<span class="gi">+        return urlparse_cached(request).scheme not in self.ignore_schemes</span>
<span class="gi">+</span>
<span class="gi">+    def should_cache_response(self, response, request):</span>
<span class="gi">+        return response.status not in self.ignore_http_codes</span>
<span class="gi">+</span>
<span class="gi">+    def is_cached_response_fresh(self, cachedresponse, request):</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def is_cached_response_valid(self, cachedresponse, response, request):</span>
<span class="gi">+        return True</span>


<span class="w"> </span>class RFC2616Policy:
<span class="gd">-    MAXAGE = 3600 * 24 * 365</span>
<span class="gi">+    MAXAGE = 3600 * 24 * 365  # one year</span>

<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.always_store = settings.getbool(&#39;HTTPCACHE_ALWAYS_STORE&#39;)</span>
<span class="gd">-        self.ignore_schemes = settings.getlist(&#39;HTTPCACHE_IGNORE_SCHEMES&#39;)</span>
<span class="gi">+        self.always_store = settings.getbool(&quot;HTTPCACHE_ALWAYS_STORE&quot;)</span>
<span class="gi">+        self.ignore_schemes = settings.getlist(&quot;HTTPCACHE_IGNORE_SCHEMES&quot;)</span>
<span class="w"> </span>        self._cc_parsed = WeakKeyDictionary()
<span class="gd">-        self.ignore_response_cache_controls = [to_bytes(cc) for cc in</span>
<span class="gd">-            settings.getlist(&#39;HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS&#39;)]</span>
<span class="gi">+        self.ignore_response_cache_controls = [</span>
<span class="gi">+            to_bytes(cc)</span>
<span class="gi">+            for cc in settings.getlist(&quot;HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS&quot;)</span>
<span class="gi">+        ]</span>

<span class="gi">+    def _parse_cachecontrol(self, r):</span>
<span class="gi">+        if r not in self._cc_parsed:</span>
<span class="gi">+            cch = r.headers.get(b&quot;Cache-Control&quot;, b&quot;&quot;)</span>
<span class="gi">+            parsed = parse_cachecontrol(cch)</span>
<span class="gi">+            if isinstance(r, Response):</span>
<span class="gi">+                for key in self.ignore_response_cache_controls:</span>
<span class="gi">+                    parsed.pop(key, None)</span>
<span class="gi">+            self._cc_parsed[r] = parsed</span>
<span class="gi">+        return self._cc_parsed[r]</span>

<span class="gd">-class DbmCacheStorage:</span>
<span class="gi">+    def should_cache_request(self, request):</span>
<span class="gi">+        if urlparse_cached(request).scheme in self.ignore_schemes:</span>
<span class="gi">+            return False</span>
<span class="gi">+        cc = self._parse_cachecontrol(request)</span>
<span class="gi">+        # obey user-agent directive &quot;Cache-Control: no-store&quot;</span>
<span class="gi">+        if b&quot;no-store&quot; in cc:</span>
<span class="gi">+            return False</span>
<span class="gi">+        # Any other is eligible for caching</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def should_cache_response(self, response, request):</span>
<span class="gi">+        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</span>
<span class="gi">+        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4</span>
<span class="gi">+        # Status code 206 is not included because cache can not deal with partial contents</span>
<span class="gi">+        cc = self._parse_cachecontrol(response)</span>
<span class="gi">+        # obey directive &quot;Cache-Control: no-store&quot;</span>
<span class="gi">+        if b&quot;no-store&quot; in cc:</span>
<span class="gi">+            return False</span>
<span class="gi">+        # Never cache 304 (Not Modified) responses</span>
<span class="gi">+        if response.status == 304:</span>
<span class="gi">+            return False</span>
<span class="gi">+        # Cache unconditionally if configured to do so</span>
<span class="gi">+        if self.always_store:</span>
<span class="gi">+            return True</span>
<span class="gi">+        # Any hint on response expiration is good</span>
<span class="gi">+        if b&quot;max-age&quot; in cc or b&quot;Expires&quot; in response.headers:</span>
<span class="gi">+            return True</span>
<span class="gi">+        # Firefox fallbacks this statuses to one year expiration if none is set</span>
<span class="gi">+        if response.status in (300, 301, 308):</span>
<span class="gi">+            return True</span>
<span class="gi">+        # Other statuses without expiration requires at least one validator</span>
<span class="gi">+        if response.status in (200, 203, 401):</span>
<span class="gi">+            return b&quot;Last-Modified&quot; in response.headers or b&quot;ETag&quot; in response.headers</span>
<span class="gi">+        # Any other is probably not eligible for caching</span>
<span class="gi">+        # Makes no sense to cache responses that does not contain expiration</span>
<span class="gi">+        # info and can not be revalidated</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def is_cached_response_fresh(self, cachedresponse, request):</span>
<span class="gi">+        cc = self._parse_cachecontrol(cachedresponse)</span>
<span class="gi">+        ccreq = self._parse_cachecontrol(request)</span>
<span class="gi">+        if b&quot;no-cache&quot; in cc or b&quot;no-cache&quot; in ccreq:</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        now = time()</span>
<span class="gi">+        freshnesslifetime = self._compute_freshness_lifetime(</span>
<span class="gi">+            cachedresponse, request, now</span>
<span class="gi">+        )</span>
<span class="gi">+        currentage = self._compute_current_age(cachedresponse, request, now)</span>
<span class="gi">+</span>
<span class="gi">+        reqmaxage = self._get_max_age(ccreq)</span>
<span class="gi">+        if reqmaxage is not None:</span>
<span class="gi">+            freshnesslifetime = min(freshnesslifetime, reqmaxage)</span>
<span class="gi">+</span>
<span class="gi">+        if currentage &lt; freshnesslifetime:</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        if b&quot;max-stale&quot; in ccreq and b&quot;must-revalidate&quot; not in cc:</span>
<span class="gi">+            # From RFC2616: &quot;Indicates that the client is willing to</span>
<span class="gi">+            # accept a response that has exceeded its expiration time.</span>
<span class="gi">+            # If max-stale is assigned a value, then the client is</span>
<span class="gi">+            # willing to accept a response that has exceeded its</span>
<span class="gi">+            # expiration time by no more than the specified number of</span>
<span class="gi">+            # seconds. If no value is assigned to max-stale, then the</span>
<span class="gi">+            # client is willing to accept a stale response of any age.&quot;</span>
<span class="gi">+            staleage = ccreq[b&quot;max-stale&quot;]</span>
<span class="gi">+            if staleage is None:</span>
<span class="gi">+                return True</span>
<span class="gi">+</span>
<span class="gi">+            try:</span>
<span class="gi">+                if currentage &lt; freshnesslifetime + max(0, int(staleage)):</span>
<span class="gi">+                    return True</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+        # Cached response is stale, try to set validators if any</span>
<span class="gi">+        self._set_conditional_validators(request, cachedresponse)</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def is_cached_response_valid(self, cachedresponse, response, request):</span>
<span class="gi">+        # Use the cached response if the new response is a server error,</span>
<span class="gi">+        # as long as the old response didn&#39;t specify must-revalidate.</span>
<span class="gi">+        if response.status &gt;= 500:</span>
<span class="gi">+            cc = self._parse_cachecontrol(cachedresponse)</span>
<span class="gi">+            if b&quot;must-revalidate&quot; not in cc:</span>
<span class="gi">+                return True</span>
<span class="gi">+</span>
<span class="gi">+        # Use the cached response if the server says it hasn&#39;t changed.</span>
<span class="gi">+        return response.status == 304</span>
<span class="gi">+</span>
<span class="gi">+    def _set_conditional_validators(self, request, cachedresponse):</span>
<span class="gi">+        if b&quot;Last-Modified&quot; in cachedresponse.headers:</span>
<span class="gi">+            request.headers[b&quot;If-Modified-Since&quot;] = cachedresponse.headers[</span>
<span class="gi">+                b&quot;Last-Modified&quot;</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+        if b&quot;ETag&quot; in cachedresponse.headers:</span>
<span class="gi">+            request.headers[b&quot;If-None-Match&quot;] = cachedresponse.headers[b&quot;ETag&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def _get_max_age(self, cc):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return max(0, int(cc[b&quot;max-age&quot;]))</span>
<span class="gi">+        except (KeyError, ValueError):</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+    def _compute_freshness_lifetime(self, response, request, now):</span>
<span class="gi">+        # Reference nsHttpResponseHead::ComputeFreshnessLifetime</span>
<span class="gi">+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706</span>
<span class="gi">+        cc = self._parse_cachecontrol(response)</span>
<span class="gi">+        maxage = self._get_max_age(cc)</span>
<span class="gi">+        if maxage is not None:</span>
<span class="gi">+            return maxage</span>
<span class="gi">+</span>
<span class="gi">+        # Parse date header or synthesize it if none exists</span>
<span class="gi">+        date = rfc1123_to_epoch(response.headers.get(b&quot;Date&quot;)) or now</span>
<span class="gi">+</span>
<span class="gi">+        # Try HTTP/1.0 Expires header</span>
<span class="gi">+        if b&quot;Expires&quot; in response.headers:</span>
<span class="gi">+            expires = rfc1123_to_epoch(response.headers[b&quot;Expires&quot;])</span>
<span class="gi">+            # When parsing Expires header fails RFC 2616 section 14.21 says we</span>
<span class="gi">+            # should treat this as an expiration time in the past.</span>
<span class="gi">+            return max(0, expires - date) if expires else 0</span>
<span class="gi">+</span>
<span class="gi">+        # Fallback to heuristic using last-modified header</span>
<span class="gi">+        # This is not in RFC but on Firefox caching implementation</span>
<span class="gi">+        lastmodified = rfc1123_to_epoch(response.headers.get(b&quot;Last-Modified&quot;))</span>
<span class="gi">+        if lastmodified and lastmodified &lt;= date:</span>
<span class="gi">+            return (date - lastmodified) / 10</span>
<span class="gi">+</span>
<span class="gi">+        # This request can be cached indefinitely</span>
<span class="gi">+        if response.status in (300, 301, 308):</span>
<span class="gi">+            return self.MAXAGE</span>

<span class="gi">+        # Insufficient information to compute freshness lifetime</span>
<span class="gi">+        return 0</span>
<span class="gi">+</span>
<span class="gi">+    def _compute_current_age(self, response, request, now):</span>
<span class="gi">+        # Reference nsHttpResponseHead::ComputeCurrentAge</span>
<span class="gi">+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658</span>
<span class="gi">+        currentage = 0</span>
<span class="gi">+        # If Date header is not set we assume it is a fast connection, and</span>
<span class="gi">+        # clock is in sync with the server</span>
<span class="gi">+        date = rfc1123_to_epoch(response.headers.get(b&quot;Date&quot;)) or now</span>
<span class="gi">+        if now &gt; date:</span>
<span class="gi">+            currentage = now - date</span>
<span class="gi">+</span>
<span class="gi">+        if b&quot;Age&quot; in response.headers:</span>
<span class="gi">+            try:</span>
<span class="gi">+                age = int(response.headers[b&quot;Age&quot;])</span>
<span class="gi">+                currentage = max(currentage, age)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+        return currentage</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class DbmCacheStorage:</span>
<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.cachedir = data_path(settings[&#39;HTTPCACHE_DIR&#39;], createdir=True)</span>
<span class="gd">-        self.expiration_secs = settings.getint(&#39;HTTPCACHE_EXPIRATION_SECS&#39;)</span>
<span class="gd">-        self.dbmodule = import_module(settings[&#39;HTTPCACHE_DBM_MODULE&#39;])</span>
<span class="gi">+        self.cachedir = data_path(settings[&quot;HTTPCACHE_DIR&quot;], createdir=True)</span>
<span class="gi">+        self.expiration_secs = settings.getint(&quot;HTTPCACHE_EXPIRATION_SECS&quot;)</span>
<span class="gi">+        self.dbmodule = import_module(settings[&quot;HTTPCACHE_DBM_MODULE&quot;])</span>
<span class="w"> </span>        self.db = None

<span class="gi">+    def open_spider(self, spider: Spider):</span>
<span class="gi">+        dbpath = Path(self.cachedir, f&quot;{spider.name}.db&quot;)</span>
<span class="gi">+        self.db = self.dbmodule.open(str(dbpath), &quot;c&quot;)</span>

<span class="gd">-class FilesystemCacheStorage:</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Using DBM cache storage in %(cachepath)s&quot;,</span>
<span class="gi">+            {&quot;cachepath&quot;: dbpath},</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        self._fingerprinter = spider.crawler.request_fingerprinter</span>
<span class="gi">+</span>
<span class="gi">+    def close_spider(self, spider):</span>
<span class="gi">+        self.db.close()</span>
<span class="gi">+</span>
<span class="gi">+    def retrieve_response(self, spider, request):</span>
<span class="gi">+        data = self._read_data(spider, request)</span>
<span class="gi">+        if data is None:</span>
<span class="gi">+            return  # not cached</span>
<span class="gi">+        url = data[&quot;url&quot;]</span>
<span class="gi">+        status = data[&quot;status&quot;]</span>
<span class="gi">+        headers = Headers(data[&quot;headers&quot;])</span>
<span class="gi">+        body = data[&quot;body&quot;]</span>
<span class="gi">+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)</span>
<span class="gi">+        response = respcls(url=url, headers=headers, status=status, body=body)</span>
<span class="gi">+        return response</span>
<span class="gi">+</span>
<span class="gi">+    def store_response(self, spider, request, response):</span>
<span class="gi">+        key = self._fingerprinter.fingerprint(request).hex()</span>
<span class="gi">+        data = {</span>
<span class="gi">+            &quot;status&quot;: response.status,</span>
<span class="gi">+            &quot;url&quot;: response.url,</span>
<span class="gi">+            &quot;headers&quot;: dict(response.headers),</span>
<span class="gi">+            &quot;body&quot;: response.body,</span>
<span class="gi">+        }</span>
<span class="gi">+        self.db[f&quot;{key}_data&quot;] = pickle.dumps(data, protocol=4)</span>
<span class="gi">+        self.db[f&quot;{key}_time&quot;] = str(time())</span>
<span class="gi">+</span>
<span class="gi">+    def _read_data(self, spider, request):</span>
<span class="gi">+        key = self._fingerprinter.fingerprint(request).hex()</span>
<span class="gi">+        db = self.db</span>
<span class="gi">+        tkey = f&quot;{key}_time&quot;</span>
<span class="gi">+        if tkey not in db:</span>
<span class="gi">+            return  # not found</span>
<span class="gi">+</span>
<span class="gi">+        ts = db[tkey]</span>
<span class="gi">+        if 0 &lt; self.expiration_secs &lt; time() - float(ts):</span>
<span class="gi">+            return  # expired</span>

<span class="gi">+        return pickle.loads(db[f&quot;{key}_data&quot;])</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class FilesystemCacheStorage:</span>
<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.cachedir = data_path(settings[&#39;HTTPCACHE_DIR&#39;])</span>
<span class="gd">-        self.expiration_secs = settings.getint(&#39;HTTPCACHE_EXPIRATION_SECS&#39;)</span>
<span class="gd">-        self.use_gzip = settings.getbool(&#39;HTTPCACHE_GZIP&#39;)</span>
<span class="gi">+        self.cachedir = data_path(settings[&quot;HTTPCACHE_DIR&quot;])</span>
<span class="gi">+        self.expiration_secs = settings.getint(&quot;HTTPCACHE_EXPIRATION_SECS&quot;)</span>
<span class="gi">+        self.use_gzip = settings.getbool(&quot;HTTPCACHE_GZIP&quot;)</span>
<span class="w"> </span>        self._open = gzip.open if self.use_gzip else open

<span class="gi">+    def open_spider(self, spider: Spider):</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Using filesystem cache storage in %(cachedir)s&quot;,</span>
<span class="gi">+            {&quot;cachedir&quot;: self.cachedir},</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        assert spider.crawler.request_fingerprinter</span>
<span class="gi">+        self._fingerprinter = spider.crawler.request_fingerprinter</span>
<span class="gi">+</span>
<span class="gi">+    def close_spider(self, spider):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="w"> </span>    def retrieve_response(self, spider: Spider, request: Request):
<span class="w"> </span>        &quot;&quot;&quot;Return response if present in cache, or None otherwise.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        metadata = self._read_meta(spider, request)</span>
<span class="gi">+        if metadata is None:</span>
<span class="gi">+            return  # not cached</span>
<span class="gi">+        rpath = Path(self._get_request_path(spider, request))</span>
<span class="gi">+        with self._open(rpath / &quot;response_body&quot;, &quot;rb&quot;) as f:</span>
<span class="gi">+            body = f.read()</span>
<span class="gi">+        with self._open(rpath / &quot;response_headers&quot;, &quot;rb&quot;) as f:</span>
<span class="gi">+            rawheaders = f.read()</span>
<span class="gi">+        url = metadata.get(&quot;response_url&quot;)</span>
<span class="gi">+        status = metadata[&quot;status&quot;]</span>
<span class="gi">+        headers = Headers(headers_raw_to_dict(rawheaders))</span>
<span class="gi">+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)</span>
<span class="gi">+        response = respcls(url=url, headers=headers, status=status, body=body)</span>
<span class="gi">+        return response</span>

<span class="w"> </span>    def store_response(self, spider: Spider, request: Request, response):
<span class="w"> </span>        &quot;&quot;&quot;Store the given response in the cache.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        rpath = Path(self._get_request_path(spider, request))</span>
<span class="gi">+        if not rpath.exists():</span>
<span class="gi">+            rpath.mkdir(parents=True)</span>
<span class="gi">+        metadata = {</span>
<span class="gi">+            &quot;url&quot;: request.url,</span>
<span class="gi">+            &quot;method&quot;: request.method,</span>
<span class="gi">+            &quot;status&quot;: response.status,</span>
<span class="gi">+            &quot;response_url&quot;: response.url,</span>
<span class="gi">+            &quot;timestamp&quot;: time(),</span>
<span class="gi">+        }</span>
<span class="gi">+        with self._open(rpath / &quot;meta&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            f.write(to_bytes(repr(metadata)))</span>
<span class="gi">+        with self._open(rpath / &quot;pickled_meta&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            pickle.dump(metadata, f, protocol=4)</span>
<span class="gi">+        with self._open(rpath / &quot;response_headers&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            f.write(headers_dict_to_raw(response.headers))</span>
<span class="gi">+        with self._open(rpath / &quot;response_body&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            f.write(response.body)</span>
<span class="gi">+        with self._open(rpath / &quot;request_headers&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            f.write(headers_dict_to_raw(request.headers))</span>
<span class="gi">+        with self._open(rpath / &quot;request_body&quot;, &quot;wb&quot;) as f:</span>
<span class="gi">+            f.write(request.body)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_request_path(self, spider: Spider, request: Request) -&gt; str:</span>
<span class="gi">+        key = self._fingerprinter.fingerprint(request).hex()</span>
<span class="gi">+        return str(Path(self.cachedir, spider.name, key[0:2], key))</span>
<span class="gi">+</span>
<span class="gi">+    def _read_meta(self, spider: Spider, request: Request):</span>
<span class="gi">+        rpath = Path(self._get_request_path(spider, request))</span>
<span class="gi">+        metapath = rpath / &quot;pickled_meta&quot;</span>
<span class="gi">+        if not metapath.exists():</span>
<span class="gi">+            return  # not found</span>
<span class="gi">+        mtime = metapath.stat().st_mtime</span>
<span class="gi">+        if 0 &lt; self.expiration_secs &lt; time() - mtime:</span>
<span class="gi">+            return  # expired</span>
<span class="gi">+        with self._open(metapath, &quot;rb&quot;) as f:</span>
<span class="gi">+            return pickle.load(f)</span>


<span class="w"> </span>def parse_cachecontrol(header):
<span class="gu">@@ -74,4 +367,17 @@ def parse_cachecontrol(header):</span>
<span class="w"> </span>    True

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    directives = {}</span>
<span class="gi">+    for directive in header.split(b&quot;,&quot;):</span>
<span class="gi">+        key, sep, val = directive.strip().partition(b&quot;=&quot;)</span>
<span class="gi">+        if key:</span>
<span class="gi">+            directives[key.lower()] = val if sep else None</span>
<span class="gi">+    return directives</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def rfc1123_to_epoch(date_str):</span>
<span class="gi">+    try:</span>
<span class="gi">+        date_str = to_unicode(date_str, encoding=&quot;ascii&quot;)</span>
<span class="gi">+        return mktime_tz(parsedate_tz(date_str))</span>
<span class="gi">+    except Exception:</span>
<span class="gi">+        return None</span>
<span class="gh">diff --git a/scrapy/extensions/logstats.py b/scrapy/extensions/logstats.py</span>
<span class="gh">index e2864b322..78874a6db 100644</span>
<span class="gd">--- a/scrapy/extensions/logstats.py</span>
<span class="gi">+++ b/scrapy/extensions/logstats.py</span>
<span class="gu">@@ -1,7 +1,10 @@</span>
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import task
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -13,3 +16,43 @@ class LogStats:</span>
<span class="w"> </span>        self.interval = interval
<span class="w"> </span>        self.multiplier = 60.0 / self.interval
<span class="w"> </span>        self.task = None
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        interval = crawler.settings.getfloat(&quot;LOGSTATS_INTERVAL&quot;)</span>
<span class="gi">+        if not interval:</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        o = cls(crawler.stats, interval)</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.pagesprev = 0</span>
<span class="gi">+        self.itemsprev = 0</span>
<span class="gi">+</span>
<span class="gi">+        self.task = task.LoopingCall(self.log, spider)</span>
<span class="gi">+        self.task.start(self.interval)</span>
<span class="gi">+</span>
<span class="gi">+    def log(self, spider):</span>
<span class="gi">+        items = self.stats.get_value(&quot;item_scraped_count&quot;, 0)</span>
<span class="gi">+        pages = self.stats.get_value(&quot;response_received_count&quot;, 0)</span>
<span class="gi">+        irate = (items - self.itemsprev) * self.multiplier</span>
<span class="gi">+        prate = (pages - self.pagesprev) * self.multiplier</span>
<span class="gi">+        self.pagesprev, self.itemsprev = pages, items</span>
<span class="gi">+</span>
<span class="gi">+        msg = (</span>
<span class="gi">+            &quot;Crawled %(pages)d pages (at %(pagerate)d pages/min), &quot;</span>
<span class="gi">+            &quot;scraped %(items)d items (at %(itemrate)d items/min)&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        log_args = {</span>
<span class="gi">+            &quot;pages&quot;: pages,</span>
<span class="gi">+            &quot;pagerate&quot;: prate,</span>
<span class="gi">+            &quot;items&quot;: items,</span>
<span class="gi">+            &quot;itemrate&quot;: irate,</span>
<span class="gi">+        }</span>
<span class="gi">+        logger.info(msg, log_args, extra={&quot;spider&quot;: spider})</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider, reason):</span>
<span class="gi">+        if self.task and self.task.running:</span>
<span class="gi">+            self.task.stop()</span>
<span class="gh">diff --git a/scrapy/extensions/memdebug.py b/scrapy/extensions/memdebug.py</span>
<span class="gh">index 8eb617202..03ede0681 100644</span>
<span class="gd">--- a/scrapy/extensions/memdebug.py</span>
<span class="gi">+++ b/scrapy/extensions/memdebug.py</span>
<span class="gu">@@ -3,13 +3,34 @@ MemoryDebugger extension</span>

<span class="w"> </span>See documentation in docs/topics/extensions.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import gc
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.trackref import live_refs


<span class="w"> </span>class MemoryDebugger:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, stats):
<span class="w"> </span>        self.stats = stats
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;MEMDEBUG_ENABLED&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        o = cls(crawler.stats)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider, reason):</span>
<span class="gi">+        gc.collect()</span>
<span class="gi">+        self.stats.set_value(</span>
<span class="gi">+            &quot;memdebug/gc_garbage_count&quot;, len(gc.garbage), spider=spider</span>
<span class="gi">+        )</span>
<span class="gi">+        for cls, wdict in live_refs.items():</span>
<span class="gi">+            if not wdict:</span>
<span class="gi">+                continue</span>
<span class="gi">+            self.stats.set_value(</span>
<span class="gi">+                f&quot;memdebug/live_refs/{cls.__name__}&quot;, len(wdict), spider=spider</span>
<span class="gi">+            )</span>
<span class="gh">diff --git a/scrapy/extensions/memusage.py b/scrapy/extensions/memusage.py</span>
<span class="gh">index 81fb5c242..ca766c938 100644</span>
<span class="gd">--- a/scrapy/extensions/memusage.py</span>
<span class="gi">+++ b/scrapy/extensions/memusage.py</span>
<span class="gu">@@ -8,37 +8,134 @@ import socket</span>
<span class="w"> </span>import sys
<span class="w"> </span>from importlib import import_module
<span class="w"> </span>from pprint import pformat
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import task
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.mail import MailSender
<span class="w"> </span>from scrapy.utils.engine import get_engine_status
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class MemoryUsage:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="gd">-        if not crawler.settings.getbool(&#39;MEMUSAGE_ENABLED&#39;):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;MEMUSAGE_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="w"> </span>        try:
<span class="gd">-            self.resource = import_module(&#39;resource&#39;)</span>
<span class="gi">+            # stdlib&#39;s resource module is only available on unix platforms.</span>
<span class="gi">+            self.resource = import_module(&quot;resource&quot;)</span>
<span class="w"> </span>        except ImportError:
<span class="w"> </span>            raise NotConfigured
<span class="gi">+</span>
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self.warned = False
<span class="gd">-        self.notify_mails = crawler.settings.getlist(&#39;MEMUSAGE_NOTIFY_MAIL&#39;)</span>
<span class="gd">-        self.limit = crawler.settings.getint(&#39;MEMUSAGE_LIMIT_MB&#39;) * 1024 * 1024</span>
<span class="gd">-        self.warning = crawler.settings.getint(&#39;MEMUSAGE_WARNING_MB&#39;</span>
<span class="gd">-            ) * 1024 * 1024</span>
<span class="gi">+        self.notify_mails = crawler.settings.getlist(&quot;MEMUSAGE_NOTIFY_MAIL&quot;)</span>
<span class="gi">+        self.limit = crawler.settings.getint(&quot;MEMUSAGE_LIMIT_MB&quot;) * 1024 * 1024</span>
<span class="gi">+        self.warning = crawler.settings.getint(&quot;MEMUSAGE_WARNING_MB&quot;) * 1024 * 1024</span>
<span class="w"> </span>        self.check_interval = crawler.settings.getfloat(
<span class="gd">-            &#39;MEMUSAGE_CHECK_INTERVAL_SECONDS&#39;)</span>
<span class="gi">+            &quot;MEMUSAGE_CHECK_INTERVAL_SECONDS&quot;</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.mail = MailSender.from_settings(crawler.settings)
<span class="gd">-        crawler.signals.connect(self.engine_started, signal=signals.</span>
<span class="gd">-            engine_started)</span>
<span class="gd">-        crawler.signals.connect(self.engine_stopped, signal=signals.</span>
<span class="gd">-            engine_stopped)</span>
<span class="gi">+        crawler.signals.connect(self.engine_started, signal=signals.engine_started)</span>
<span class="gi">+        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def get_virtual_size(self):</span>
<span class="gi">+        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss</span>
<span class="gi">+        if sys.platform != &quot;darwin&quot;:</span>
<span class="gi">+            # on macOS ru_maxrss is in bytes, on Linux it is in KB</span>
<span class="gi">+            size *= 1024</span>
<span class="gi">+        return size</span>
<span class="gi">+</span>
<span class="gi">+    def engine_started(self):</span>
<span class="gi">+        self.crawler.stats.set_value(&quot;memusage/startup&quot;, self.get_virtual_size())</span>
<span class="gi">+        self.tasks = []</span>
<span class="gi">+        tsk = task.LoopingCall(self.update)</span>
<span class="gi">+        self.tasks.append(tsk)</span>
<span class="gi">+        tsk.start(self.check_interval, now=True)</span>
<span class="gi">+        if self.limit:</span>
<span class="gi">+            tsk = task.LoopingCall(self._check_limit)</span>
<span class="gi">+            self.tasks.append(tsk)</span>
<span class="gi">+            tsk.start(self.check_interval, now=True)</span>
<span class="gi">+        if self.warning:</span>
<span class="gi">+            tsk = task.LoopingCall(self._check_warning)</span>
<span class="gi">+            self.tasks.append(tsk)</span>
<span class="gi">+            tsk.start(self.check_interval, now=True)</span>
<span class="gi">+</span>
<span class="gi">+    def engine_stopped(self):</span>
<span class="gi">+        for tsk in self.tasks:</span>
<span class="gi">+            if tsk.running:</span>
<span class="gi">+                tsk.stop()</span>
<span class="gi">+</span>
<span class="gi">+    def update(self):</span>
<span class="gi">+        self.crawler.stats.max_value(&quot;memusage/max&quot;, self.get_virtual_size())</span>
<span class="gi">+</span>
<span class="gi">+    def _check_limit(self):</span>
<span class="gi">+        peak_mem_usage = self.get_virtual_size()</span>
<span class="gi">+        if peak_mem_usage &gt; self.limit:</span>
<span class="gi">+            self.crawler.stats.set_value(&quot;memusage/limit_reached&quot;, 1)</span>
<span class="gi">+            mem = self.limit / 1024 / 1024</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...&quot;,</span>
<span class="gi">+                {&quot;memusage&quot;: mem},</span>
<span class="gi">+                extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+            )</span>
<span class="gi">+            if self.notify_mails:</span>
<span class="gi">+                subj = (</span>
<span class="gi">+                    f&quot;{self.crawler.settings[&#39;BOT_NAME&#39;]} terminated: &quot;</span>
<span class="gi">+                    f&quot;memory usage exceeded {mem}MiB at {socket.gethostname()}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                self._send_report(self.notify_mails, subj)</span>
<span class="gi">+                self.crawler.stats.set_value(&quot;memusage/limit_notified&quot;, 1)</span>
<span class="gi">+</span>
<span class="gi">+            if self.crawler.engine.spider is not None:</span>
<span class="gi">+                self.crawler.engine.close_spider(</span>
<span class="gi">+                    self.crawler.engine.spider, &quot;memusage_exceeded&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                self.crawler.stop()</span>
<span class="gi">+        else:</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;Peak memory usage is %(virtualsize)dMiB&quot;,</span>
<span class="gi">+                {&quot;virtualsize&quot;: peak_mem_usage / 1024 / 1024},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _check_warning(self):</span>
<span class="gi">+        if self.warned:  # warn only once</span>
<span class="gi">+            return</span>
<span class="gi">+        if self.get_virtual_size() &gt; self.warning:</span>
<span class="gi">+            self.crawler.stats.set_value(&quot;memusage/warning_reached&quot;, 1)</span>
<span class="gi">+            mem = self.warning / 1024 / 1024</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;Memory usage reached %(memusage)dMiB&quot;,</span>
<span class="gi">+                {&quot;memusage&quot;: mem},</span>
<span class="gi">+                extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+            )</span>
<span class="gi">+            if self.notify_mails:</span>
<span class="gi">+                subj = (</span>
<span class="gi">+                    f&quot;{self.crawler.settings[&#39;BOT_NAME&#39;]} warning: &quot;</span>
<span class="gi">+                    f&quot;memory usage reached {mem}MiB at {socket.gethostname()}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                self._send_report(self.notify_mails, subj)</span>
<span class="gi">+                self.crawler.stats.set_value(&quot;memusage/warning_notified&quot;, 1)</span>
<span class="gi">+            self.warned = True</span>

<span class="w"> </span>    def _send_report(self, rcpts, subject):
<span class="w"> </span>        &quot;&quot;&quot;send notification mail with some additional useful info&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        stats = self.crawler.stats</span>
<span class="gi">+        s = f&quot;Memory usage at engine startup : {stats.get_value(&#39;memusage/startup&#39;) / 1024 / 1024}M\r\n&quot;</span>
<span class="gi">+        s += f&quot;Maximum memory usage          : {stats.get_value(&#39;memusage/max&#39;) / 1024 / 1024}M\r\n&quot;</span>
<span class="gi">+        s += f&quot;Current memory usage          : {self.get_virtual_size() / 1024 / 1024}M\r\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+        s += (</span>
<span class="gi">+            &quot;ENGINE STATUS ------------------------------------------------------- \r\n&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        s += &quot;\r\n&quot;</span>
<span class="gi">+        s += pformat(get_engine_status(self.crawler.engine))</span>
<span class="gi">+        s += &quot;\r\n&quot;</span>
<span class="gi">+        self.mail.send(rcpts, subject, s)</span>
<span class="gh">diff --git a/scrapy/extensions/periodic_log.py b/scrapy/extensions/periodic_log.py</span>
<span class="gh">index 6703689b9..2d557f123 100644</span>
<span class="gd">--- a/scrapy/extensions/periodic_log.py</span>
<span class="gi">+++ b/scrapy/extensions/periodic_log.py</span>
<span class="gu">@@ -1,26 +1,140 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>from datetime import datetime, timezone
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import task
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.serialize import ScrapyJSONEncoder
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class PeriodicLog:
<span class="w"> </span>    &quot;&quot;&quot;Log basic scraping stats periodically&quot;&quot;&quot;

<span class="gd">-    def __init__(self, stats, interval=60.0, ext_stats={}, ext_delta={},</span>
<span class="gd">-        ext_timing_enabled=False):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        stats,</span>
<span class="gi">+        interval=60.0,</span>
<span class="gi">+        ext_stats={},</span>
<span class="gi">+        ext_delta={},</span>
<span class="gi">+        ext_timing_enabled=False,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.stats = stats
<span class="w"> </span>        self.interval = interval
<span class="w"> </span>        self.multiplier = 60.0 / self.interval
<span class="w"> </span>        self.task = None
<span class="w"> </span>        self.encoder = ScrapyJSONEncoder(sort_keys=True, indent=4)
<span class="w"> </span>        self.ext_stats_enabled = bool(ext_stats)
<span class="gd">-        self.ext_stats_include = ext_stats.get(&#39;include&#39;, [])</span>
<span class="gd">-        self.ext_stats_exclude = ext_stats.get(&#39;exclude&#39;, [])</span>
<span class="gi">+        self.ext_stats_include = ext_stats.get(&quot;include&quot;, [])</span>
<span class="gi">+        self.ext_stats_exclude = ext_stats.get(&quot;exclude&quot;, [])</span>
<span class="w"> </span>        self.ext_delta_enabled = bool(ext_delta)
<span class="gd">-        self.ext_delta_include = ext_delta.get(&#39;include&#39;, [])</span>
<span class="gd">-        self.ext_delta_exclude = ext_delta.get(&#39;exclude&#39;, [])</span>
<span class="gi">+        self.ext_delta_include = ext_delta.get(&quot;include&quot;, [])</span>
<span class="gi">+        self.ext_delta_exclude = ext_delta.get(&quot;exclude&quot;, [])</span>
<span class="w"> </span>        self.ext_timing_enabled = ext_timing_enabled
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        interval = crawler.settings.getfloat(&quot;LOGSTATS_INTERVAL&quot;)</span>
<span class="gi">+        if not interval:</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        try:</span>
<span class="gi">+            ext_stats = crawler.settings.getdict(&quot;PERIODIC_LOG_STATS&quot;)</span>
<span class="gi">+        except (TypeError, ValueError):</span>
<span class="gi">+            ext_stats = (</span>
<span class="gi">+                {&quot;enabled&quot;: True}</span>
<span class="gi">+                if crawler.settings.getbool(&quot;PERIODIC_LOG_STATS&quot;)</span>
<span class="gi">+                else None</span>
<span class="gi">+            )</span>
<span class="gi">+        try:</span>
<span class="gi">+            ext_delta = crawler.settings.getdict(&quot;PERIODIC_LOG_DELTA&quot;)</span>
<span class="gi">+        except (TypeError, ValueError):</span>
<span class="gi">+            ext_delta = (</span>
<span class="gi">+                {&quot;enabled&quot;: True}</span>
<span class="gi">+                if crawler.settings.getbool(&quot;PERIODIC_LOG_DELTA&quot;)</span>
<span class="gi">+                else None</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        ext_timing_enabled = crawler.settings.getbool(</span>
<span class="gi">+            &quot;PERIODIC_LOG_TIMING_ENABLED&quot;, False</span>
<span class="gi">+        )</span>
<span class="gi">+        if not (ext_stats or ext_delta or ext_timing_enabled):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        o = cls(</span>
<span class="gi">+            crawler.stats,</span>
<span class="gi">+            interval,</span>
<span class="gi">+            ext_stats,</span>
<span class="gi">+            ext_delta,</span>
<span class="gi">+            ext_timing_enabled,</span>
<span class="gi">+        )</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.time_prev = datetime.now(tz=timezone.utc)</span>
<span class="gi">+        self.delta_prev = {}</span>
<span class="gi">+        self.stats_prev = {}</span>
<span class="gi">+</span>
<span class="gi">+        self.task = task.LoopingCall(self.log)</span>
<span class="gi">+        self.task.start(self.interval)</span>
<span class="gi">+</span>
<span class="gi">+    def log(self):</span>
<span class="gi">+        data = {}</span>
<span class="gi">+        if self.ext_timing_enabled:</span>
<span class="gi">+            data.update(self.log_timing())</span>
<span class="gi">+        if self.ext_delta_enabled:</span>
<span class="gi">+            data.update(self.log_delta())</span>
<span class="gi">+        if self.ext_stats_enabled:</span>
<span class="gi">+            data.update(self.log_crawler_stats())</span>
<span class="gi">+        logger.info(self.encoder.encode(data))</span>
<span class="gi">+</span>
<span class="gi">+    def log_delta(self):</span>
<span class="gi">+        num_stats = {</span>
<span class="gi">+            k: v</span>
<span class="gi">+            for k, v in self.stats._stats.items()</span>
<span class="gi">+            if isinstance(v, (int, float))</span>
<span class="gi">+            and self.param_allowed(k, self.ext_delta_include, self.ext_delta_exclude)</span>
<span class="gi">+        }</span>
<span class="gi">+        delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}</span>
<span class="gi">+        self.delta_prev = num_stats</span>
<span class="gi">+        return {&quot;delta&quot;: delta}</span>
<span class="gi">+</span>
<span class="gi">+    def log_timing(self):</span>
<span class="gi">+        now = datetime.now(tz=timezone.utc)</span>
<span class="gi">+        time = {</span>
<span class="gi">+            &quot;log_interval&quot;: self.interval,</span>
<span class="gi">+            &quot;start_time&quot;: self.stats._stats[&quot;start_time&quot;],</span>
<span class="gi">+            &quot;utcnow&quot;: now,</span>
<span class="gi">+            &quot;log_interval_real&quot;: (now - self.time_prev).total_seconds(),</span>
<span class="gi">+            &quot;elapsed&quot;: (now - self.stats._stats[&quot;start_time&quot;]).total_seconds(),</span>
<span class="gi">+        }</span>
<span class="gi">+        self.time_prev = now</span>
<span class="gi">+        return {&quot;time&quot;: time}</span>
<span class="gi">+</span>
<span class="gi">+    def log_crawler_stats(self):</span>
<span class="gi">+        stats = {</span>
<span class="gi">+            k: v</span>
<span class="gi">+            for k, v in self.stats._stats.items()</span>
<span class="gi">+            if self.param_allowed(k, self.ext_stats_include, self.ext_stats_exclude)</span>
<span class="gi">+        }</span>
<span class="gi">+        return {&quot;stats&quot;: stats}</span>
<span class="gi">+</span>
<span class="gi">+    def param_allowed(self, stat_name, include, exclude):</span>
<span class="gi">+        if not include and not exclude:</span>
<span class="gi">+            return True</span>
<span class="gi">+        for p in exclude:</span>
<span class="gi">+            if p in stat_name:</span>
<span class="gi">+                return False</span>
<span class="gi">+        if exclude and not include:</span>
<span class="gi">+            return True</span>
<span class="gi">+        for p in include:</span>
<span class="gi">+            if p in stat_name:</span>
<span class="gi">+                return True</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider, reason):</span>
<span class="gi">+        self.log()</span>
<span class="gi">+        if self.task and self.task.running:</span>
<span class="gi">+            self.task.stop()</span>
<span class="gh">diff --git a/scrapy/extensions/postprocessing.py b/scrapy/extensions/postprocessing.py</span>
<span class="gh">index 32f5ff6bc..17969c5b0 100644</span>
<span class="gd">--- a/scrapy/extensions/postprocessing.py</span>
<span class="gi">+++ b/scrapy/extensions/postprocessing.py</span>
<span class="gu">@@ -6,6 +6,7 @@ from gzip import GzipFile</span>
<span class="w"> </span>from io import IOBase
<span class="w"> </span>from lzma import LZMAFile
<span class="w"> </span>from typing import Any, BinaryIO, Dict, List
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.misc import load_object


<span class="gu">@@ -22,14 +23,25 @@ class GzipPlugin:</span>
<span class="w"> </span>    See :py:class:`gzip.GzipFile` for more info about parameters.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt;None:</span>
<span class="gi">+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt; None:</span>
<span class="w"> </span>        self.file = file
<span class="w"> </span>        self.feed_options = feed_options
<span class="gd">-        compress_level = self.feed_options.get(&#39;gzip_compresslevel&#39;, 9)</span>
<span class="gd">-        mtime = self.feed_options.get(&#39;gzip_mtime&#39;)</span>
<span class="gd">-        filename = self.feed_options.get(&#39;gzip_filename&#39;)</span>
<span class="gd">-        self.gzipfile = GzipFile(fileobj=self.file, mode=&#39;wb&#39;,</span>
<span class="gd">-            compresslevel=compress_level, mtime=mtime, filename=filename)</span>
<span class="gi">+        compress_level = self.feed_options.get(&quot;gzip_compresslevel&quot;, 9)</span>
<span class="gi">+        mtime = self.feed_options.get(&quot;gzip_mtime&quot;)</span>
<span class="gi">+        filename = self.feed_options.get(&quot;gzip_filename&quot;)</span>
<span class="gi">+        self.gzipfile = GzipFile(</span>
<span class="gi">+            fileobj=self.file,</span>
<span class="gi">+            mode=&quot;wb&quot;,</span>
<span class="gi">+            compresslevel=compress_level,</span>
<span class="gi">+            mtime=mtime,</span>
<span class="gi">+            filename=filename,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def write(self, data: bytes) -&gt; int:</span>
<span class="gi">+        return self.gzipfile.write(data)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; None:</span>
<span class="gi">+        self.gzipfile.close()</span>


<span class="w"> </span>class Bz2Plugin:
<span class="gu">@@ -43,12 +55,19 @@ class Bz2Plugin:</span>
<span class="w"> </span>    See :py:class:`bz2.BZ2File` for more info about parameters.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt;None:</span>
<span class="gi">+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt; None:</span>
<span class="w"> </span>        self.file = file
<span class="w"> </span>        self.feed_options = feed_options
<span class="gd">-        compress_level = self.feed_options.get(&#39;bz2_compresslevel&#39;, 9)</span>
<span class="gd">-        self.bz2file = BZ2File(filename=self.file, mode=&#39;wb&#39;, compresslevel</span>
<span class="gd">-            =compress_level)</span>
<span class="gi">+        compress_level = self.feed_options.get(&quot;bz2_compresslevel&quot;, 9)</span>
<span class="gi">+        self.bz2file = BZ2File(</span>
<span class="gi">+            filename=self.file, mode=&quot;wb&quot;, compresslevel=compress_level</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def write(self, data: bytes) -&gt; int:</span>
<span class="gi">+        return self.bz2file.write(data)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; None:</span>
<span class="gi">+        self.bz2file.close()</span>


<span class="w"> </span>class LZMAPlugin:
<span class="gu">@@ -68,17 +87,33 @@ class LZMAPlugin:</span>
<span class="w"> </span>    See :py:class:`lzma.LZMAFile` for more info about parameters.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt;None:</span>
<span class="gi">+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -&gt; None:</span>
<span class="w"> </span>        self.file = file
<span class="w"> </span>        self.feed_options = feed_options
<span class="gd">-        format = self.feed_options.get(&#39;lzma_format&#39;)</span>
<span class="gd">-        check = self.feed_options.get(&#39;lzma_check&#39;, -1)</span>
<span class="gd">-        preset = self.feed_options.get(&#39;lzma_preset&#39;)</span>
<span class="gd">-        filters = self.feed_options.get(&#39;lzma_filters&#39;)</span>
<span class="gd">-        self.lzmafile = LZMAFile(filename=self.file, mode=&#39;wb&#39;, format=</span>
<span class="gd">-            format, check=check, preset=preset, filters=filters)</span>
<span class="gd">-</span>

<span class="gi">+        format = self.feed_options.get(&quot;lzma_format&quot;)</span>
<span class="gi">+        check = self.feed_options.get(&quot;lzma_check&quot;, -1)</span>
<span class="gi">+        preset = self.feed_options.get(&quot;lzma_preset&quot;)</span>
<span class="gi">+        filters = self.feed_options.get(&quot;lzma_filters&quot;)</span>
<span class="gi">+        self.lzmafile = LZMAFile(</span>
<span class="gi">+            filename=self.file,</span>
<span class="gi">+            mode=&quot;wb&quot;,</span>
<span class="gi">+            format=format,</span>
<span class="gi">+            check=check,</span>
<span class="gi">+            preset=preset,</span>
<span class="gi">+            filters=filters,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def write(self, data: bytes) -&gt; int:</span>
<span class="gi">+        return self.lzmafile.write(data)</span>
<span class="gi">+</span>
<span class="gi">+    def close(self) -&gt; None:</span>
<span class="gi">+        self.lzmafile.close()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# io.IOBase is subclassed here, so that exporters can use the PostProcessingManager</span>
<span class="gi">+# instance as a file like writable object. This could be needed by some exporters</span>
<span class="gi">+# such as CsvItemExporter which wraps the feed storage with io.TextIOWrapper.</span>
<span class="w"> </span>class PostProcessingManager(IOBase):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    This will manage and use declared plugins to process data in a
<span class="gu">@@ -89,14 +124,15 @@ class PostProcessingManager(IOBase):</span>
<span class="w"> </span>    :type file: file like object
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, plugins: List[Any], file: BinaryIO, feed_options:</span>
<span class="gd">-        Dict[str, Any]) -&gt;None:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="w"> </span>        self.plugins = self._load_plugins(plugins)
<span class="w"> </span>        self.file = file
<span class="w"> </span>        self.feed_options = feed_options
<span class="w"> </span>        self.head_plugin = self._get_head_plugin()

<span class="gd">-    def write(self, data: bytes) -&gt;int:</span>
<span class="gi">+    def write(self, data: bytes) -&gt; int:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Uses all the declared plugins to process data first, then writes
<span class="w"> </span>        the processed data to target file.
<span class="gu">@@ -105,10 +141,26 @@ class PostProcessingManager(IOBase):</span>
<span class="w"> </span>        :return: returns number of bytes written
<span class="w"> </span>        :rtype: int
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.head_plugin.write(data)</span>
<span class="gi">+</span>
<span class="gi">+    def tell(self) -&gt; int:</span>
<span class="gi">+        return self.file.tell()</span>

<span class="gd">-    def close(self) -&gt;None:</span>
<span class="gi">+    def close(self) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Close the target file along with all the plugins.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.head_plugin.close()</span>
<span class="gi">+</span>
<span class="gi">+    def writable(self) -&gt; bool:</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def _load_plugins(self, plugins: List[Any]) -&gt; List[Any]:</span>
<span class="gi">+        plugins = [load_object(plugin) for plugin in plugins]</span>
<span class="gi">+        return plugins</span>
<span class="gi">+</span>
<span class="gi">+    def _get_head_plugin(self) -&gt; Any:</span>
<span class="gi">+        prev = self.file</span>
<span class="gi">+        for plugin in self.plugins[::-1]:</span>
<span class="gi">+            prev = plugin(prev, self.feed_options)</span>
<span class="gi">+        return prev</span>
<span class="gh">diff --git a/scrapy/extensions/spiderstate.py b/scrapy/extensions/spiderstate.py</span>
<span class="gh">index 903837a0c..929a3be70 100644</span>
<span class="gd">--- a/scrapy/extensions/spiderstate.py</span>
<span class="gi">+++ b/scrapy/extensions/spiderstate.py</span>
<span class="gu">@@ -1,5 +1,6 @@</span>
<span class="w"> </span>import pickle
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.job import job_dir
<span class="gu">@@ -10,3 +11,30 @@ class SpiderState:</span>

<span class="w"> </span>    def __init__(self, jobdir=None):
<span class="w"> </span>        self.jobdir = jobdir
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        jobdir = job_dir(crawler.settings)</span>
<span class="gi">+        if not jobdir:</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+</span>
<span class="gi">+        obj = cls(jobdir)</span>
<span class="gi">+        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        return obj</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider):</span>
<span class="gi">+        if self.jobdir:</span>
<span class="gi">+            with Path(self.statefn).open(&quot;wb&quot;) as f:</span>
<span class="gi">+                pickle.dump(spider.state, f, protocol=4)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        if self.jobdir and Path(self.statefn).exists():</span>
<span class="gi">+            with Path(self.statefn).open(&quot;rb&quot;) as f:</span>
<span class="gi">+                spider.state = pickle.load(f)</span>
<span class="gi">+        else:</span>
<span class="gi">+            spider.state = {}</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def statefn(self) -&gt; str:</span>
<span class="gi">+        return str(Path(self.jobdir, &quot;spider.state&quot;))</span>
<span class="gh">diff --git a/scrapy/extensions/statsmailer.py b/scrapy/extensions/statsmailer.py</span>
<span class="gh">index b0e2395d9..58610c25e 100644</span>
<span class="gd">--- a/scrapy/extensions/statsmailer.py</span>
<span class="gi">+++ b/scrapy/extensions/statsmailer.py</span>
<span class="gu">@@ -3,14 +3,32 @@ StatsMailer extension sends an email when a spider finishes scraping.</span>

<span class="w"> </span>Use STATSMAILER_RCPTS setting to enable and give the recipient mail address
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.mail import MailSender


<span class="w"> </span>class StatsMailer:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, stats, recipients, mail):
<span class="w"> </span>        self.stats = stats
<span class="w"> </span>        self.recipients = recipients
<span class="w"> </span>        self.mail = mail
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        recipients = crawler.settings.getlist(&quot;STATSMAILER_RCPTS&quot;)</span>
<span class="gi">+        if not recipients:</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        mail = MailSender.from_settings(crawler.settings)</span>
<span class="gi">+        o = cls(crawler.stats, recipients, mail)</span>
<span class="gi">+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def spider_closed(self, spider):</span>
<span class="gi">+        spider_stats = self.stats.get_stats(spider)</span>
<span class="gi">+        body = &quot;Global stats\n\n&quot;</span>
<span class="gi">+        body += &quot;\n&quot;.join(f&quot;{k:&lt;50} : {v}&quot; for k, v in self.stats.get_stats().items())</span>
<span class="gi">+        body += f&quot;\n\n{spider.name} stats\n\n&quot;</span>
<span class="gi">+        body += &quot;\n&quot;.join(f&quot;{k:&lt;50} : {v}&quot; for k, v in spider_stats.items())</span>
<span class="gi">+        return self.mail.send(self.recipients, f&quot;Scrapy stats for: {spider.name}&quot;, body)</span>
<span class="gh">diff --git a/scrapy/extensions/telnet.py b/scrapy/extensions/telnet.py</span>
<span class="gh">index d866ecf30..c92b7f5fe 100644</span>
<span class="gd">--- a/scrapy/extensions/telnet.py</span>
<span class="gi">+++ b/scrapy/extensions/telnet.py</span>
<span class="gu">@@ -3,50 +3,113 @@ Scrapy Telnet Console extension</span>

<span class="w"> </span>See documentation in docs/topics/telnetconsole.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import binascii
<span class="w"> </span>import logging
<span class="w"> </span>import os
<span class="w"> </span>import pprint
<span class="w"> </span>import traceback
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import protocol
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    from twisted.conch import manhole, telnet
<span class="w"> </span>    from twisted.conch.insults import insults
<span class="gi">+</span>
<span class="w"> </span>    TWISTED_CONCH_AVAILABLE = True
<span class="w"> </span>except (ImportError, SyntaxError):
<span class="w"> </span>    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()
<span class="w"> </span>    TWISTED_CONCH_AVAILABLE = False
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.utils.decorators import defers
<span class="w"> </span>from scrapy.utils.engine import print_engine_status
<span class="w"> </span>from scrapy.utils.reactor import listen_tcp
<span class="w"> </span>from scrapy.utils.trackref import print_live_refs
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gi">+</span>
<span class="gi">+# signal to update telnet variables</span>
<span class="gi">+# args: telnet_vars</span>
<span class="w"> </span>update_telnet_vars = object()


<span class="w"> </span>class TelnetConsole(protocol.ServerFactory):
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="gd">-        if not crawler.settings.getbool(&#39;TELNETCONSOLE_ENABLED&#39;):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;TELNETCONSOLE_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="w"> </span>        if not TWISTED_CONCH_AVAILABLE:
<span class="w"> </span>            raise NotConfigured(
<span class="gd">-                &quot;&quot;&quot;TELNETCONSOLE_ENABLED setting is True but required twisted modules failed to import:</span>
<span class="gd">-&quot;&quot;&quot;</span>
<span class="gd">-                 + _TWISTED_CONCH_TRACEBACK)</span>
<span class="gi">+                &quot;TELNETCONSOLE_ENABLED setting is True but required twisted &quot;</span>
<span class="gi">+                &quot;modules failed to import:\n&quot; + _TWISTED_CONCH_TRACEBACK</span>
<span class="gi">+            )</span>
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self.noisy = False
<span class="gd">-        self.portrange = [int(x) for x in crawler.settings.getlist(</span>
<span class="gd">-            &#39;TELNETCONSOLE_PORT&#39;)]</span>
<span class="gd">-        self.host = crawler.settings[&#39;TELNETCONSOLE_HOST&#39;]</span>
<span class="gd">-        self.username = crawler.settings[&#39;TELNETCONSOLE_USERNAME&#39;]</span>
<span class="gd">-        self.password = crawler.settings[&#39;TELNETCONSOLE_PASSWORD&#39;]</span>
<span class="gi">+        self.portrange = [</span>
<span class="gi">+            int(x) for x in crawler.settings.getlist(&quot;TELNETCONSOLE_PORT&quot;)</span>
<span class="gi">+        ]</span>
<span class="gi">+        self.host = crawler.settings[&quot;TELNETCONSOLE_HOST&quot;]</span>
<span class="gi">+        self.username = crawler.settings[&quot;TELNETCONSOLE_USERNAME&quot;]</span>
<span class="gi">+        self.password = crawler.settings[&quot;TELNETCONSOLE_PASSWORD&quot;]</span>
<span class="gi">+</span>
<span class="w"> </span>        if not self.password:
<span class="gd">-            self.password = binascii.hexlify(os.urandom(8)).decode(&#39;utf8&#39;)</span>
<span class="gd">-            logger.info(&#39;Telnet Password: %s&#39;, self.password)</span>
<span class="gd">-        self.crawler.signals.connect(self.start_listening, signals.</span>
<span class="gd">-            engine_started)</span>
<span class="gd">-        self.crawler.signals.connect(self.stop_listening, signals.</span>
<span class="gd">-            engine_stopped)</span>
<span class="gi">+            self.password = binascii.hexlify(os.urandom(8)).decode(&quot;utf8&quot;)</span>
<span class="gi">+            logger.info(&quot;Telnet Password: %s&quot;, self.password)</span>
<span class="gi">+</span>
<span class="gi">+        self.crawler.signals.connect(self.start_listening, signals.engine_started)</span>
<span class="gi">+        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def start_listening(self):</span>
<span class="gi">+        self.port = listen_tcp(self.portrange, self.host, self)</span>
<span class="gi">+        h = self.port.getHost()</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Telnet console listening on %(host)s:%(port)d&quot;,</span>
<span class="gi">+            {&quot;host&quot;: h.host, &quot;port&quot;: h.port},</span>
<span class="gi">+            extra={&quot;crawler&quot;: self.crawler},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def stop_listening(self):</span>
<span class="gi">+        self.port.stopListening()</span>
<span class="gi">+</span>
<span class="gi">+    def protocol(self):</span>
<span class="gi">+        class Portal:</span>
<span class="gi">+            &quot;&quot;&quot;An implementation of IPortal&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+            @defers</span>
<span class="gi">+            def login(self_, credentials, mind, *interfaces):</span>
<span class="gi">+                if not (</span>
<span class="gi">+                    credentials.username == self.username.encode(&quot;utf8&quot;)</span>
<span class="gi">+                    and credentials.checkPassword(self.password.encode(&quot;utf8&quot;))</span>
<span class="gi">+                ):</span>
<span class="gi">+                    raise ValueError(&quot;Invalid credentials&quot;)</span>
<span class="gi">+</span>
<span class="gi">+                protocol = telnet.TelnetBootstrapProtocol(</span>
<span class="gi">+                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()</span>
<span class="gi">+                )</span>
<span class="gi">+                return (interfaces[0], protocol, lambda: None)</span>
<span class="gi">+</span>
<span class="gi">+        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())</span>
<span class="gi">+</span>
<span class="gi">+    def _get_telnet_vars(self):</span>
<span class="gi">+        # Note: if you add entries here also update topics/telnetconsole.rst</span>
<span class="gi">+        telnet_vars = {</span>
<span class="gi">+            &quot;engine&quot;: self.crawler.engine,</span>
<span class="gi">+            &quot;spider&quot;: self.crawler.engine.spider,</span>
<span class="gi">+            &quot;slot&quot;: self.crawler.engine.slot,</span>
<span class="gi">+            &quot;crawler&quot;: self.crawler,</span>
<span class="gi">+            &quot;extensions&quot;: self.crawler.extensions,</span>
<span class="gi">+            &quot;stats&quot;: self.crawler.stats,</span>
<span class="gi">+            &quot;settings&quot;: self.crawler.settings,</span>
<span class="gi">+            &quot;est&quot;: lambda: print_engine_status(self.crawler.engine),</span>
<span class="gi">+            &quot;p&quot;: pprint.pprint,</span>
<span class="gi">+            &quot;prefs&quot;: print_live_refs,</span>
<span class="gi">+            &quot;help&quot;: &quot;This is Scrapy telnet console. For more info see: &quot;</span>
<span class="gi">+            &quot;https://docs.scrapy.org/en/latest/topics/telnetconsole.html&quot;,</span>
<span class="gi">+        }</span>
<span class="gi">+        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)</span>
<span class="gi">+        return telnet_vars</span>
<span class="gh">diff --git a/scrapy/extensions/throttle.py b/scrapy/extensions/throttle.py</span>
<span class="gh">index 4920a7cc7..396800775 100644</span>
<span class="gd">--- a/scrapy/extensions/throttle.py</span>
<span class="gi">+++ b/scrapy/extensions/throttle.py</span>
<span class="gu">@@ -1,23 +1,101 @@</span>
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class AutoThrottle:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="w"> </span>        self.crawler = crawler
<span class="gd">-        if not crawler.settings.getbool(&#39;AUTOTHROTTLE_ENABLED&#39;):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;AUTOTHROTTLE_ENABLED&quot;):</span>
<span class="w"> </span>            raise NotConfigured
<span class="gd">-        self.debug = crawler.settings.getbool(&#39;AUTOTHROTTLE_DEBUG&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        self.debug = crawler.settings.getbool(&quot;AUTOTHROTTLE_DEBUG&quot;)</span>
<span class="w"> </span>        self.target_concurrency = crawler.settings.getfloat(
<span class="gd">-            &#39;AUTOTHROTTLE_TARGET_CONCURRENCY&#39;)</span>
<span class="gd">-        crawler.signals.connect(self._spider_opened, signal=signals.</span>
<span class="gd">-            spider_opened)</span>
<span class="gd">-        crawler.signals.connect(self._response_downloaded, signal=signals.</span>
<span class="gd">-            response_downloaded)</span>
<span class="gi">+            &quot;AUTOTHROTTLE_TARGET_CONCURRENCY&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        crawler.signals.connect(</span>
<span class="gi">+            self._response_downloaded, signal=signals.response_downloaded</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def _spider_opened(self, spider):</span>
<span class="gi">+        self.mindelay = self._min_delay(spider)</span>
<span class="gi">+        self.maxdelay = self._max_delay(spider)</span>
<span class="gi">+        spider.download_delay = self._start_delay(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _min_delay(self, spider):</span>
<span class="gi">+        s = self.crawler.settings</span>
<span class="gi">+        return getattr(spider, &quot;download_delay&quot;, s.getfloat(&quot;DOWNLOAD_DELAY&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def _max_delay(self, spider):</span>
<span class="gi">+        return self.crawler.settings.getfloat(&quot;AUTOTHROTTLE_MAX_DELAY&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _start_delay(self, spider):</span>
<span class="gi">+        return max(</span>
<span class="gi">+            self.mindelay, self.crawler.settings.getfloat(&quot;AUTOTHROTTLE_START_DELAY&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _response_downloaded(self, response, request, spider):</span>
<span class="gi">+        key, slot = self._get_slot(request, spider)</span>
<span class="gi">+        latency = request.meta.get(&quot;download_latency&quot;)</span>
<span class="gi">+        if latency is None or slot is None:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        olddelay = slot.delay</span>
<span class="gi">+        self._adjust_delay(slot, latency, response)</span>
<span class="gi">+        if self.debug:</span>
<span class="gi">+            diff = slot.delay - olddelay</span>
<span class="gi">+            size = len(response.body)</span>
<span class="gi">+            conc = len(slot.transferring)</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;slot: %(slot)s | conc:%(concurrency)2d | &quot;</span>
<span class="gi">+                &quot;delay:%(delay)5d ms (%(delaydiff)+d) | &quot;</span>
<span class="gi">+                &quot;latency:%(latency)5d ms | size:%(size)6d bytes&quot;,</span>
<span class="gi">+                {</span>
<span class="gi">+                    &quot;slot&quot;: key,</span>
<span class="gi">+                    &quot;concurrency&quot;: conc,</span>
<span class="gi">+                    &quot;delay&quot;: slot.delay * 1000,</span>
<span class="gi">+                    &quot;delaydiff&quot;: diff * 1000,</span>
<span class="gi">+                    &quot;latency&quot;: latency * 1000,</span>
<span class="gi">+                    &quot;size&quot;: size,</span>
<span class="gi">+                },</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _get_slot(self, request, spider):</span>
<span class="gi">+        key = request.meta.get(&quot;download_slot&quot;)</span>
<span class="gi">+        return key, self.crawler.engine.downloader.slots.get(key)</span>

<span class="w"> </span>    def _adjust_delay(self, slot, latency, response):
<span class="w"> </span>        &quot;&quot;&quot;Define delay adjustment policy&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # If a server needs `latency` seconds to respond then</span>
<span class="gi">+        # we should send a request each `latency/N` seconds</span>
<span class="gi">+        # to have N requests processed in parallel</span>
<span class="gi">+        target_delay = latency / self.target_concurrency</span>
<span class="gi">+</span>
<span class="gi">+        # Adjust the delay to make it closer to target_delay</span>
<span class="gi">+        new_delay = (slot.delay + target_delay) / 2.0</span>
<span class="gi">+</span>
<span class="gi">+        # If target delay is bigger than old delay, then use it instead of mean.</span>
<span class="gi">+        # It works better with problematic sites.</span>
<span class="gi">+        new_delay = max(target_delay, new_delay)</span>
<span class="gi">+</span>
<span class="gi">+        # Make sure self.mindelay &lt;= new_delay &lt;= self.max_delay</span>
<span class="gi">+        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)</span>
<span class="gi">+</span>
<span class="gi">+        # Dont adjust delay if response status != 200 and new delay is smaller</span>
<span class="gi">+        # than old one, as error pages (and redirections) are usually small and</span>
<span class="gi">+        # so tend to reduce latency, thus provoking a positive feedback by</span>
<span class="gi">+        # reducing delay instead of increase.</span>
<span class="gi">+        if response.status != 200 and new_delay &lt;= slot.delay:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        slot.delay = new_delay</span>
<span class="gh">diff --git a/scrapy/http/common.py b/scrapy/http/common.py</span>
<span class="gh">index e69de29bb..bc8861574 100644</span>
<span class="gd">--- a/scrapy/http/common.py</span>
<span class="gi">+++ b/scrapy/http/common.py</span>
<span class="gu">@@ -0,0 +1,7 @@</span>
<span class="gi">+def obsolete_setter(setter, attrname):</span>
<span class="gi">+    def newsetter(self, value):</span>
<span class="gi">+        c = self.__class__.__name__</span>
<span class="gi">+        msg = f&quot;{c}.{attrname} is not modifiable, use {c}.replace() instead&quot;</span>
<span class="gi">+        raise AttributeError(msg)</span>
<span class="gi">+</span>
<span class="gi">+    return newsetter</span>
<span class="gh">diff --git a/scrapy/http/cookies.py b/scrapy/http/cookies.py</span>
<span class="gh">index 25e4927de..15f25f69d 100644</span>
<span class="gd">--- a/scrapy/http/cookies.py</span>
<span class="gi">+++ b/scrapy/http/cookies.py</span>
<span class="gu">@@ -2,13 +2,16 @@ import re</span>
<span class="w"> </span>import time
<span class="w"> </span>from http.cookiejar import CookieJar as _CookieJar
<span class="w"> </span>from http.cookiejar import DefaultCookiePolicy
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="gd">-IPV4_RE = re.compile(&#39;\\.\\d+$&#39;, re.ASCII)</span>

<span class="gi">+# Defined in the http.cookiejar module, but undocumented:</span>
<span class="gi">+# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527</span>
<span class="gi">+IPV4_RE = re.compile(r&quot;\.\d+$&quot;, re.ASCII)</span>

<span class="gd">-class CookieJar:</span>

<span class="gi">+class CookieJar:</span>
<span class="w"> </span>    def __init__(self, policy=None, check_expired_frequency=10000):
<span class="w"> </span>        self.policy = policy or DefaultCookiePolicy()
<span class="w"> </span>        self.jar = _CookieJar(self.policy)
<span class="gu">@@ -16,12 +19,73 @@ class CookieJar:</span>
<span class="w"> </span>        self.check_expired_frequency = check_expired_frequency
<span class="w"> </span>        self.processed = 0

<span class="gi">+    def extract_cookies(self, response, request):</span>
<span class="gi">+        wreq = WrappedRequest(request)</span>
<span class="gi">+        wrsp = WrappedResponse(response)</span>
<span class="gi">+        return self.jar.extract_cookies(wrsp, wreq)</span>
<span class="gi">+</span>
<span class="gi">+    def add_cookie_header(self, request):</span>
<span class="gi">+        wreq = WrappedRequest(request)</span>
<span class="gi">+        self.policy._now = self.jar._now = int(time.time())</span>
<span class="gi">+</span>
<span class="gi">+        # the cookiejar implementation iterates through all domains</span>
<span class="gi">+        # instead we restrict to potential matches on the domain</span>
<span class="gi">+        req_host = urlparse_cached(request).hostname</span>
<span class="gi">+        if not req_host:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if not IPV4_RE.search(req_host):</span>
<span class="gi">+            hosts = potential_domain_matches(req_host)</span>
<span class="gi">+            if &quot;.&quot; not in req_host:</span>
<span class="gi">+                hosts += [req_host + &quot;.local&quot;]</span>
<span class="gi">+        else:</span>
<span class="gi">+            hosts = [req_host]</span>
<span class="gi">+</span>
<span class="gi">+        cookies = []</span>
<span class="gi">+        for host in hosts:</span>
<span class="gi">+            if host in self.jar._cookies:</span>
<span class="gi">+                cookies += self.jar._cookies_for_domain(host, wreq)</span>
<span class="gi">+</span>
<span class="gi">+        attrs = self.jar._cookie_attrs(cookies)</span>
<span class="gi">+        if attrs:</span>
<span class="gi">+            if not wreq.has_header(&quot;Cookie&quot;):</span>
<span class="gi">+                wreq.add_unredirected_header(&quot;Cookie&quot;, &quot;; &quot;.join(attrs))</span>
<span class="gi">+</span>
<span class="gi">+        self.processed += 1</span>
<span class="gi">+        if self.processed % self.check_expired_frequency == 0:</span>
<span class="gi">+            # This is still quite inefficient for large number of cookies</span>
<span class="gi">+            self.jar.clear_expired_cookies()</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def _cookies(self):</span>
<span class="gi">+        return self.jar._cookies</span>
<span class="gi">+</span>
<span class="gi">+    def clear_session_cookies(self, *args, **kwargs):</span>
<span class="gi">+        return self.jar.clear_session_cookies(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def clear(self, domain=None, path=None, name=None):</span>
<span class="gi">+        return self.jar.clear(domain, path, name)</span>
<span class="gi">+</span>
<span class="w"> </span>    def __iter__(self):
<span class="w"> </span>        return iter(self.jar)

<span class="w"> </span>    def __len__(self):
<span class="w"> </span>        return len(self.jar)

<span class="gi">+    def set_policy(self, pol):</span>
<span class="gi">+        return self.jar.set_policy(pol)</span>
<span class="gi">+</span>
<span class="gi">+    def make_cookies(self, response, request):</span>
<span class="gi">+        wreq = WrappedRequest(request)</span>
<span class="gi">+        wrsp = WrappedResponse(response)</span>
<span class="gi">+        return self.jar.make_cookies(wrsp, wreq)</span>
<span class="gi">+</span>
<span class="gi">+    def set_cookie(self, cookie):</span>
<span class="gi">+        self.jar.set_cookie(cookie)</span>
<span class="gi">+</span>
<span class="gi">+    def set_cookie_if_ok(self, cookie, request):</span>
<span class="gi">+        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))</span>
<span class="gi">+</span>

<span class="w"> </span>def potential_domain_matches(domain):
<span class="w"> </span>    &quot;&quot;&quot;Potential domain matches for a cookie
<span class="gu">@@ -30,11 +94,24 @@ def potential_domain_matches(domain):</span>
<span class="w"> </span>    [&#39;www.example.com&#39;, &#39;example.com&#39;, &#39;.www.example.com&#39;, &#39;.example.com&#39;]

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    matches = [domain]</span>
<span class="gi">+    try:</span>
<span class="gi">+        start = domain.index(&quot;.&quot;) + 1</span>
<span class="gi">+        end = domain.rindex(&quot;.&quot;)</span>
<span class="gi">+        while start &lt; end:</span>
<span class="gi">+            matches.append(domain[start:])</span>
<span class="gi">+            start = domain.index(&quot;.&quot;, start) + 1</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        pass</span>
<span class="gi">+    return matches + [&quot;.&quot; + d for d in matches]</span>


<span class="w"> </span>class _DummyLock:
<span class="gd">-    pass</span>
<span class="gi">+    def acquire(self):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def release(self):</span>
<span class="gi">+        pass</span>


<span class="w"> </span>class WrappedRequest:
<span class="gu">@@ -46,6 +123,15 @@ class WrappedRequest:</span>
<span class="w"> </span>    def __init__(self, request):
<span class="w"> </span>        self.request = request

<span class="gi">+    def get_full_url(self):</span>
<span class="gi">+        return self.request.url</span>
<span class="gi">+</span>
<span class="gi">+    def get_host(self):</span>
<span class="gi">+        return urlparse_cached(self.request).netloc</span>
<span class="gi">+</span>
<span class="gi">+    def get_type(self):</span>
<span class="gi">+        return urlparse_cached(self.request).scheme</span>
<span class="gi">+</span>
<span class="w"> </span>    def is_unverifiable(self):
<span class="w"> </span>        &quot;&quot;&quot;Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.

<span class="gu">@@ -54,10 +140,56 @@ class WrappedRequest:</span>
<span class="w"> </span>        HTML document, and the user had no option to approve the automatic
<span class="w"> </span>        fetching of the image, this should be true.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.request.meta.get(&quot;is_unverifiable&quot;, False)</span>

<span class="gi">+    @property</span>
<span class="gi">+    def full_url(self):</span>
<span class="gi">+        return self.get_full_url()</span>

<span class="gd">-class WrappedResponse:</span>
<span class="gi">+    @property</span>
<span class="gi">+    def host(self):</span>
<span class="gi">+        return self.get_host()</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def type(self):</span>
<span class="gi">+        return self.get_type()</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def unverifiable(self):</span>
<span class="gi">+        return self.is_unverifiable()</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def origin_req_host(self):</span>
<span class="gi">+        return urlparse_cached(self.request).hostname</span>
<span class="gi">+</span>
<span class="gi">+    def has_header(self, name):</span>
<span class="gi">+        return name in self.request.headers</span>

<span class="gi">+    def get_header(self, name, default=None):</span>
<span class="gi">+        value = self.request.headers.get(name, default)</span>
<span class="gi">+        return to_unicode(value, errors=&quot;replace&quot;) if value is not None else None</span>
<span class="gi">+</span>
<span class="gi">+    def header_items(self):</span>
<span class="gi">+        return [</span>
<span class="gi">+            (</span>
<span class="gi">+                to_unicode(k, errors=&quot;replace&quot;),</span>
<span class="gi">+                [to_unicode(x, errors=&quot;replace&quot;) for x in v],</span>
<span class="gi">+            )</span>
<span class="gi">+            for k, v in self.request.headers.items()</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+    def add_unredirected_header(self, name, value):</span>
<span class="gi">+        self.request.headers.appendlist(name, value)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class WrappedResponse:</span>
<span class="w"> </span>    def __init__(self, response):
<span class="w"> </span>        self.response = response
<span class="gi">+</span>
<span class="gi">+    def info(self):</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def get_all(self, name, default=None):</span>
<span class="gi">+        return [</span>
<span class="gi">+            to_unicode(v, errors=&quot;replace&quot;) for v in self.response.headers.getlist(name)</span>
<span class="gi">+        ]</span>
<span class="gh">diff --git a/scrapy/http/headers.py b/scrapy/http/headers.py</span>
<span class="gh">index 730065335..822597c84 100644</span>
<span class="gd">--- a/scrapy/http/headers.py</span>
<span class="gi">+++ b/scrapy/http/headers.py</span>
<span class="gu">@@ -1,5 +1,7 @@</span>
<span class="w"> </span>from collections.abc import Mapping
<span class="gi">+</span>
<span class="w"> </span>from w3lib.http import headers_dict_to_raw
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict
<span class="w"> </span>from scrapy.utils.python import to_unicode

<span class="gu">@@ -7,17 +9,40 @@ from scrapy.utils.python import to_unicode</span>
<span class="w"> </span>class Headers(CaselessDict):
<span class="w"> </span>    &quot;&quot;&quot;Case insensitive http headers dictionary&quot;&quot;&quot;

<span class="gd">-    def __init__(self, seq=None, encoding=&#39;utf-8&#39;):</span>
<span class="gi">+    def __init__(self, seq=None, encoding=&quot;utf-8&quot;):</span>
<span class="w"> </span>        self.encoding = encoding
<span class="w"> </span>        super().__init__(seq)

<span class="gi">+    def update(self, seq):</span>
<span class="gi">+        seq = seq.items() if isinstance(seq, Mapping) else seq</span>
<span class="gi">+        iseq = {}</span>
<span class="gi">+        for k, v in seq:</span>
<span class="gi">+            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))</span>
<span class="gi">+        super().update(iseq)</span>
<span class="gi">+</span>
<span class="w"> </span>    def normkey(self, key):
<span class="w"> </span>        &quot;&quot;&quot;Normalize key to bytes&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._tobytes(key.title())</span>

<span class="w"> </span>    def normvalue(self, value):
<span class="w"> </span>        &quot;&quot;&quot;Normalize values to bytes&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if value is None:</span>
<span class="gi">+            value = []</span>
<span class="gi">+        elif isinstance(value, (str, bytes)):</span>
<span class="gi">+            value = [value]</span>
<span class="gi">+        elif not hasattr(value, &quot;__iter__&quot;):</span>
<span class="gi">+            value = [value]</span>
<span class="gi">+</span>
<span class="gi">+        return [self._tobytes(x) for x in value]</span>
<span class="gi">+</span>
<span class="gi">+    def _tobytes(self, x):</span>
<span class="gi">+        if isinstance(x, bytes):</span>
<span class="gi">+            return x</span>
<span class="gi">+        if isinstance(x, str):</span>
<span class="gi">+            return x.encode(self.encoding)</span>
<span class="gi">+        if isinstance(x, int):</span>
<span class="gi">+            return str(x).encode(self.encoding)</span>
<span class="gi">+        raise TypeError(f&quot;Unsupported value type: {type(x)}&quot;)</span>

<span class="w"> </span>    def __getitem__(self, key):
<span class="w"> </span>        try:
<span class="gu">@@ -25,12 +50,53 @@ class Headers(CaselessDict):</span>
<span class="w"> </span>        except IndexError:
<span class="w"> </span>            return None

<span class="gi">+    def get(self, key, def_val=None):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return super().get(key, def_val)[-1]</span>
<span class="gi">+        except IndexError:</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+    def getlist(self, key, def_val=None):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return super().__getitem__(key)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            if def_val is not None:</span>
<span class="gi">+                return self.normvalue(def_val)</span>
<span class="gi">+            return []</span>
<span class="gi">+</span>
<span class="gi">+    def setlist(self, key, list_):</span>
<span class="gi">+        self[key] = list_</span>
<span class="gi">+</span>
<span class="gi">+    def setlistdefault(self, key, default_list=()):</span>
<span class="gi">+        return self.setdefault(key, default_list)</span>
<span class="gi">+</span>
<span class="gi">+    def appendlist(self, key, value):</span>
<span class="gi">+        lst = self.getlist(key)</span>
<span class="gi">+        lst.extend(self.normvalue(value))</span>
<span class="gi">+        self[key] = lst</span>
<span class="gi">+</span>
<span class="gi">+    def items(self):</span>
<span class="gi">+        return ((k, self.getlist(k)) for k in self.keys())</span>
<span class="gi">+</span>
<span class="gi">+    def values(self):</span>
<span class="gi">+        return [self[k] for k in self.keys()]</span>
<span class="gi">+</span>
<span class="gi">+    def to_string(self):</span>
<span class="gi">+        return headers_dict_to_raw(self)</span>
<span class="gi">+</span>
<span class="w"> </span>    def to_unicode_dict(self):
<span class="w"> </span>        &quot;&quot;&quot;Return headers as a CaselessDict with unicode keys
<span class="w"> </span>        and unicode values. Multiple values are joined with &#39;,&#39;.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return CaseInsensitiveDict(</span>
<span class="gi">+            (</span>
<span class="gi">+                to_unicode(key, encoding=self.encoding),</span>
<span class="gi">+                to_unicode(b&quot;,&quot;.join(value), encoding=self.encoding),</span>
<span class="gi">+            )</span>
<span class="gi">+            for key, value in self.items()</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __copy__(self):
<span class="w"> </span>        return self.__class__(self)
<span class="gi">+</span>
<span class="w"> </span>    copy = __copy__
<span class="gh">diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py</span>
<span class="gh">index 351539ea3..2d1f33edd 100644</span>
<span class="gd">--- a/scrapy/http/request/form.py</span>
<span class="gi">+++ b/scrapy/http/request/form.py</span>
<span class="gu">@@ -4,59 +4,255 @@ This module implements the FormRequest class which is a more convenient class</span>

<span class="w"> </span>See documentation in docs/topics/request-response.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union, cast
<span class="w"> </span>from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit
<span class="gd">-from lxml.html import FormElement</span>
<span class="gd">-from lxml.html import InputElement</span>
<span class="gd">-from lxml.html import MultipleSelectOptions</span>
<span class="gd">-from lxml.html import SelectElement</span>
<span class="gd">-from lxml.html import TextareaElement</span>
<span class="gi">+</span>
<span class="gi">+from lxml.html import FormElement  # nosec</span>
<span class="gi">+from lxml.html import InputElement  # nosec</span>
<span class="gi">+from lxml.html import MultipleSelectOptions  # nosec</span>
<span class="gi">+from lxml.html import SelectElement  # nosec</span>
<span class="gi">+from lxml.html import TextareaElement  # nosec</span>
<span class="w"> </span>from w3lib.html import strip_html5_whitespace
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.http.response.text import TextResponse
<span class="w"> </span>from scrapy.utils.python import is_listlike, to_bytes
<span class="gd">-FormRequestTypeVar = TypeVar(&#39;FormRequestTypeVar&#39;, bound=&#39;FormRequest&#39;)</span>
<span class="gi">+</span>
<span class="gi">+FormRequestTypeVar = TypeVar(&quot;FormRequestTypeVar&quot;, bound=&quot;FormRequest&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>FormdataKVType = Tuple[str, Union[str, Iterable[str]]]
<span class="w"> </span>FormdataType = Optional[Union[dict, List[FormdataKVType]]]


<span class="w"> </span>class FormRequest(Request):
<span class="gd">-    valid_form_methods = [&#39;GET&#39;, &#39;POST&#39;]</span>
<span class="gi">+    valid_form_methods = [&quot;GET&quot;, &quot;POST&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, *args, formdata: FormdataType = None, **kwargs) -&gt; None:</span>
<span class="gi">+        if formdata and kwargs.get(&quot;method&quot;) is None:</span>
<span class="gi">+            kwargs[&quot;method&quot;] = &quot;POST&quot;</span>

<span class="gd">-    def __init__(self, *args, formdata: FormdataType=None, **kwargs) -&gt;None:</span>
<span class="gd">-        if formdata and kwargs.get(&#39;method&#39;) is None:</span>
<span class="gd">-            kwargs[&#39;method&#39;] = &#39;POST&#39;</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="gi">+</span>
<span class="w"> </span>        if formdata:
<span class="gd">-            items = formdata.items() if isinstance(formdata, dict</span>
<span class="gd">-                ) else formdata</span>
<span class="gi">+            items = formdata.items() if isinstance(formdata, dict) else formdata</span>
<span class="w"> </span>            form_query_str = _urlencode(items, self.encoding)
<span class="gd">-            if self.method == &#39;POST&#39;:</span>
<span class="gd">-                self.headers.setdefault(b&#39;Content-Type&#39;,</span>
<span class="gd">-                    b&#39;application/x-www-form-urlencoded&#39;)</span>
<span class="gi">+            if self.method == &quot;POST&quot;:</span>
<span class="gi">+                self.headers.setdefault(</span>
<span class="gi">+                    b&quot;Content-Type&quot;, b&quot;application/x-www-form-urlencoded&quot;</span>
<span class="gi">+                )</span>
<span class="w"> </span>                self._set_body(form_query_str)
<span class="w"> </span>            else:
<span class="gd">-                self._set_url(urlunsplit(urlsplit(self.url)._replace(query=</span>
<span class="gd">-                    form_query_str)))</span>
<span class="gi">+                self._set_url(</span>
<span class="gi">+                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_response(</span>
<span class="gi">+        cls: Type[FormRequestTypeVar],</span>
<span class="gi">+        response: TextResponse,</span>
<span class="gi">+        formname: Optional[str] = None,</span>
<span class="gi">+        formid: Optional[str] = None,</span>
<span class="gi">+        formnumber: int = 0,</span>
<span class="gi">+        formdata: FormdataType = None,</span>
<span class="gi">+        clickdata: Optional[dict] = None,</span>
<span class="gi">+        dont_click: bool = False,</span>
<span class="gi">+        formxpath: Optional[str] = None,</span>
<span class="gi">+        formcss: Optional[str] = None,</span>
<span class="gi">+        **kwargs,</span>
<span class="gi">+    ) -&gt; FormRequestTypeVar:</span>
<span class="gi">+        kwargs.setdefault(&quot;encoding&quot;, response.encoding)</span>
<span class="gi">+</span>
<span class="gi">+        if formcss is not None:</span>
<span class="gi">+            from parsel.csstranslator import HTMLTranslator</span>
<span class="gi">+</span>
<span class="gi">+            formxpath = HTMLTranslator().css_to_xpath(formcss)</span>
<span class="gi">+</span>
<span class="gi">+        form = _get_form(response, formname, formid, formnumber, formxpath)</span>
<span class="gi">+        formdata = _get_inputs(form, formdata, dont_click, clickdata)</span>
<span class="gi">+        url = _get_form_url(form, kwargs.pop(&quot;url&quot;, None))</span>
<span class="gi">+</span>
<span class="gi">+        method = kwargs.pop(&quot;method&quot;, form.method)</span>
<span class="gi">+        if method is not None:</span>
<span class="gi">+            method = method.upper()</span>
<span class="gi">+            if method not in cls.valid_form_methods:</span>
<span class="gi">+                method = &quot;GET&quot;</span>
<span class="gi">+</span>
<span class="gi">+        return cls(url=url, method=method, formdata=formdata, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_form_url(form: FormElement, url: Optional[str]) -&gt; str:</span>
<span class="gi">+    assert form.base_url is not None  # typing</span>
<span class="gi">+    if url is None:</span>
<span class="gi">+        action = form.get(&quot;action&quot;)</span>
<span class="gi">+        if action is None:</span>
<span class="gi">+            return form.base_url</span>
<span class="gi">+        return urljoin(form.base_url, strip_html5_whitespace(action))</span>
<span class="gi">+    return urljoin(form.base_url, url)</span>
<span class="gi">+</span>

<span class="gi">+def _urlencode(seq: Iterable[FormdataKVType], enc: str) -&gt; str:</span>
<span class="gi">+    values = [</span>
<span class="gi">+        (to_bytes(k, enc), to_bytes(v, enc))</span>
<span class="gi">+        for k, vs in seq</span>
<span class="gi">+        for v in (cast(Iterable[str], vs) if is_listlike(vs) else [cast(str, vs)])</span>
<span class="gi">+    ]</span>
<span class="gi">+    return urlencode(values, doseq=True)</span>

<span class="gd">-def _get_form(response: TextResponse, formname: Optional[str], formid:</span>
<span class="gd">-    Optional[str], formnumber: int, formxpath: Optional[str]) -&gt;FormElement:</span>
<span class="gi">+</span>
<span class="gi">+def _get_form(</span>
<span class="gi">+    response: TextResponse,</span>
<span class="gi">+    formname: Optional[str],</span>
<span class="gi">+    formid: Optional[str],</span>
<span class="gi">+    formnumber: int,</span>
<span class="gi">+    formxpath: Optional[str],</span>
<span class="gi">+) -&gt; FormElement:</span>
<span class="w"> </span>    &quot;&quot;&quot;Find the wanted form element within the given response.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    root = response.selector.root</span>
<span class="gi">+    forms = root.xpath(&quot;//form&quot;)</span>
<span class="gi">+    if not forms:</span>
<span class="gi">+        raise ValueError(f&quot;No &lt;form&gt; element found in {response}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if formname is not None:</span>
<span class="gi">+        f = root.xpath(f&#39;//form[@name=&quot;{formname}&quot;]&#39;)</span>
<span class="gi">+        if f:</span>
<span class="gi">+            return f[0]</span>
<span class="gi">+</span>
<span class="gi">+    if formid is not None:</span>
<span class="gi">+        f = root.xpath(f&#39;//form[@id=&quot;{formid}&quot;]&#39;)</span>
<span class="gi">+        if f:</span>
<span class="gi">+            return f[0]</span>
<span class="gi">+</span>
<span class="gi">+    # Get form element from xpath, if not found, go up</span>
<span class="gi">+    if formxpath is not None:</span>
<span class="gi">+        nodes = root.xpath(formxpath)</span>
<span class="gi">+        if nodes:</span>
<span class="gi">+            el = nodes[0]</span>
<span class="gi">+            while True:</span>
<span class="gi">+                if el.tag == &quot;form&quot;:</span>
<span class="gi">+                    return el</span>
<span class="gi">+                el = el.getparent()</span>
<span class="gi">+                if el is None:</span>
<span class="gi">+                    break</span>
<span class="gi">+        raise ValueError(f&quot;No &lt;form&gt; element found with {formxpath}&quot;)</span>

<span class="gi">+    # If we get here, it means that either formname was None or invalid</span>
<span class="gi">+    try:</span>
<span class="gi">+        form = forms[formnumber]</span>
<span class="gi">+    except IndexError:</span>
<span class="gi">+        raise IndexError(f&quot;Form number {formnumber} not found in {response}&quot;)</span>
<span class="gi">+    else:</span>
<span class="gi">+        return form</span>

<span class="gd">-def _get_inputs(form: FormElement, formdata: FormdataType, dont_click: bool,</span>
<span class="gd">-    clickdata: Optional[dict]) -&gt;List[FormdataKVType]:</span>
<span class="gi">+</span>
<span class="gi">+def _get_inputs(</span>
<span class="gi">+    form: FormElement,</span>
<span class="gi">+    formdata: FormdataType,</span>
<span class="gi">+    dont_click: bool,</span>
<span class="gi">+    clickdata: Optional[dict],</span>
<span class="gi">+) -&gt; List[FormdataKVType]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a list of key-value pairs for the inputs found in the given form.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        formdata_keys = dict(formdata or ()).keys()</span>
<span class="gi">+    except (ValueError, TypeError):</span>
<span class="gi">+        raise ValueError(&quot;formdata should be a dict or iterable of tuples&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if not formdata:</span>
<span class="gi">+        formdata = []</span>
<span class="gi">+    inputs = form.xpath(</span>
<span class="gi">+        &quot;descendant::textarea&quot;</span>
<span class="gi">+        &quot;|descendant::select&quot;</span>
<span class="gi">+        &quot;|descendant::input[not(@type) or @type[&quot;</span>
<span class="gi">+        &#39; not(re:test(., &quot;^(?:submit|image|reset)$&quot;, &quot;i&quot;))&#39;</span>
<span class="gi">+        &quot; and (../@checked or&quot;</span>
<span class="gi">+        &#39;  not(re:test(., &quot;^(?:checkbox|radio)$&quot;, &quot;i&quot;)))]]&#39;,</span>
<span class="gi">+        namespaces={&quot;re&quot;: &quot;http://exslt.org/regular-expressions&quot;},</span>
<span class="gi">+    )</span>
<span class="gi">+    values: List[FormdataKVType] = [</span>
<span class="gi">+        (k, &quot;&quot; if v is None else v)</span>
<span class="gi">+        for k, v in (_value(e) for e in inputs)</span>
<span class="gi">+        if k and k not in formdata_keys</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    if not dont_click:</span>
<span class="gi">+        clickable = _get_clickable(clickdata, form)</span>
<span class="gi">+        if clickable and clickable[0] not in formdata and not clickable[0] is None:</span>
<span class="gi">+            values.append(clickable)</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(formdata, dict):</span>
<span class="gi">+        formdata = formdata.items()  # type: ignore[assignment]</span>

<span class="gi">+    values.extend((k, v) for k, v in formdata if v is not None)</span>
<span class="gi">+    return values</span>

<span class="gd">-def _get_clickable(clickdata: Optional[dict], form: FormElement) -&gt;Optional[</span>
<span class="gd">-    Tuple[str, str]]:</span>
<span class="gi">+</span>
<span class="gi">+def _value(</span>
<span class="gi">+    ele: Union[InputElement, SelectElement, TextareaElement]</span>
<span class="gi">+) -&gt; Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:</span>
<span class="gi">+    n = ele.name</span>
<span class="gi">+    v = ele.value</span>
<span class="gi">+    if ele.tag == &quot;select&quot;:</span>
<span class="gi">+        return _select_value(cast(SelectElement, ele), n, v)</span>
<span class="gi">+    return n, v</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _select_value(</span>
<span class="gi">+    ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]</span>
<span class="gi">+) -&gt; Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:</span>
<span class="gi">+    multiple = ele.multiple</span>
<span class="gi">+    if v is None and not multiple:</span>
<span class="gi">+        # Match browser behaviour on simple select tag without options selected</span>
<span class="gi">+        # And for select tags without options</span>
<span class="gi">+        o = ele.value_options</span>
<span class="gi">+        return (n, o[0]) if o else (None, None)</span>
<span class="gi">+    return n, v</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_clickable(</span>
<span class="gi">+    clickdata: Optional[dict], form: FormElement</span>
<span class="gi">+) -&gt; Optional[Tuple[str, str]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Returns the clickable element specified in clickdata,
<span class="w"> </span>    if the latter is given. If not, it returns the first
<span class="w"> </span>    clickable element found
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    clickables = list(</span>
<span class="gi">+        form.xpath(</span>
<span class="gi">+            &#39;descendant::input[re:test(@type, &quot;^(submit|image)$&quot;, &quot;i&quot;)]&#39;</span>
<span class="gi">+            &#39;|descendant::button[not(@type) or re:test(@type, &quot;^submit$&quot;, &quot;i&quot;)]&#39;,</span>
<span class="gi">+            namespaces={&quot;re&quot;: &quot;http://exslt.org/regular-expressions&quot;},</span>
<span class="gi">+        )</span>
<span class="gi">+    )</span>
<span class="gi">+    if not clickables:</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    # If we don&#39;t have clickdata, we just use the first clickable element</span>
<span class="gi">+    if clickdata is None:</span>
<span class="gi">+        el = clickables[0]</span>
<span class="gi">+        return (el.get(&quot;name&quot;), el.get(&quot;value&quot;) or &quot;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # If clickdata is given, we compare it to the clickable elements to find a</span>
<span class="gi">+    # match. We first look to see if the number is specified in clickdata,</span>
<span class="gi">+    # because that uniquely identifies the element</span>
<span class="gi">+    nr = clickdata.get(&quot;nr&quot;, None)</span>
<span class="gi">+    if nr is not None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            el = list(form.inputs)[nr]</span>
<span class="gi">+        except IndexError:</span>
<span class="gi">+            pass</span>
<span class="gi">+        else:</span>
<span class="gi">+            return (el.get(&quot;name&quot;), el.get(&quot;value&quot;) or &quot;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # We didn&#39;t find it, so now we build an XPath expression out of the other</span>
<span class="gi">+    # arguments, because they can be used as such</span>
<span class="gi">+    xpath = &quot;.//*&quot; + &quot;&quot;.join(f&#39;[@{k}=&quot;{v}&quot;]&#39; for k, v in clickdata.items())</span>
<span class="gi">+    el = form.xpath(xpath)</span>
<span class="gi">+    if len(el) == 1:</span>
<span class="gi">+        return (el[0].get(&quot;name&quot;), el[0].get(&quot;value&quot;) or &quot;&quot;)</span>
<span class="gi">+    if len(el) &gt; 1:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;Multiple elements found ({el!r}) matching the &quot;</span>
<span class="gi">+            f&quot;criteria in clickdata: {clickdata!r}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(f&quot;No clickable element matching clickdata: {clickdata!r}&quot;)</span>
<span class="gh">diff --git a/scrapy/http/request/json_request.py b/scrapy/http/request/json_request.py</span>
<span class="gh">index 7a8404484..510c903db 100644</span>
<span class="gd">--- a/scrapy/http/request/json_request.py</span>
<span class="gi">+++ b/scrapy/http/request/json_request.py</span>
<span class="gu">@@ -4,40 +4,60 @@ This module implements the JsonRequest class which is a more convenient class</span>

<span class="w"> </span>See documentation in docs/topics/request-response.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import copy
<span class="w"> </span>import json
<span class="w"> </span>import warnings
<span class="w"> </span>from typing import Optional, Tuple
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.utils.deprecate import create_deprecated_class


<span class="w"> </span>class JsonRequest(Request):
<span class="gd">-    attributes: Tuple[str, ...] = Request.attributes + (&#39;dumps_kwargs&#39;,)</span>
<span class="gi">+    attributes: Tuple[str, ...] = Request.attributes + (&quot;dumps_kwargs&quot;,)</span>

<span class="gd">-    def __init__(self, *args, dumps_kwargs: Optional[dict]=None, **kwargs</span>
<span class="gd">-        ) -&gt;None:</span>
<span class="gd">-        dumps_kwargs = copy.deepcopy(dumps_kwargs</span>
<span class="gd">-            ) if dumps_kwargs is not None else {}</span>
<span class="gd">-        dumps_kwargs.setdefault(&#39;sort_keys&#39;, True)</span>
<span class="gi">+    def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -&gt; None:</span>
<span class="gi">+        dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}</span>
<span class="gi">+        dumps_kwargs.setdefault(&quot;sort_keys&quot;, True)</span>
<span class="w"> </span>        self._dumps_kwargs = dumps_kwargs
<span class="gd">-        body_passed = kwargs.get(&#39;body&#39;, None) is not None</span>
<span class="gd">-        data = kwargs.pop(&#39;data&#39;, None)</span>
<span class="gi">+</span>
<span class="gi">+        body_passed = kwargs.get(&quot;body&quot;, None) is not None</span>
<span class="gi">+        data = kwargs.pop(&quot;data&quot;, None)</span>
<span class="w"> </span>        data_passed = data is not None
<span class="gi">+</span>
<span class="w"> </span>        if body_passed and data_passed:
<span class="gd">-            warnings.warn(&#39;Both body and data passed. data will be ignored&#39;)</span>
<span class="gi">+            warnings.warn(&quot;Both body and data passed. data will be ignored&quot;)</span>
<span class="w"> </span>        elif not body_passed and data_passed:
<span class="gd">-            kwargs[&#39;body&#39;] = self._dumps(data)</span>
<span class="gd">-            if &#39;method&#39; not in kwargs:</span>
<span class="gd">-                kwargs[&#39;method&#39;] = &#39;POST&#39;</span>
<span class="gi">+            kwargs[&quot;body&quot;] = self._dumps(data)</span>
<span class="gi">+            if &quot;method&quot; not in kwargs:</span>
<span class="gi">+                kwargs[&quot;method&quot;] = &quot;POST&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="gd">-        self.headers.setdefault(&#39;Content-Type&#39;, &#39;application/json&#39;)</span>
<span class="gd">-        self.headers.setdefault(&#39;Accept&#39;,</span>
<span class="gd">-            &#39;application/json, text/javascript, */*; q=0.01&#39;)</span>
<span class="gi">+        self.headers.setdefault(&quot;Content-Type&quot;, &quot;application/json&quot;)</span>
<span class="gi">+        self.headers.setdefault(</span>
<span class="gi">+            &quot;Accept&quot;, &quot;application/json, text/javascript, */*; q=0.01&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def dumps_kwargs(self) -&gt; dict:</span>
<span class="gi">+        return self._dumps_kwargs</span>
<span class="gi">+</span>
<span class="gi">+    def replace(self, *args, **kwargs) -&gt; Request:</span>
<span class="gi">+        body_passed = kwargs.get(&quot;body&quot;, None) is not None</span>
<span class="gi">+        data = kwargs.pop(&quot;data&quot;, None)</span>
<span class="gi">+        data_passed = data is not None</span>
<span class="gi">+</span>
<span class="gi">+        if body_passed and data_passed:</span>
<span class="gi">+            warnings.warn(&quot;Both body and data passed. data will be ignored&quot;)</span>
<span class="gi">+        elif not body_passed and data_passed:</span>
<span class="gi">+            kwargs[&quot;body&quot;] = self._dumps(data)</span>
<span class="gi">+</span>
<span class="gi">+        return super().replace(*args, **kwargs)</span>

<span class="gd">-    def _dumps(self, data: dict) -&gt;str:</span>
<span class="gi">+    def _dumps(self, data: dict) -&gt; str:</span>
<span class="w"> </span>        &quot;&quot;&quot;Convert to JSON&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return json.dumps(data, **self._dumps_kwargs)</span>


<span class="gd">-JSONRequest = create_deprecated_class(&#39;JSONRequest&#39;, JsonRequest)</span>
<span class="gi">+JSONRequest = create_deprecated_class(&quot;JSONRequest&quot;, JsonRequest)</span>
<span class="gh">diff --git a/scrapy/http/request/rpc.py b/scrapy/http/request/rpc.py</span>
<span class="gh">index 2ed828664..59767de7a 100644</span>
<span class="gd">--- a/scrapy/http/request/rpc.py</span>
<span class="gi">+++ b/scrapy/http/request/rpc.py</span>
<span class="gu">@@ -6,22 +6,32 @@ See documentation in docs/topics/request-response.rst</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import xmlrpc.client as xmlrpclib
<span class="w"> </span>from typing import Optional
<span class="gi">+</span>
<span class="w"> </span>import defusedxml.xmlrpc
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.request import Request
<span class="w"> </span>from scrapy.utils.python import get_func_args
<span class="gi">+</span>
<span class="w"> </span>defusedxml.xmlrpc.monkey_patch()
<span class="gi">+</span>
<span class="w"> </span>DUMPS_ARGS = get_func_args(xmlrpclib.dumps)


<span class="w"> </span>class XmlRpcRequest(Request):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *args, encoding: Optional[str]=None, **kwargs):</span>
<span class="gd">-        if &#39;body&#39; not in kwargs and &#39;params&#39; in kwargs:</span>
<span class="gi">+    def __init__(self, *args, encoding: Optional[str] = None, **kwargs):</span>
<span class="gi">+        if &quot;body&quot; not in kwargs and &quot;params&quot; in kwargs:</span>
<span class="w"> </span>            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)
<span class="gd">-            kwargs[&#39;body&#39;] = xmlrpclib.dumps(**kw)</span>
<span class="gd">-        kwargs.setdefault(&#39;method&#39;, &#39;POST&#39;)</span>
<span class="gd">-        kwargs.setdefault(&#39;dont_filter&#39;, True)</span>
<span class="gi">+            kwargs[&quot;body&quot;] = xmlrpclib.dumps(**kw)</span>
<span class="gi">+</span>
<span class="gi">+        # spec defines that requests must use POST method</span>
<span class="gi">+        kwargs.setdefault(&quot;method&quot;, &quot;POST&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # xmlrpc query multiples times over the same url</span>
<span class="gi">+        kwargs.setdefault(&quot;dont_filter&quot;, True)</span>
<span class="gi">+</span>
<span class="gi">+        # restore encoding</span>
<span class="w"> </span>        if encoding is not None:
<span class="gd">-            kwargs[&#39;encoding&#39;] = encoding</span>
<span class="gi">+            kwargs[&quot;encoding&quot;] = encoding</span>
<span class="gi">+</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="gd">-        self.headers.setdefault(&#39;Content-Type&#39;, &#39;text/xml&#39;)</span>
<span class="gi">+        self.headers.setdefault(&quot;Content-Type&quot;, &quot;text/xml&quot;)</span>
<span class="gh">diff --git a/scrapy/http/response/html.py b/scrapy/http/response/html.py</span>
<span class="gh">index d55895aa3..7eed052c2 100644</span>
<span class="gd">--- a/scrapy/http/response/html.py</span>
<span class="gi">+++ b/scrapy/http/response/html.py</span>
<span class="gu">@@ -4,6 +4,7 @@ discovering through HTML encoding declarations to the TextResponse class.</span>

<span class="w"> </span>See documentation in docs/topics/request-response.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.response.text import TextResponse


<span class="gh">diff --git a/scrapy/http/response/text.py b/scrapy/http/response/text.py</span>
<span class="gh">index 4b8b63972..47d7bc10f 100644</span>
<span class="gd">--- a/scrapy/http/response/text.py</span>
<span class="gi">+++ b/scrapy/http/response/text.py</span>
<span class="gu">@@ -5,55 +5,176 @@ discovering (through HTTP headers) to base Response class.</span>
<span class="w"> </span>See documentation in docs/topics/request-response.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import json
<span class="w"> </span>from contextlib import suppress
<span class="w"> </span>from typing import TYPE_CHECKING, Any, Generator, Optional, Tuple
<span class="w"> </span>from urllib.parse import urljoin
<span class="gi">+</span>
<span class="w"> </span>import parsel
<span class="gd">-from w3lib.encoding import html_body_declared_encoding, html_to_unicode, http_content_type_encoding, read_bom, resolve_encoding</span>
<span class="gi">+from w3lib.encoding import (</span>
<span class="gi">+    html_body_declared_encoding,</span>
<span class="gi">+    html_to_unicode,</span>
<span class="gi">+    http_content_type_encoding,</span>
<span class="gi">+    read_bom,</span>
<span class="gi">+    resolve_encoding,</span>
<span class="gi">+)</span>
<span class="w"> </span>from w3lib.html import strip_html5_whitespace
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.http.response import Response
<span class="w"> </span>from scrapy.utils.python import memoizemethod_noargs, to_unicode
<span class="w"> </span>from scrapy.utils.response import get_base_url
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.selector import Selector
<span class="gi">+</span>
<span class="w"> </span>_NONE = object()


<span class="w"> </span>class TextResponse(Response):
<span class="gd">-    _DEFAULT_ENCODING = &#39;ascii&#39;</span>
<span class="gi">+    _DEFAULT_ENCODING = &quot;ascii&quot;</span>
<span class="w"> </span>    _cached_decoded_json = _NONE
<span class="gd">-    attributes: Tuple[str, ...] = Response.attributes + (&#39;encoding&#39;,)</span>
<span class="gi">+</span>
<span class="gi">+    attributes: Tuple[str, ...] = Response.attributes + (&quot;encoding&quot;,)</span>

<span class="w"> </span>    def __init__(self, *args: Any, **kwargs: Any):
<span class="gd">-        self._encoding = kwargs.pop(&#39;encoding&#39;, None)</span>
<span class="gi">+        self._encoding = kwargs.pop(&quot;encoding&quot;, None)</span>
<span class="w"> </span>        self._cached_benc: Optional[str] = None
<span class="w"> </span>        self._cached_ubody: Optional[str] = None
<span class="w"> </span>        self._cached_selector: Optional[Selector] = None
<span class="w"> </span>        super().__init__(*args, **kwargs)

<span class="gi">+    def _set_url(self, url):</span>
<span class="gi">+        if isinstance(url, str):</span>
<span class="gi">+            self._url = to_unicode(url, self.encoding)</span>
<span class="gi">+        else:</span>
<span class="gi">+            super()._set_url(url)</span>
<span class="gi">+</span>
<span class="gi">+    def _set_body(self, body):</span>
<span class="gi">+        self._body = b&quot;&quot;  # used by encoding detection</span>
<span class="gi">+        if isinstance(body, str):</span>
<span class="gi">+            if self._encoding is None:</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    &quot;Cannot convert unicode body - &quot;</span>
<span class="gi">+                    f&quot;{type(self).__name__} has no encoding&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            self._body = body.encode(self._encoding)</span>
<span class="gi">+        else:</span>
<span class="gi">+            super()._set_body(body)</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def encoding(self):</span>
<span class="gi">+        return self._declared_encoding() or self._body_inferred_encoding()</span>
<span class="gi">+</span>
<span class="gi">+    def _declared_encoding(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._encoding</span>
<span class="gi">+            or self._bom_encoding()</span>
<span class="gi">+            or self._headers_encoding()</span>
<span class="gi">+            or self._body_declared_encoding()</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>    def json(self):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        .. versionadded:: 2.2

<span class="w"> </span>        Deserialize a JSON document to a Python object.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self._cached_decoded_json is _NONE:</span>
<span class="gi">+            self._cached_decoded_json = json.loads(self.body)</span>
<span class="gi">+        return self._cached_decoded_json</span>

<span class="w"> </span>    @property
<span class="gd">-    def text(self) -&gt;str:</span>
<span class="gi">+    def text(self) -&gt; str:</span>
<span class="w"> </span>        &quot;&quot;&quot;Body as unicode&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # access self.encoding before _cached_ubody to make sure</span>
<span class="gi">+        # _body_inferred_encoding is called</span>
<span class="gi">+        benc = self.encoding</span>
<span class="gi">+        if self._cached_ubody is None:</span>
<span class="gi">+            charset = f&quot;charset={benc}&quot;</span>
<span class="gi">+            self._cached_ubody = html_to_unicode(charset, self.body)[1]</span>
<span class="gi">+        return self._cached_ubody</span>

<span class="w"> </span>    def urljoin(self, url):
<span class="w"> </span>        &quot;&quot;&quot;Join this Response&#39;s url with a possible relative url to form an
<span class="w"> </span>        absolute interpretation of the latter.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return urljoin(get_base_url(self), url)</span>
<span class="gi">+</span>
<span class="gi">+    @memoizemethod_noargs</span>
<span class="gi">+    def _headers_encoding(self):</span>
<span class="gi">+        content_type = self.headers.get(b&quot;Content-Type&quot;, b&quot;&quot;)</span>
<span class="gi">+        return http_content_type_encoding(to_unicode(content_type, encoding=&quot;latin-1&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def _body_inferred_encoding(self):</span>
<span class="gi">+        if self._cached_benc is None:</span>
<span class="gi">+            content_type = to_unicode(</span>
<span class="gi">+                self.headers.get(b&quot;Content-Type&quot;, b&quot;&quot;), encoding=&quot;latin-1&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            benc, ubody = html_to_unicode(</span>
<span class="gi">+                content_type,</span>
<span class="gi">+                self.body,</span>
<span class="gi">+                auto_detect_fun=self._auto_detect_fun,</span>
<span class="gi">+                default_encoding=self._DEFAULT_ENCODING,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._cached_benc = benc</span>
<span class="gi">+            self._cached_ubody = ubody</span>
<span class="gi">+        return self._cached_benc</span>

<span class="gd">-    def follow(self, url, callback=None, method=&#39;GET&#39;, headers=None, body=</span>
<span class="gd">-        None, cookies=None, meta=None, encoding=None, priority=0,</span>
<span class="gd">-        dont_filter=False, errback=None, cb_kwargs=None, flags=None) -&gt;Request:</span>
<span class="gi">+    def _auto_detect_fun(self, text):</span>
<span class="gi">+        for enc in (self._DEFAULT_ENCODING, &quot;utf-8&quot;, &quot;cp1252&quot;):</span>
<span class="gi">+            try:</span>
<span class="gi">+                text.decode(enc)</span>
<span class="gi">+            except UnicodeError:</span>
<span class="gi">+                continue</span>
<span class="gi">+            return resolve_encoding(enc)</span>
<span class="gi">+</span>
<span class="gi">+    @memoizemethod_noargs</span>
<span class="gi">+    def _body_declared_encoding(self):</span>
<span class="gi">+        return html_body_declared_encoding(self.body)</span>
<span class="gi">+</span>
<span class="gi">+    @memoizemethod_noargs</span>
<span class="gi">+    def _bom_encoding(self):</span>
<span class="gi">+        return read_bom(self.body)[0]</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def selector(self):</span>
<span class="gi">+        from scrapy.selector import Selector</span>
<span class="gi">+</span>
<span class="gi">+        if self._cached_selector is None:</span>
<span class="gi">+            self._cached_selector = Selector(self)</span>
<span class="gi">+        return self._cached_selector</span>
<span class="gi">+</span>
<span class="gi">+    def jmespath(self, query, **kwargs):</span>
<span class="gi">+        if not hasattr(self.selector, &quot;jmespath&quot;):  # type: ignore[attr-defined]</span>
<span class="gi">+            raise AttributeError(</span>
<span class="gi">+                &quot;Please install parsel &gt;= 1.8.1 to get jmespath support&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        return self.selector.jmespath(query, **kwargs)  # type: ignore[attr-defined]</span>
<span class="gi">+</span>
<span class="gi">+    def xpath(self, query, **kwargs):</span>
<span class="gi">+        return self.selector.xpath(query, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def css(self, query):</span>
<span class="gi">+        return self.selector.css(query)</span>
<span class="gi">+</span>
<span class="gi">+    def follow(</span>
<span class="gi">+        self,</span>
<span class="gi">+        url,</span>
<span class="gi">+        callback=None,</span>
<span class="gi">+        method=&quot;GET&quot;,</span>
<span class="gi">+        headers=None,</span>
<span class="gi">+        body=None,</span>
<span class="gi">+        cookies=None,</span>
<span class="gi">+        meta=None,</span>
<span class="gi">+        encoding=None,</span>
<span class="gi">+        priority=0,</span>
<span class="gi">+        dont_filter=False,</span>
<span class="gi">+        errback=None,</span>
<span class="gi">+        cb_kwargs=None,</span>
<span class="gi">+        flags=None,</span>
<span class="gi">+    ) -&gt; Request:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return a :class:`~.Request` instance to follow a link ``url``.
<span class="w"> </span>        It accepts the same arguments as ``Request.__init__`` method,
<span class="gu">@@ -70,12 +191,45 @@ class TextResponse(Response):</span>

<span class="w"> </span>        See :ref:`response-follow-example` for usage examples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(url, parsel.Selector):</span>
<span class="gi">+            url = _url_from_selector(url)</span>
<span class="gi">+        elif isinstance(url, parsel.SelectorList):</span>
<span class="gi">+            raise ValueError(&quot;SelectorList is not supported&quot;)</span>
<span class="gi">+        encoding = self.encoding if encoding is None else encoding</span>
<span class="gi">+        return super().follow(</span>
<span class="gi">+            url=url,</span>
<span class="gi">+            callback=callback,</span>
<span class="gi">+            method=method,</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            body=body,</span>
<span class="gi">+            cookies=cookies,</span>
<span class="gi">+            meta=meta,</span>
<span class="gi">+            encoding=encoding,</span>
<span class="gi">+            priority=priority,</span>
<span class="gi">+            dont_filter=dont_filter,</span>
<span class="gi">+            errback=errback,</span>
<span class="gi">+            cb_kwargs=cb_kwargs,</span>
<span class="gi">+            flags=flags,</span>
<span class="gi">+        )</span>

<span class="gd">-    def follow_all(self, urls=None, callback=None, method=&#39;GET&#39;, headers=</span>
<span class="gd">-        None, body=None, cookies=None, meta=None, encoding=None, priority=0,</span>
<span class="gd">-        dont_filter=False, errback=None, cb_kwargs=None, flags=None, css=</span>
<span class="gd">-        None, xpath=None) -&gt;Generator[Request, None, None]:</span>
<span class="gi">+    def follow_all(</span>
<span class="gi">+        self,</span>
<span class="gi">+        urls=None,</span>
<span class="gi">+        callback=None,</span>
<span class="gi">+        method=&quot;GET&quot;,</span>
<span class="gi">+        headers=None,</span>
<span class="gi">+        body=None,</span>
<span class="gi">+        cookies=None,</span>
<span class="gi">+        meta=None,</span>
<span class="gi">+        encoding=None,</span>
<span class="gi">+        priority=0,</span>
<span class="gi">+        dont_filter=False,</span>
<span class="gi">+        errback=None,</span>
<span class="gi">+        cb_kwargs=None,</span>
<span class="gi">+        flags=None,</span>
<span class="gi">+        css=None,</span>
<span class="gi">+        xpath=None,</span>
<span class="gi">+    ) -&gt; Generator[Request, None, None]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        A generator that produces :class:`~.Request` instances to follow all
<span class="w"> </span>        links in ``urls``. It accepts the same arguments as the :class:`~.Request`&#39;s
<span class="gu">@@ -99,10 +253,57 @@ class TextResponse(Response):</span>
<span class="w"> </span>        selectors from which links cannot be obtained (for instance, anchor tags without an
<span class="w"> </span>        ``href`` attribute)
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        arguments = [x for x in (urls, css, xpath) if x is not None]</span>
<span class="gi">+        if len(arguments) != 1:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Please supply exactly one of the following arguments: urls, css, xpath&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        if not urls:</span>
<span class="gi">+            if css:</span>
<span class="gi">+                urls = self.css(css)</span>
<span class="gi">+            if xpath:</span>
<span class="gi">+                urls = self.xpath(xpath)</span>
<span class="gi">+        if isinstance(urls, parsel.SelectorList):</span>
<span class="gi">+            selectors = urls</span>
<span class="gi">+            urls = []</span>
<span class="gi">+            for sel in selectors:</span>
<span class="gi">+                with suppress(_InvalidSelector):</span>
<span class="gi">+                    urls.append(_url_from_selector(sel))</span>
<span class="gi">+        return super().follow_all(</span>
<span class="gi">+            urls=urls,</span>
<span class="gi">+            callback=callback,</span>
<span class="gi">+            method=method,</span>
<span class="gi">+            headers=headers,</span>
<span class="gi">+            body=body,</span>
<span class="gi">+            cookies=cookies,</span>
<span class="gi">+            meta=meta,</span>
<span class="gi">+            encoding=encoding,</span>
<span class="gi">+            priority=priority,</span>
<span class="gi">+            dont_filter=dont_filter,</span>
<span class="gi">+            errback=errback,</span>
<span class="gi">+            cb_kwargs=cb_kwargs,</span>
<span class="gi">+            flags=flags,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class _InvalidSelector(ValueError):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Raised when a URL cannot be obtained from a Selector
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _url_from_selector(sel):</span>
<span class="gi">+    # type: (parsel.Selector) -&gt; str</span>
<span class="gi">+    if isinstance(sel.root, str):</span>
<span class="gi">+        # e.g. ::attr(href) result</span>
<span class="gi">+        return strip_html5_whitespace(sel.root)</span>
<span class="gi">+    if not hasattr(sel.root, &quot;tag&quot;):</span>
<span class="gi">+        raise _InvalidSelector(f&quot;Unsupported selector: {sel}&quot;)</span>
<span class="gi">+    if sel.root.tag not in (&quot;a&quot;, &quot;link&quot;):</span>
<span class="gi">+        raise _InvalidSelector(</span>
<span class="gi">+            &quot;Only &lt;a&gt; and &lt;link&gt; elements are supported; &quot; f&quot;got &lt;{sel.root.tag}&gt;&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    href = sel.root.get(&quot;href&quot;)</span>
<span class="gi">+    if href is None:</span>
<span class="gi">+        raise _InvalidSelector(f&quot;&lt;{sel.root.tag}&gt; element has no href attribute: {sel}&quot;)</span>
<span class="gi">+    return strip_html5_whitespace(href)</span>
<span class="gh">diff --git a/scrapy/http/response/xml.py b/scrapy/http/response/xml.py</span>
<span class="gh">index 9ff465ec5..abf474a2f 100644</span>
<span class="gd">--- a/scrapy/http/response/xml.py</span>
<span class="gi">+++ b/scrapy/http/response/xml.py</span>
<span class="gu">@@ -4,6 +4,7 @@ discovering through XML encoding declarations to the TextResponse class.</span>

<span class="w"> </span>See documentation in docs/topics/request-response.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.response.text import TextResponse


<span class="gh">diff --git a/scrapy/interfaces.py b/scrapy/interfaces.py</span>
<span class="gh">index 151522c8c..9a2c5f170 100644</span>
<span class="gd">--- a/scrapy/interfaces.py</span>
<span class="gi">+++ b/scrapy/interfaces.py</span>
<span class="gu">@@ -2,21 +2,16 @@ from zope.interface import Interface</span>


<span class="w"> </span>class ISpiderLoader(Interface):
<span class="gd">-</span>
<span class="w"> </span>    def from_settings(settings):
<span class="w"> </span>        &quot;&quot;&quot;Return an instance of the class for the given settings&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def load(spider_name):
<span class="w"> </span>        &quot;&quot;&quot;Return the Spider class for the given spider name. If the spider
<span class="w"> </span>        name is not found, it must raise a KeyError.&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def list():
<span class="w"> </span>        &quot;&quot;&quot;Return a list with the names of all spiders available in the
<span class="w"> </span>        project&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def find_by_request(request):
<span class="w"> </span>        &quot;&quot;&quot;Return the list of spiders names that can handle the given request&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gh">diff --git a/scrapy/item.py b/scrapy/item.py</span>
<span class="gh">index aae295969..d3eb90b7b 100644</span>
<span class="gd">--- a/scrapy/item.py</span>
<span class="gi">+++ b/scrapy/item.py</span>
<span class="gu">@@ -3,11 +3,13 @@ Scrapy Item</span>

<span class="w"> </span>See documentation in docs/topics/item.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from abc import ABCMeta
<span class="w"> </span>from collections.abc import MutableMapping
<span class="w"> </span>from copy import deepcopy
<span class="w"> </span>from pprint import pformat
<span class="w"> </span>from typing import Dict
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.trackref import object_ref


<span class="gu">@@ -22,11 +24,11 @@ class ItemMeta(ABCMeta):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __new__(mcs, class_name, bases, attrs):
<span class="gd">-        classcell = attrs.pop(&#39;__classcell__&#39;, None)</span>
<span class="gd">-        new_bases = tuple(base._class for base in bases if hasattr(base,</span>
<span class="gd">-            &#39;_class&#39;))</span>
<span class="gd">-        _class = super().__new__(mcs, &#39;x_&#39; + class_name, new_bases, attrs)</span>
<span class="gd">-        fields = getattr(_class, &#39;fields&#39;, {})</span>
<span class="gi">+        classcell = attrs.pop(&quot;__classcell__&quot;, None)</span>
<span class="gi">+        new_bases = tuple(base._class for base in bases if hasattr(base, &quot;_class&quot;))</span>
<span class="gi">+        _class = super().__new__(mcs, &quot;x_&quot; + class_name, new_bases, attrs)</span>
<span class="gi">+</span>
<span class="gi">+        fields = getattr(_class, &quot;fields&quot;, {})</span>
<span class="w"> </span>        new_attrs = {}
<span class="w"> </span>        for n in dir(_class):
<span class="w"> </span>            v = getattr(_class, n)
<span class="gu">@@ -34,10 +36,11 @@ class ItemMeta(ABCMeta):</span>
<span class="w"> </span>                fields[n] = v
<span class="w"> </span>            elif n in attrs:
<span class="w"> </span>                new_attrs[n] = attrs[n]
<span class="gd">-        new_attrs[&#39;fields&#39;] = fields</span>
<span class="gd">-        new_attrs[&#39;_class&#39;] = _class</span>
<span class="gi">+</span>
<span class="gi">+        new_attrs[&quot;fields&quot;] = fields</span>
<span class="gi">+        new_attrs[&quot;_class&quot;] = _class</span>
<span class="w"> </span>        if classcell is not None:
<span class="gd">-            new_attrs[&#39;__classcell__&#39;] = classcell</span>
<span class="gi">+            new_attrs[&quot;__classcell__&quot;] = classcell</span>
<span class="w"> </span>        return super().__new__(mcs, class_name, bases, new_attrs)


<span class="gu">@@ -63,11 +66,12 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):</span>
<span class="w"> </span>    Unlike instances of :class:`dict`, instances of :class:`Item` may be
<span class="w"> </span>    :ref:`tracked &lt;topics-leaks-trackrefs&gt;` to debug memory leaks.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    fields: Dict[str, Field]

<span class="w"> </span>    def __init__(self, *args, **kwargs):
<span class="w"> </span>        self._values = {}
<span class="gd">-        if args or kwargs:</span>
<span class="gi">+        if args or kwargs:  # avoid creating dict for most common case</span>
<span class="w"> </span>            for k, v in dict(*args, **kwargs).items():
<span class="w"> </span>                self[k] = v

<span class="gu">@@ -78,21 +82,19 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):</span>
<span class="w"> </span>        if key in self.fields:
<span class="w"> </span>            self._values[key] = value
<span class="w"> </span>        else:
<span class="gd">-            raise KeyError(</span>
<span class="gd">-                f&#39;{self.__class__.__name__} does not support field: {key}&#39;)</span>
<span class="gi">+            raise KeyError(f&quot;{self.__class__.__name__} does not support field: {key}&quot;)</span>

<span class="w"> </span>    def __delitem__(self, key):
<span class="w"> </span>        del self._values[key]

<span class="w"> </span>    def __getattr__(self, name):
<span class="w"> </span>        if name in self.fields:
<span class="gd">-            raise AttributeError(f&#39;Use item[{name!r}] to get field value&#39;)</span>
<span class="gi">+            raise AttributeError(f&quot;Use item[{name!r}] to get field value&quot;)</span>
<span class="w"> </span>        raise AttributeError(name)

<span class="w"> </span>    def __setattr__(self, name, value):
<span class="gd">-        if not name.startswith(&#39;_&#39;):</span>
<span class="gd">-            raise AttributeError(</span>
<span class="gd">-                f&#39;Use item[{name!r}] = {value!r} to set field value&#39;)</span>
<span class="gi">+        if not name.startswith(&quot;_&quot;):</span>
<span class="gi">+            raise AttributeError(f&quot;Use item[{name!r}] = {value!r} to set field value&quot;)</span>
<span class="w"> </span>        super().__setattr__(name, value)

<span class="w"> </span>    def __len__(self):
<span class="gu">@@ -100,11 +102,18 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):</span>

<span class="w"> </span>    def __iter__(self):
<span class="w"> </span>        return iter(self._values)
<span class="gi">+</span>
<span class="w"> </span>    __hash__ = object_ref.__hash__

<span class="gi">+    def keys(self):</span>
<span class="gi">+        return self._values.keys()</span>
<span class="gi">+</span>
<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        return pformat(dict(self))

<span class="gi">+    def copy(self):</span>
<span class="gi">+        return self.__class__(self)</span>
<span class="gi">+</span>
<span class="w"> </span>    def deepcopy(self):
<span class="w"> </span>        &quot;&quot;&quot;Return a :func:`~copy.deepcopy` of this item.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return deepcopy(self)</span>
<span class="gh">diff --git a/scrapy/link.py b/scrapy/link.py</span>
<span class="gh">index 2bc6f207e..0868ae5ef 100644</span>
<span class="gd">--- a/scrapy/link.py</span>
<span class="gi">+++ b/scrapy/link.py</span>
<span class="gu">@@ -24,29 +24,37 @@ class Link:</span>
<span class="w"> </span>    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute
<span class="w"> </span>                    of the anchor tag.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    __slots__ = [&#39;url&#39;, &#39;text&#39;, &#39;fragment&#39;, &#39;nofollow&#39;]</span>

<span class="gd">-    def __init__(self, url: str, text: str=&#39;&#39;, fragment: str=&#39;&#39;, nofollow:</span>
<span class="gd">-        bool=False):</span>
<span class="gi">+    __slots__ = [&quot;url&quot;, &quot;text&quot;, &quot;fragment&quot;, &quot;nofollow&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, url: str, text: str = &quot;&quot;, fragment: str = &quot;&quot;, nofollow: bool = False</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if not isinstance(url, str):
<span class="w"> </span>            got = url.__class__.__name__
<span class="gd">-            raise TypeError(f&#39;Link urls must be str objects, got {got}&#39;)</span>
<span class="gi">+            raise TypeError(f&quot;Link urls must be str objects, got {got}&quot;)</span>
<span class="w"> </span>        self.url: str = url
<span class="w"> </span>        self.text: str = text
<span class="w"> </span>        self.fragment: str = fragment
<span class="w"> </span>        self.nofollow: bool = nofollow

<span class="gd">-    def __eq__(self, other: Any) -&gt;bool:</span>
<span class="gi">+    def __eq__(self, other: Any) -&gt; bool:</span>
<span class="w"> </span>        if not isinstance(other, Link):
<span class="w"> </span>            raise NotImplementedError
<span class="gd">-        return (self.url == other.url and self.text == other.text and self.</span>
<span class="gd">-            fragment == other.fragment and self.nofollow == other.nofollow)</span>
<span class="gi">+        return (</span>
<span class="gi">+            self.url == other.url</span>
<span class="gi">+            and self.text == other.text</span>
<span class="gi">+            and self.fragment == other.fragment</span>
<span class="gi">+            and self.nofollow == other.nofollow</span>
<span class="gi">+        )</span>

<span class="gd">-    def __hash__(self) -&gt;int:</span>
<span class="gd">-        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(</span>
<span class="gd">-            self.nofollow)</span>
<span class="gi">+    def __hash__(self) -&gt; int:</span>
<span class="gi">+        return (</span>
<span class="gi">+            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)</span>
<span class="gi">+        )</span>

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="w"> </span>        return (
<span class="gd">-            f&#39;Link(url={self.url!r}, text={self.text!r}, fragment={self.fragment!r}, nofollow={self.nofollow!r})&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+            f&quot;Link(url={self.url!r}, text={self.text!r}, &quot;</span>
<span class="gi">+            f&quot;fragment={self.fragment!r}, nofollow={self.nofollow!r})&quot;</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/linkextractors/lxmlhtml.py b/scrapy/linkextractors/lxmlhtml.py</span>
<span class="gh">index 4c22c2389..de032fdd8 100644</span>
<span class="gd">--- a/scrapy/linkextractors/lxmlhtml.py</span>
<span class="gi">+++ b/scrapy/linkextractors/lxmlhtml.py</span>
<span class="gu">@@ -5,69 +5,226 @@ import logging</span>
<span class="w"> </span>import operator
<span class="w"> </span>from functools import partial
<span class="w"> </span>from urllib.parse import urljoin, urlparse
<span class="gd">-from lxml import etree</span>
<span class="gi">+</span>
<span class="gi">+from lxml import etree  # nosec</span>
<span class="w"> </span>from parsel.csstranslator import HTMLTranslator
<span class="w"> </span>from w3lib.html import strip_html5_whitespace
<span class="w"> </span>from w3lib.url import canonicalize_url, safe_url_string
<span class="gi">+</span>
<span class="w"> </span>from scrapy.link import Link
<span class="gd">-from scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, _re_type, re</span>
<span class="gi">+from scrapy.linkextractors import (</span>
<span class="gi">+    IGNORED_EXTENSIONS,</span>
<span class="gi">+    _is_valid_url,</span>
<span class="gi">+    _matches,</span>
<span class="gi">+    _re_type,</span>
<span class="gi">+    re,</span>
<span class="gi">+)</span>
<span class="w"> </span>from scrapy.utils.misc import arg_to_iter, rel_has_nofollow
<span class="w"> </span>from scrapy.utils.python import unique as unique_list
<span class="w"> </span>from scrapy.utils.response import get_base_url
<span class="w"> </span>from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gd">-XHTML_NAMESPACE = &#39;http://www.w3.org/1999/xhtml&#39;</span>
<span class="gd">-_collect_string_content = etree.XPath(&#39;string()&#39;)</span>

<span class="gi">+# from lxml/src/lxml/html/__init__.py</span>
<span class="gi">+XHTML_NAMESPACE = &quot;http://www.w3.org/1999/xhtml&quot;</span>
<span class="gi">+</span>
<span class="gi">+_collect_string_content = etree.XPath(&quot;string()&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _nons(tag):</span>
<span class="gi">+    if isinstance(tag, str):</span>
<span class="gi">+        if tag[0] == &quot;{&quot; and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:</span>
<span class="gi">+            return tag.split(&quot;}&quot;)[-1]</span>
<span class="gi">+    return tag</span>

<span class="gd">-class LxmlParserLinkExtractor:</span>

<span class="gd">-    def __init__(self, tag=&#39;a&#39;, attr=&#39;href&#39;, process=None, unique=False,</span>
<span class="gd">-        strip=True, canonicalized=False):</span>
<span class="gi">+def _identity(x):</span>
<span class="gi">+    return x</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _canonicalize_link_url(link):</span>
<span class="gi">+    return canonicalize_url(link.url, keep_fragments=True)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class LxmlParserLinkExtractor:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        tag=&quot;a&quot;,</span>
<span class="gi">+        attr=&quot;href&quot;,</span>
<span class="gi">+        process=None,</span>
<span class="gi">+        unique=False,</span>
<span class="gi">+        strip=True,</span>
<span class="gi">+        canonicalized=False,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.scan_tag = tag if callable(tag) else partial(operator.eq, tag)
<span class="w"> </span>        self.scan_attr = attr if callable(attr) else partial(operator.eq, attr)
<span class="w"> </span>        self.process_attr = process if callable(process) else _identity
<span class="w"> </span>        self.unique = unique
<span class="w"> </span>        self.strip = strip
<span class="gd">-        self.link_key = operator.attrgetter(&#39;url&#39;</span>
<span class="gd">-            ) if canonicalized else _canonicalize_link_url</span>
<span class="gi">+        self.link_key = (</span>
<span class="gi">+            operator.attrgetter(&quot;url&quot;) if canonicalized else _canonicalize_link_url</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _iter_links(self, document):</span>
<span class="gi">+        for el in document.iter(etree.Element):</span>
<span class="gi">+            if not self.scan_tag(_nons(el.tag)):</span>
<span class="gi">+                continue</span>
<span class="gi">+            attribs = el.attrib</span>
<span class="gi">+            for attrib in attribs:</span>
<span class="gi">+                if not self.scan_attr(attrib):</span>
<span class="gi">+                    continue</span>
<span class="gi">+                yield (el, attrib, attribs[attrib])</span>
<span class="gi">+</span>
<span class="gi">+    def _extract_links(self, selector, response_url, response_encoding, base_url):</span>
<span class="gi">+        links = []</span>
<span class="gi">+        # hacky way to get the underlying lxml parsed document</span>
<span class="gi">+        for el, attr, attr_val in self._iter_links(selector.root):</span>
<span class="gi">+            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)</span>
<span class="gi">+            try:</span>
<span class="gi">+                if self.strip:</span>
<span class="gi">+                    attr_val = strip_html5_whitespace(attr_val)</span>
<span class="gi">+                attr_val = urljoin(base_url, attr_val)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                continue  # skipping bogus links</span>
<span class="gi">+            else:</span>
<span class="gi">+                url = self.process_attr(attr_val)</span>
<span class="gi">+                if url is None:</span>
<span class="gi">+                    continue</span>
<span class="gi">+            try:</span>
<span class="gi">+                url = safe_url_string(url, encoding=response_encoding)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                logger.debug(f&quot;Skipping extraction of link with bad URL {url!r}&quot;)</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            # to fix relative links after process_value</span>
<span class="gi">+            url = urljoin(response_url, url)</span>
<span class="gi">+            link = Link(</span>
<span class="gi">+                url,</span>
<span class="gi">+                _collect_string_content(el) or &quot;&quot;,</span>
<span class="gi">+                nofollow=rel_has_nofollow(el.get(&quot;rel&quot;)),</span>
<span class="gi">+            )</span>
<span class="gi">+            links.append(link)</span>
<span class="gi">+        return self._deduplicate_if_needed(links)</span>
<span class="gi">+</span>
<span class="gi">+    def extract_links(self, response):</span>
<span class="gi">+        base_url = get_base_url(response)</span>
<span class="gi">+        return self._extract_links(</span>
<span class="gi">+            response.selector, response.url, response.encoding, base_url</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _process_links(self, links):
<span class="w"> </span>        &quot;&quot;&quot;Normalize and filter extracted links

<span class="w"> </span>        The subclass should override it if necessary
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._deduplicate_if_needed(links)</span>
<span class="gi">+</span>
<span class="gi">+    def _deduplicate_if_needed(self, links):</span>
<span class="gi">+        if self.unique:</span>
<span class="gi">+            return unique_list(links, key=self.link_key)</span>
<span class="gi">+        return links</span>


<span class="w"> </span>class LxmlLinkExtractor:
<span class="w"> </span>    _csstranslator = HTMLTranslator()

<span class="gd">-    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(),</span>
<span class="gd">-        restrict_xpaths=(), tags=(&#39;a&#39;, &#39;area&#39;), attrs=(&#39;href&#39;,),</span>
<span class="gd">-        canonicalize=False, unique=True, process_value=None,</span>
<span class="gd">-        deny_extensions=None, restrict_css=(), strip=True, restrict_text=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        allow=(),</span>
<span class="gi">+        deny=(),</span>
<span class="gi">+        allow_domains=(),</span>
<span class="gi">+        deny_domains=(),</span>
<span class="gi">+        restrict_xpaths=(),</span>
<span class="gi">+        tags=(&quot;a&quot;, &quot;area&quot;),</span>
<span class="gi">+        attrs=(&quot;href&quot;,),</span>
<span class="gi">+        canonicalize=False,</span>
<span class="gi">+        unique=True,</span>
<span class="gi">+        process_value=None,</span>
<span class="gi">+        deny_extensions=None,</span>
<span class="gi">+        restrict_css=(),</span>
<span class="gi">+        strip=True,</span>
<span class="gi">+        restrict_text=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))
<span class="gd">-        self.link_extractor = LxmlParserLinkExtractor(tag=partial(operator.</span>
<span class="gd">-            contains, tags), attr=partial(operator.contains, attrs), unique</span>
<span class="gd">-            =unique, process=process_value, strip=strip, canonicalized=not</span>
<span class="gd">-            canonicalize)</span>
<span class="gd">-        self.allow_res = [(x if isinstance(x, _re_type) else re.compile(x)) for</span>
<span class="gd">-            x in arg_to_iter(allow)]</span>
<span class="gd">-        self.deny_res = [(x if isinstance(x, _re_type) else re.compile(x)) for</span>
<span class="gd">-            x in arg_to_iter(deny)]</span>
<span class="gi">+        self.link_extractor = LxmlParserLinkExtractor(</span>
<span class="gi">+            tag=partial(operator.contains, tags),</span>
<span class="gi">+            attr=partial(operator.contains, attrs),</span>
<span class="gi">+            unique=unique,</span>
<span class="gi">+            process=process_value,</span>
<span class="gi">+            strip=strip,</span>
<span class="gi">+            canonicalized=not canonicalize,</span>
<span class="gi">+        )</span>
<span class="gi">+        self.allow_res = [</span>
<span class="gi">+            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)</span>
<span class="gi">+        ]</span>
<span class="gi">+        self.deny_res = [</span>
<span class="gi">+            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="w"> </span>        self.allow_domains = set(arg_to_iter(allow_domains))
<span class="w"> </span>        self.deny_domains = set(arg_to_iter(deny_domains))
<span class="gi">+</span>
<span class="w"> </span>        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))
<span class="gd">-        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,</span>
<span class="gd">-            arg_to_iter(restrict_css)))</span>
<span class="gi">+        self.restrict_xpaths += tuple(</span>
<span class="gi">+            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>        if deny_extensions is None:
<span class="w"> </span>            deny_extensions = IGNORED_EXTENSIONS
<span class="w"> </span>        self.canonicalize = canonicalize
<span class="gd">-        self.deny_extensions = {(&#39;.&#39; + e) for e in arg_to_iter(deny_extensions)</span>
<span class="gd">-            }</span>
<span class="gd">-        self.restrict_text = [(x if isinstance(x, _re_type) else re.compile</span>
<span class="gd">-            (x)) for x in arg_to_iter(restrict_text)]</span>
<span class="gi">+        self.deny_extensions = {&quot;.&quot; + e for e in arg_to_iter(deny_extensions)}</span>
<span class="gi">+        self.restrict_text = [</span>
<span class="gi">+            x if isinstance(x, _re_type) else re.compile(x)</span>
<span class="gi">+            for x in arg_to_iter(restrict_text)</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+    def _link_allowed(self, link):</span>
<span class="gi">+        if not _is_valid_url(link.url):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.allow_res and not _matches(link.url, self.allow_res):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.deny_res and _matches(link.url, self.deny_res):</span>
<span class="gi">+            return False</span>
<span class="gi">+        parsed_url = urlparse(link.url)</span>
<span class="gi">+        if self.allow_domains and not url_is_from_any_domain(</span>
<span class="gi">+            parsed_url, self.allow_domains</span>
<span class="gi">+        ):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.deny_extensions and url_has_any_extension(</span>
<span class="gi">+            parsed_url, self.deny_extensions</span>
<span class="gi">+        ):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.restrict_text and not _matches(link.text, self.restrict_text):</span>
<span class="gi">+            return False</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    def matches(self, url):</span>
<span class="gi">+        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        allowed = (</span>
<span class="gi">+            (regex.search(url) for regex in self.allow_res)</span>
<span class="gi">+            if self.allow_res</span>
<span class="gi">+            else [True]</span>
<span class="gi">+        )</span>
<span class="gi">+        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []</span>
<span class="gi">+        return any(allowed) and not any(denied)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_links(self, links):</span>
<span class="gi">+        links = [x for x in links if self._link_allowed(x)]</span>
<span class="gi">+        if self.canonicalize:</span>
<span class="gi">+            for link in links:</span>
<span class="gi">+                link.url = canonicalize_url(link.url)</span>
<span class="gi">+        links = self.link_extractor._process_links(links)</span>
<span class="gi">+        return links</span>
<span class="gi">+</span>
<span class="gi">+    def _extract_links(self, *args, **kwargs):</span>
<span class="gi">+        return self.link_extractor._extract_links(*args, **kwargs)</span>

<span class="w"> </span>    def extract_links(self, response):
<span class="w"> </span>        &quot;&quot;&quot;Returns a list of :class:`~scrapy.link.Link` objects from the
<span class="gu">@@ -79,4 +236,17 @@ class LxmlLinkExtractor:</span>
<span class="w"> </span>        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,
<span class="w"> </span>        otherwise they are returned.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        base_url = get_base_url(response)</span>
<span class="gi">+        if self.restrict_xpaths:</span>
<span class="gi">+            docs = [</span>
<span class="gi">+                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)</span>
<span class="gi">+            ]</span>
<span class="gi">+        else:</span>
<span class="gi">+            docs = [response.selector]</span>
<span class="gi">+        all_links = []</span>
<span class="gi">+        for doc in docs:</span>
<span class="gi">+            links = self._extract_links(doc, response.url, response.encoding, base_url)</span>
<span class="gi">+            all_links.extend(self._process_links(links))</span>
<span class="gi">+        if self.link_extractor.unique:</span>
<span class="gi">+            return unique_list(all_links, key=self.link_extractor.link_key)</span>
<span class="gi">+        return all_links</span>
<span class="gh">diff --git a/scrapy/loader/common.py b/scrapy/loader/common.py</span>
<span class="gh">index f37e2fc91..3e8644e0c 100644</span>
<span class="gd">--- a/scrapy/loader/common.py</span>
<span class="gi">+++ b/scrapy/loader/common.py</span>
<span class="gu">@@ -1,6 +1,9 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Common functions used in Item Loaders code&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>from itemloaders import common
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.deprecate import ScrapyDeprecationWarning


<span class="gu">@@ -8,4 +11,11 @@ def wrap_loader_context(function, context):</span>
<span class="w"> </span>    &quot;&quot;&quot;Wrap functions that receive loader_context to contain the context
<span class="w"> </span>    &quot;pre-loaded&quot; and expose a interface that receives only one argument
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    warnings.warn(</span>
<span class="gi">+        &quot;scrapy.loader.common.wrap_loader_context has moved to a new library.&quot;</span>
<span class="gi">+        &quot;Please update your reference to itemloaders.common.wrap_loader_context&quot;,</span>
<span class="gi">+        ScrapyDeprecationWarning,</span>
<span class="gi">+        stacklevel=2,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    return common.wrap_loader_context(function, context)</span>
<span class="gh">diff --git a/scrapy/loader/processors.py b/scrapy/loader/processors.py</span>
<span class="gh">index 1b5404f6e..b82c6d5c7 100644</span>
<span class="gd">--- a/scrapy/loader/processors.py</span>
<span class="gi">+++ b/scrapy/loader/processors.py</span>
<span class="gu">@@ -4,10 +4,17 @@ This module provides some commonly used processors for Item Loaders.</span>
<span class="w"> </span>See documentation in docs/topics/loaders.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from itemloaders import processors
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.deprecate import create_deprecated_class
<span class="gd">-MapCompose = create_deprecated_class(&#39;MapCompose&#39;, processors.MapCompose)</span>
<span class="gd">-Compose = create_deprecated_class(&#39;Compose&#39;, processors.Compose)</span>
<span class="gd">-TakeFirst = create_deprecated_class(&#39;TakeFirst&#39;, processors.TakeFirst)</span>
<span class="gd">-Identity = create_deprecated_class(&#39;Identity&#39;, processors.Identity)</span>
<span class="gd">-SelectJmes = create_deprecated_class(&#39;SelectJmes&#39;, processors.SelectJmes)</span>
<span class="gd">-Join = create_deprecated_class(&#39;Join&#39;, processors.Join)</span>
<span class="gi">+</span>
<span class="gi">+MapCompose = create_deprecated_class(&quot;MapCompose&quot;, processors.MapCompose)</span>
<span class="gi">+</span>
<span class="gi">+Compose = create_deprecated_class(&quot;Compose&quot;, processors.Compose)</span>
<span class="gi">+</span>
<span class="gi">+TakeFirst = create_deprecated_class(&quot;TakeFirst&quot;, processors.TakeFirst)</span>
<span class="gi">+</span>
<span class="gi">+Identity = create_deprecated_class(&quot;Identity&quot;, processors.Identity)</span>
<span class="gi">+</span>
<span class="gi">+SelectJmes = create_deprecated_class(&quot;SelectJmes&quot;, processors.SelectJmes)</span>
<span class="gi">+</span>
<span class="gi">+Join = create_deprecated_class(&quot;Join&quot;, processors.Join)</span>
<span class="gh">diff --git a/scrapy/logformatter.py b/scrapy/logformatter.py</span>
<span class="gh">index e59eb2a97..d720b2f38 100644</span>
<span class="gd">--- a/scrapy/logformatter.py</span>
<span class="gi">+++ b/scrapy/logformatter.py</span>
<span class="gu">@@ -1,23 +1,29 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import os
<span class="w"> </span>from typing import TYPE_CHECKING, Any, Dict, Optional, Union
<span class="gi">+</span>
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request, Spider
<span class="w"> </span>from scrapy.http import Response
<span class="w"> </span>from scrapy.utils.request import referer_str
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gd">-SCRAPEDMSG = &#39;Scraped from %(src)s&#39; + os.linesep + &#39;%(item)s&#39;</span>
<span class="gd">-DROPPEDMSG = &#39;Dropped: %(exception)s&#39; + os.linesep + &#39;%(item)s&#39;</span>
<span class="gd">-CRAWLEDMSG = (</span>
<span class="gd">-    &#39;Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s&#39;</span>
<span class="gd">-    )</span>
<span class="gd">-ITEMERRORMSG = &#39;Error processing %(item)s&#39;</span>
<span class="gd">-SPIDERERRORMSG = &#39;Spider error processing %(request)s (referer: %(referer)s)&#39;</span>
<span class="gd">-DOWNLOADERRORMSG_SHORT = &#39;Error downloading %(request)s&#39;</span>
<span class="gd">-DOWNLOADERRORMSG_LONG = &#39;Error downloading %(request)s: %(errmsg)s&#39;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+SCRAPEDMSG = &quot;Scraped from %(src)s&quot; + os.linesep + &quot;%(item)s&quot;</span>
<span class="gi">+DROPPEDMSG = &quot;Dropped: %(exception)s&quot; + os.linesep + &quot;%(item)s&quot;</span>
<span class="gi">+CRAWLEDMSG = &quot;Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s&quot;</span>
<span class="gi">+ITEMERRORMSG = &quot;Error processing %(item)s&quot;</span>
<span class="gi">+SPIDERERRORMSG = &quot;Spider error processing %(request)s (referer: %(referer)s)&quot;</span>
<span class="gi">+DOWNLOADERRORMSG_SHORT = &quot;Error downloading %(request)s&quot;</span>
<span class="gi">+DOWNLOADERRORMSG_LONG = &quot;Error downloading %(request)s: %(errmsg)s&quot;</span>


<span class="w"> </span>class LogFormatter:
<span class="gu">@@ -58,43 +64,115 @@ class LogFormatter:</span>
<span class="w"> </span>                    }
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def crawled(self, request: Request, response: Response, spider: Spider</span>
<span class="gd">-        ) -&gt;dict:</span>
<span class="gi">+    def crawled(self, request: Request, response: Response, spider: Spider) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs a message when the crawler finds a webpage.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def scraped(self, item: Any, response: Union[Response, Failure], spider:</span>
<span class="gd">-        Spider) -&gt;dict:</span>
<span class="gi">+        request_flags = f&quot; {str(request.flags)}&quot; if request.flags else &quot;&quot;</span>
<span class="gi">+        response_flags = f&quot; {str(response.flags)}&quot; if response.flags else &quot;&quot;</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.DEBUG,</span>
<span class="gi">+            &quot;msg&quot;: CRAWLEDMSG,</span>
<span class="gi">+            &quot;args&quot;: {</span>
<span class="gi">+                &quot;status&quot;: response.status,</span>
<span class="gi">+                &quot;request&quot;: request,</span>
<span class="gi">+                &quot;request_flags&quot;: request_flags,</span>
<span class="gi">+                &quot;referer&quot;: referer_str(request),</span>
<span class="gi">+                &quot;response_flags&quot;: response_flags,</span>
<span class="gi">+                # backward compatibility with Scrapy logformatter below 1.4 version</span>
<span class="gi">+                &quot;flags&quot;: response_flags,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def scraped(</span>
<span class="gi">+        self, item: Any, response: Union[Response, Failure], spider: Spider</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs a message when an item is scraped by a spider.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def dropped(self, item: Any, exception: BaseException, response:</span>
<span class="gd">-        Response, spider: Spider) -&gt;dict:</span>
<span class="gi">+        src: Any</span>
<span class="gi">+        if isinstance(response, Failure):</span>
<span class="gi">+            src = response.getErrorMessage()</span>
<span class="gi">+        else:</span>
<span class="gi">+            src = response</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.DEBUG,</span>
<span class="gi">+            &quot;msg&quot;: SCRAPEDMSG,</span>
<span class="gi">+            &quot;args&quot;: {</span>
<span class="gi">+                &quot;src&quot;: src,</span>
<span class="gi">+                &quot;item&quot;: item,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def dropped(</span>
<span class="gi">+        self, item: Any, exception: BaseException, response: Response, spider: Spider</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs a message when an item is dropped while it is passing through the item pipeline.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def item_error(self, item: Any, exception: BaseException, response:</span>
<span class="gd">-        Response, spider: Spider) -&gt;dict:</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.WARNING,</span>
<span class="gi">+            &quot;msg&quot;: DROPPEDMSG,</span>
<span class="gi">+            &quot;args&quot;: {</span>
<span class="gi">+                &quot;exception&quot;: exception,</span>
<span class="gi">+                &quot;item&quot;: item,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def item_error(</span>
<span class="gi">+        self, item: Any, exception: BaseException, response: Response, spider: Spider</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs a message when an item causes an error while it is passing
<span class="w"> </span>        through the item pipeline.

<span class="w"> </span>        .. versionadded:: 2.0
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def spider_error(self, failure: Failure, request: Request, response:</span>
<span class="gd">-        Union[Response, Failure], spider: Spider) -&gt;dict:</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.ERROR,</span>
<span class="gi">+            &quot;msg&quot;: ITEMERRORMSG,</span>
<span class="gi">+            &quot;args&quot;: {</span>
<span class="gi">+                &quot;item&quot;: item,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def spider_error(</span>
<span class="gi">+        self,</span>
<span class="gi">+        failure: Failure,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        response: Union[Response, Failure],</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs an error message from a spider.

<span class="w"> </span>        .. versionadded:: 2.0
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def download_error(self, failure: Failure, request: Request, spider:</span>
<span class="gd">-        Spider, errmsg: Optional[str]=None) -&gt;dict:</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.ERROR,</span>
<span class="gi">+            &quot;msg&quot;: SPIDERERRORMSG,</span>
<span class="gi">+            &quot;args&quot;: {</span>
<span class="gi">+                &quot;request&quot;: request,</span>
<span class="gi">+                &quot;referer&quot;: referer_str(request),</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def download_error(</span>
<span class="gi">+        self,</span>
<span class="gi">+        failure: Failure,</span>
<span class="gi">+        request: Request,</span>
<span class="gi">+        spider: Spider,</span>
<span class="gi">+        errmsg: Optional[str] = None,</span>
<span class="gi">+    ) -&gt; dict:</span>
<span class="w"> </span>        &quot;&quot;&quot;Logs a download error message from a spider (typically coming from
<span class="w"> </span>        the engine).

<span class="w"> </span>        .. versionadded:: 2.0
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        args: Dict[str, Any] = {&quot;request&quot;: request}</span>
<span class="gi">+        if errmsg:</span>
<span class="gi">+            msg = DOWNLOADERRORMSG_LONG</span>
<span class="gi">+            args[&quot;errmsg&quot;] = errmsg</span>
<span class="gi">+        else:</span>
<span class="gi">+            msg = DOWNLOADERRORMSG_SHORT</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;level&quot;: logging.ERROR,</span>
<span class="gi">+            &quot;msg&quot;: msg,</span>
<span class="gi">+            &quot;args&quot;: args,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler: Crawler) -&gt; Self:</span>
<span class="gi">+        return cls()</span>
<span class="gh">diff --git a/scrapy/mail.py b/scrapy/mail.py</span>
<span class="gh">index b149b42bb..237327451 100644</span>
<span class="gd">--- a/scrapy/mail.py</span>
<span class="gi">+++ b/scrapy/mail.py</span>
<span class="gu">@@ -11,20 +11,40 @@ from email.mime.nonmultipart import MIMENonMultipart</span>
<span class="w"> </span>from email.mime.text import MIMEText
<span class="w"> </span>from email.utils import formatdate
<span class="w"> </span>from io import BytesIO
<span class="gi">+</span>
<span class="w"> </span>from twisted import version as twisted_version
<span class="w"> </span>from twisted.internet import defer, ssl
<span class="w"> </span>from twisted.python.versions import Version
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.misc import arg_to_iter
<span class="w"> </span>from scrapy.utils.python import to_bytes
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gd">-COMMASPACE = &#39;, &#39;</span>


<span class="gd">-class MailSender:</span>
<span class="gi">+# Defined in the email.utils module, but undocumented:</span>
<span class="gi">+# https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42</span>
<span class="gi">+COMMASPACE = &quot;, &quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _to_bytes_or_none(text):</span>
<span class="gi">+    if text is None:</span>
<span class="gi">+        return None</span>
<span class="gi">+    return to_bytes(text)</span>
<span class="gi">+</span>

<span class="gd">-    def __init__(self, smtphost=&#39;localhost&#39;, mailfrom=&#39;scrapy@localhost&#39;,</span>
<span class="gd">-        smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=</span>
<span class="gd">-        False, debug=False):</span>
<span class="gi">+class MailSender:</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        smtphost=&quot;localhost&quot;,</span>
<span class="gi">+        mailfrom=&quot;scrapy@localhost&quot;,</span>
<span class="gi">+        smtpuser=None,</span>
<span class="gi">+        smtppass=None,</span>
<span class="gi">+        smtpport=25,</span>
<span class="gi">+        smtptls=False,</span>
<span class="gi">+        smtpssl=False,</span>
<span class="gi">+        debug=False,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.smtphost = smtphost
<span class="w"> </span>        self.smtpport = smtpport
<span class="w"> </span>        self.smtpuser = _to_bytes_or_none(smtpuser)
<span class="gu">@@ -33,3 +53,156 @@ class MailSender:</span>
<span class="w"> </span>        self.smtpssl = smtpssl
<span class="w"> </span>        self.mailfrom = mailfrom
<span class="w"> </span>        self.debug = debug
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings):</span>
<span class="gi">+        return cls(</span>
<span class="gi">+            smtphost=settings[&quot;MAIL_HOST&quot;],</span>
<span class="gi">+            mailfrom=settings[&quot;MAIL_FROM&quot;],</span>
<span class="gi">+            smtpuser=settings[&quot;MAIL_USER&quot;],</span>
<span class="gi">+            smtppass=settings[&quot;MAIL_PASS&quot;],</span>
<span class="gi">+            smtpport=settings.getint(&quot;MAIL_PORT&quot;),</span>
<span class="gi">+            smtptls=settings.getbool(&quot;MAIL_TLS&quot;),</span>
<span class="gi">+            smtpssl=settings.getbool(&quot;MAIL_SSL&quot;),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def send(</span>
<span class="gi">+        self,</span>
<span class="gi">+        to,</span>
<span class="gi">+        subject,</span>
<span class="gi">+        body,</span>
<span class="gi">+        cc=None,</span>
<span class="gi">+        attachs=(),</span>
<span class="gi">+        mimetype=&quot;text/plain&quot;,</span>
<span class="gi">+        charset=None,</span>
<span class="gi">+        _callback=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        if attachs:</span>
<span class="gi">+            msg = MIMEMultipart()</span>
<span class="gi">+        else:</span>
<span class="gi">+            msg = MIMENonMultipart(*mimetype.split(&quot;/&quot;, 1))</span>
<span class="gi">+</span>
<span class="gi">+        to = list(arg_to_iter(to))</span>
<span class="gi">+        cc = list(arg_to_iter(cc))</span>
<span class="gi">+</span>
<span class="gi">+        msg[&quot;From&quot;] = self.mailfrom</span>
<span class="gi">+        msg[&quot;To&quot;] = COMMASPACE.join(to)</span>
<span class="gi">+        msg[&quot;Date&quot;] = formatdate(localtime=True)</span>
<span class="gi">+        msg[&quot;Subject&quot;] = subject</span>
<span class="gi">+        rcpts = to[:]</span>
<span class="gi">+        if cc:</span>
<span class="gi">+            rcpts.extend(cc)</span>
<span class="gi">+            msg[&quot;Cc&quot;] = COMMASPACE.join(cc)</span>
<span class="gi">+</span>
<span class="gi">+        if attachs:</span>
<span class="gi">+            if charset:</span>
<span class="gi">+                msg.set_charset(charset)</span>
<span class="gi">+            msg.attach(MIMEText(body, &quot;plain&quot;, charset or &quot;us-ascii&quot;))</span>
<span class="gi">+            for attach_name, mimetype, f in attachs:</span>
<span class="gi">+                part = MIMEBase(*mimetype.split(&quot;/&quot;))</span>
<span class="gi">+                part.set_payload(f.read())</span>
<span class="gi">+                Encoders.encode_base64(part)</span>
<span class="gi">+                part.add_header(</span>
<span class="gi">+                    &quot;Content-Disposition&quot;, &quot;attachment&quot;, filename=attach_name</span>
<span class="gi">+                )</span>
<span class="gi">+                msg.attach(part)</span>
<span class="gi">+        else:</span>
<span class="gi">+            msg.set_payload(body, charset)</span>
<span class="gi">+</span>
<span class="gi">+        if _callback:</span>
<span class="gi">+            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)</span>
<span class="gi">+</span>
<span class="gi">+        if self.debug:</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s &quot;</span>
<span class="gi">+                &#39;Subject=&quot;%(mailsubject)s&quot; Attachs=%(mailattachs)d&#39;,</span>
<span class="gi">+                {</span>
<span class="gi">+                    &quot;mailto&quot;: to,</span>
<span class="gi">+                    &quot;mailcc&quot;: cc,</span>
<span class="gi">+                    &quot;mailsubject&quot;: subject,</span>
<span class="gi">+                    &quot;mailattachs&quot;: len(attachs),</span>
<span class="gi">+                },</span>
<span class="gi">+            )</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or &quot;utf-8&quot;))</span>
<span class="gi">+        dfd.addCallbacks(</span>
<span class="gi">+            callback=self._sent_ok,</span>
<span class="gi">+            errback=self._sent_failed,</span>
<span class="gi">+            callbackArgs=[to, cc, subject, len(attachs)],</span>
<span class="gi">+            errbackArgs=[to, cc, subject, len(attachs)],</span>
<span class="gi">+        )</span>
<span class="gi">+        reactor.addSystemEventTrigger(&quot;before&quot;, &quot;shutdown&quot;, lambda: dfd)</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def _sent_ok(self, result, to, cc, subject, nattachs):</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Mail sent OK: To=%(mailto)s Cc=%(mailcc)s &quot;</span>
<span class="gi">+            &#39;Subject=&quot;%(mailsubject)s&quot; Attachs=%(mailattachs)d&#39;,</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;mailto&quot;: to,</span>
<span class="gi">+                &quot;mailcc&quot;: cc,</span>
<span class="gi">+                &quot;mailsubject&quot;: subject,</span>
<span class="gi">+                &quot;mailattachs&quot;: nattachs,</span>
<span class="gi">+            },</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _sent_failed(self, failure, to, cc, subject, nattachs):</span>
<span class="gi">+        errstr = str(failure.value)</span>
<span class="gi">+        logger.error(</span>
<span class="gi">+            &quot;Unable to send mail: To=%(mailto)s Cc=%(mailcc)s &quot;</span>
<span class="gi">+            &#39;Subject=&quot;%(mailsubject)s&quot; Attachs=%(mailattachs)d&#39;</span>
<span class="gi">+            &quot;- %(mailerr)s&quot;,</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;mailto&quot;: to,</span>
<span class="gi">+                &quot;mailcc&quot;: cc,</span>
<span class="gi">+                &quot;mailsubject&quot;: subject,</span>
<span class="gi">+                &quot;mailattachs&quot;: nattachs,</span>
<span class="gi">+                &quot;mailerr&quot;: errstr,</span>
<span class="gi">+            },</span>
<span class="gi">+        )</span>
<span class="gi">+        return failure</span>
<span class="gi">+</span>
<span class="gi">+    def _sendmail(self, to_addrs, msg):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        msg = BytesIO(msg)</span>
<span class="gi">+        d = defer.Deferred()</span>
<span class="gi">+</span>
<span class="gi">+        factory = self._create_sender_factory(to_addrs, msg, d)</span>
<span class="gi">+</span>
<span class="gi">+        if self.smtpssl:</span>
<span class="gi">+            reactor.connectSSL(</span>
<span class="gi">+                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            reactor.connectTCP(self.smtphost, self.smtpport, factory)</span>
<span class="gi">+</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _create_sender_factory(self, to_addrs, msg, d):</span>
<span class="gi">+        from twisted.mail.smtp import ESMTPSenderFactory</span>
<span class="gi">+</span>
<span class="gi">+        factory_keywords = {</span>
<span class="gi">+            &quot;heloFallback&quot;: True,</span>
<span class="gi">+            &quot;requireAuthentication&quot;: False,</span>
<span class="gi">+            &quot;requireTransportSecurity&quot;: self.smtptls,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+        # Newer versions of twisted require the hostname to use STARTTLS</span>
<span class="gi">+        if twisted_version &gt;= Version(&quot;twisted&quot;, 21, 2, 0):</span>
<span class="gi">+            factory_keywords[&quot;hostname&quot;] = self.smtphost</span>
<span class="gi">+</span>
<span class="gi">+        factory = ESMTPSenderFactory(</span>
<span class="gi">+            self.smtpuser,</span>
<span class="gi">+            self.smtppass,</span>
<span class="gi">+            self.mailfrom,</span>
<span class="gi">+            to_addrs,</span>
<span class="gi">+            msg,</span>
<span class="gi">+            d,</span>
<span class="gi">+            **factory_keywords</span>
<span class="gi">+        )</span>
<span class="gi">+        factory.noisy = False</span>
<span class="gi">+        return factory</span>
<span class="gh">diff --git a/scrapy/middleware.py b/scrapy/middleware.py</span>
<span class="gh">index 38f8c46ff..090588130 100644</span>
<span class="gd">--- a/scrapy/middleware.py</span>
<span class="gi">+++ b/scrapy/middleware.py</span>
<span class="gu">@@ -1,27 +1,110 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import pprint
<span class="w"> </span>from collections import defaultdict, deque
<span class="gd">-from typing import TYPE_CHECKING, Any, Callable, Deque, Dict, Iterable, List, Optional, Tuple, Union, cast</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Deque,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.defer import process_chain, process_parallel
<span class="w"> </span>from scrapy.utils.misc import create_instance, load_object
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class MiddlewareManager:
<span class="w"> </span>    &quot;&quot;&quot;Base class for implementing middleware managers&quot;&quot;&quot;
<span class="gd">-    component_name = &#39;foo middleware&#39;</span>

<span class="gd">-    def __init__(self, *middlewares: Any) -&gt;None:</span>
<span class="gi">+    component_name = &quot;foo middleware&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, *middlewares: Any) -&gt; None:</span>
<span class="w"> </span>        self.middlewares = middlewares
<span class="gd">-        self.methods: Dict[str, Deque[Union[None, Callable, Tuple[Callable,</span>
<span class="gd">-            Callable]]]] = defaultdict(deque)</span>
<span class="gi">+        # Only process_spider_output and process_spider_exception can be None.</span>
<span class="gi">+        # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.</span>
<span class="gi">+        self.methods: Dict[</span>
<span class="gi">+            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]</span>
<span class="gi">+        ] = defaultdict(deque)</span>
<span class="w"> </span>        for mw in middlewares:
<span class="w"> </span>            self._add_middleware(mw)
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _get_mwlist_from_settings(cls, settings: Settings) -&gt; List[Any]:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(</span>
<span class="gi">+        cls, settings: Settings, crawler: Optional[Crawler] = None</span>
<span class="gi">+    ) -&gt; Self:</span>
<span class="gi">+        mwlist = cls._get_mwlist_from_settings(settings)</span>
<span class="gi">+        middlewares = []</span>
<span class="gi">+        enabled = []</span>
<span class="gi">+        for clspath in mwlist:</span>
<span class="gi">+            try:</span>
<span class="gi">+                mwcls = load_object(clspath)</span>
<span class="gi">+                mw = create_instance(mwcls, settings, crawler)</span>
<span class="gi">+                middlewares.append(mw)</span>
<span class="gi">+                enabled.append(clspath)</span>
<span class="gi">+            except NotConfigured as e:</span>
<span class="gi">+                if e.args:</span>
<span class="gi">+                    logger.warning(</span>
<span class="gi">+                        &quot;Disabled %(clspath)s: %(eargs)s&quot;,</span>
<span class="gi">+                        {&quot;clspath&quot;: clspath, &quot;eargs&quot;: e.args[0]},</span>
<span class="gi">+                        extra={&quot;crawler&quot;: crawler},</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+        logger.info(</span>
<span class="gi">+            &quot;Enabled %(componentname)ss:\n%(enabledlist)s&quot;,</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;componentname&quot;: cls.component_name,</span>
<span class="gi">+                &quot;enabledlist&quot;: pprint.pformat(enabled),</span>
<span class="gi">+            },</span>
<span class="gi">+            extra={&quot;crawler&quot;: crawler},</span>
<span class="gi">+        )</span>
<span class="gi">+        return cls(*middlewares)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler: Crawler) -&gt; Self:</span>
<span class="gi">+        return cls.from_settings(crawler.settings, crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def _add_middleware(self, mw: Any) -&gt; None:</span>
<span class="gi">+        if hasattr(mw, &quot;open_spider&quot;):</span>
<span class="gi">+            self.methods[&quot;open_spider&quot;].append(mw.open_spider)</span>
<span class="gi">+        if hasattr(mw, &quot;close_spider&quot;):</span>
<span class="gi">+            self.methods[&quot;close_spider&quot;].appendleft(mw.close_spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_parallel(self, methodname: str, obj: Any, *args: Any) -&gt; Deferred:</span>
<span class="gi">+        methods = cast(Iterable[Callable], self.methods[methodname])</span>
<span class="gi">+        return process_parallel(methods, obj, *args)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_chain(self, methodname: str, obj: Any, *args: Any) -&gt; Deferred:</span>
<span class="gi">+        methods = cast(Iterable[Callable], self.methods[methodname])</span>
<span class="gi">+        return process_chain(methods, obj, *args)</span>
<span class="gi">+</span>
<span class="gi">+    def open_spider(self, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        return self._process_parallel(&quot;open_spider&quot;, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def close_spider(self, spider: Spider) -&gt; Deferred:</span>
<span class="gi">+        return self._process_parallel(&quot;close_spider&quot;, spider)</span>
<span class="gh">diff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py</span>
<span class="gh">index 3c976dd8f..5c09ab37e 100644</span>
<span class="gd">--- a/scrapy/pipelines/files.py</span>
<span class="gi">+++ b/scrapy/pipelines/files.py</span>
<span class="gu">@@ -18,8 +18,10 @@ from os import PathLike</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import DefaultDict, Optional, Set, Union
<span class="w"> </span>from urllib.parse import urlparse
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter
<span class="w"> </span>from twisted.internet import defer, threads
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest, NotConfigured
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.http.request import NO_CALLBACK
<span class="gu">@@ -32,23 +34,57 @@ from scrapy.utils.log import failure_to_exc_info</span>
<span class="w"> </span>from scrapy.utils.misc import md5sum
<span class="w"> </span>from scrapy.utils.python import to_bytes
<span class="w"> </span>from scrapy.utils.request import referer_str
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gi">+def _to_string(path: Union[str, PathLike]) -&gt; str:</span>
<span class="gi">+    return str(path)  # convert a Path object to string</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class FileException(Exception):
<span class="w"> </span>    &quot;&quot;&quot;General media error exception&quot;&quot;&quot;


<span class="w"> </span>class FSFilesStore:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, basedir: Union[str, PathLike]):
<span class="w"> </span>        basedir = _to_string(basedir)
<span class="gd">-        if &#39;://&#39; in basedir:</span>
<span class="gd">-            basedir = basedir.split(&#39;://&#39;, 1)[1]</span>
<span class="gi">+        if &quot;://&quot; in basedir:</span>
<span class="gi">+            basedir = basedir.split(&quot;://&quot;, 1)[1]</span>
<span class="w"> </span>        self.basedir = basedir
<span class="w"> </span>        self._mkdir(Path(self.basedir))
<span class="w"> </span>        self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)

<span class="gi">+    def persist_file(</span>
<span class="gi">+        self, path: Union[str, PathLike], buf, info, meta=None, headers=None</span>
<span class="gi">+    ):</span>
<span class="gi">+        absolute_path = self._get_filesystem_path(path)</span>
<span class="gi">+        self._mkdir(absolute_path.parent, info)</span>
<span class="gi">+        absolute_path.write_bytes(buf.getvalue())</span>
<span class="gi">+</span>
<span class="gi">+    def stat_file(self, path: Union[str, PathLike], info):</span>
<span class="gi">+        absolute_path = self._get_filesystem_path(path)</span>
<span class="gi">+        try:</span>
<span class="gi">+            last_modified = absolute_path.stat().st_mtime</span>
<span class="gi">+        except os.error:</span>
<span class="gi">+            return {}</span>
<span class="gi">+</span>
<span class="gi">+        with absolute_path.open(&quot;rb&quot;) as f:</span>
<span class="gi">+            checksum = md5sum(f)</span>
<span class="gi">+</span>
<span class="gi">+        return {&quot;last_modified&quot;: last_modified, &quot;checksum&quot;: checksum}</span>
<span class="gi">+</span>
<span class="gi">+    def _get_filesystem_path(self, path: Union[str, PathLike]) -&gt; Path:</span>
<span class="gi">+        path_comps = _to_string(path).split(&quot;/&quot;)</span>
<span class="gi">+        return Path(self.basedir, *path_comps)</span>
<span class="gi">+</span>
<span class="gi">+    def _mkdir(self, dirname: Path, domain: Optional[str] = None):</span>
<span class="gi">+        seen = self.created_directories[domain] if domain else set()</span>
<span class="gi">+        if str(dirname) not in seen:</span>
<span class="gi">+            if not dirname.exists():</span>
<span class="gi">+                dirname.mkdir(parents=True)</span>
<span class="gi">+            seen.add(str(dirname))</span>
<span class="gi">+</span>

<span class="w"> </span>class S3FilesStore:
<span class="w"> </span>    AWS_ACCESS_KEY_ID = None
<span class="gu">@@ -58,53 +94,170 @@ class S3FilesStore:</span>
<span class="w"> </span>    AWS_REGION_NAME = None
<span class="w"> </span>    AWS_USE_SSL = None
<span class="w"> </span>    AWS_VERIFY = None
<span class="gd">-    POLICY = &#39;private&#39;</span>
<span class="gd">-    HEADERS = {&#39;Cache-Control&#39;: &#39;max-age=172800&#39;}</span>
<span class="gi">+</span>
<span class="gi">+    POLICY = &quot;private&quot;  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings</span>
<span class="gi">+    HEADERS = {</span>
<span class="gi">+        &quot;Cache-Control&quot;: &quot;max-age=172800&quot;,</span>
<span class="gi">+    }</span>

<span class="w"> </span>    def __init__(self, uri):
<span class="w"> </span>        if not is_botocore_available():
<span class="gd">-            raise NotConfigured(&#39;missing botocore library&#39;)</span>
<span class="gi">+            raise NotConfigured(&quot;missing botocore library&quot;)</span>
<span class="w"> </span>        import botocore.session
<span class="gi">+</span>
<span class="w"> </span>        session = botocore.session.get_session()
<span class="gd">-        self.s3_client = session.create_client(&#39;s3&#39;, aws_access_key_id=self</span>
<span class="gd">-            .AWS_ACCESS_KEY_ID, aws_secret_access_key=self.</span>
<span class="gd">-            AWS_SECRET_ACCESS_KEY, aws_session_token=self.AWS_SESSION_TOKEN,</span>
<span class="gd">-            endpoint_url=self.AWS_ENDPOINT_URL, region_name=self.</span>
<span class="gd">-            AWS_REGION_NAME, use_ssl=self.AWS_USE_SSL, verify=self.AWS_VERIFY)</span>
<span class="gd">-        if not uri.startswith(&#39;s3://&#39;):</span>
<span class="gi">+        self.s3_client = session.create_client(</span>
<span class="gi">+            &quot;s3&quot;,</span>
<span class="gi">+            aws_access_key_id=self.AWS_ACCESS_KEY_ID,</span>
<span class="gi">+            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,</span>
<span class="gi">+            aws_session_token=self.AWS_SESSION_TOKEN,</span>
<span class="gi">+            endpoint_url=self.AWS_ENDPOINT_URL,</span>
<span class="gi">+            region_name=self.AWS_REGION_NAME,</span>
<span class="gi">+            use_ssl=self.AWS_USE_SSL,</span>
<span class="gi">+            verify=self.AWS_VERIFY,</span>
<span class="gi">+        )</span>
<span class="gi">+        if not uri.startswith(&quot;s3://&quot;):</span>
<span class="w"> </span>            raise ValueError(f&quot;Incorrect URI scheme in {uri}, expected &#39;s3&#39;&quot;)
<span class="gd">-        self.bucket, self.prefix = uri[5:].split(&#39;/&#39;, 1)</span>
<span class="gi">+        self.bucket, self.prefix = uri[5:].split(&quot;/&quot;, 1)</span>
<span class="gi">+</span>
<span class="gi">+    def stat_file(self, path, info):</span>
<span class="gi">+        def _onsuccess(boto_key):</span>
<span class="gi">+            checksum = boto_key[&quot;ETag&quot;].strip(&#39;&quot;&#39;)</span>
<span class="gi">+            last_modified = boto_key[&quot;LastModified&quot;]</span>
<span class="gi">+            modified_stamp = time.mktime(last_modified.timetuple())</span>
<span class="gi">+            return {&quot;checksum&quot;: checksum, &quot;last_modified&quot;: modified_stamp}</span>
<span class="gi">+</span>
<span class="gi">+        return self._get_boto_key(path).addCallback(_onsuccess)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_boto_key(self, path):</span>
<span class="gi">+        key_name = f&quot;{self.prefix}{path}&quot;</span>
<span class="gi">+        return threads.deferToThread(</span>
<span class="gi">+            self.s3_client.head_object, Bucket=self.bucket, Key=key_name</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def persist_file(self, path, buf, info, meta=None, headers=None):
<span class="w"> </span>        &quot;&quot;&quot;Upload file to S3 storage&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        key_name = f&quot;{self.prefix}{path}&quot;</span>
<span class="gi">+        buf.seek(0)</span>
<span class="gi">+        extra = self._headers_to_botocore_kwargs(self.HEADERS)</span>
<span class="gi">+        if headers:</span>
<span class="gi">+            extra.update(self._headers_to_botocore_kwargs(headers))</span>
<span class="gi">+        return threads.deferToThread(</span>
<span class="gi">+            self.s3_client.put_object,</span>
<span class="gi">+            Bucket=self.bucket,</span>
<span class="gi">+            Key=key_name,</span>
<span class="gi">+            Body=buf,</span>
<span class="gi">+            Metadata={k: str(v) for k, v in (meta or {}).items()},</span>
<span class="gi">+            ACL=self.POLICY,</span>
<span class="gi">+            **extra,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _headers_to_botocore_kwargs(self, headers):
<span class="w"> </span>        &quot;&quot;&quot;Convert headers to botocore keyword arguments.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # This is required while we need to support both boto and botocore.</span>
<span class="gi">+        mapping = CaseInsensitiveDict(</span>
<span class="gi">+            {</span>
<span class="gi">+                &quot;Content-Type&quot;: &quot;ContentType&quot;,</span>
<span class="gi">+                &quot;Cache-Control&quot;: &quot;CacheControl&quot;,</span>
<span class="gi">+                &quot;Content-Disposition&quot;: &quot;ContentDisposition&quot;,</span>
<span class="gi">+                &quot;Content-Encoding&quot;: &quot;ContentEncoding&quot;,</span>
<span class="gi">+                &quot;Content-Language&quot;: &quot;ContentLanguage&quot;,</span>
<span class="gi">+                &quot;Content-Length&quot;: &quot;ContentLength&quot;,</span>
<span class="gi">+                &quot;Content-MD5&quot;: &quot;ContentMD5&quot;,</span>
<span class="gi">+                &quot;Expires&quot;: &quot;Expires&quot;,</span>
<span class="gi">+                &quot;X-Amz-Grant-Full-Control&quot;: &quot;GrantFullControl&quot;,</span>
<span class="gi">+                &quot;X-Amz-Grant-Read&quot;: &quot;GrantRead&quot;,</span>
<span class="gi">+                &quot;X-Amz-Grant-Read-ACP&quot;: &quot;GrantReadACP&quot;,</span>
<span class="gi">+                &quot;X-Amz-Grant-Write-ACP&quot;: &quot;GrantWriteACP&quot;,</span>
<span class="gi">+                &quot;X-Amz-Object-Lock-Legal-Hold&quot;: &quot;ObjectLockLegalHoldStatus&quot;,</span>
<span class="gi">+                &quot;X-Amz-Object-Lock-Mode&quot;: &quot;ObjectLockMode&quot;,</span>
<span class="gi">+                &quot;X-Amz-Object-Lock-Retain-Until-Date&quot;: &quot;ObjectLockRetainUntilDate&quot;,</span>
<span class="gi">+                &quot;X-Amz-Request-Payer&quot;: &quot;RequestPayer&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption&quot;: &quot;ServerSideEncryption&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id&quot;: &quot;SSEKMSKeyId&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption-Context&quot;: &quot;SSEKMSEncryptionContext&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption-Customer-Algorithm&quot;: &quot;SSECustomerAlgorithm&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption-Customer-Key&quot;: &quot;SSECustomerKey&quot;,</span>
<span class="gi">+                &quot;X-Amz-Server-Side-Encryption-Customer-Key-Md5&quot;: &quot;SSECustomerKeyMD5&quot;,</span>
<span class="gi">+                &quot;X-Amz-Storage-Class&quot;: &quot;StorageClass&quot;,</span>
<span class="gi">+                &quot;X-Amz-Tagging&quot;: &quot;Tagging&quot;,</span>
<span class="gi">+                &quot;X-Amz-Website-Redirect-Location&quot;: &quot;WebsiteRedirectLocation&quot;,</span>
<span class="gi">+            }</span>
<span class="gi">+        )</span>
<span class="gi">+        extra = {}</span>
<span class="gi">+        for key, value in headers.items():</span>
<span class="gi">+            try:</span>
<span class="gi">+                kwarg = mapping[key]</span>
<span class="gi">+            except KeyError:</span>
<span class="gi">+                raise TypeError(f&#39;Header &quot;{key}&quot; is not supported by botocore&#39;)</span>
<span class="gi">+            else:</span>
<span class="gi">+                extra[kwarg] = value</span>
<span class="gi">+        return extra</span>


<span class="w"> </span>class GCSFilesStore:
<span class="w"> </span>    GCS_PROJECT_ID = None
<span class="gd">-    CACHE_CONTROL = &#39;max-age=172800&#39;</span>
<span class="gi">+</span>
<span class="gi">+    CACHE_CONTROL = &quot;max-age=172800&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # The bucket&#39;s default object ACL will be applied to the object.</span>
<span class="gi">+    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.</span>
<span class="w"> </span>    POLICY = None

<span class="w"> </span>    def __init__(self, uri):
<span class="w"> </span>        from google.cloud import storage
<span class="gi">+</span>
<span class="w"> </span>        client = storage.Client(project=self.GCS_PROJECT_ID)
<span class="gd">-        bucket, prefix = uri[5:].split(&#39;/&#39;, 1)</span>
<span class="gi">+        bucket, prefix = uri[5:].split(&quot;/&quot;, 1)</span>
<span class="w"> </span>        self.bucket = client.bucket(bucket)
<span class="w"> </span>        self.prefix = prefix
<span class="gd">-        permissions = self.bucket.test_iam_permissions([</span>
<span class="gd">-            &#39;storage.objects.get&#39;, &#39;storage.objects.create&#39;])</span>
<span class="gd">-        if &#39;storage.objects.get&#39; not in permissions:</span>
<span class="gi">+        permissions = self.bucket.test_iam_permissions(</span>
<span class="gi">+            [&quot;storage.objects.get&quot;, &quot;storage.objects.create&quot;]</span>
<span class="gi">+        )</span>
<span class="gi">+        if &quot;storage.objects.get&quot; not in permissions:</span>
<span class="w"> </span>            logger.warning(
<span class="gd">-                &quot;No &#39;storage.objects.get&#39; permission for GSC bucket %(bucket)s. Checking if files are up to date will be impossible. Files will be downloaded every time.&quot;</span>
<span class="gd">-                , {&#39;bucket&#39;: bucket})</span>
<span class="gd">-        if &#39;storage.objects.create&#39; not in permissions:</span>
<span class="gi">+                &quot;No &#39;storage.objects.get&#39; permission for GSC bucket %(bucket)s. &quot;</span>
<span class="gi">+                &quot;Checking if files are up to date will be impossible. Files will be downloaded every time.&quot;,</span>
<span class="gi">+                {&quot;bucket&quot;: bucket},</span>
<span class="gi">+            )</span>
<span class="gi">+        if &quot;storage.objects.create&quot; not in permissions:</span>
<span class="w"> </span>            logger.error(
<span class="gd">-                &quot;No &#39;storage.objects.create&#39; permission for GSC bucket %(bucket)s. Saving files will be impossible!&quot;</span>
<span class="gd">-                , {&#39;bucket&#39;: bucket})</span>
<span class="gi">+                &quot;No &#39;storage.objects.create&#39; permission for GSC bucket %(bucket)s. Saving files will be impossible!&quot;,</span>
<span class="gi">+                {&quot;bucket&quot;: bucket},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def stat_file(self, path, info):</span>
<span class="gi">+        def _onsuccess(blob):</span>
<span class="gi">+            if blob:</span>
<span class="gi">+                checksum = base64.b64decode(blob.md5_hash).hex()</span>
<span class="gi">+                last_modified = time.mktime(blob.updated.timetuple())</span>
<span class="gi">+                return {&quot;checksum&quot;: checksum, &quot;last_modified&quot;: last_modified}</span>
<span class="gi">+            return {}</span>
<span class="gi">+</span>
<span class="gi">+        blob_path = self._get_blob_path(path)</span>
<span class="gi">+        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(</span>
<span class="gi">+            _onsuccess</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _get_content_type(self, headers):</span>
<span class="gi">+        if headers and &quot;Content-Type&quot; in headers:</span>
<span class="gi">+            return headers[&quot;Content-Type&quot;]</span>
<span class="gi">+        return &quot;application/octet-stream&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def _get_blob_path(self, path):</span>
<span class="gi">+        return self.prefix + path</span>
<span class="gi">+</span>
<span class="gi">+    def persist_file(self, path, buf, info, meta=None, headers=None):</span>
<span class="gi">+        blob_path = self._get_blob_path(path)</span>
<span class="gi">+        blob = self.bucket.blob(blob_path)</span>
<span class="gi">+        blob.cache_control = self.CACHE_CONTROL</span>
<span class="gi">+        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}</span>
<span class="gi">+        return threads.deferToThread(</span>
<span class="gi">+            blob.upload_from_string,</span>
<span class="gi">+            data=buf.getvalue(),</span>
<span class="gi">+            content_type=self._get_content_type(headers),</span>
<span class="gi">+            predefined_acl=self.POLICY,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class FTPFilesStore:
<span class="gu">@@ -113,7 +266,7 @@ class FTPFilesStore:</span>
<span class="w"> </span>    USE_ACTIVE_MODE = None

<span class="w"> </span>    def __init__(self, uri):
<span class="gd">-        if not uri.startswith(&#39;ftp://&#39;):</span>
<span class="gi">+        if not uri.startswith(&quot;ftp://&quot;):</span>
<span class="w"> </span>            raise ValueError(f&quot;Incorrect URI scheme in {uri}, expected &#39;ftp&#39;&quot;)
<span class="w"> </span>        u = urlparse(uri)
<span class="w"> </span>        self.port = u.port
<span class="gu">@@ -121,7 +274,39 @@ class FTPFilesStore:</span>
<span class="w"> </span>        self.port = int(u.port or 21)
<span class="w"> </span>        self.username = u.username or self.FTP_USERNAME
<span class="w"> </span>        self.password = u.password or self.FTP_PASSWORD
<span class="gd">-        self.basedir = u.path.rstrip(&#39;/&#39;)</span>
<span class="gi">+        self.basedir = u.path.rstrip(&quot;/&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def persist_file(self, path, buf, info, meta=None, headers=None):</span>
<span class="gi">+        path = f&quot;{self.basedir}/{path}&quot;</span>
<span class="gi">+        return threads.deferToThread(</span>
<span class="gi">+            ftp_store_file,</span>
<span class="gi">+            path=path,</span>
<span class="gi">+            file=buf,</span>
<span class="gi">+            host=self.host,</span>
<span class="gi">+            port=self.port,</span>
<span class="gi">+            username=self.username,</span>
<span class="gi">+            password=self.password,</span>
<span class="gi">+            use_active_mode=self.USE_ACTIVE_MODE,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def stat_file(self, path, info):</span>
<span class="gi">+        def _stat_file(path):</span>
<span class="gi">+            try:</span>
<span class="gi">+                ftp = FTP()</span>
<span class="gi">+                ftp.connect(self.host, self.port)</span>
<span class="gi">+                ftp.login(self.username, self.password)</span>
<span class="gi">+                if self.USE_ACTIVE_MODE:</span>
<span class="gi">+                    ftp.set_pasv(False)</span>
<span class="gi">+                file_path = f&quot;{self.basedir}/{path}&quot;</span>
<span class="gi">+                last_modified = float(ftp.voidcmd(f&quot;MDTM {file_path}&quot;)[4:].strip())</span>
<span class="gi">+                m = hashlib.md5()</span>
<span class="gi">+                ftp.retrbinary(f&quot;RETR {file_path}&quot;, m.update)</span>
<span class="gi">+                return {&quot;last_modified&quot;: last_modified, &quot;checksum&quot;: m.hexdigest()}</span>
<span class="gi">+            # The file doesn&#39;t exist</span>
<span class="gi">+            except Exception:</span>
<span class="gi">+                return {}</span>
<span class="gi">+</span>
<span class="gi">+        return threads.deferToThread(_stat_file, path)</span>


<span class="w"> </span>class FilesPipeline(MediaPipeline):
<span class="gu">@@ -142,30 +327,226 @@ class FilesPipeline(MediaPipeline):</span>
<span class="w"> </span>        refresh it in case of change.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    MEDIA_NAME = &#39;file&#39;</span>
<span class="gi">+</span>
<span class="gi">+    MEDIA_NAME = &quot;file&quot;</span>
<span class="w"> </span>    EXPIRES = 90
<span class="gd">-    STORE_SCHEMES = {&#39;&#39;: FSFilesStore, &#39;file&#39;: FSFilesStore, &#39;s3&#39;:</span>
<span class="gd">-        S3FilesStore, &#39;gs&#39;: GCSFilesStore, &#39;ftp&#39;: FTPFilesStore}</span>
<span class="gd">-    DEFAULT_FILES_URLS_FIELD = &#39;file_urls&#39;</span>
<span class="gd">-    DEFAULT_FILES_RESULT_FIELD = &#39;files&#39;</span>
<span class="gi">+    STORE_SCHEMES = {</span>
<span class="gi">+        &quot;&quot;: FSFilesStore,</span>
<span class="gi">+        &quot;file&quot;: FSFilesStore,</span>
<span class="gi">+        &quot;s3&quot;: S3FilesStore,</span>
<span class="gi">+        &quot;gs&quot;: GCSFilesStore,</span>
<span class="gi">+        &quot;ftp&quot;: FTPFilesStore,</span>
<span class="gi">+    }</span>
<span class="gi">+    DEFAULT_FILES_URLS_FIELD = &quot;file_urls&quot;</span>
<span class="gi">+    DEFAULT_FILES_RESULT_FIELD = &quot;files&quot;</span>

<span class="w"> </span>    def __init__(self, store_uri, download_func=None, settings=None):
<span class="w"> </span>        store_uri = _to_string(store_uri)
<span class="w"> </span>        if not store_uri:
<span class="w"> </span>            raise NotConfigured
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(settings, dict) or settings is None:
<span class="w"> </span>            settings = Settings(settings)
<span class="gd">-        cls_name = &#39;FilesPipeline&#39;</span>
<span class="gi">+        cls_name = &quot;FilesPipeline&quot;</span>
<span class="w"> </span>        self.store = self._get_store(store_uri)
<span class="gd">-        resolve = functools.partial(self._key_for_pipe, base_class_name=</span>
<span class="gd">-            cls_name, settings=settings)</span>
<span class="gd">-        self.expires = settings.getint(resolve(&#39;FILES_EXPIRES&#39;), self.EXPIRES)</span>
<span class="gd">-        if not hasattr(self, &#39;FILES_URLS_FIELD&#39;):</span>
<span class="gi">+        resolve = functools.partial(</span>
<span class="gi">+            self._key_for_pipe, base_class_name=cls_name, settings=settings</span>
<span class="gi">+        )</span>
<span class="gi">+        self.expires = settings.getint(resolve(&quot;FILES_EXPIRES&quot;), self.EXPIRES)</span>
<span class="gi">+        if not hasattr(self, &quot;FILES_URLS_FIELD&quot;):</span>
<span class="w"> </span>            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD
<span class="gd">-        if not hasattr(self, &#39;FILES_RESULT_FIELD&#39;):</span>
<span class="gi">+        if not hasattr(self, &quot;FILES_RESULT_FIELD&quot;):</span>
<span class="w"> </span>            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD
<span class="gd">-        self.files_urls_field = settings.get(resolve(&#39;FILES_URLS_FIELD&#39;),</span>
<span class="gd">-            self.FILES_URLS_FIELD)</span>
<span class="gd">-        self.files_result_field = settings.get(resolve(&#39;FILES_RESULT_FIELD&#39;</span>
<span class="gd">-            ), self.FILES_RESULT_FIELD)</span>
<span class="gi">+        self.files_urls_field = settings.get(</span>
<span class="gi">+            resolve(&quot;FILES_URLS_FIELD&quot;), self.FILES_URLS_FIELD</span>
<span class="gi">+        )</span>
<span class="gi">+        self.files_result_field = settings.get(</span>
<span class="gi">+            resolve(&quot;FILES_RESULT_FIELD&quot;), self.FILES_RESULT_FIELD</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>        super().__init__(download_func=download_func, settings=settings)
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings):</span>
<span class="gi">+        s3store = cls.STORE_SCHEMES[&quot;s3&quot;]</span>
<span class="gi">+        s3store.AWS_ACCESS_KEY_ID = settings[&quot;AWS_ACCESS_KEY_ID&quot;]</span>
<span class="gi">+        s3store.AWS_SECRET_ACCESS_KEY = settings[&quot;AWS_SECRET_ACCESS_KEY&quot;]</span>
<span class="gi">+        s3store.AWS_SESSION_TOKEN = settings[&quot;AWS_SESSION_TOKEN&quot;]</span>
<span class="gi">+        s3store.AWS_ENDPOINT_URL = settings[&quot;AWS_ENDPOINT_URL&quot;]</span>
<span class="gi">+        s3store.AWS_REGION_NAME = settings[&quot;AWS_REGION_NAME&quot;]</span>
<span class="gi">+        s3store.AWS_USE_SSL = settings[&quot;AWS_USE_SSL&quot;]</span>
<span class="gi">+        s3store.AWS_VERIFY = settings[&quot;AWS_VERIFY&quot;]</span>
<span class="gi">+        s3store.POLICY = settings[&quot;FILES_STORE_S3_ACL&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        gcs_store = cls.STORE_SCHEMES[&quot;gs&quot;]</span>
<span class="gi">+        gcs_store.GCS_PROJECT_ID = settings[&quot;GCS_PROJECT_ID&quot;]</span>
<span class="gi">+        gcs_store.POLICY = settings[&quot;FILES_STORE_GCS_ACL&quot;] or None</span>
<span class="gi">+</span>
<span class="gi">+        ftp_store = cls.STORE_SCHEMES[&quot;ftp&quot;]</span>
<span class="gi">+        ftp_store.FTP_USERNAME = settings[&quot;FTP_USER&quot;]</span>
<span class="gi">+        ftp_store.FTP_PASSWORD = settings[&quot;FTP_PASSWORD&quot;]</span>
<span class="gi">+        ftp_store.USE_ACTIVE_MODE = settings.getbool(&quot;FEED_STORAGE_FTP_ACTIVE&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        store_uri = settings[&quot;FILES_STORE&quot;]</span>
<span class="gi">+        return cls(store_uri, settings=settings)</span>
<span class="gi">+</span>
<span class="gi">+    def _get_store(self, uri: str):</span>
<span class="gi">+        if Path(uri).is_absolute():  # to support win32 paths like: C:\\some\dir</span>
<span class="gi">+            scheme = &quot;file&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            scheme = urlparse(uri).scheme</span>
<span class="gi">+        store_cls = self.STORE_SCHEMES[scheme]</span>
<span class="gi">+        return store_cls(uri)</span>
<span class="gi">+</span>
<span class="gi">+    def media_to_download(self, request, info, *, item=None):</span>
<span class="gi">+        def _onsuccess(result):</span>
<span class="gi">+            if not result:</span>
<span class="gi">+                return  # returning None force download</span>
<span class="gi">+</span>
<span class="gi">+            last_modified = result.get(&quot;last_modified&quot;, None)</span>
<span class="gi">+            if not last_modified:</span>
<span class="gi">+                return  # returning None force download</span>
<span class="gi">+</span>
<span class="gi">+            age_seconds = time.time() - last_modified</span>
<span class="gi">+            age_days = age_seconds / 60 / 60 / 24</span>
<span class="gi">+            if age_days &gt; self.expires:</span>
<span class="gi">+                return  # returning None force download</span>
<span class="gi">+</span>
<span class="gi">+            referer = referer_str(request)</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;File (uptodate): Downloaded %(medianame)s from %(request)s &quot;</span>
<span class="gi">+                &quot;referred in &lt;%(referer)s&gt;&quot;,</span>
<span class="gi">+                {&quot;medianame&quot;: self.MEDIA_NAME, &quot;request&quot;: request, &quot;referer&quot;: referer},</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            self.inc_stats(info.spider, &quot;uptodate&quot;)</span>
<span class="gi">+</span>
<span class="gi">+            checksum = result.get(&quot;checksum&quot;, None)</span>
<span class="gi">+            return {</span>
<span class="gi">+                &quot;url&quot;: request.url,</span>
<span class="gi">+                &quot;path&quot;: path,</span>
<span class="gi">+                &quot;checksum&quot;: checksum,</span>
<span class="gi">+                &quot;status&quot;: &quot;uptodate&quot;,</span>
<span class="gi">+            }</span>
<span class="gi">+</span>
<span class="gi">+        path = self.file_path(request, info=info, item=item)</span>
<span class="gi">+        dfd = defer.maybeDeferred(self.store.stat_file, path, info)</span>
<span class="gi">+        dfd.addCallbacks(_onsuccess, lambda _: None)</span>
<span class="gi">+        dfd.addErrback(</span>
<span class="gi">+            lambda f: logger.error(</span>
<span class="gi">+                self.__class__.__name__ + &quot;.store.stat_file&quot;,</span>
<span class="gi">+                exc_info=failure_to_exc_info(f),</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def media_failed(self, failure, request, info):</span>
<span class="gi">+        if not isinstance(failure.value, IgnoreRequest):</span>
<span class="gi">+            referer = referer_str(request)</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;File (unknown-error): Error downloading %(medianame)s from &quot;</span>
<span class="gi">+                &quot;%(request)s referred in &lt;%(referer)s&gt;: %(exception)s&quot;,</span>
<span class="gi">+                {</span>
<span class="gi">+                    &quot;medianame&quot;: self.MEDIA_NAME,</span>
<span class="gi">+                    &quot;request&quot;: request,</span>
<span class="gi">+                    &quot;referer&quot;: referer,</span>
<span class="gi">+                    &quot;exception&quot;: failure.value,</span>
<span class="gi">+                },</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        raise FileException</span>
<span class="gi">+</span>
<span class="gi">+    def media_downloaded(self, response, request, info, *, item=None):</span>
<span class="gi">+        referer = referer_str(request)</span>
<span class="gi">+</span>
<span class="gi">+        if response.status != 200:</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;File (code: %(status)s): Error downloading file from &quot;</span>
<span class="gi">+                &quot;%(request)s referred in &lt;%(referer)s&gt;&quot;,</span>
<span class="gi">+                {&quot;status&quot;: response.status, &quot;request&quot;: request, &quot;referer&quot;: referer},</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            raise FileException(&quot;download-error&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if not response.body:</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;File (empty-content): Empty file from %(request)s referred &quot;</span>
<span class="gi">+                &quot;in &lt;%(referer)s&gt;: no-content&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request, &quot;referer&quot;: referer},</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            raise FileException(&quot;empty-content&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        status = &quot;cached&quot; if &quot;cached&quot; in response.flags else &quot;downloaded&quot;</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;File (%(status)s): Downloaded file from %(request)s referred in &quot;</span>
<span class="gi">+            &quot;&lt;%(referer)s&gt;&quot;,</span>
<span class="gi">+            {&quot;status&quot;: status, &quot;request&quot;: request, &quot;referer&quot;: referer},</span>
<span class="gi">+            extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        self.inc_stats(info.spider, status)</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            path = self.file_path(request, response=response, info=info, item=item)</span>
<span class="gi">+            checksum = self.file_downloaded(response, request, info, item=item)</span>
<span class="gi">+        except FileException as exc:</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;File (error): Error processing file from %(request)s &quot;</span>
<span class="gi">+                &quot;referred in &lt;%(referer)s&gt;: %(errormsg)s&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request, &quot;referer&quot;: referer, &quot;errormsg&quot;: str(exc)},</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+                exc_info=True,</span>
<span class="gi">+            )</span>
<span class="gi">+            raise</span>
<span class="gi">+        except Exception as exc:</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;File (unknown-error): Error processing file from %(request)s &quot;</span>
<span class="gi">+                &quot;referred in &lt;%(referer)s&gt;&quot;,</span>
<span class="gi">+                {&quot;request&quot;: request, &quot;referer&quot;: referer},</span>
<span class="gi">+                exc_info=True,</span>
<span class="gi">+                extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            raise FileException(str(exc))</span>
<span class="gi">+</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;url&quot;: request.url,</span>
<span class="gi">+            &quot;path&quot;: path,</span>
<span class="gi">+            &quot;checksum&quot;: checksum,</span>
<span class="gi">+            &quot;status&quot;: status,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    def inc_stats(self, spider, status):</span>
<span class="gi">+        spider.crawler.stats.inc_value(&quot;file_count&quot;, spider=spider)</span>
<span class="gi">+        spider.crawler.stats.inc_value(f&quot;file_status_count/{status}&quot;, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    # Overridable Interface</span>
<span class="gi">+    def get_media_requests(self, item, info):</span>
<span class="gi">+        urls = ItemAdapter(item).get(self.files_urls_field, [])</span>
<span class="gi">+        return [Request(u, callback=NO_CALLBACK) for u in urls]</span>
<span class="gi">+</span>
<span class="gi">+    def file_downloaded(self, response, request, info, *, item=None):</span>
<span class="gi">+        path = self.file_path(request, response=response, info=info, item=item)</span>
<span class="gi">+        buf = BytesIO(response.body)</span>
<span class="gi">+        checksum = md5sum(buf)</span>
<span class="gi">+        buf.seek(0)</span>
<span class="gi">+        self.store.persist_file(path, buf, info)</span>
<span class="gi">+        return checksum</span>
<span class="gi">+</span>
<span class="gi">+    def item_completed(self, results, item, info):</span>
<span class="gi">+        with suppress(KeyError):</span>
<span class="gi">+            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]</span>
<span class="gi">+        return item</span>
<span class="gi">+</span>
<span class="gi">+    def file_path(self, request, response=None, info=None, *, item=None):</span>
<span class="gi">+        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()</span>
<span class="gi">+        media_ext = Path(request.url).suffix</span>
<span class="gi">+        # Handles empty and wild extensions by trying to guess the</span>
<span class="gi">+        # mime type then extension or default to empty string otherwise</span>
<span class="gi">+        if media_ext not in mimetypes.types_map:</span>
<span class="gi">+            media_ext = &quot;&quot;</span>
<span class="gi">+            media_type = mimetypes.guess_type(request.url)[0]</span>
<span class="gi">+            if media_type:</span>
<span class="gi">+                media_ext = mimetypes.guess_extension(media_type)</span>
<span class="gi">+        return f&quot;full/{media_guid}{media_ext}&quot;</span>
<span class="gh">diff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py</span>
<span class="gh">index 4c93a08c0..9d18144ee 100644</span>
<span class="gd">--- a/scrapy/pipelines/images.py</span>
<span class="gi">+++ b/scrapy/pipelines/images.py</span>
<span class="gu">@@ -8,11 +8,15 @@ import hashlib</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from contextlib import suppress
<span class="w"> </span>from io import BytesIO
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.http.request import NO_CALLBACK
<span class="w"> </span>from scrapy.pipelines.files import FileException, FilesPipeline
<span class="gi">+</span>
<span class="gi">+# TODO: from scrapy.pipelines.media import MediaPipeline</span>
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.misc import md5sum
<span class="w"> </span>from scrapy.utils.python import get_func_args, to_bytes
<span class="gu">@@ -22,8 +26,11 @@ class NoimagesDrop(DropItem):</span>
<span class="w"> </span>    &quot;&quot;&quot;Product with no images exception&quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, *args, **kwargs):
<span class="gd">-        warnings.warn(&#39;The NoimagesDrop class is deprecated&#39;, category=</span>
<span class="gd">-            ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;The NoimagesDrop class is deprecated&quot;,</span>
<span class="gi">+            category=ScrapyDeprecationWarning,</span>
<span class="gi">+            stacklevel=2,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)


<span class="gu">@@ -33,39 +40,192 @@ class ImageException(FileException):</span>

<span class="w"> </span>class ImagesPipeline(FilesPipeline):
<span class="w"> </span>    &quot;&quot;&quot;Abstract pipeline that implement the image thumbnail generation logic&quot;&quot;&quot;
<span class="gd">-    MEDIA_NAME = &#39;image&#39;</span>
<span class="gi">+</span>
<span class="gi">+    MEDIA_NAME = &quot;image&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # Uppercase attributes kept for backward compatibility with code that subclasses</span>
<span class="gi">+    # ImagesPipeline. They may be overridden by settings.</span>
<span class="w"> </span>    MIN_WIDTH = 0
<span class="w"> </span>    MIN_HEIGHT = 0
<span class="w"> </span>    EXPIRES = 90
<span class="w"> </span>    THUMBS = {}
<span class="gd">-    DEFAULT_IMAGES_URLS_FIELD = &#39;image_urls&#39;</span>
<span class="gd">-    DEFAULT_IMAGES_RESULT_FIELD = &#39;images&#39;</span>
<span class="gi">+    DEFAULT_IMAGES_URLS_FIELD = &quot;image_urls&quot;</span>
<span class="gi">+    DEFAULT_IMAGES_RESULT_FIELD = &quot;images&quot;</span>

<span class="w"> </span>    def __init__(self, store_uri, download_func=None, settings=None):
<span class="w"> </span>        try:
<span class="w"> </span>            from PIL import Image
<span class="gi">+</span>
<span class="w"> </span>            self._Image = Image
<span class="w"> </span>        except ImportError:
<span class="w"> </span>            raise NotConfigured(
<span class="gd">-                &#39;ImagesPipeline requires installing Pillow 4.0.0 or later&#39;)</span>
<span class="gd">-        super().__init__(store_uri, settings=settings, download_func=</span>
<span class="gd">-            download_func)</span>
<span class="gi">+                &quot;ImagesPipeline requires installing Pillow 4.0.0 or later&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        super().__init__(store_uri, settings=settings, download_func=download_func)</span>
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(settings, dict) or settings is None:
<span class="w"> </span>            settings = Settings(settings)
<span class="gd">-        resolve = functools.partial(self._key_for_pipe, base_class_name=</span>
<span class="gd">-            &#39;ImagesPipeline&#39;, settings=settings)</span>
<span class="gd">-        self.expires = settings.getint(resolve(&#39;IMAGES_EXPIRES&#39;), self.EXPIRES)</span>
<span class="gd">-        if not hasattr(self, &#39;IMAGES_RESULT_FIELD&#39;):</span>
<span class="gi">+</span>
<span class="gi">+        resolve = functools.partial(</span>
<span class="gi">+            self._key_for_pipe,</span>
<span class="gi">+            base_class_name=&quot;ImagesPipeline&quot;,</span>
<span class="gi">+            settings=settings,</span>
<span class="gi">+        )</span>
<span class="gi">+        self.expires = settings.getint(resolve(&quot;IMAGES_EXPIRES&quot;), self.EXPIRES)</span>
<span class="gi">+</span>
<span class="gi">+        if not hasattr(self, &quot;IMAGES_RESULT_FIELD&quot;):</span>
<span class="w"> </span>            self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD
<span class="gd">-        if not hasattr(self, &#39;IMAGES_URLS_FIELD&#39;):</span>
<span class="gi">+        if not hasattr(self, &quot;IMAGES_URLS_FIELD&quot;):</span>
<span class="w"> </span>            self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD
<span class="gd">-        self.images_urls_field = settings.get(resolve(&#39;IMAGES_URLS_FIELD&#39;),</span>
<span class="gd">-            self.IMAGES_URLS_FIELD)</span>
<span class="gd">-        self.images_result_field = settings.get(resolve(</span>
<span class="gd">-            &#39;IMAGES_RESULT_FIELD&#39;), self.IMAGES_RESULT_FIELD)</span>
<span class="gd">-        self.min_width = settings.getint(resolve(&#39;IMAGES_MIN_WIDTH&#39;), self.</span>
<span class="gd">-            MIN_WIDTH)</span>
<span class="gd">-        self.min_height = settings.getint(resolve(&#39;IMAGES_MIN_HEIGHT&#39;),</span>
<span class="gd">-            self.MIN_HEIGHT)</span>
<span class="gd">-        self.thumbs = settings.get(resolve(&#39;IMAGES_THUMBS&#39;), self.THUMBS)</span>
<span class="gi">+</span>
<span class="gi">+        self.images_urls_field = settings.get(</span>
<span class="gi">+            resolve(&quot;IMAGES_URLS_FIELD&quot;), self.IMAGES_URLS_FIELD</span>
<span class="gi">+        )</span>
<span class="gi">+        self.images_result_field = settings.get(</span>
<span class="gi">+            resolve(&quot;IMAGES_RESULT_FIELD&quot;), self.IMAGES_RESULT_FIELD</span>
<span class="gi">+        )</span>
<span class="gi">+        self.min_width = settings.getint(resolve(&quot;IMAGES_MIN_WIDTH&quot;), self.MIN_WIDTH)</span>
<span class="gi">+        self.min_height = settings.getint(resolve(&quot;IMAGES_MIN_HEIGHT&quot;), self.MIN_HEIGHT)</span>
<span class="gi">+        self.thumbs = settings.get(resolve(&quot;IMAGES_THUMBS&quot;), self.THUMBS)</span>
<span class="gi">+</span>
<span class="w"> </span>        self._deprecated_convert_image = None
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings):</span>
<span class="gi">+        s3store = cls.STORE_SCHEMES[&quot;s3&quot;]</span>
<span class="gi">+        s3store.AWS_ACCESS_KEY_ID = settings[&quot;AWS_ACCESS_KEY_ID&quot;]</span>
<span class="gi">+        s3store.AWS_SECRET_ACCESS_KEY = settings[&quot;AWS_SECRET_ACCESS_KEY&quot;]</span>
<span class="gi">+        s3store.AWS_SESSION_TOKEN = settings[&quot;AWS_SESSION_TOKEN&quot;]</span>
<span class="gi">+        s3store.AWS_ENDPOINT_URL = settings[&quot;AWS_ENDPOINT_URL&quot;]</span>
<span class="gi">+        s3store.AWS_REGION_NAME = settings[&quot;AWS_REGION_NAME&quot;]</span>
<span class="gi">+        s3store.AWS_USE_SSL = settings[&quot;AWS_USE_SSL&quot;]</span>
<span class="gi">+        s3store.AWS_VERIFY = settings[&quot;AWS_VERIFY&quot;]</span>
<span class="gi">+        s3store.POLICY = settings[&quot;IMAGES_STORE_S3_ACL&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        gcs_store = cls.STORE_SCHEMES[&quot;gs&quot;]</span>
<span class="gi">+        gcs_store.GCS_PROJECT_ID = settings[&quot;GCS_PROJECT_ID&quot;]</span>
<span class="gi">+        gcs_store.POLICY = settings[&quot;IMAGES_STORE_GCS_ACL&quot;] or None</span>
<span class="gi">+</span>
<span class="gi">+        ftp_store = cls.STORE_SCHEMES[&quot;ftp&quot;]</span>
<span class="gi">+        ftp_store.FTP_USERNAME = settings[&quot;FTP_USER&quot;]</span>
<span class="gi">+        ftp_store.FTP_PASSWORD = settings[&quot;FTP_PASSWORD&quot;]</span>
<span class="gi">+        ftp_store.USE_ACTIVE_MODE = settings.getbool(&quot;FEED_STORAGE_FTP_ACTIVE&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        store_uri = settings[&quot;IMAGES_STORE&quot;]</span>
<span class="gi">+        return cls(store_uri, settings=settings)</span>
<span class="gi">+</span>
<span class="gi">+    def file_downloaded(self, response, request, info, *, item=None):</span>
<span class="gi">+        return self.image_downloaded(response, request, info, item=item)</span>
<span class="gi">+</span>
<span class="gi">+    def image_downloaded(self, response, request, info, *, item=None):</span>
<span class="gi">+        checksum = None</span>
<span class="gi">+        for path, image, buf in self.get_images(response, request, info, item=item):</span>
<span class="gi">+            if checksum is None:</span>
<span class="gi">+                buf.seek(0)</span>
<span class="gi">+                checksum = md5sum(buf)</span>
<span class="gi">+            width, height = image.size</span>
<span class="gi">+            self.store.persist_file(</span>
<span class="gi">+                path,</span>
<span class="gi">+                buf,</span>
<span class="gi">+                info,</span>
<span class="gi">+                meta={&quot;width&quot;: width, &quot;height&quot;: height},</span>
<span class="gi">+                headers={&quot;Content-Type&quot;: &quot;image/jpeg&quot;},</span>
<span class="gi">+            )</span>
<span class="gi">+        return checksum</span>
<span class="gi">+</span>
<span class="gi">+    def get_images(self, response, request, info, *, item=None):</span>
<span class="gi">+        path = self.file_path(request, response=response, info=info, item=item)</span>
<span class="gi">+        orig_image = self._Image.open(BytesIO(response.body))</span>
<span class="gi">+</span>
<span class="gi">+        width, height = orig_image.size</span>
<span class="gi">+        if width &lt; self.min_width or height &lt; self.min_height:</span>
<span class="gi">+            raise ImageException(</span>
<span class="gi">+                &quot;Image too small &quot;</span>
<span class="gi">+                f&quot;({width}x{height} &lt; &quot;</span>
<span class="gi">+                f&quot;{self.min_width}x{self.min_height})&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if self._deprecated_convert_image is None:</span>
<span class="gi">+            self._deprecated_convert_image = &quot;response_body&quot; not in get_func_args(</span>
<span class="gi">+                self.convert_image</span>
<span class="gi">+            )</span>
<span class="gi">+            if self._deprecated_convert_image:</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    f&quot;{self.__class__.__name__}.convert_image() method overridden in a deprecated way, &quot;</span>
<span class="gi">+                    &quot;overridden method does not accept response_body argument.&quot;,</span>
<span class="gi">+                    category=ScrapyDeprecationWarning,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        if self._deprecated_convert_image:</span>
<span class="gi">+            image, buf = self.convert_image(orig_image)</span>
<span class="gi">+        else:</span>
<span class="gi">+            image, buf = self.convert_image(</span>
<span class="gi">+                orig_image, response_body=BytesIO(response.body)</span>
<span class="gi">+            )</span>
<span class="gi">+        yield path, image, buf</span>
<span class="gi">+</span>
<span class="gi">+        for thumb_id, size in self.thumbs.items():</span>
<span class="gi">+            thumb_path = self.thumb_path(</span>
<span class="gi">+                request, thumb_id, response=response, info=info, item=item</span>
<span class="gi">+            )</span>
<span class="gi">+            if self._deprecated_convert_image:</span>
<span class="gi">+                thumb_image, thumb_buf = self.convert_image(image, size)</span>
<span class="gi">+            else:</span>
<span class="gi">+                thumb_image, thumb_buf = self.convert_image(image, size, buf)</span>
<span class="gi">+            yield thumb_path, thumb_image, thumb_buf</span>
<span class="gi">+</span>
<span class="gi">+    def convert_image(self, image, size=None, response_body=None):</span>
<span class="gi">+        if response_body is None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;{self.__class__.__name__}.convert_image() method called in a deprecated way, &quot;</span>
<span class="gi">+                &quot;method called without response_body argument.&quot;,</span>
<span class="gi">+                category=ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if image.format in (&quot;PNG&quot;, &quot;WEBP&quot;) and image.mode == &quot;RGBA&quot;:</span>
<span class="gi">+            background = self._Image.new(&quot;RGBA&quot;, image.size, (255, 255, 255))</span>
<span class="gi">+            background.paste(image, image)</span>
<span class="gi">+            image = background.convert(&quot;RGB&quot;)</span>
<span class="gi">+        elif image.mode == &quot;P&quot;:</span>
<span class="gi">+            image = image.convert(&quot;RGBA&quot;)</span>
<span class="gi">+            background = self._Image.new(&quot;RGBA&quot;, image.size, (255, 255, 255))</span>
<span class="gi">+            background.paste(image, image)</span>
<span class="gi">+            image = background.convert(&quot;RGB&quot;)</span>
<span class="gi">+        elif image.mode != &quot;RGB&quot;:</span>
<span class="gi">+            image = image.convert(&quot;RGB&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if size:</span>
<span class="gi">+            image = image.copy()</span>
<span class="gi">+            try:</span>
<span class="gi">+                # Image.Resampling.LANCZOS was added in Pillow 9.1.0</span>
<span class="gi">+                # remove this try except block,</span>
<span class="gi">+                # when updating the minimum requirements for Pillow.</span>
<span class="gi">+                resampling_filter = self._Image.Resampling.LANCZOS</span>
<span class="gi">+            except AttributeError:</span>
<span class="gi">+                resampling_filter = self._Image.ANTIALIAS</span>
<span class="gi">+            image.thumbnail(size, resampling_filter)</span>
<span class="gi">+        elif response_body is not None and image.format == &quot;JPEG&quot;:</span>
<span class="gi">+            return image, response_body</span>
<span class="gi">+</span>
<span class="gi">+        buf = BytesIO()</span>
<span class="gi">+        image.save(buf, &quot;JPEG&quot;)</span>
<span class="gi">+        return image, buf</span>
<span class="gi">+</span>
<span class="gi">+    def get_media_requests(self, item, info):</span>
<span class="gi">+        urls = ItemAdapter(item).get(self.images_urls_field, [])</span>
<span class="gi">+        return [Request(u, callback=NO_CALLBACK) for u in urls]</span>
<span class="gi">+</span>
<span class="gi">+    def item_completed(self, results, item, info):</span>
<span class="gi">+        with suppress(KeyError):</span>
<span class="gi">+            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]</span>
<span class="gi">+        return item</span>
<span class="gi">+</span>
<span class="gi">+    def file_path(self, request, response=None, info=None, *, item=None):</span>
<span class="gi">+        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()</span>
<span class="gi">+        return f&quot;full/{image_guid}.jpg&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):</span>
<span class="gi">+        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()</span>
<span class="gi">+        return f&quot;thumbs/{thumb_id}/{thumb_guid}.jpg&quot;</span>
<span class="gh">diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py</span>
<span class="gh">index f88dbcfe8..153047acf 100644</span>
<span class="gd">--- a/scrapy/pipelines/media.py</span>
<span class="gi">+++ b/scrapy/pipelines/media.py</span>
<span class="gu">@@ -3,8 +3,10 @@ import logging</span>
<span class="w"> </span>from collections import defaultdict
<span class="w"> </span>from inspect import signature
<span class="w"> </span>from warnings import warn
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred, DeferredList
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http.request import NO_CALLBACK
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.datatypes import SequenceExclude
<span class="gu">@@ -12,15 +14,18 @@ from scrapy.utils.defer import defer_result, mustbe_deferred</span>
<span class="w"> </span>from scrapy.utils.deprecate import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info
<span class="w"> </span>from scrapy.utils.misc import arg_to_iter
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gi">+def _DUMMY_CALLBACK(response):</span>
<span class="gi">+    return response</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class MediaPipeline:
<span class="w"> </span>    LOG_FAILED_RESULTS = True

<span class="gd">-</span>
<span class="w"> </span>    class SpiderInfo:
<span class="gd">-</span>
<span class="w"> </span>        def __init__(self, spider):
<span class="w"> </span>            self.spider = spider
<span class="w"> </span>            self.downloading = set()
<span class="gu">@@ -30,15 +35,23 @@ class MediaPipeline:</span>
<span class="w"> </span>    def __init__(self, download_func=None, settings=None):
<span class="w"> </span>        self.download_func = download_func
<span class="w"> </span>        self._expects_item = {}
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(settings, dict) or settings is None:
<span class="w"> </span>            settings = Settings(settings)
<span class="gd">-        resolve = functools.partial(self._key_for_pipe, base_class_name=</span>
<span class="gd">-            &#39;MediaPipeline&#39;, settings=settings)</span>
<span class="gd">-        self.allow_redirects = settings.getbool(resolve(</span>
<span class="gd">-            &#39;MEDIA_ALLOW_REDIRECTS&#39;), False)</span>
<span class="gi">+        resolve = functools.partial(</span>
<span class="gi">+            self._key_for_pipe, base_class_name=&quot;MediaPipeline&quot;, settings=settings</span>
<span class="gi">+        )</span>
<span class="gi">+        self.allow_redirects = settings.getbool(resolve(&quot;MEDIA_ALLOW_REDIRECTS&quot;), False)</span>
<span class="w"> </span>        self._handle_statuses(self.allow_redirects)
<span class="gi">+</span>
<span class="gi">+        # Check if deprecated methods are being used and make them compatible</span>
<span class="w"> </span>        self._make_compatible()

<span class="gi">+    def _handle_statuses(self, allow_redirects):</span>
<span class="gi">+        self.handle_httpstatus_list = None</span>
<span class="gi">+        if allow_redirects:</span>
<span class="gi">+            self.handle_httpstatus_list = SequenceExclude(range(300, 400))</span>
<span class="gi">+</span>
<span class="w"> </span>    def _key_for_pipe(self, key, base_class_name=None, settings=None):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        &gt;&gt;&gt; MediaPipeline()._key_for_pipe(&quot;IMAGES&quot;)
<span class="gu">@@ -48,16 +61,186 @@ class MediaPipeline:</span>
<span class="w"> </span>        &gt;&gt;&gt; MyPipe()._key_for_pipe(&quot;IMAGES&quot;, base_class_name=&quot;MediaPipeline&quot;)
<span class="w"> </span>        &#39;MYPIPE_IMAGES&#39;
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        class_name = self.__class__.__name__</span>
<span class="gi">+        formatted_key = f&quot;{class_name.upper()}_{key}&quot;</span>
<span class="gi">+        if (</span>
<span class="gi">+            not base_class_name</span>
<span class="gi">+            or class_name == base_class_name</span>
<span class="gi">+            or settings</span>
<span class="gi">+            and not settings.get(formatted_key)</span>
<span class="gi">+        ):</span>
<span class="gi">+            return key</span>
<span class="gi">+        return formatted_key</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        try:</span>
<span class="gi">+            pipe = cls.from_settings(crawler.settings)</span>
<span class="gi">+        except AttributeError:</span>
<span class="gi">+            pipe = cls()</span>
<span class="gi">+        pipe.crawler = crawler</span>
<span class="gi">+        pipe._fingerprinter = crawler.request_fingerprinter</span>
<span class="gi">+        return pipe</span>
<span class="gi">+</span>
<span class="gi">+    def open_spider(self, spider):</span>
<span class="gi">+        self.spiderinfo = self.SpiderInfo(spider)</span>
<span class="gi">+</span>
<span class="gi">+    def process_item(self, item, spider):</span>
<span class="gi">+        info = self.spiderinfo</span>
<span class="gi">+        requests = arg_to_iter(self.get_media_requests(item, info))</span>
<span class="gi">+        dlist = [self._process_request(r, info, item) for r in requests]</span>
<span class="gi">+        dfd = DeferredList(dlist, consumeErrors=True)</span>
<span class="gi">+        return dfd.addCallback(self.item_completed, item, info)</span>
<span class="gi">+</span>
<span class="gi">+    def _process_request(self, request, info, item):</span>
<span class="gi">+        fp = self._fingerprinter.fingerprint(request)</span>
<span class="gi">+        if not request.callback or request.callback is NO_CALLBACK:</span>
<span class="gi">+            cb = _DUMMY_CALLBACK</span>
<span class="gi">+        else:</span>
<span class="gi">+            cb = request.callback</span>
<span class="gi">+        eb = request.errback</span>
<span class="gi">+        request.callback = NO_CALLBACK</span>
<span class="gi">+        request.errback = None</span>
<span class="gi">+</span>
<span class="gi">+        # Return cached result if request was already seen</span>
<span class="gi">+        if fp in info.downloaded:</span>
<span class="gi">+            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)</span>
<span class="gi">+</span>
<span class="gi">+        # Otherwise, wait for result</span>
<span class="gi">+        wad = Deferred().addCallbacks(cb, eb)</span>
<span class="gi">+        info.waiting[fp].append(wad)</span>
<span class="gi">+</span>
<span class="gi">+        # Check if request is downloading right now to avoid doing it twice</span>
<span class="gi">+        if fp in info.downloading:</span>
<span class="gi">+            return wad</span>
<span class="gi">+</span>
<span class="gi">+        # Download request checking media_to_download hook output first</span>
<span class="gi">+        info.downloading.add(fp)</span>
<span class="gi">+        dfd = mustbe_deferred(self.media_to_download, request, info, item=item)</span>
<span class="gi">+        dfd.addCallback(self._check_media_to_download, request, info, item=item)</span>
<span class="gi">+        dfd.addErrback(self._log_exception)</span>
<span class="gi">+        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)</span>
<span class="gi">+        return dfd.addBoth(lambda _: wad)  # it must return wad at last</span>
<span class="gi">+</span>
<span class="gi">+    def _log_exception(self, result):</span>
<span class="gi">+        logger.exception(result)</span>
<span class="gi">+        return result</span>

<span class="w"> </span>    def _make_compatible(self):
<span class="w"> </span>        &quot;&quot;&quot;Make overridable methods of MediaPipeline and subclasses backwards compatible&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        methods = [</span>
<span class="gi">+            &quot;file_path&quot;,</span>
<span class="gi">+            &quot;thumb_path&quot;,</span>
<span class="gi">+            &quot;media_to_download&quot;,</span>
<span class="gi">+            &quot;media_downloaded&quot;,</span>
<span class="gi">+            &quot;file_downloaded&quot;,</span>
<span class="gi">+            &quot;image_downloaded&quot;,</span>
<span class="gi">+            &quot;get_images&quot;,</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+        for method_name in methods:</span>
<span class="gi">+            method = getattr(self, method_name, None)</span>
<span class="gi">+            if callable(method):</span>
<span class="gi">+                setattr(self, method_name, self._compatible(method))</span>

<span class="w"> </span>    def _compatible(self, func):
<span class="w"> </span>        &quot;&quot;&quot;Wrapper for overridable methods to allow backwards compatibility&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_signature(func)</span>
<span class="gi">+</span>
<span class="gi">+        @functools.wraps(func)</span>
<span class="gi">+        def wrapper(*args, **kwargs):</span>
<span class="gi">+            if self._expects_item[func.__name__]:</span>
<span class="gi">+                return func(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+            kwargs.pop(&quot;item&quot;, None)</span>
<span class="gi">+            return func(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+        return wrapper</span>
<span class="gi">+</span>
<span class="gi">+    def _check_signature(self, func):</span>
<span class="gi">+        sig = signature(func)</span>
<span class="gi">+        self._expects_item[func.__name__] = True</span>

<span class="gi">+        if &quot;item&quot; not in sig.parameters:</span>
<span class="gi">+            old_params = str(sig)[1:-1]</span>
<span class="gi">+            new_params = old_params + &quot;, *, item=None&quot;</span>
<span class="gi">+            warn(</span>
<span class="gi">+                f&quot;{func.__name__}(self, {old_params}) is deprecated, &quot;</span>
<span class="gi">+                f&quot;please use {func.__name__}(self, {new_params})&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._expects_item[func.__name__] = False</span>
<span class="gi">+</span>
<span class="gi">+    def _modify_media_request(self, request):</span>
<span class="gi">+        if self.handle_httpstatus_list:</span>
<span class="gi">+            request.meta[&quot;handle_httpstatus_list&quot;] = self.handle_httpstatus_list</span>
<span class="gi">+        else:</span>
<span class="gi">+            request.meta[&quot;handle_httpstatus_all&quot;] = True</span>
<span class="gi">+</span>
<span class="gi">+    def _check_media_to_download(self, result, request, info, item):</span>
<span class="gi">+        if result is not None:</span>
<span class="gi">+            return result</span>
<span class="gi">+        if self.download_func:</span>
<span class="gi">+            # this ugly code was left only to support tests. TODO: remove</span>
<span class="gi">+            dfd = mustbe_deferred(self.download_func, request, info.spider)</span>
<span class="gi">+            dfd.addCallbacks(</span>
<span class="gi">+                callback=self.media_downloaded,</span>
<span class="gi">+                callbackArgs=(request, info),</span>
<span class="gi">+                callbackKeywords={&quot;item&quot;: item},</span>
<span class="gi">+                errback=self.media_failed,</span>
<span class="gi">+                errbackArgs=(request, info),</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._modify_media_request(request)</span>
<span class="gi">+            dfd = self.crawler.engine.download(request)</span>
<span class="gi">+            dfd.addCallbacks(</span>
<span class="gi">+                callback=self.media_downloaded,</span>
<span class="gi">+                callbackArgs=(request, info),</span>
<span class="gi">+                callbackKeywords={&quot;item&quot;: item},</span>
<span class="gi">+                errback=self.media_failed,</span>
<span class="gi">+                errbackArgs=(request, info),</span>
<span class="gi">+            )</span>
<span class="gi">+        return dfd</span>
<span class="gi">+</span>
<span class="gi">+    def _cache_result_and_execute_waiters(self, result, fp, info):</span>
<span class="gi">+        if isinstance(result, Failure):</span>
<span class="gi">+            # minimize cached information for failure</span>
<span class="gi">+            result.cleanFailure()</span>
<span class="gi">+            result.frames = []</span>
<span class="gi">+            result.stack = None</span>
<span class="gi">+</span>
<span class="gi">+            # This code fixes a memory leak by avoiding to keep references to</span>
<span class="gi">+            # the Request and Response objects on the Media Pipeline cache.</span>
<span class="gi">+            #</span>
<span class="gi">+            # What happens when the media_downloaded callback raises an</span>
<span class="gi">+            # exception, for example a FileException(&#39;download-error&#39;) when</span>
<span class="gi">+            # the Response status code is not 200 OK, is that the original</span>
<span class="gi">+            # StopIteration exception (which in turn contains the failed</span>
<span class="gi">+            # Response and by extension, the original Request) gets encapsulated</span>
<span class="gi">+            # within the FileException context.</span>
<span class="gi">+            #</span>
<span class="gi">+            # Originally, Scrapy was using twisted.internet.defer.returnValue</span>
<span class="gi">+            # inside functions decorated with twisted.internet.defer.inlineCallbacks,</span>
<span class="gi">+            # encapsulating the returned Response in a _DefGen_Return exception</span>
<span class="gi">+            # instead of a StopIteration.</span>
<span class="gi">+            #</span>
<span class="gi">+            # To avoid keeping references to the Response and therefore Request</span>
<span class="gi">+            # objects on the Media Pipeline cache, we should wipe the context of</span>
<span class="gi">+            # the encapsulated exception when it is a StopIteration instance</span>
<span class="gi">+            #</span>
<span class="gi">+            # This problem does not occur in Python 2.7 since we don&#39;t have</span>
<span class="gi">+            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).</span>
<span class="gi">+            context = getattr(result.value, &quot;__context__&quot;, None)</span>
<span class="gi">+            if isinstance(context, StopIteration):</span>
<span class="gi">+                setattr(result.value, &quot;__context__&quot;, None)</span>
<span class="gi">+</span>
<span class="gi">+        info.downloading.remove(fp)</span>
<span class="gi">+        info.downloaded[fp] = result  # cache result</span>
<span class="gi">+        for wad in info.waiting.pop(fp):</span>
<span class="gi">+            defer_result(result).chainDeferred(wad)</span>
<span class="gi">+</span>
<span class="gi">+    # Overridable Interface</span>
<span class="w"> </span>    def media_to_download(self, request, info, *, item=None):
<span class="w"> </span>        &quot;&quot;&quot;Check request before starting download&quot;&quot;&quot;
<span class="w"> </span>        pass
<span class="gu">@@ -68,15 +251,24 @@ class MediaPipeline:</span>

<span class="w"> </span>    def media_downloaded(self, response, request, info, *, item=None):
<span class="w"> </span>        &quot;&quot;&quot;Handler for success downloads&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return response</span>

<span class="w"> </span>    def media_failed(self, failure, request, info):
<span class="w"> </span>        &quot;&quot;&quot;Handler for failed downloads&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return failure</span>

<span class="w"> </span>    def item_completed(self, results, item, info):
<span class="w"> </span>        &quot;&quot;&quot;Called per item when all media requests has been processed&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.LOG_FAILED_RESULTS:</span>
<span class="gi">+            for ok, value in results:</span>
<span class="gi">+                if not ok:</span>
<span class="gi">+                    logger.error(</span>
<span class="gi">+                        &quot;%(class)s found errors processing %(item)s&quot;,</span>
<span class="gi">+                        {&quot;class&quot;: self.__class__.__name__, &quot;item&quot;: item},</span>
<span class="gi">+                        exc_info=failure_to_exc_info(value),</span>
<span class="gi">+                        extra={&quot;spider&quot;: info.spider},</span>
<span class="gi">+                    )</span>
<span class="gi">+        return item</span>

<span class="w"> </span>    def file_path(self, request, response=None, info=None, *, item=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns the path where downloaded media should be stored&quot;&quot;&quot;
<span class="gh">diff --git a/scrapy/pqueues.py b/scrapy/pqueues.py</span>
<span class="gh">index c9f7c822d..62a9af477 100644</span>
<span class="gd">--- a/scrapy/pqueues.py</span>
<span class="gi">+++ b/scrapy/pqueues.py</span>
<span class="gu">@@ -1,6 +1,8 @@</span>
<span class="w"> </span>import hashlib
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.misc import create_instance
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -15,7 +17,11 @@ def _path_safe(text):</span>
<span class="w"> </span>    &gt;&gt;&gt; _path_safe(&#39;some@symbol?&#39;).startswith(&#39;some_symbol_&#39;)
<span class="w"> </span>    True
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pathable_slot = &quot;&quot;.join([c if c.isalnum() or c in &quot;-._&quot; else &quot;_&quot; for c in text])</span>
<span class="gi">+    # as we replace some letters we can get collision for different slots</span>
<span class="gi">+    # add we add unique part</span>
<span class="gi">+    unique_slot = hashlib.md5(text.encode(&quot;utf8&quot;)).hexdigest()</span>
<span class="gi">+    return &quot;-&quot;.join([pathable_slot, unique_slot])</span>


<span class="w"> </span>class ScrapyPriorityQueue:
<span class="gu">@@ -44,6 +50,10 @@ class ScrapyPriorityQueue:</span>

<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):</span>
<span class="gi">+        return cls(crawler, downstream_queue_cls, key, startprios)</span>
<span class="gi">+</span>
<span class="w"> </span>    def __init__(self, crawler, downstream_queue_cls, key, startprios=()):
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self.downstream_queue_cls = downstream_queue_cls
<span class="gu">@@ -52,6 +62,47 @@ class ScrapyPriorityQueue:</span>
<span class="w"> </span>        self.curprio = None
<span class="w"> </span>        self.init_prios(startprios)

<span class="gi">+    def init_prios(self, startprios):</span>
<span class="gi">+        if not startprios:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        for priority in startprios:</span>
<span class="gi">+            self.queues[priority] = self.qfactory(priority)</span>
<span class="gi">+</span>
<span class="gi">+        self.curprio = min(startprios)</span>
<span class="gi">+</span>
<span class="gi">+    def qfactory(self, key):</span>
<span class="gi">+        return create_instance(</span>
<span class="gi">+            self.downstream_queue_cls,</span>
<span class="gi">+            None,</span>
<span class="gi">+            self.crawler,</span>
<span class="gi">+            self.key + &quot;/&quot; + str(key),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def priority(self, request):</span>
<span class="gi">+        return -request.priority</span>
<span class="gi">+</span>
<span class="gi">+    def push(self, request):</span>
<span class="gi">+        priority = self.priority(request)</span>
<span class="gi">+        if priority not in self.queues:</span>
<span class="gi">+            self.queues[priority] = self.qfactory(priority)</span>
<span class="gi">+        q = self.queues[priority]</span>
<span class="gi">+        q.push(request)  # this may fail (eg. serialization error)</span>
<span class="gi">+        if self.curprio is None or priority &lt; self.curprio:</span>
<span class="gi">+            self.curprio = priority</span>
<span class="gi">+</span>
<span class="gi">+    def pop(self):</span>
<span class="gi">+        if self.curprio is None:</span>
<span class="gi">+            return</span>
<span class="gi">+        q = self.queues[self.curprio]</span>
<span class="gi">+        m = q.pop()</span>
<span class="gi">+        if not q:</span>
<span class="gi">+            del self.queues[self.curprio]</span>
<span class="gi">+            q.close()</span>
<span class="gi">+            prios = [p for p, q in self.queues.items() if q]</span>
<span class="gi">+            self.curprio = min(prios) if prios else None</span>
<span class="gi">+        return m</span>
<span class="gi">+</span>
<span class="w"> </span>    def peek(self):
<span class="w"> </span>        &quot;&quot;&quot;Returns the next object to be returned by :meth:`pop`,
<span class="w"> </span>        but without removing it from the queue.
<span class="gu">@@ -59,20 +110,37 @@ class ScrapyPriorityQueue:</span>
<span class="w"> </span>        Raises :exc:`NotImplementedError` if the underlying queue class does
<span class="w"> </span>        not implement a ``peek`` method, which is optional for queues.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.curprio is None:</span>
<span class="gi">+            return None</span>
<span class="gi">+        queue = self.queues[self.curprio]</span>
<span class="gi">+        return queue.peek()</span>
<span class="gi">+</span>
<span class="gi">+    def close(self):</span>
<span class="gi">+        active = []</span>
<span class="gi">+        for p, q in self.queues.items():</span>
<span class="gi">+            active.append(p)</span>
<span class="gi">+            q.close()</span>
<span class="gi">+        return active</span>

<span class="w"> </span>    def __len__(self):
<span class="w"> </span>        return sum(len(x) for x in self.queues.values()) if self.queues else 0


<span class="w"> </span>class DownloaderInterface:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, crawler):
<span class="w"> </span>        self.downloader = crawler.engine.downloader

<span class="gi">+    def stats(self, possible_slots):</span>
<span class="gi">+        return [(self._active_downloads(slot), slot) for slot in possible_slots]</span>
<span class="gi">+</span>
<span class="gi">+    def get_slot_key(self, request):</span>
<span class="gi">+        return self.downloader._get_slot_key(request, None)</span>
<span class="gi">+</span>
<span class="w"> </span>    def _active_downloads(self, slot):
<span class="w"> </span>        &quot;&quot;&quot;Return a number of requests in a Downloader for a given slot&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if slot not in self.downloader.slots:</span>
<span class="gi">+            return 0</span>
<span class="gi">+        return len(self.downloader.slots[slot].active)</span>


<span class="w"> </span>class DownloaderAwarePriorityQueue:
<span class="gu">@@ -81,23 +149,64 @@ class DownloaderAwarePriorityQueue:</span>
<span class="w"> </span>    first.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):</span>
<span class="gi">+        return cls(crawler, downstream_queue_cls, key, startprios)</span>
<span class="gi">+</span>
<span class="w"> </span>    def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):
<span class="gd">-        if crawler.settings.getint(&#39;CONCURRENT_REQUESTS_PER_IP&#39;) != 0:</span>
<span class="gi">+        if crawler.settings.getint(&quot;CONCURRENT_REQUESTS_PER_IP&quot;) != 0:</span>
<span class="w"> </span>            raise ValueError(
<span class="w"> </span>                f&#39;&quot;{self.__class__}&quot; does not support CONCURRENT_REQUESTS_PER_IP&#39;
<span class="gd">-                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="w"> </span>        if slot_startprios and not isinstance(slot_startprios, dict):
<span class="w"> </span>            raise ValueError(
<span class="gd">-                f&#39;DownloaderAwarePriorityQueue accepts ``slot_startprios`` as a dict; {slot_startprios.__class__!r} instance is passed. Most likely, it means the state iscreated by an incompatible priority queue. Only a crawl started with the same priority queue class can be resumed.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                &quot;DownloaderAwarePriorityQueue accepts &quot;</span>
<span class="gi">+                &quot;``slot_startprios`` as a dict; &quot;</span>
<span class="gi">+                f&quot;{slot_startprios.__class__!r} instance &quot;</span>
<span class="gi">+                &quot;is passed. Most likely, it means the state is&quot;</span>
<span class="gi">+                &quot;created by an incompatible priority queue. &quot;</span>
<span class="gi">+                &quot;Only a crawl started with the same priority &quot;</span>
<span class="gi">+                &quot;queue class can be resumed.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="w"> </span>        self._downloader_interface = DownloaderInterface(crawler)
<span class="w"> </span>        self.downstream_queue_cls = downstream_queue_cls
<span class="w"> </span>        self.key = key
<span class="w"> </span>        self.crawler = crawler
<span class="gd">-        self.pqueues = {}</span>
<span class="gi">+</span>
<span class="gi">+        self.pqueues = {}  # slot -&gt; priority queue</span>
<span class="w"> </span>        for slot, startprios in (slot_startprios or {}).items():
<span class="w"> </span>            self.pqueues[slot] = self.pqfactory(slot, startprios)

<span class="gi">+    def pqfactory(self, slot, startprios=()):</span>
<span class="gi">+        return ScrapyPriorityQueue(</span>
<span class="gi">+            self.crawler,</span>
<span class="gi">+            self.downstream_queue_cls,</span>
<span class="gi">+            self.key + &quot;/&quot; + _path_safe(slot),</span>
<span class="gi">+            startprios,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def pop(self):</span>
<span class="gi">+        stats = self._downloader_interface.stats(self.pqueues)</span>
<span class="gi">+</span>
<span class="gi">+        if not stats:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        slot = min(stats)[1]</span>
<span class="gi">+        queue = self.pqueues[slot]</span>
<span class="gi">+        request = queue.pop()</span>
<span class="gi">+        if len(queue) == 0:</span>
<span class="gi">+            del self.pqueues[slot]</span>
<span class="gi">+        return request</span>
<span class="gi">+</span>
<span class="gi">+    def push(self, request):</span>
<span class="gi">+        slot = self._downloader_interface.get_slot_key(request)</span>
<span class="gi">+        if slot not in self.pqueues:</span>
<span class="gi">+            self.pqueues[slot] = self.pqfactory(slot)</span>
<span class="gi">+        queue = self.pqueues[slot]</span>
<span class="gi">+        queue.push(request)</span>
<span class="gi">+</span>
<span class="w"> </span>    def peek(self):
<span class="w"> </span>        &quot;&quot;&quot;Returns the next object to be returned by :meth:`pop`,
<span class="w"> </span>        but without removing it from the queue.
<span class="gu">@@ -105,11 +214,20 @@ class DownloaderAwarePriorityQueue:</span>
<span class="w"> </span>        Raises :exc:`NotImplementedError` if the underlying queue class does
<span class="w"> </span>        not implement a ``peek`` method, which is optional for queues.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        stats = self._downloader_interface.stats(self.pqueues)</span>
<span class="gi">+        if not stats:</span>
<span class="gi">+            return None</span>
<span class="gi">+        slot = min(stats)[1]</span>
<span class="gi">+        queue = self.pqueues[slot]</span>
<span class="gi">+        return queue.peek()</span>
<span class="gi">+</span>
<span class="gi">+    def close(self):</span>
<span class="gi">+        active = {slot: queue.close() for slot, queue in self.pqueues.items()}</span>
<span class="gi">+        self.pqueues.clear()</span>
<span class="gi">+        return active</span>

<span class="w"> </span>    def __len__(self):
<span class="gd">-        return sum(len(x) for x in self.pqueues.values()</span>
<span class="gd">-            ) if self.pqueues else 0</span>
<span class="gi">+        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0</span>

<span class="w"> </span>    def __contains__(self, slot):
<span class="w"> </span>        return slot in self.pqueues
<span class="gh">diff --git a/scrapy/resolver.py b/scrapy/resolver.py</span>
<span class="gh">index c5fa8c6de..e2e8beff4 100644</span>
<span class="gd">--- a/scrapy/resolver.py</span>
<span class="gi">+++ b/scrapy/resolver.py</span>
<span class="gu">@@ -1,9 +1,18 @@</span>
<span class="w"> </span>from typing import Any
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer
<span class="w"> </span>from twisted.internet.base import ThreadedResolver
<span class="gd">-from twisted.internet.interfaces import IHostnameResolver, IHostResolution, IResolutionReceiver, IResolverSimple</span>
<span class="gi">+from twisted.internet.interfaces import (</span>
<span class="gi">+    IHostnameResolver,</span>
<span class="gi">+    IHostResolution,</span>
<span class="gi">+    IResolutionReceiver,</span>
<span class="gi">+    IResolverSimple,</span>
<span class="gi">+)</span>
<span class="w"> </span>from zope.interface.declarations import implementer, provider
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.datatypes import LocalCache
<span class="gi">+</span>
<span class="gi">+# TODO: cache misses</span>
<span class="w"> </span>dnscache: LocalCache[str, Any] = LocalCache(10000)


<span class="gu">@@ -18,22 +27,64 @@ class CachingThreadedResolver(ThreadedResolver):</span>
<span class="w"> </span>        dnscache.limit = cache_size
<span class="w"> </span>        self.timeout = timeout

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, reactor):</span>
<span class="gi">+        if crawler.settings.getbool(&quot;DNSCACHE_ENABLED&quot;):</span>
<span class="gi">+            cache_size = crawler.settings.getint(&quot;DNSCACHE_SIZE&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            cache_size = 0</span>
<span class="gi">+        return cls(reactor, cache_size, crawler.settings.getfloat(&quot;DNS_TIMEOUT&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    def install_on_reactor(self):</span>
<span class="gi">+        self.reactor.installResolver(self)</span>
<span class="gi">+</span>
<span class="gi">+    def getHostByName(self, name: str, timeout=None):</span>
<span class="gi">+        if name in dnscache:</span>
<span class="gi">+            return defer.succeed(dnscache[name])</span>
<span class="gi">+        # in Twisted&lt;=16.6, getHostByName() is always called with</span>
<span class="gi">+        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),</span>
<span class="gi">+        # so the input argument above is simply overridden</span>
<span class="gi">+        # to enforce Scrapy&#39;s DNS_TIMEOUT setting&#39;s value</span>
<span class="gi">+        timeout = (self.timeout,)</span>
<span class="gi">+        d = super().getHostByName(name, timeout)</span>
<span class="gi">+        if dnscache.limit:</span>
<span class="gi">+            d.addCallback(self._cache_result, name)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _cache_result(self, result, name):</span>
<span class="gi">+        dnscache[name] = result</span>
<span class="gi">+        return result</span>
<span class="gi">+</span>

<span class="w"> </span>@implementer(IHostResolution)
<span class="w"> </span>class HostResolution:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, name):
<span class="w"> </span>        self.name = name

<span class="gi">+    def cancel(self):</span>
<span class="gi">+        raise NotImplementedError()</span>
<span class="gi">+</span>

<span class="w"> </span>@provider(IResolutionReceiver)
<span class="w"> </span>class _CachingResolutionReceiver:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, resolutionReceiver, hostName):
<span class="w"> </span>        self.resolutionReceiver = resolutionReceiver
<span class="w"> </span>        self.hostName = hostName
<span class="w"> </span>        self.addresses = []

<span class="gi">+    def resolutionBegan(self, resolution):</span>
<span class="gi">+        self.resolutionReceiver.resolutionBegan(resolution)</span>
<span class="gi">+        self.resolution = resolution</span>
<span class="gi">+</span>
<span class="gi">+    def addressResolved(self, address):</span>
<span class="gi">+        self.resolutionReceiver.addressResolved(address)</span>
<span class="gi">+        self.addresses.append(address)</span>
<span class="gi">+</span>
<span class="gi">+    def resolutionComplete(self):</span>
<span class="gi">+        self.resolutionReceiver.resolutionComplete()</span>
<span class="gi">+        if self.addresses:</span>
<span class="gi">+            dnscache[self.hostName] = self.addresses</span>
<span class="gi">+</span>

<span class="w"> </span>@implementer(IHostnameResolver)
<span class="w"> </span>class CachingHostnameResolver:
<span class="gu">@@ -46,3 +97,39 @@ class CachingHostnameResolver:</span>
<span class="w"> </span>        self.reactor = reactor
<span class="w"> </span>        self.original_resolver = reactor.nameResolver
<span class="w"> </span>        dnscache.limit = cache_size
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, reactor):</span>
<span class="gi">+        if crawler.settings.getbool(&quot;DNSCACHE_ENABLED&quot;):</span>
<span class="gi">+            cache_size = crawler.settings.getint(&quot;DNSCACHE_SIZE&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            cache_size = 0</span>
<span class="gi">+        return cls(reactor, cache_size)</span>
<span class="gi">+</span>
<span class="gi">+    def install_on_reactor(self):</span>
<span class="gi">+        self.reactor.installNameResolver(self)</span>
<span class="gi">+</span>
<span class="gi">+    def resolveHostName(</span>
<span class="gi">+        self,</span>
<span class="gi">+        resolutionReceiver,</span>
<span class="gi">+        hostName: str,</span>
<span class="gi">+        portNumber=0,</span>
<span class="gi">+        addressTypes=None,</span>
<span class="gi">+        transportSemantics=&quot;TCP&quot;,</span>
<span class="gi">+    ):</span>
<span class="gi">+        try:</span>
<span class="gi">+            addresses = dnscache[hostName]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            return self.original_resolver.resolveHostName(</span>
<span class="gi">+                _CachingResolutionReceiver(resolutionReceiver, hostName),</span>
<span class="gi">+                hostName,</span>
<span class="gi">+                portNumber,</span>
<span class="gi">+                addressTypes,</span>
<span class="gi">+                transportSemantics,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            resolutionReceiver.resolutionBegan(HostResolution(hostName))</span>
<span class="gi">+            for addr in addresses:</span>
<span class="gi">+                resolutionReceiver.addressResolved(addr)</span>
<span class="gi">+            resolutionReceiver.resolutionComplete()</span>
<span class="gi">+            return resolutionReceiver</span>
<span class="gh">diff --git a/scrapy/responsetypes.py b/scrapy/responsetypes.py</span>
<span class="gh">index 31e10e18d..9e411d4aa 100644</span>
<span class="gd">--- a/scrapy/responsetypes.py</span>
<span class="gi">+++ b/scrapy/responsetypes.py</span>
<span class="gu">@@ -6,69 +6,134 @@ from io import StringIO</span>
<span class="w"> </span>from mimetypes import MimeTypes
<span class="w"> </span>from pkgutil import get_data
<span class="w"> </span>from typing import Dict, Mapping, Optional, Type, Union
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Response
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="w"> </span>from scrapy.utils.python import binary_is_text, to_bytes, to_unicode


<span class="w"> </span>class ResponseTypes:
<span class="gd">-    CLASSES = {&#39;text/html&#39;: &#39;scrapy.http.HtmlResponse&#39;,</span>
<span class="gd">-        &#39;application/atom+xml&#39;: &#39;scrapy.http.XmlResponse&#39;,</span>
<span class="gd">-        &#39;application/rdf+xml&#39;: &#39;scrapy.http.XmlResponse&#39;,</span>
<span class="gd">-        &#39;application/rss+xml&#39;: &#39;scrapy.http.XmlResponse&#39;,</span>
<span class="gd">-        &#39;application/xhtml+xml&#39;: &#39;scrapy.http.HtmlResponse&#39;,</span>
<span class="gd">-        &#39;application/vnd.wap.xhtml+xml&#39;: &#39;scrapy.http.HtmlResponse&#39;,</span>
<span class="gd">-        &#39;application/xml&#39;: &#39;scrapy.http.XmlResponse&#39;, &#39;application/json&#39;:</span>
<span class="gd">-        &#39;scrapy.http.TextResponse&#39;, &#39;application/x-json&#39;:</span>
<span class="gd">-        &#39;scrapy.http.TextResponse&#39;, &#39;application/json-amazonui-streaming&#39;:</span>
<span class="gd">-        &#39;scrapy.http.TextResponse&#39;, &#39;application/javascript&#39;:</span>
<span class="gd">-        &#39;scrapy.http.TextResponse&#39;, &#39;application/x-javascript&#39;:</span>
<span class="gd">-        &#39;scrapy.http.TextResponse&#39;, &#39;text/xml&#39;: &#39;scrapy.http.XmlResponse&#39;,</span>
<span class="gd">-        &#39;text/*&#39;: &#39;scrapy.http.TextResponse&#39;}</span>
<span class="gi">+    CLASSES = {</span>
<span class="gi">+        &quot;text/html&quot;: &quot;scrapy.http.HtmlResponse&quot;,</span>
<span class="gi">+        &quot;application/atom+xml&quot;: &quot;scrapy.http.XmlResponse&quot;,</span>
<span class="gi">+        &quot;application/rdf+xml&quot;: &quot;scrapy.http.XmlResponse&quot;,</span>
<span class="gi">+        &quot;application/rss+xml&quot;: &quot;scrapy.http.XmlResponse&quot;,</span>
<span class="gi">+        &quot;application/xhtml+xml&quot;: &quot;scrapy.http.HtmlResponse&quot;,</span>
<span class="gi">+        &quot;application/vnd.wap.xhtml+xml&quot;: &quot;scrapy.http.HtmlResponse&quot;,</span>
<span class="gi">+        &quot;application/xml&quot;: &quot;scrapy.http.XmlResponse&quot;,</span>
<span class="gi">+        &quot;application/json&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+        &quot;application/x-json&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+        &quot;application/json-amazonui-streaming&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+        &quot;application/javascript&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+        &quot;application/x-javascript&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+        &quot;text/xml&quot;: &quot;scrapy.http.XmlResponse&quot;,</span>
<span class="gi">+        &quot;text/*&quot;: &quot;scrapy.http.TextResponse&quot;,</span>
<span class="gi">+    }</span>

<span class="gd">-    def __init__(self) -&gt;None:</span>
<span class="gi">+    def __init__(self) -&gt; None:</span>
<span class="w"> </span>        self.classes: Dict[str, Type[Response]] = {}
<span class="w"> </span>        self.mimetypes: MimeTypes = MimeTypes()
<span class="gd">-        mimedata = get_data(&#39;scrapy&#39;, &#39;mime.types&#39;)</span>
<span class="gi">+        mimedata = get_data(&quot;scrapy&quot;, &quot;mime.types&quot;)</span>
<span class="w"> </span>        if not mimedata:
<span class="w"> </span>            raise ValueError(
<span class="gd">-                &#39;The mime.types file is not found in the Scrapy installation&#39;)</span>
<span class="gd">-        self.mimetypes.readfp(StringIO(mimedata.decode(&#39;utf8&#39;)))</span>
<span class="gi">+                &quot;The mime.types file is not found in the Scrapy installation&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        self.mimetypes.readfp(StringIO(mimedata.decode(&quot;utf8&quot;)))</span>
<span class="w"> </span>        for mimetype, cls in self.CLASSES.items():
<span class="w"> </span>            self.classes[mimetype] = load_object(cls)

<span class="gd">-    def from_mimetype(self, mimetype: str) -&gt;Type[Response]:</span>
<span class="gi">+    def from_mimetype(self, mimetype: str) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return the most appropriate Response class for the given mimetype&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if mimetype is None:</span>
<span class="gi">+            return Response</span>
<span class="gi">+        if mimetype in self.classes:</span>
<span class="gi">+            return self.classes[mimetype]</span>
<span class="gi">+        basetype = f&quot;{mimetype.split(&#39;/&#39;)[0]}/*&quot;</span>
<span class="gi">+        return self.classes.get(basetype, Response)</span>

<span class="gd">-    def from_content_type(self, content_type: Union[str, bytes],</span>
<span class="gd">-        content_encoding: Optional[bytes]=None) -&gt;Type[Response]:</span>
<span class="gi">+    def from_content_type(</span>
<span class="gi">+        self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None</span>
<span class="gi">+    ) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return the most appropriate Response class from an HTTP Content-Type
<span class="w"> </span>        header&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if content_encoding:</span>
<span class="gi">+            return Response</span>
<span class="gi">+        mimetype = (</span>
<span class="gi">+            to_unicode(content_type, encoding=&quot;latin-1&quot;).split(&quot;;&quot;)[0].strip().lower()</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.from_mimetype(mimetype)</span>
<span class="gi">+</span>
<span class="gi">+    def from_content_disposition(</span>
<span class="gi">+        self, content_disposition: Union[str, bytes]</span>
<span class="gi">+    ) -&gt; Type[Response]:</span>
<span class="gi">+        try:</span>
<span class="gi">+            filename = (</span>
<span class="gi">+                to_unicode(content_disposition, encoding=&quot;latin-1&quot;, errors=&quot;replace&quot;)</span>
<span class="gi">+                .split(&quot;;&quot;)[1]</span>
<span class="gi">+                .split(&quot;=&quot;)[1]</span>
<span class="gi">+                .strip(&quot;\&quot;&#39;&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+            return self.from_filename(filename)</span>
<span class="gi">+        except IndexError:</span>
<span class="gi">+            return Response</span>

<span class="gd">-    def from_headers(self, headers: Mapping[bytes, bytes]) -&gt;Type[Response]:</span>
<span class="gi">+    def from_headers(self, headers: Mapping[bytes, bytes]) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return the most appropriate Response class by looking at the HTTP
<span class="w"> </span>        headers&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        cls = Response</span>
<span class="gi">+        if b&quot;Content-Type&quot; in headers:</span>
<span class="gi">+            cls = self.from_content_type(</span>
<span class="gi">+                content_type=headers[b&quot;Content-Type&quot;],</span>
<span class="gi">+                content_encoding=headers.get(b&quot;Content-Encoding&quot;),</span>
<span class="gi">+            )</span>
<span class="gi">+        if cls is Response and b&quot;Content-Disposition&quot; in headers:</span>
<span class="gi">+            cls = self.from_content_disposition(headers[b&quot;Content-Disposition&quot;])</span>
<span class="gi">+        return cls</span>

<span class="gd">-    def from_filename(self, filename: str) -&gt;Type[Response]:</span>
<span class="gi">+    def from_filename(self, filename: str) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Return the most appropriate Response class from a file name&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        mimetype, encoding = self.mimetypes.guess_type(filename)</span>
<span class="gi">+        if mimetype and not encoding:</span>
<span class="gi">+            return self.from_mimetype(mimetype)</span>
<span class="gi">+        return Response</span>

<span class="gd">-    def from_body(self, body: bytes) -&gt;Type[Response]:</span>
<span class="gi">+    def from_body(self, body: bytes) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Try to guess the appropriate response based on the body content.
<span class="w"> </span>        This method is a bit magic and could be improved in the future, but
<span class="w"> </span>        it&#39;s not meant to be used except for special cases where response types
<span class="w"> </span>        cannot be guess using more straightforward methods.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        chunk = body[:5000]</span>
<span class="gi">+        chunk = to_bytes(chunk)</span>
<span class="gi">+        if not binary_is_text(chunk):</span>
<span class="gi">+            return self.from_mimetype(&quot;application/octet-stream&quot;)</span>
<span class="gi">+        lowercase_chunk = chunk.lower()</span>
<span class="gi">+        if b&quot;&lt;html&gt;&quot; in lowercase_chunk:</span>
<span class="gi">+            return self.from_mimetype(&quot;text/html&quot;)</span>
<span class="gi">+        if b&quot;&lt;?xml&quot; in lowercase_chunk:</span>
<span class="gi">+            return self.from_mimetype(&quot;text/xml&quot;)</span>
<span class="gi">+        if b&quot;&lt;!doctype html&gt;&quot; in lowercase_chunk:</span>
<span class="gi">+            return self.from_mimetype(&quot;text/html&quot;)</span>
<span class="gi">+        return self.from_mimetype(&quot;text&quot;)</span>

<span class="gd">-    def from_args(self, headers: Optional[Mapping[bytes, bytes]]=None, url:</span>
<span class="gd">-        Optional[str]=None, filename: Optional[str]=None, body: Optional[</span>
<span class="gd">-        bytes]=None) -&gt;Type[Response]:</span>
<span class="gi">+    def from_args(</span>
<span class="gi">+        self,</span>
<span class="gi">+        headers: Optional[Mapping[bytes, bytes]] = None,</span>
<span class="gi">+        url: Optional[str] = None,</span>
<span class="gi">+        filename: Optional[str] = None,</span>
<span class="gi">+        body: Optional[bytes] = None,</span>
<span class="gi">+    ) -&gt; Type[Response]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Guess the most appropriate Response class based on
<span class="w"> </span>        the given arguments.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        cls = Response</span>
<span class="gi">+        if headers is not None:</span>
<span class="gi">+            cls = self.from_headers(headers)</span>
<span class="gi">+        if cls is Response and url is not None:</span>
<span class="gi">+            cls = self.from_filename(url)</span>
<span class="gi">+        if cls is Response and filename is not None:</span>
<span class="gi">+            cls = self.from_filename(filename)</span>
<span class="gi">+        if cls is Response and body is not None:</span>
<span class="gi">+            cls = self.from_body(body)</span>
<span class="gi">+        return cls</span>


<span class="w"> </span>responsetypes = ResponseTypes()
<span class="gh">diff --git a/scrapy/robotstxt.py b/scrapy/robotstxt.py</span>
<span class="gh">index a5916df9a..ea943c364 100644</span>
<span class="gd">--- a/scrapy/robotstxt.py</span>
<span class="gi">+++ b/scrapy/robotstxt.py</span>
<span class="gu">@@ -1,12 +1,32 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>import sys
<span class="w"> </span>from abc import ABCMeta, abstractmethod
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gd">-class RobotParser(metaclass=ABCMeta):</span>
<span class="gi">+def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):</span>
<span class="gi">+    try:</span>
<span class="gi">+        if to_native_str_type:</span>
<span class="gi">+            robotstxt_body = to_unicode(robotstxt_body)</span>
<span class="gi">+        else:</span>
<span class="gi">+            robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;, errors=&quot;ignore&quot;)</span>
<span class="gi">+    except UnicodeDecodeError:</span>
<span class="gi">+        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.</span>
<span class="gi">+        # Switch to &#39;allow all&#39; state.</span>
<span class="gi">+        logger.warning(</span>
<span class="gi">+            &quot;Failure while parsing robots.txt. File either contains garbage or &quot;</span>
<span class="gi">+            &quot;is in an encoding other than UTF-8, treating it as an empty file.&quot;,</span>
<span class="gi">+            exc_info=sys.exc_info(),</span>
<span class="gi">+            extra={&quot;spider&quot;: spider},</span>
<span class="gi">+        )</span>
<span class="gi">+        robotstxt_body = &quot;&quot;</span>
<span class="gi">+    return robotstxt_body</span>

<span class="gi">+</span>
<span class="gi">+class RobotParser(metaclass=ABCMeta):</span>
<span class="w"> </span>    @classmethod
<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def from_crawler(cls, crawler, robotstxt_body):
<span class="gu">@@ -35,38 +55,81 @@ class RobotParser(metaclass=ABCMeta):</span>


<span class="w"> </span>class PythonRobotParser(RobotParser):
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, robotstxt_body, spider):
<span class="w"> </span>        from urllib.robotparser import RobotFileParser
<span class="gi">+</span>
<span class="w"> </span>        self.spider = spider
<span class="gd">-        robotstxt_body = decode_robotstxt(robotstxt_body, spider,</span>
<span class="gd">-            to_native_str_type=True)</span>
<span class="gi">+        robotstxt_body = decode_robotstxt(</span>
<span class="gi">+            robotstxt_body, spider, to_native_str_type=True</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.rp = RobotFileParser()
<span class="w"> </span>        self.rp.parse(robotstxt_body.splitlines())

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, robotstxt_body):</span>
<span class="gi">+        spider = None if not crawler else crawler.spider</span>
<span class="gi">+        o = cls(robotstxt_body, spider)</span>
<span class="gi">+        return o</span>

<span class="gd">-class ReppyRobotParser(RobotParser):</span>
<span class="gi">+    def allowed(self, url, user_agent):</span>
<span class="gi">+        user_agent = to_unicode(user_agent)</span>
<span class="gi">+        url = to_unicode(url)</span>
<span class="gi">+        return self.rp.can_fetch(user_agent, url)</span>

<span class="gi">+</span>
<span class="gi">+class ReppyRobotParser(RobotParser):</span>
<span class="w"> </span>    def __init__(self, robotstxt_body, spider):
<span class="w"> </span>        from reppy.robots import Robots
<span class="gi">+</span>
<span class="w"> </span>        self.spider = spider
<span class="gd">-        self.rp = Robots.parse(&#39;&#39;, robotstxt_body)</span>
<span class="gi">+        self.rp = Robots.parse(&quot;&quot;, robotstxt_body)</span>

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, robotstxt_body):</span>
<span class="gi">+        spider = None if not crawler else crawler.spider</span>
<span class="gi">+        o = cls(robotstxt_body, spider)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def allowed(self, url, user_agent):</span>
<span class="gi">+        return self.rp.allowed(url, user_agent)</span>

<span class="gd">-class RerpRobotParser(RobotParser):</span>

<span class="gi">+class RerpRobotParser(RobotParser):</span>
<span class="w"> </span>    def __init__(self, robotstxt_body, spider):
<span class="w"> </span>        from robotexclusionrulesparser import RobotExclusionRulesParser
<span class="gi">+</span>
<span class="w"> </span>        self.spider = spider
<span class="w"> </span>        self.rp = RobotExclusionRulesParser()
<span class="w"> </span>        robotstxt_body = decode_robotstxt(robotstxt_body, spider)
<span class="w"> </span>        self.rp.parse(robotstxt_body)

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, robotstxt_body):</span>
<span class="gi">+        spider = None if not crawler else crawler.spider</span>
<span class="gi">+        o = cls(robotstxt_body, spider)</span>
<span class="gi">+        return o</span>

<span class="gd">-class ProtegoRobotParser(RobotParser):</span>
<span class="gi">+    def allowed(self, url, user_agent):</span>
<span class="gi">+        user_agent = to_unicode(user_agent)</span>
<span class="gi">+        url = to_unicode(url)</span>
<span class="gi">+        return self.rp.is_allowed(user_agent, url)</span>

<span class="gi">+</span>
<span class="gi">+class ProtegoRobotParser(RobotParser):</span>
<span class="w"> </span>    def __init__(self, robotstxt_body, spider):
<span class="w"> </span>        from protego import Protego
<span class="gi">+</span>
<span class="w"> </span>        self.spider = spider
<span class="w"> </span>        robotstxt_body = decode_robotstxt(robotstxt_body, spider)
<span class="w"> </span>        self.rp = Protego.parse(robotstxt_body)
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, robotstxt_body):</span>
<span class="gi">+        spider = None if not crawler else crawler.spider</span>
<span class="gi">+        o = cls(robotstxt_body, spider)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def allowed(self, url, user_agent):</span>
<span class="gi">+        user_agent = to_unicode(user_agent)</span>
<span class="gi">+        url = to_unicode(url)</span>
<span class="gi">+        return self.rp.can_fetch(url, user_agent)</span>
<span class="gh">diff --git a/scrapy/selector/unified.py b/scrapy/selector/unified.py</span>
<span class="gh">index 0c13d190a..5ad4724c0 100644</span>
<span class="gd">--- a/scrapy/selector/unified.py</span>
<span class="gi">+++ b/scrapy/selector/unified.py</span>
<span class="gu">@@ -2,15 +2,30 @@</span>
<span class="w"> </span>XPath selectors based on lxml
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from typing import Any, Optional, Type, Union
<span class="gi">+</span>
<span class="w"> </span>from parsel import Selector as _ParselSelector
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import HtmlResponse, TextResponse, XmlResponse
<span class="w"> </span>from scrapy.utils.python import to_bytes
<span class="w"> </span>from scrapy.utils.response import get_base_url
<span class="w"> </span>from scrapy.utils.trackref import object_ref
<span class="gd">-__all__ = [&#39;Selector&#39;, &#39;SelectorList&#39;]</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;Selector&quot;, &quot;SelectorList&quot;]</span>
<span class="gi">+</span>
<span class="w"> </span>_NOT_SET = object()


<span class="gi">+def _st(response: Optional[TextResponse], st: Optional[str]) -&gt; str:</span>
<span class="gi">+    if st is None:</span>
<span class="gi">+        return &quot;xml&quot; if isinstance(response, XmlResponse) else &quot;html&quot;</span>
<span class="gi">+    return st</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _response_from_text(text: Union[str, bytes], st: Optional[str]) -&gt; TextResponse:</span>
<span class="gi">+    rt: Type[TextResponse] = XmlResponse if st == &quot;xml&quot; else HtmlResponse</span>
<span class="gi">+    return rt(url=&quot;about:blank&quot;, encoding=&quot;utf-8&quot;, body=to_bytes(text, &quot;utf-8&quot;))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class SelectorList(_ParselSelector.selectorlist_cls, object_ref):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    The :class:`SelectorList` class is a subclass of the builtin ``list``
<span class="gu">@@ -48,23 +63,36 @@ class Selector(_ParselSelector, object_ref):</span>
<span class="w"> </span>    Otherwise, if ``type`` is set, the selector type will be forced and no
<span class="w"> </span>    detection will occur.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    __slots__ = [&#39;response&#39;]</span>
<span class="gi">+</span>
<span class="gi">+    __slots__ = [&quot;response&quot;]</span>
<span class="w"> </span>    selectorlist_cls = SelectorList

<span class="gd">-    def __init__(self, response: Optional[TextResponse]=None, text:</span>
<span class="gd">-        Optional[str]=None, type: Optional[str]=None, root: Optional[Any]=</span>
<span class="gd">-        _NOT_SET, **kwargs: Any):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        response: Optional[TextResponse] = None,</span>
<span class="gi">+        text: Optional[str] = None,</span>
<span class="gi">+        type: Optional[str] = None,</span>
<span class="gi">+        root: Optional[Any] = _NOT_SET,</span>
<span class="gi">+        **kwargs: Any,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if response is not None and text is not None:
<span class="w"> </span>            raise ValueError(
<span class="gd">-                f&#39;{self.__class__.__name__}.__init__() received both response and text&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                f&quot;{self.__class__.__name__}.__init__() received &quot;</span>
<span class="gi">+                &quot;both response and text&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="w"> </span>        st = _st(response, type)
<span class="gi">+</span>
<span class="w"> </span>        if text is not None:
<span class="w"> </span>            response = _response_from_text(text, st)
<span class="gi">+</span>
<span class="w"> </span>        if response is not None:
<span class="w"> </span>            text = response.text
<span class="gd">-            kwargs.setdefault(&#39;base_url&#39;, get_base_url(response))</span>
<span class="gi">+            kwargs.setdefault(&quot;base_url&quot;, get_base_url(response))</span>
<span class="gi">+</span>
<span class="w"> </span>        self.response = response
<span class="gi">+</span>
<span class="w"> </span>        if root is not _NOT_SET:
<span class="gd">-            kwargs[&#39;root&#39;] = root</span>
<span class="gi">+            kwargs[&quot;root&quot;] = root</span>
<span class="gi">+</span>
<span class="w"> </span>        super().__init__(text=text, type=st, **kwargs)
<span class="gh">diff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py</span>
<span class="gh">index 417c6e729..6affd2125 100644</span>
<span class="gd">--- a/scrapy/settings/default_settings.py</span>
<span class="gi">+++ b/scrapy/settings/default_settings.py</span>
<span class="gu">@@ -12,234 +12,327 @@ Scrapy developers, if you add a setting here remember to:</span>
<span class="w"> </span>  (docs/topics/settings.rst)

<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import sys
<span class="w"> </span>from importlib import import_module
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>ADDONS = {}
<span class="gi">+</span>
<span class="w"> </span>AJAXCRAWL_ENABLED = False
<span class="gi">+</span>
<span class="w"> </span>ASYNCIO_EVENT_LOOP = None
<span class="gi">+</span>
<span class="w"> </span>AUTOTHROTTLE_ENABLED = False
<span class="w"> </span>AUTOTHROTTLE_DEBUG = False
<span class="w"> </span>AUTOTHROTTLE_MAX_DELAY = 60.0
<span class="w"> </span>AUTOTHROTTLE_START_DELAY = 5.0
<span class="w"> </span>AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
<span class="gd">-BOT_NAME = &#39;scrapybot&#39;</span>
<span class="gi">+</span>
<span class="gi">+BOT_NAME = &quot;scrapybot&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>CLOSESPIDER_TIMEOUT = 0
<span class="w"> </span>CLOSESPIDER_PAGECOUNT = 0
<span class="w"> </span>CLOSESPIDER_ITEMCOUNT = 0
<span class="w"> </span>CLOSESPIDER_ERRORCOUNT = 0
<span class="gd">-COMMANDS_MODULE = &#39;&#39;</span>
<span class="gi">+</span>
<span class="gi">+COMMANDS_MODULE = &quot;&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>COMPRESSION_ENABLED = True
<span class="gi">+</span>
<span class="w"> </span>CONCURRENT_ITEMS = 100
<span class="gi">+</span>
<span class="w"> </span>CONCURRENT_REQUESTS = 16
<span class="w"> </span>CONCURRENT_REQUESTS_PER_DOMAIN = 8
<span class="w"> </span>CONCURRENT_REQUESTS_PER_IP = 0
<span class="gi">+</span>
<span class="w"> </span>COOKIES_ENABLED = True
<span class="w"> </span>COOKIES_DEBUG = False
<span class="gd">-DEFAULT_ITEM_CLASS = &#39;scrapy.item.Item&#39;</span>
<span class="gd">-DEFAULT_REQUEST_HEADERS = {&#39;Accept&#39;:</span>
<span class="gd">-    &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,</span>
<span class="gd">-    &#39;Accept-Language&#39;: &#39;en&#39;}</span>
<span class="gi">+</span>
<span class="gi">+DEFAULT_ITEM_CLASS = &quot;scrapy.item.Item&quot;</span>
<span class="gi">+</span>
<span class="gi">+DEFAULT_REQUEST_HEADERS = {</span>
<span class="gi">+    &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;,</span>
<span class="gi">+    &quot;Accept-Language&quot;: &quot;en&quot;,</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="w"> </span>DEPTH_LIMIT = 0
<span class="w"> </span>DEPTH_STATS_VERBOSE = False
<span class="w"> </span>DEPTH_PRIORITY = 0
<span class="gi">+</span>
<span class="w"> </span>DNSCACHE_ENABLED = True
<span class="w"> </span>DNSCACHE_SIZE = 10000
<span class="gd">-DNS_RESOLVER = &#39;scrapy.resolver.CachingThreadedResolver&#39;</span>
<span class="gi">+DNS_RESOLVER = &quot;scrapy.resolver.CachingThreadedResolver&quot;</span>
<span class="w"> </span>DNS_TIMEOUT = 60
<span class="gi">+</span>
<span class="w"> </span>DOWNLOAD_DELAY = 0
<span class="gi">+</span>
<span class="w"> </span>DOWNLOAD_HANDLERS = {}
<span class="gd">-DOWNLOAD_HANDLERS_BASE = {&#39;data&#39;:</span>
<span class="gd">-    &#39;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&#39;,</span>
<span class="gd">-    &#39;file&#39;: &#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;,</span>
<span class="gd">-    &#39;http&#39;: &#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;,</span>
<span class="gd">-    &#39;https&#39;: &#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;,</span>
<span class="gd">-    &#39;s3&#39;: &#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;, &#39;ftp&#39;:</span>
<span class="gd">-    &#39;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&#39;}</span>
<span class="gd">-DOWNLOAD_TIMEOUT = 180</span>
<span class="gd">-DOWNLOAD_MAXSIZE = 1024 * 1024 * 1024</span>
<span class="gd">-DOWNLOAD_WARNSIZE = 32 * 1024 * 1024</span>
<span class="gi">+DOWNLOAD_HANDLERS_BASE = {</span>
<span class="gi">+    &quot;data&quot;: &quot;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&quot;,</span>
<span class="gi">+    &quot;file&quot;: &quot;scrapy.core.downloader.handlers.file.FileDownloadHandler&quot;,</span>
<span class="gi">+    &quot;http&quot;: &quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;,</span>
<span class="gi">+    &quot;https&quot;: &quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;,</span>
<span class="gi">+    &quot;s3&quot;: &quot;scrapy.core.downloader.handlers.s3.S3DownloadHandler&quot;,</span>
<span class="gi">+    &quot;ftp&quot;: &quot;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&quot;,</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+DOWNLOAD_TIMEOUT = 180  # 3mins</span>
<span class="gi">+</span>
<span class="gi">+DOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m</span>
<span class="gi">+DOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m</span>
<span class="gi">+</span>
<span class="w"> </span>DOWNLOAD_FAIL_ON_DATALOSS = True
<span class="gd">-DOWNLOADER = &#39;scrapy.core.downloader.Downloader&#39;</span>
<span class="gi">+</span>
<span class="gi">+DOWNLOADER = &quot;scrapy.core.downloader.Downloader&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>DOWNLOADER_HTTPCLIENTFACTORY = (
<span class="gd">-    &#39;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&#39;)</span>
<span class="gi">+    &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot;</span>
<span class="gi">+)</span>
<span class="w"> </span>DOWNLOADER_CLIENTCONTEXTFACTORY = (
<span class="gd">-    &#39;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&#39;)</span>
<span class="gd">-DOWNLOADER_CLIENT_TLS_CIPHERS = &#39;DEFAULT&#39;</span>
<span class="gd">-DOWNLOADER_CLIENT_TLS_METHOD = &#39;TLS&#39;</span>
<span class="gi">+    &quot;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&quot;</span>
<span class="gi">+)</span>
<span class="gi">+DOWNLOADER_CLIENT_TLS_CIPHERS = &quot;DEFAULT&quot;</span>
<span class="gi">+# Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:</span>
<span class="gi">+DOWNLOADER_CLIENT_TLS_METHOD = &quot;TLS&quot;</span>
<span class="w"> </span>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False
<span class="gi">+</span>
<span class="w"> </span>DOWNLOADER_MIDDLEWARES = {}
<span class="gi">+</span>
<span class="w"> </span>DOWNLOADER_MIDDLEWARES_BASE = {
<span class="gd">-    &#39;scrapy.downloadermiddlewares.offsite.OffsiteMiddleware&#39;: 50,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;: 300,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;:</span>
<span class="gd">-    350,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;:</span>
<span class="gd">-    400, &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 500,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 550,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;: 560,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;: 580,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;:</span>
<span class="gd">-    590, &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;: 600,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;: 700,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;: 750,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;: 850,</span>
<span class="gd">-    &#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;: 900}</span>
<span class="gi">+    # Engine side</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.offsite.OffsiteMiddleware&quot;: 50,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;: 100,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;: 300,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;: 350,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;: 400,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: 500,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;: 550,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;: 560,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;: 580,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 590,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;: 600,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;: 700,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;: 750,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;: 850,</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;: 900,</span>
<span class="gi">+    # Downloader side</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="w"> </span>DOWNLOADER_STATS = True
<span class="gd">-DUPEFILTER_CLASS = &#39;scrapy.dupefilters.RFPDupeFilter&#39;</span>
<span class="gd">-EDITOR = &#39;vi&#39;</span>
<span class="gd">-if sys.platform == &#39;win32&#39;:</span>
<span class="gd">-    EDITOR = &#39;%s -m idlelib.idle&#39;</span>
<span class="gi">+</span>
<span class="gi">+DUPEFILTER_CLASS = &quot;scrapy.dupefilters.RFPDupeFilter&quot;</span>
<span class="gi">+</span>
<span class="gi">+EDITOR = &quot;vi&quot;</span>
<span class="gi">+if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+    EDITOR = &quot;%s -m idlelib.idle&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>EXTENSIONS = {}
<span class="gd">-EXTENSIONS_BASE = {&#39;scrapy.extensions.corestats.CoreStats&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.memusage.MemoryUsage&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.memdebug.MemoryDebugger&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.closespider.CloseSpider&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.feedexport.FeedExporter&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.logstats.LogStats&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.spiderstate.SpiderState&#39;: 0,</span>
<span class="gd">-    &#39;scrapy.extensions.throttle.AutoThrottle&#39;: 0}</span>
<span class="gi">+</span>
<span class="gi">+EXTENSIONS_BASE = {</span>
<span class="gi">+    &quot;scrapy.extensions.corestats.CoreStats&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.telnet.TelnetConsole&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.memusage.MemoryUsage&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.memdebug.MemoryDebugger&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.closespider.CloseSpider&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.feedexport.FeedExporter&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.logstats.LogStats&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.spiderstate.SpiderState&quot;: 0,</span>
<span class="gi">+    &quot;scrapy.extensions.throttle.AutoThrottle&quot;: 0,</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="w"> </span>FEED_TEMPDIR = None
<span class="w"> </span>FEEDS = {}
<span class="gd">-FEED_URI_PARAMS = None</span>
<span class="gi">+FEED_URI_PARAMS = None  # a function to extend uri arguments</span>
<span class="w"> </span>FEED_STORE_EMPTY = True
<span class="w"> </span>FEED_EXPORT_ENCODING = None
<span class="w"> </span>FEED_EXPORT_FIELDS = None
<span class="w"> </span>FEED_STORAGES = {}
<span class="gd">-FEED_STORAGES_BASE = {&#39;&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;,</span>
<span class="gd">-    &#39;file&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;, &#39;ftp&#39;:</span>
<span class="gd">-    &#39;scrapy.extensions.feedexport.FTPFeedStorage&#39;, &#39;gs&#39;:</span>
<span class="gd">-    &#39;scrapy.extensions.feedexport.GCSFeedStorage&#39;, &#39;s3&#39;:</span>
<span class="gd">-    &#39;scrapy.extensions.feedexport.S3FeedStorage&#39;, &#39;stdout&#39;:</span>
<span class="gd">-    &#39;scrapy.extensions.feedexport.StdoutFeedStorage&#39;}</span>
<span class="gi">+FEED_STORAGES_BASE = {</span>
<span class="gi">+    &quot;&quot;: &quot;scrapy.extensions.feedexport.FileFeedStorage&quot;,</span>
<span class="gi">+    &quot;file&quot;: &quot;scrapy.extensions.feedexport.FileFeedStorage&quot;,</span>
<span class="gi">+    &quot;ftp&quot;: &quot;scrapy.extensions.feedexport.FTPFeedStorage&quot;,</span>
<span class="gi">+    &quot;gs&quot;: &quot;scrapy.extensions.feedexport.GCSFeedStorage&quot;,</span>
<span class="gi">+    &quot;s3&quot;: &quot;scrapy.extensions.feedexport.S3FeedStorage&quot;,</span>
<span class="gi">+    &quot;stdout&quot;: &quot;scrapy.extensions.feedexport.StdoutFeedStorage&quot;,</span>
<span class="gi">+}</span>
<span class="w"> </span>FEED_EXPORT_BATCH_ITEM_COUNT = 0
<span class="w"> </span>FEED_EXPORTERS = {}
<span class="gd">-FEED_EXPORTERS_BASE = {&#39;json&#39;: &#39;scrapy.exporters.JsonItemExporter&#39;,</span>
<span class="gd">-    &#39;jsonlines&#39;: &#39;scrapy.exporters.JsonLinesItemExporter&#39;, &#39;jsonl&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.JsonLinesItemExporter&#39;, &#39;jl&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.JsonLinesItemExporter&#39;, &#39;csv&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.CsvItemExporter&#39;, &#39;xml&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.XmlItemExporter&#39;, &#39;marshal&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.MarshalItemExporter&#39;, &#39;pickle&#39;:</span>
<span class="gd">-    &#39;scrapy.exporters.PickleItemExporter&#39;}</span>
<span class="gi">+FEED_EXPORTERS_BASE = {</span>
<span class="gi">+    &quot;json&quot;: &quot;scrapy.exporters.JsonItemExporter&quot;,</span>
<span class="gi">+    &quot;jsonlines&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,</span>
<span class="gi">+    &quot;jsonl&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,</span>
<span class="gi">+    &quot;jl&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,</span>
<span class="gi">+    &quot;csv&quot;: &quot;scrapy.exporters.CsvItemExporter&quot;,</span>
<span class="gi">+    &quot;xml&quot;: &quot;scrapy.exporters.XmlItemExporter&quot;,</span>
<span class="gi">+    &quot;marshal&quot;: &quot;scrapy.exporters.MarshalItemExporter&quot;,</span>
<span class="gi">+    &quot;pickle&quot;: &quot;scrapy.exporters.PickleItemExporter&quot;,</span>
<span class="gi">+}</span>
<span class="w"> </span>FEED_EXPORT_INDENT = 0
<span class="gi">+</span>
<span class="w"> </span>FEED_STORAGE_FTP_ACTIVE = False
<span class="gd">-FEED_STORAGE_GCS_ACL = &#39;&#39;</span>
<span class="gd">-FEED_STORAGE_S3_ACL = &#39;&#39;</span>
<span class="gd">-FILES_STORE_S3_ACL = &#39;private&#39;</span>
<span class="gd">-FILES_STORE_GCS_ACL = &#39;&#39;</span>
<span class="gd">-FTP_USER = &#39;anonymous&#39;</span>
<span class="gd">-FTP_PASSWORD = &#39;guest&#39;</span>
<span class="gi">+FEED_STORAGE_GCS_ACL = &quot;&quot;</span>
<span class="gi">+FEED_STORAGE_S3_ACL = &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+FILES_STORE_S3_ACL = &quot;private&quot;</span>
<span class="gi">+FILES_STORE_GCS_ACL = &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+FTP_USER = &quot;anonymous&quot;</span>
<span class="gi">+FTP_PASSWORD = &quot;guest&quot;</span>
<span class="w"> </span>FTP_PASSIVE_MODE = True
<span class="gi">+</span>
<span class="w"> </span>GCS_PROJECT_ID = None
<span class="gi">+</span>
<span class="w"> </span>HTTPCACHE_ENABLED = False
<span class="gd">-HTTPCACHE_DIR = &#39;httpcache&#39;</span>
<span class="gi">+HTTPCACHE_DIR = &quot;httpcache&quot;</span>
<span class="w"> </span>HTTPCACHE_IGNORE_MISSING = False
<span class="gd">-HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;</span>
<span class="gi">+HTTPCACHE_STORAGE = &quot;scrapy.extensions.httpcache.FilesystemCacheStorage&quot;</span>
<span class="w"> </span>HTTPCACHE_EXPIRATION_SECS = 0
<span class="w"> </span>HTTPCACHE_ALWAYS_STORE = False
<span class="w"> </span>HTTPCACHE_IGNORE_HTTP_CODES = []
<span class="gd">-HTTPCACHE_IGNORE_SCHEMES = [&#39;file&#39;]</span>
<span class="gi">+HTTPCACHE_IGNORE_SCHEMES = [&quot;file&quot;]</span>
<span class="w"> </span>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []
<span class="gd">-HTTPCACHE_DBM_MODULE = &#39;dbm&#39;</span>
<span class="gd">-HTTPCACHE_POLICY = &#39;scrapy.extensions.httpcache.DummyPolicy&#39;</span>
<span class="gi">+HTTPCACHE_DBM_MODULE = &quot;dbm&quot;</span>
<span class="gi">+HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot;</span>
<span class="w"> </span>HTTPCACHE_GZIP = False
<span class="gi">+</span>
<span class="w"> </span>HTTPPROXY_ENABLED = True
<span class="gd">-HTTPPROXY_AUTH_ENCODING = &#39;latin-1&#39;</span>
<span class="gd">-IMAGES_STORE_S3_ACL = &#39;private&#39;</span>
<span class="gd">-IMAGES_STORE_GCS_ACL = &#39;&#39;</span>
<span class="gd">-ITEM_PROCESSOR = &#39;scrapy.pipelines.ItemPipelineManager&#39;</span>
<span class="gi">+HTTPPROXY_AUTH_ENCODING = &quot;latin-1&quot;</span>
<span class="gi">+</span>
<span class="gi">+IMAGES_STORE_S3_ACL = &quot;private&quot;</span>
<span class="gi">+IMAGES_STORE_GCS_ACL = &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+ITEM_PROCESSOR = &quot;scrapy.pipelines.ItemPipelineManager&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>ITEM_PIPELINES = {}
<span class="w"> </span>ITEM_PIPELINES_BASE = {}
<span class="gi">+</span>
<span class="w"> </span>JOBDIR = None
<span class="gi">+</span>
<span class="w"> </span>LOG_ENABLED = True
<span class="gd">-LOG_ENCODING = &#39;utf-8&#39;</span>
<span class="gd">-LOG_FORMATTER = &#39;scrapy.logformatter.LogFormatter&#39;</span>
<span class="gd">-LOG_FORMAT = &#39;%(asctime)s [%(name)s] %(levelname)s: %(message)s&#39;</span>
<span class="gd">-LOG_DATEFORMAT = &#39;%Y-%m-%d %H:%M:%S&#39;</span>
<span class="gi">+LOG_ENCODING = &quot;utf-8&quot;</span>
<span class="gi">+LOG_FORMATTER = &quot;scrapy.logformatter.LogFormatter&quot;</span>
<span class="gi">+LOG_FORMAT = &quot;%(asctime)s [%(name)s] %(levelname)s: %(message)s&quot;</span>
<span class="gi">+LOG_DATEFORMAT = &quot;%Y-%m-%d %H:%M:%S&quot;</span>
<span class="w"> </span>LOG_STDOUT = False
<span class="gd">-LOG_LEVEL = &#39;DEBUG&#39;</span>
<span class="gi">+LOG_LEVEL = &quot;DEBUG&quot;</span>
<span class="w"> </span>LOG_FILE = None
<span class="w"> </span>LOG_FILE_APPEND = True
<span class="w"> </span>LOG_SHORT_NAMES = False
<span class="gi">+</span>
<span class="w"> </span>SCHEDULER_DEBUG = False
<span class="gi">+</span>
<span class="w"> </span>LOGSTATS_INTERVAL = 60.0
<span class="gd">-MAIL_HOST = &#39;localhost&#39;</span>
<span class="gi">+</span>
<span class="gi">+MAIL_HOST = &quot;localhost&quot;</span>
<span class="w"> </span>MAIL_PORT = 25
<span class="gd">-MAIL_FROM = &#39;scrapy@localhost&#39;</span>
<span class="gi">+MAIL_FROM = &quot;scrapy@localhost&quot;</span>
<span class="w"> </span>MAIL_PASS = None
<span class="w"> </span>MAIL_USER = None
<span class="gd">-MEMDEBUG_ENABLED = False</span>
<span class="gd">-MEMDEBUG_NOTIFY = []</span>
<span class="gi">+</span>
<span class="gi">+MEMDEBUG_ENABLED = False  # enable memory debugging</span>
<span class="gi">+MEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown</span>
<span class="gi">+</span>
<span class="w"> </span>MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0
<span class="w"> </span>MEMUSAGE_ENABLED = True
<span class="w"> </span>MEMUSAGE_LIMIT_MB = 0
<span class="w"> </span>MEMUSAGE_NOTIFY_MAIL = []
<span class="w"> </span>MEMUSAGE_WARNING_MB = 0
<span class="gi">+</span>
<span class="w"> </span>METAREFRESH_ENABLED = True
<span class="gd">-METAREFRESH_IGNORE_TAGS = [&#39;noscript&#39;]</span>
<span class="gi">+METAREFRESH_IGNORE_TAGS = [&quot;noscript&quot;]</span>
<span class="w"> </span>METAREFRESH_MAXDELAY = 100
<span class="gd">-NEWSPIDER_MODULE = &#39;&#39;</span>
<span class="gi">+</span>
<span class="gi">+NEWSPIDER_MODULE = &quot;&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>PERIODIC_LOG_DELTA = None
<span class="w"> </span>PERIODIC_LOG_STATS = None
<span class="w"> </span>PERIODIC_LOG_TIMING_ENABLED = False
<span class="gi">+</span>
<span class="w"> </span>RANDOMIZE_DOWNLOAD_DELAY = True
<span class="gi">+</span>
<span class="w"> </span>REACTOR_THREADPOOL_MAXSIZE = 10
<span class="gi">+</span>
<span class="w"> </span>REDIRECT_ENABLED = True
<span class="gd">-REDIRECT_MAX_TIMES = 20</span>
<span class="gi">+REDIRECT_MAX_TIMES = 20  # uses Firefox default setting</span>
<span class="w"> </span>REDIRECT_PRIORITY_ADJUST = +2
<span class="gi">+</span>
<span class="w"> </span>REFERER_ENABLED = True
<span class="gd">-REFERRER_POLICY = &#39;scrapy.spidermiddlewares.referer.DefaultReferrerPolicy&#39;</span>
<span class="gd">-REQUEST_FINGERPRINTER_CLASS = &#39;scrapy.utils.request.RequestFingerprinter&#39;</span>
<span class="gd">-REQUEST_FINGERPRINTER_IMPLEMENTATION = &#39;2.6&#39;</span>
<span class="gi">+REFERRER_POLICY = &quot;scrapy.spidermiddlewares.referer.DefaultReferrerPolicy&quot;</span>
<span class="gi">+</span>
<span class="gi">+REQUEST_FINGERPRINTER_CLASS = &quot;scrapy.utils.request.RequestFingerprinter&quot;</span>
<span class="gi">+REQUEST_FINGERPRINTER_IMPLEMENTATION = &quot;2.6&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>RETRY_ENABLED = True
<span class="gd">-RETRY_TIMES = 2</span>
<span class="gi">+RETRY_TIMES = 2  # initial response + 2 retries = 3 requests</span>
<span class="w"> </span>RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]
<span class="w"> </span>RETRY_PRIORITY_ADJUST = -1
<span class="gd">-RETRY_EXCEPTIONS = [&#39;twisted.internet.defer.TimeoutError&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.TimeoutError&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.DNSLookupError&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.ConnectionRefusedError&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.ConnectionDone&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.ConnectError&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.ConnectionLost&#39;,</span>
<span class="gd">-    &#39;twisted.internet.error.TCPTimedOutError&#39;,</span>
<span class="gd">-    &#39;twisted.web.client.ResponseFailed&#39;, OSError,</span>
<span class="gd">-    &#39;scrapy.core.downloader.handlers.http11.TunnelError&#39;]</span>
<span class="gi">+RETRY_EXCEPTIONS = [</span>
<span class="gi">+    &quot;twisted.internet.defer.TimeoutError&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.TimeoutError&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.DNSLookupError&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.ConnectionRefusedError&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.ConnectionDone&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.ConnectError&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.ConnectionLost&quot;,</span>
<span class="gi">+    &quot;twisted.internet.error.TCPTimedOutError&quot;,</span>
<span class="gi">+    &quot;twisted.web.client.ResponseFailed&quot;,</span>
<span class="gi">+    # OSError is raised by the HttpCompression middleware when trying to</span>
<span class="gi">+    # decompress an empty response</span>
<span class="gi">+    OSError,</span>
<span class="gi">+    &quot;scrapy.core.downloader.handlers.http11.TunnelError&quot;,</span>
<span class="gi">+]</span>
<span class="gi">+</span>
<span class="w"> </span>ROBOTSTXT_OBEY = False
<span class="gd">-ROBOTSTXT_PARSER = &#39;scrapy.robotstxt.ProtegoRobotParser&#39;</span>
<span class="gi">+ROBOTSTXT_PARSER = &quot;scrapy.robotstxt.ProtegoRobotParser&quot;</span>
<span class="w"> </span>ROBOTSTXT_USER_AGENT = None
<span class="gd">-SCHEDULER = &#39;scrapy.core.scheduler.Scheduler&#39;</span>
<span class="gd">-SCHEDULER_DISK_QUEUE = &#39;scrapy.squeues.PickleLifoDiskQueue&#39;</span>
<span class="gd">-SCHEDULER_MEMORY_QUEUE = &#39;scrapy.squeues.LifoMemoryQueue&#39;</span>
<span class="gd">-SCHEDULER_PRIORITY_QUEUE = &#39;scrapy.pqueues.ScrapyPriorityQueue&#39;</span>
<span class="gi">+</span>
<span class="gi">+SCHEDULER = &quot;scrapy.core.scheduler.Scheduler&quot;</span>
<span class="gi">+SCHEDULER_DISK_QUEUE = &quot;scrapy.squeues.PickleLifoDiskQueue&quot;</span>
<span class="gi">+SCHEDULER_MEMORY_QUEUE = &quot;scrapy.squeues.LifoMemoryQueue&quot;</span>
<span class="gi">+SCHEDULER_PRIORITY_QUEUE = &quot;scrapy.pqueues.ScrapyPriorityQueue&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000
<span class="gd">-SPIDER_LOADER_CLASS = &#39;scrapy.spiderloader.SpiderLoader&#39;</span>
<span class="gi">+</span>
<span class="gi">+SPIDER_LOADER_CLASS = &quot;scrapy.spiderloader.SpiderLoader&quot;</span>
<span class="w"> </span>SPIDER_LOADER_WARN_ONLY = False
<span class="gi">+</span>
<span class="w"> </span>SPIDER_MIDDLEWARES = {}
<span class="gi">+</span>
<span class="w"> </span>SPIDER_MIDDLEWARES_BASE = {
<span class="gd">-    &#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;: 50,</span>
<span class="gd">-    &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;: 700,</span>
<span class="gd">-    &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;: 800,</span>
<span class="gd">-    &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;: 900}</span>
<span class="gi">+    # Engine side</span>
<span class="gi">+    &quot;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&quot;: 50,</span>
<span class="gi">+    &quot;scrapy.spidermiddlewares.referer.RefererMiddleware&quot;: 700,</span>
<span class="gi">+    &quot;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&quot;: 800,</span>
<span class="gi">+    &quot;scrapy.spidermiddlewares.depth.DepthMiddleware&quot;: 900,</span>
<span class="gi">+    # Spider side</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="w"> </span>SPIDER_MODULES = []
<span class="gd">-STATS_CLASS = &#39;scrapy.statscollectors.MemoryStatsCollector&#39;</span>
<span class="gi">+</span>
<span class="gi">+STATS_CLASS = &quot;scrapy.statscollectors.MemoryStatsCollector&quot;</span>
<span class="w"> </span>STATS_DUMP = True
<span class="gi">+</span>
<span class="w"> </span>STATSMAILER_RCPTS = []
<span class="gd">-TEMPLATES_DIR = str((Path(__file__).parent / &#39;..&#39; / &#39;templates&#39;).resolve())</span>
<span class="gi">+</span>
<span class="gi">+TEMPLATES_DIR = str((Path(__file__).parent / &quot;..&quot; / &quot;templates&quot;).resolve())</span>
<span class="gi">+</span>
<span class="w"> </span>URLLENGTH_LIMIT = 2083
<span class="gd">-USER_AGENT = (</span>
<span class="gd">-    f&quot;Scrapy/{import_module(&#39;scrapy&#39;).__version__} (+https://scrapy.org)&quot;)</span>
<span class="gi">+</span>
<span class="gi">+USER_AGENT = f&#39;Scrapy/{import_module(&quot;scrapy&quot;).__version__} (+https://scrapy.org)&#39;</span>
<span class="gi">+</span>
<span class="w"> </span>TELNETCONSOLE_ENABLED = 1
<span class="w"> </span>TELNETCONSOLE_PORT = [6023, 6073]
<span class="gd">-TELNETCONSOLE_HOST = &#39;127.0.0.1&#39;</span>
<span class="gd">-TELNETCONSOLE_USERNAME = &#39;scrapy&#39;</span>
<span class="gi">+TELNETCONSOLE_HOST = &quot;127.0.0.1&quot;</span>
<span class="gi">+TELNETCONSOLE_USERNAME = &quot;scrapy&quot;</span>
<span class="w"> </span>TELNETCONSOLE_PASSWORD = None
<span class="gi">+</span>
<span class="w"> </span>TWISTED_REACTOR = None
<span class="gi">+</span>
<span class="w"> </span>SPIDER_CONTRACTS = {}
<span class="gd">-SPIDER_CONTRACTS_BASE = {&#39;scrapy.contracts.default.UrlContract&#39;: 1,</span>
<span class="gd">-    &#39;scrapy.contracts.default.CallbackKeywordArgumentsContract&#39;: 1,</span>
<span class="gd">-    &#39;scrapy.contracts.default.ReturnsContract&#39;: 2,</span>
<span class="gd">-    &#39;scrapy.contracts.default.ScrapesContract&#39;: 3}</span>
<span class="gi">+SPIDER_CONTRACTS_BASE = {</span>
<span class="gi">+    &quot;scrapy.contracts.default.UrlContract&quot;: 1,</span>
<span class="gi">+    &quot;scrapy.contracts.default.CallbackKeywordArgumentsContract&quot;: 1,</span>
<span class="gi">+    &quot;scrapy.contracts.default.ReturnsContract&quot;: 2,</span>
<span class="gi">+    &quot;scrapy.contracts.default.ScrapesContract&quot;: 3,</span>
<span class="gi">+}</span>
<span class="gh">diff --git a/scrapy/shell.py b/scrapy/shell.py</span>
<span class="gh">index f8a94309b..bb3b1461c 100644</span>
<span class="gd">--- a/scrapy/shell.py</span>
<span class="gi">+++ b/scrapy/shell.py</span>
<span class="gu">@@ -5,10 +5,12 @@ See documentation in docs/topics/shell.rst</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import os
<span class="w"> </span>import signal
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import is_item
<span class="w"> </span>from twisted.internet import defer, threads
<span class="w"> </span>from twisted.python import threadable
<span class="w"> </span>from w3lib.url import any_to_uri
<span class="gi">+</span>
<span class="w"> </span>from scrapy.crawler import Crawler
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest
<span class="w"> </span>from scrapy.http import Request, Response
<span class="gu">@@ -23,21 +25,159 @@ from scrapy.utils.response import open_in_browser</span>


<span class="w"> </span>class Shell:
<span class="gd">-    relevant_classes = Crawler, Spider, Request, Response, Settings</span>
<span class="gi">+    relevant_classes = (Crawler, Spider, Request, Response, Settings)</span>

<span class="w"> </span>    def __init__(self, crawler, update_vars=None, code=None):
<span class="w"> </span>        self.crawler = crawler
<span class="w"> </span>        self.update_vars = update_vars or (lambda x: None)
<span class="gd">-        self.item_class = load_object(crawler.settings[&#39;DEFAULT_ITEM_CLASS&#39;])</span>
<span class="gi">+        self.item_class = load_object(crawler.settings[&quot;DEFAULT_ITEM_CLASS&quot;])</span>
<span class="w"> </span>        self.spider = None
<span class="w"> </span>        self.inthread = not threadable.isInIOThread()
<span class="w"> </span>        self.code = code
<span class="w"> </span>        self.vars = {}

<span class="gi">+    def start(self, url=None, request=None, response=None, spider=None, redirect=True):</span>
<span class="gi">+        # disable accidental Ctrl-C key press from shutting down the engine</span>
<span class="gi">+        signal.signal(signal.SIGINT, signal.SIG_IGN)</span>
<span class="gi">+        if url:</span>
<span class="gi">+            self.fetch(url, spider, redirect=redirect)</span>
<span class="gi">+        elif request:</span>
<span class="gi">+            self.fetch(request, spider)</span>
<span class="gi">+        elif response:</span>
<span class="gi">+            request = response.request</span>
<span class="gi">+            self.populate_vars(response, request, spider)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.populate_vars()</span>
<span class="gi">+        if self.code:</span>
<span class="gi">+            print(eval(self.code, globals(), self.vars))</span>
<span class="gi">+        else:</span>
<span class="gi">+            &quot;&quot;&quot;</span>
<span class="gi">+            Detect interactive shell setting in scrapy.cfg</span>
<span class="gi">+            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg</span>
<span class="gi">+            [settings]</span>
<span class="gi">+            # shell can be one of ipython, bpython or python;</span>
<span class="gi">+            # to be used as the interactive python console, if available.</span>
<span class="gi">+            # (default is ipython, fallbacks in the order listed above)</span>
<span class="gi">+            shell = python</span>
<span class="gi">+            &quot;&quot;&quot;</span>
<span class="gi">+            cfg = get_config()</span>
<span class="gi">+            section, option = &quot;settings&quot;, &quot;shell&quot;</span>
<span class="gi">+            env = os.environ.get(&quot;SCRAPY_PYTHON_SHELL&quot;)</span>
<span class="gi">+            shells = []</span>
<span class="gi">+            if env:</span>
<span class="gi">+                shells += env.strip().lower().split(&quot;,&quot;)</span>
<span class="gi">+            elif cfg.has_option(section, option):</span>
<span class="gi">+                shells += [cfg.get(section, option).strip().lower()]</span>
<span class="gi">+            else:  # try all by default</span>
<span class="gi">+                shells += DEFAULT_PYTHON_SHELLS.keys()</span>
<span class="gi">+            # always add standard shell as fallback</span>
<span class="gi">+            shells += [&quot;python&quot;]</span>
<span class="gi">+            start_python_console(</span>
<span class="gi">+                self.vars, shells=shells, banner=self.vars.pop(&quot;banner&quot;, &quot;&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _schedule(self, request, spider):</span>
<span class="gi">+        if is_asyncio_reactor_installed():</span>
<span class="gi">+            # set the asyncio event loop for the current thread</span>
<span class="gi">+            event_loop_path = self.crawler.settings[&quot;ASYNCIO_EVENT_LOOP&quot;]</span>
<span class="gi">+            set_asyncio_event_loop(event_loop_path)</span>
<span class="gi">+        spider = self._open_spider(request, spider)</span>
<span class="gi">+        d = _request_deferred(request)</span>
<span class="gi">+        d.addCallback(lambda x: (x, spider))</span>
<span class="gi">+        self.crawler.engine.crawl(request)</span>
<span class="gi">+        return d</span>
<span class="gi">+</span>
<span class="gi">+    def _open_spider(self, request, spider):</span>
<span class="gi">+        if self.spider:</span>
<span class="gi">+            return self.spider</span>
<span class="gi">+</span>
<span class="gi">+        if spider is None:</span>
<span class="gi">+            spider = self.crawler.spider or self.crawler._create_spider()</span>
<span class="gi">+</span>
<span class="gi">+        self.crawler.spider = spider</span>
<span class="gi">+        self.crawler.engine.open_spider(spider, close_if_idle=False)</span>
<span class="gi">+        self.spider = spider</span>
<span class="gi">+        return spider</span>
<span class="gi">+</span>
<span class="gi">+    def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(request_or_url, Request):</span>
<span class="gi">+            request = request_or_url</span>
<span class="gi">+        else:</span>
<span class="gi">+            url = any_to_uri(request_or_url)</span>
<span class="gi">+            request = Request(url, dont_filter=True, **kwargs)</span>
<span class="gi">+            if redirect:</span>
<span class="gi">+                request.meta[&quot;handle_httpstatus_list&quot;] = SequenceExclude(</span>
<span class="gi">+                    range(300, 400)</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                request.meta[&quot;handle_httpstatus_all&quot;] = True</span>
<span class="gi">+        response = None</span>
<span class="gi">+        try:</span>
<span class="gi">+            response, spider = threads.blockingCallFromThread(</span>
<span class="gi">+                reactor, self._schedule, request, spider</span>
<span class="gi">+            )</span>
<span class="gi">+        except IgnoreRequest:</span>
<span class="gi">+            pass</span>
<span class="gi">+        self.populate_vars(response, request, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def populate_vars(self, response=None, request=None, spider=None):</span>
<span class="gi">+        import scrapy</span>
<span class="gi">+</span>
<span class="gi">+        self.vars[&quot;scrapy&quot;] = scrapy</span>
<span class="gi">+        self.vars[&quot;crawler&quot;] = self.crawler</span>
<span class="gi">+        self.vars[&quot;item&quot;] = self.item_class()</span>
<span class="gi">+        self.vars[&quot;settings&quot;] = self.crawler.settings</span>
<span class="gi">+        self.vars[&quot;spider&quot;] = spider</span>
<span class="gi">+        self.vars[&quot;request&quot;] = request</span>
<span class="gi">+        self.vars[&quot;response&quot;] = response</span>
<span class="gi">+        if self.inthread:</span>
<span class="gi">+            self.vars[&quot;fetch&quot;] = self.fetch</span>
<span class="gi">+        self.vars[&quot;view&quot;] = open_in_browser</span>
<span class="gi">+        self.vars[&quot;shelp&quot;] = self.print_help</span>
<span class="gi">+        self.update_vars(self.vars)</span>
<span class="gi">+        if not self.code:</span>
<span class="gi">+            self.vars[&quot;banner&quot;] = self.get_help()</span>
<span class="gi">+</span>
<span class="gi">+    def print_help(self):</span>
<span class="gi">+        print(self.get_help())</span>
<span class="gi">+</span>
<span class="gi">+    def get_help(self):</span>
<span class="gi">+        b = []</span>
<span class="gi">+        b.append(&quot;Available Scrapy objects:&quot;)</span>
<span class="gi">+        b.append(</span>
<span class="gi">+            &quot;  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        for k, v in sorted(self.vars.items()):</span>
<span class="gi">+            if self._is_relevant(v):</span>
<span class="gi">+                b.append(f&quot;  {k:&lt;10} {v}&quot;)</span>
<span class="gi">+        b.append(&quot;Useful shortcuts:&quot;)</span>
<span class="gi">+        if self.inthread:</span>
<span class="gi">+            b.append(</span>
<span class="gi">+                &quot;  fetch(url[, redirect=True]) &quot;</span>
<span class="gi">+                &quot;Fetch URL and update local objects (by default, redirects are followed)&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            b.append(</span>
<span class="gi">+                &quot;  fetch(req)                  &quot;</span>
<span class="gi">+                &quot;Fetch a scrapy.Request and update local objects &quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        b.append(&quot;  shelp()           Shell help (print this help)&quot;)</span>
<span class="gi">+        b.append(&quot;  view(response)    View response in a browser&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        return &quot;\n&quot;.join(f&quot;[s] {line}&quot; for line in b)</span>
<span class="gi">+</span>
<span class="gi">+    def _is_relevant(self, value):</span>
<span class="gi">+        return isinstance(value, self.relevant_classes) or is_item(value)</span>
<span class="gi">+</span>

<span class="w"> </span>def inspect_response(response, spider):
<span class="w"> </span>    &quot;&quot;&quot;Open a shell to inspect the given response&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Shell.start removes the SIGINT handler, so save it and re-add it after</span>
<span class="gi">+    # the shell has closed</span>
<span class="gi">+    sigint_handler = signal.getsignal(signal.SIGINT)</span>
<span class="gi">+    Shell(spider.crawler).start(response=response, spider=spider)</span>
<span class="gi">+    signal.signal(signal.SIGINT, sigint_handler)</span>


<span class="w"> </span>def _request_deferred(request):
<span class="gu">@@ -51,4 +191,18 @@ def _request_deferred(request):</span>

<span class="w"> </span>    WARNING: Do not call request.replace() until after the deferred is called.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    request_callback = request.callback</span>
<span class="gi">+    request_errback = request.errback</span>
<span class="gi">+</span>
<span class="gi">+    def _restore_callbacks(result):</span>
<span class="gi">+        request.callback = request_callback</span>
<span class="gi">+        request.errback = request_errback</span>
<span class="gi">+        return result</span>
<span class="gi">+</span>
<span class="gi">+    d = defer.Deferred()</span>
<span class="gi">+    d.addBoth(_restore_callbacks)</span>
<span class="gi">+    if request.callback:</span>
<span class="gi">+        d.addCallbacks(request.callback, request.errback)</span>
<span class="gi">+</span>
<span class="gi">+    request.callback, request.errback = d.callback, d.errback</span>
<span class="gi">+    return d</span>
<span class="gh">diff --git a/scrapy/signalmanager.py b/scrapy/signalmanager.py</span>
<span class="gh">index e85b12c05..f6df191d8 100644</span>
<span class="gd">--- a/scrapy/signalmanager.py</span>
<span class="gi">+++ b/scrapy/signalmanager.py</span>
<span class="gu">@@ -1,15 +1,16 @@</span>
<span class="w"> </span>from typing import Any, List, Tuple
<span class="gi">+</span>
<span class="w"> </span>from pydispatch import dispatcher
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils import signal as _signal


<span class="w"> </span>class SignalManager:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, sender: Any=dispatcher.Anonymous):</span>
<span class="gi">+    def __init__(self, sender: Any = dispatcher.Anonymous):</span>
<span class="w"> </span>        self.sender: Any = sender

<span class="gd">-    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -&gt;None:</span>
<span class="gi">+    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Connect a receiver function to a signal.

<span class="gu">@@ -23,27 +24,29 @@ class SignalManager:</span>
<span class="w"> </span>        :param signal: the signal to connect to
<span class="w"> </span>        :type signal: object
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        kwargs.setdefault(&quot;sender&quot;, self.sender)</span>
<span class="gi">+        dispatcher.connect(receiver, signal, **kwargs)</span>

<span class="gd">-    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -&gt;None:</span>
<span class="gi">+    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Disconnect a receiver function from a signal. This has the
<span class="w"> </span>        opposite effect of the :meth:`connect` method, and the arguments
<span class="w"> </span>        are the same.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        kwargs.setdefault(&quot;sender&quot;, self.sender)</span>
<span class="gi">+        dispatcher.disconnect(receiver, signal, **kwargs)</span>

<span class="gd">-    def send_catch_log(self, signal: Any, **kwargs: Any) -&gt;List[Tuple[Any, Any]</span>
<span class="gd">-        ]:</span>
<span class="gi">+    def send_catch_log(self, signal: Any, **kwargs: Any) -&gt; List[Tuple[Any, Any]]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Send a signal, catch exceptions and log them.

<span class="w"> </span>        The keyword arguments are passed to the signal handlers (connected
<span class="w"> </span>        through the :meth:`connect` method).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        kwargs.setdefault(&quot;sender&quot;, self.sender)</span>
<span class="gi">+        return _signal.send_catch_log(signal, **kwargs)</span>

<span class="gd">-    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -&gt;Deferred:</span>
<span class="gi">+    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -&gt; Deferred:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Like :meth:`send_catch_log` but supports returning
<span class="w"> </span>        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.
<span class="gu">@@ -54,13 +57,15 @@ class SignalManager:</span>
<span class="w"> </span>        The keyword arguments are passed to the signal handlers (connected
<span class="w"> </span>        through the :meth:`connect` method).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        kwargs.setdefault(&quot;sender&quot;, self.sender)</span>
<span class="gi">+        return _signal.send_catch_log_deferred(signal, **kwargs)</span>

<span class="gd">-    def disconnect_all(self, signal: Any, **kwargs: Any) -&gt;None:</span>
<span class="gi">+    def disconnect_all(self, signal: Any, **kwargs: Any) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Disconnect all receivers from the given signal.

<span class="w"> </span>        :param signal: the signal to disconnect from
<span class="w"> </span>        :type signal: object
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        kwargs.setdefault(&quot;sender&quot;, self.sender)</span>
<span class="gi">+        _signal.disconnect_all(signal, **kwargs)</span>
<span class="gh">diff --git a/scrapy/signals.py b/scrapy/signals.py</span>
<span class="gh">index 0d08d829c..0090f1c8b 100644</span>
<span class="gd">--- a/scrapy/signals.py</span>
<span class="gi">+++ b/scrapy/signals.py</span>
<span class="gu">@@ -4,6 +4,7 @@ Scrapy signals</span>
<span class="w"> </span>These signals are documented in docs/topics/signals.rst. Please don&#39;t add new
<span class="w"> </span>signals here without documenting them there.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>engine_started = object()
<span class="w"> </span>engine_stopped = object()
<span class="w"> </span>spider_opened = object()
<span class="gu">@@ -23,8 +24,12 @@ item_dropped = object()</span>
<span class="w"> </span>item_error = object()
<span class="w"> </span>feed_slot_closed = object()
<span class="w"> </span>feed_exporter_closed = object()
<span class="gi">+</span>
<span class="gi">+# for backward compatibility</span>
<span class="w"> </span>stats_spider_opened = spider_opened
<span class="w"> </span>stats_spider_closing = spider_closed
<span class="w"> </span>stats_spider_closed = spider_closed
<span class="gi">+</span>
<span class="w"> </span>item_passed = item_scraped
<span class="gi">+</span>
<span class="w"> </span>request_received = request_scheduled
<span class="gh">diff --git a/scrapy/spiderloader.py b/scrapy/spiderloader.py</span>
<span class="gh">index 9d53190bb..d855c962c 100644</span>
<span class="gd">--- a/scrapy/spiderloader.py</span>
<span class="gi">+++ b/scrapy/spiderloader.py</span>
<span class="gu">@@ -1,16 +1,21 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import traceback
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import defaultdict
<span class="w"> </span>from types import ModuleType
<span class="w"> </span>from typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type
<span class="gi">+</span>
<span class="w"> </span>from zope.interface import implementer
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request, Spider
<span class="w"> </span>from scrapy.interfaces import ISpiderLoader
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="w"> </span>from scrapy.utils.misc import walk_modules
<span class="w"> </span>from scrapy.utils.spider import iter_spider_classes
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self


<span class="gu">@@ -22,28 +27,77 @@ class SpiderLoader:</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, settings: BaseSettings):
<span class="gd">-        self.spider_modules: List[str] = settings.getlist(&#39;SPIDER_MODULES&#39;)</span>
<span class="gd">-        self.warn_only: bool = settings.getbool(&#39;SPIDER_LOADER_WARN_ONLY&#39;)</span>
<span class="gi">+        self.spider_modules: List[str] = settings.getlist(&quot;SPIDER_MODULES&quot;)</span>
<span class="gi">+        self.warn_only: bool = settings.getbool(&quot;SPIDER_LOADER_WARN_ONLY&quot;)</span>
<span class="w"> </span>        self._spiders: Dict[str, Type[Spider]] = {}
<span class="gd">-        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list</span>
<span class="gd">-            )</span>
<span class="gi">+        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)</span>
<span class="w"> </span>        self._load_all_spiders()

<span class="gd">-    def load(self, spider_name: str) -&gt;Type[Spider]:</span>
<span class="gi">+    def _check_name_duplicates(self) -&gt; None:</span>
<span class="gi">+        dupes = []</span>
<span class="gi">+        for name, locations in self._found.items():</span>
<span class="gi">+            dupes.extend(</span>
<span class="gi">+                [</span>
<span class="gi">+                    f&quot;  {cls} named {name!r} (in {mod})&quot;</span>
<span class="gi">+                    for mod, cls in locations</span>
<span class="gi">+                    if len(locations) &gt; 1</span>
<span class="gi">+                ]</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if dupes:</span>
<span class="gi">+            dupes_string = &quot;\n\n&quot;.join(dupes)</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;There are several spiders with the same name:\n\n&quot;</span>
<span class="gi">+                f&quot;{dupes_string}\n\n  This can cause unexpected behavior.&quot;,</span>
<span class="gi">+                category=UserWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _load_spiders(self, module: ModuleType) -&gt; None:</span>
<span class="gi">+        for spcls in iter_spider_classes(module):</span>
<span class="gi">+            self._found[spcls.name].append((module.__name__, spcls.__name__))</span>
<span class="gi">+            self._spiders[spcls.name] = spcls</span>
<span class="gi">+</span>
<span class="gi">+    def _load_all_spiders(self) -&gt; None:</span>
<span class="gi">+        for name in self.spider_modules:</span>
<span class="gi">+            try:</span>
<span class="gi">+                for module in walk_modules(name):</span>
<span class="gi">+                    self._load_spiders(module)</span>
<span class="gi">+            except ImportError:</span>
<span class="gi">+                if self.warn_only:</span>
<span class="gi">+                    warnings.warn(</span>
<span class="gi">+                        f&quot;\n{traceback.format_exc()}Could not load spiders &quot;</span>
<span class="gi">+                        f&quot;from module &#39;{name}&#39;. &quot;</span>
<span class="gi">+                        &quot;See above traceback for details.&quot;,</span>
<span class="gi">+                        category=RuntimeWarning,</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    raise</span>
<span class="gi">+        self._check_name_duplicates()</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings: BaseSettings) -&gt; Self:</span>
<span class="gi">+        return cls(settings)</span>
<span class="gi">+</span>
<span class="gi">+    def load(self, spider_name: str) -&gt; Type[Spider]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return the Spider class for the given spider name. If the spider
<span class="w"> </span>        name is not found, raise a KeyError.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            return self._spiders[spider_name]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            raise KeyError(f&quot;Spider not found: {spider_name}&quot;)</span>

<span class="gd">-    def find_by_request(self, request: Request) -&gt;List[str]:</span>
<span class="gi">+    def find_by_request(self, request: Request) -&gt; List[str]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return the list of spider names that can handle the given request.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return [</span>
<span class="gi">+            name for name, cls in self._spiders.items() if cls.handles_request(request)</span>
<span class="gi">+        ]</span>

<span class="gd">-    def list(self) -&gt;List[str]:</span>
<span class="gi">+    def list(self) -&gt; List[str]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Return a list with the names of all spiders available in the project.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return list(self._spiders.keys())</span>
<span class="gh">diff --git a/scrapy/spidermiddlewares/depth.py b/scrapy/spidermiddlewares/depth.py</span>
<span class="gh">index 6b9fdb9ee..eadc7c6ab 100644</span>
<span class="gd">--- a/scrapy/spidermiddlewares/depth.py</span>
<span class="gi">+++ b/scrapy/spidermiddlewares/depth.py</span>
<span class="gu">@@ -3,15 +3,61 @@ Depth Spider Middleware</span>

<span class="w"> </span>See documentation in docs/topics/spider-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class DepthMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):
<span class="w"> </span>        self.maxdepth = maxdepth
<span class="w"> </span>        self.stats = stats
<span class="w"> </span>        self.verbose_stats = verbose_stats
<span class="w"> </span>        self.prio = prio
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        settings = crawler.settings</span>
<span class="gi">+        maxdepth = settings.getint(&quot;DEPTH_LIMIT&quot;)</span>
<span class="gi">+        verbose = settings.getbool(&quot;DEPTH_STATS_VERBOSE&quot;)</span>
<span class="gi">+        prio = settings.getint(&quot;DEPTH_PRIORITY&quot;)</span>
<span class="gi">+        return cls(maxdepth, crawler.stats, verbose, prio)</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_output(self, response, result, spider):</span>
<span class="gi">+        self._init_depth(response, spider)</span>
<span class="gi">+        return (r for r in result or () if self._filter(r, response, spider))</span>
<span class="gi">+</span>
<span class="gi">+    async def process_spider_output_async(self, response, result, spider):</span>
<span class="gi">+        self._init_depth(response, spider)</span>
<span class="gi">+        async for r in result or ():</span>
<span class="gi">+            if self._filter(r, response, spider):</span>
<span class="gi">+                yield r</span>
<span class="gi">+</span>
<span class="gi">+    def _init_depth(self, response, spider):</span>
<span class="gi">+        # base case (depth=0)</span>
<span class="gi">+        if &quot;depth&quot; not in response.meta:</span>
<span class="gi">+            response.meta[&quot;depth&quot;] = 0</span>
<span class="gi">+            if self.verbose_stats:</span>
<span class="gi">+                self.stats.inc_value(&quot;request_depth_count/0&quot;, spider=spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _filter(self, request, response, spider):</span>
<span class="gi">+        if not isinstance(request, Request):</span>
<span class="gi">+            return True</span>
<span class="gi">+        depth = response.meta[&quot;depth&quot;] + 1</span>
<span class="gi">+        request.meta[&quot;depth&quot;] = depth</span>
<span class="gi">+        if self.prio:</span>
<span class="gi">+            request.priority -= depth * self.prio</span>
<span class="gi">+        if self.maxdepth and depth &gt; self.maxdepth:</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Ignoring link (depth &gt; %(maxdepth)d): %(requrl)s &quot;,</span>
<span class="gi">+                {&quot;maxdepth&quot;: self.maxdepth, &quot;requrl&quot;: request.url},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            return False</span>
<span class="gi">+        if self.verbose_stats:</span>
<span class="gi">+            self.stats.inc_value(f&quot;request_depth_count/{depth}&quot;, spider=spider)</span>
<span class="gi">+        self.stats.max_value(&quot;request_depth_max&quot;, depth, spider=spider)</span>
<span class="gi">+        return True</span>
<span class="gh">diff --git a/scrapy/spidermiddlewares/httperror.py b/scrapy/spidermiddlewares/httperror.py</span>
<span class="gh">index 001661412..0d3e5fe0b 100644</span>
<span class="gd">--- a/scrapy/spidermiddlewares/httperror.py</span>
<span class="gi">+++ b/scrapy/spidermiddlewares/httperror.py</span>
<span class="gu">@@ -4,7 +4,9 @@ HttpError Spider Middleware</span>
<span class="w"> </span>See documentation in docs/topics/spider-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gu">@@ -17,8 +19,41 @@ class HttpError(IgnoreRequest):</span>


<span class="w"> </span>class HttpErrorMiddleware:
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler.settings)</span>

<span class="w"> </span>    def __init__(self, settings):
<span class="gd">-        self.handle_httpstatus_all = settings.getbool(&#39;HTTPERROR_ALLOW_ALL&#39;)</span>
<span class="gd">-        self.handle_httpstatus_list = settings.getlist(</span>
<span class="gd">-            &#39;HTTPERROR_ALLOWED_CODES&#39;)</span>
<span class="gi">+        self.handle_httpstatus_all = settings.getbool(&quot;HTTPERROR_ALLOW_ALL&quot;)</span>
<span class="gi">+        self.handle_httpstatus_list = settings.getlist(&quot;HTTPERROR_ALLOWED_CODES&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_input(self, response, spider):</span>
<span class="gi">+        if 200 &lt;= response.status &lt; 300:  # common case</span>
<span class="gi">+            return</span>
<span class="gi">+        meta = response.meta</span>
<span class="gi">+        if meta.get(&quot;handle_httpstatus_all&quot;, False):</span>
<span class="gi">+            return</span>
<span class="gi">+        if &quot;handle_httpstatus_list&quot; in meta:</span>
<span class="gi">+            allowed_statuses = meta[&quot;handle_httpstatus_list&quot;]</span>
<span class="gi">+        elif self.handle_httpstatus_all:</span>
<span class="gi">+            return</span>
<span class="gi">+        else:</span>
<span class="gi">+            allowed_statuses = getattr(</span>
<span class="gi">+                spider, &quot;handle_httpstatus_list&quot;, self.handle_httpstatus_list</span>
<span class="gi">+            )</span>
<span class="gi">+        if response.status in allowed_statuses:</span>
<span class="gi">+            return</span>
<span class="gi">+        raise HttpError(response, &quot;Ignoring non-200 response&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_exception(self, response, exception, spider):</span>
<span class="gi">+        if isinstance(exception, HttpError):</span>
<span class="gi">+            spider.crawler.stats.inc_value(&quot;httperror/response_ignored_count&quot;)</span>
<span class="gi">+            spider.crawler.stats.inc_value(</span>
<span class="gi">+                f&quot;httperror/response_ignored_status_count/{response.status}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;Ignoring response %(response)r: HTTP status code is not handled or not allowed&quot;,</span>
<span class="gi">+                {&quot;response&quot;: response},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            return []</span>
<span class="gh">diff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py</span>
<span class="gh">index 5b86596e3..243055d89 100644</span>
<span class="gd">--- a/scrapy/spidermiddlewares/offsite.py</span>
<span class="gi">+++ b/scrapy/spidermiddlewares/offsite.py</span>
<span class="gu">@@ -6,24 +6,93 @@ See documentation in docs/topics/spider-middleware.rst</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.http import Request
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="gi">+</span>
<span class="w"> </span>warnings.warn(
<span class="gd">-    &#39;The scrapy.spidermiddlewares.offsite module is deprecated, use scrapy.downloadermiddlewares.offsite instead.&#39;</span>
<span class="gd">-    , ScrapyDeprecationWarning)</span>
<span class="gi">+    &quot;The scrapy.spidermiddlewares.offsite module is deprecated, use &quot;</span>
<span class="gi">+    &quot;scrapy.downloadermiddlewares.offsite instead.&quot;,</span>
<span class="gi">+    ScrapyDeprecationWarning,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class OffsiteMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, stats):
<span class="w"> </span>        self.stats = stats

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        o = cls(crawler.stats)</span>
<span class="gi">+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span>
<span class="gi">+        return o</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_output(self, response, result, spider):</span>
<span class="gi">+        return (r for r in result or () if self._filter(r, spider))</span>
<span class="gi">+</span>
<span class="gi">+    async def process_spider_output_async(self, response, result, spider):</span>
<span class="gi">+        async for r in result or ():</span>
<span class="gi">+            if self._filter(r, spider):</span>
<span class="gi">+                yield r</span>
<span class="gi">+</span>
<span class="gi">+    def _filter(self, request, spider) -&gt; bool:</span>
<span class="gi">+        if not isinstance(request, Request):</span>
<span class="gi">+            return True</span>
<span class="gi">+        if request.dont_filter or self.should_follow(request, spider):</span>
<span class="gi">+            return True</span>
<span class="gi">+        domain = urlparse_cached(request).hostname</span>
<span class="gi">+        if domain and domain not in self.domains_seen:</span>
<span class="gi">+            self.domains_seen.add(domain)</span>
<span class="gi">+            logger.debug(</span>
<span class="gi">+                &quot;Filtered offsite request to %(domain)r: %(request)s&quot;,</span>
<span class="gi">+                {&quot;domain&quot;: domain, &quot;request&quot;: request},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            self.stats.inc_value(&quot;offsite/domains&quot;, spider=spider)</span>
<span class="gi">+        self.stats.inc_value(&quot;offsite/filtered&quot;, spider=spider)</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def should_follow(self, request, spider):</span>
<span class="gi">+        regex = self.host_regex</span>
<span class="gi">+        # hostname can be None for wrong urls (like javascript links)</span>
<span class="gi">+        host = urlparse_cached(request).hostname or &quot;&quot;</span>
<span class="gi">+        return bool(regex.search(host))</span>
<span class="gi">+</span>
<span class="w"> </span>    def get_host_regex(self, spider):
<span class="w"> </span>        &quot;&quot;&quot;Override this method to implement a different offsite policy&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        allowed_domains = getattr(spider, &quot;allowed_domains&quot;, None)</span>
<span class="gi">+        if not allowed_domains:</span>
<span class="gi">+            return re.compile(&quot;&quot;)  # allow all by default</span>
<span class="gi">+        url_pattern = re.compile(r&quot;^https?://.*$&quot;)</span>
<span class="gi">+        port_pattern = re.compile(r&quot;:\d+$&quot;)</span>
<span class="gi">+        domains = []</span>
<span class="gi">+        for domain in allowed_domains:</span>
<span class="gi">+            if domain is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            if url_pattern.match(domain):</span>
<span class="gi">+                message = (</span>
<span class="gi">+                    &quot;allowed_domains accepts only domains, not URLs. &quot;</span>
<span class="gi">+                    f&quot;Ignoring URL entry {domain} in allowed_domains.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                warnings.warn(message, URLWarning)</span>
<span class="gi">+            elif port_pattern.search(domain):</span>
<span class="gi">+                message = (</span>
<span class="gi">+                    &quot;allowed_domains accepts only domains without ports. &quot;</span>
<span class="gi">+                    f&quot;Ignoring entry {domain} in allowed_domains.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                warnings.warn(message, PortWarning)</span>
<span class="gi">+            else:</span>
<span class="gi">+                domains.append(re.escape(domain))</span>
<span class="gi">+        regex = rf&#39;^(.*\.)?({&quot;|&quot;.join(domains)})$&#39;</span>
<span class="gi">+        return re.compile(regex)</span>
<span class="gi">+</span>
<span class="gi">+    def spider_opened(self, spider):</span>
<span class="gi">+        self.host_regex = self.get_host_regex(spider)</span>
<span class="gi">+        self.domains_seen = set()</span>


<span class="w"> </span>class URLWarning(Warning):
<span class="gh">diff --git a/scrapy/spidermiddlewares/referer.py b/scrapy/spidermiddlewares/referer.py</span>
<span class="gh">index a92a7e327..fd91e658b 100644</span>
<span class="gd">--- a/scrapy/spidermiddlewares/referer.py</span>
<span class="gi">+++ b/scrapy/spidermiddlewares/referer.py</span>
<span class="gu">@@ -5,29 +5,49 @@ originated it.</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from typing import Tuple
<span class="w"> </span>from urllib.parse import urlparse
<span class="gi">+</span>
<span class="w"> </span>from w3lib.url import safe_url_string
<span class="gi">+</span>
<span class="w"> </span>from scrapy import signals
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.http import Request, Response
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="w"> </span>from scrapy.utils.url import strip_url
<span class="gd">-LOCAL_SCHEMES = &#39;about&#39;, &#39;blob&#39;, &#39;data&#39;, &#39;filesystem&#39;</span>
<span class="gd">-POLICY_NO_REFERRER = &#39;no-referrer&#39;</span>
<span class="gd">-POLICY_NO_REFERRER_WHEN_DOWNGRADE = &#39;no-referrer-when-downgrade&#39;</span>
<span class="gd">-POLICY_SAME_ORIGIN = &#39;same-origin&#39;</span>
<span class="gd">-POLICY_ORIGIN = &#39;origin&#39;</span>
<span class="gd">-POLICY_STRICT_ORIGIN = &#39;strict-origin&#39;</span>
<span class="gd">-POLICY_ORIGIN_WHEN_CROSS_ORIGIN = &#39;origin-when-cross-origin&#39;</span>
<span class="gd">-POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN = &#39;strict-origin-when-cross-origin&#39;</span>
<span class="gd">-POLICY_UNSAFE_URL = &#39;unsafe-url&#39;</span>
<span class="gd">-POLICY_SCRAPY_DEFAULT = &#39;scrapy-default&#39;</span>
<span class="gi">+</span>
<span class="gi">+LOCAL_SCHEMES = (</span>
<span class="gi">+    &quot;about&quot;,</span>
<span class="gi">+    &quot;blob&quot;,</span>
<span class="gi">+    &quot;data&quot;,</span>
<span class="gi">+    &quot;filesystem&quot;,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+POLICY_NO_REFERRER = &quot;no-referrer&quot;</span>
<span class="gi">+POLICY_NO_REFERRER_WHEN_DOWNGRADE = &quot;no-referrer-when-downgrade&quot;</span>
<span class="gi">+POLICY_SAME_ORIGIN = &quot;same-origin&quot;</span>
<span class="gi">+POLICY_ORIGIN = &quot;origin&quot;</span>
<span class="gi">+POLICY_STRICT_ORIGIN = &quot;strict-origin&quot;</span>
<span class="gi">+POLICY_ORIGIN_WHEN_CROSS_ORIGIN = &quot;origin-when-cross-origin&quot;</span>
<span class="gi">+POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN = &quot;strict-origin-when-cross-origin&quot;</span>
<span class="gi">+POLICY_UNSAFE_URL = &quot;unsafe-url&quot;</span>
<span class="gi">+POLICY_SCRAPY_DEFAULT = &quot;scrapy-default&quot;</span>


<span class="w"> </span>class ReferrerPolicy:
<span class="w"> </span>    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES
<span class="w"> </span>    name: str

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        raise NotImplementedError()</span>
<span class="gi">+</span>
<span class="gi">+    def stripped_referrer(self, url):</span>
<span class="gi">+        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:</span>
<span class="gi">+            return self.strip_url(url)</span>
<span class="gi">+</span>
<span class="gi">+    def origin_referrer(self, url):</span>
<span class="gi">+        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:</span>
<span class="gi">+            return self.origin(url)</span>
<span class="gi">+</span>
<span class="w"> </span>    def strip_url(self, url, origin_only=False):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        https://www.w3.org/TR/referrer-policy/#strip-url
<span class="gu">@@ -42,11 +62,29 @@ class ReferrerPolicy:</span>
<span class="w"> </span>            Set url&#39;s query to null.
<span class="w"> </span>        Return url.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not url:</span>
<span class="gi">+            return None</span>
<span class="gi">+        return strip_url(</span>
<span class="gi">+            url,</span>
<span class="gi">+            strip_credentials=True,</span>
<span class="gi">+            strip_fragment=True,</span>
<span class="gi">+            strip_default_port=True,</span>
<span class="gi">+            origin_only=origin_only,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def origin(self, url):
<span class="w"> </span>        &quot;&quot;&quot;Return serialized origin (scheme, host, path) for a request or response URL.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.strip_url(url, origin_only=True)</span>
<span class="gi">+</span>
<span class="gi">+    def potentially_trustworthy(self, url):</span>
<span class="gi">+        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy</span>
<span class="gi">+        parsed_url = urlparse(url)</span>
<span class="gi">+        if parsed_url.scheme in (&quot;data&quot;,):</span>
<span class="gi">+            return False</span>
<span class="gi">+        return self.tls_protected(url)</span>
<span class="gi">+</span>
<span class="gi">+    def tls_protected(self, url):</span>
<span class="gi">+        return urlparse(url).scheme in (&quot;https&quot;, &quot;ftps&quot;)</span>


<span class="w"> </span>class NoReferrerPolicy(ReferrerPolicy):
<span class="gu">@@ -57,8 +95,12 @@ class NoReferrerPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    is to be sent along with requests made from a particular request client to any origin.
<span class="w"> </span>    The header will be omitted entirely.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_NO_REFERRER

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>

<span class="w"> </span>class NoReferrerWhenDowngradePolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -74,8 +116,13 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):</span>

<span class="w"> </span>    This is a user agent&#39;s default behavior, if no policy is otherwise specified.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        if not self.tls_protected(response_url) or self.tls_protected(request_url):</span>
<span class="gi">+            return self.stripped_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class SameOriginPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -87,8 +134,13 @@ class SameOriginPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    Cross-origin requests, on the other hand, will contain no referrer information.
<span class="w"> </span>    A Referer HTTP header will not be sent.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_SAME_ORIGIN

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        if self.origin(response_url) == self.origin(request_url):</span>
<span class="gi">+            return self.stripped_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class OriginPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -99,8 +151,12 @@ class OriginPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    when making both same-origin requests and cross-origin requests
<span class="w"> </span>    from a particular request client.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_ORIGIN

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        return self.origin_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class StrictOriginPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -115,8 +171,17 @@ class StrictOriginPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    on the other hand, will contain no referrer information.
<span class="w"> </span>    A Referer HTTP header will not be sent.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_STRICT_ORIGIN

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.tls_protected(response_url)</span>
<span class="gi">+            and self.potentially_trustworthy(request_url)</span>
<span class="gi">+            or not self.tls_protected(response_url)</span>
<span class="gi">+        ):</span>
<span class="gi">+            return self.origin_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class OriginWhenCrossOriginPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -129,8 +194,15 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    is sent as referrer information when making cross-origin requests
<span class="w"> </span>    from a particular request client.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        origin = self.origin(response_url)</span>
<span class="gi">+        if origin == self.origin(request_url):</span>
<span class="gi">+            return self.stripped_referrer(response_url)</span>
<span class="gi">+        return origin</span>
<span class="gi">+</span>

<span class="w"> </span>class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -149,8 +221,20 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    on the other hand, will contain no referrer information.
<span class="w"> </span>    A Referer HTTP header will not be sent.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        origin = self.origin(response_url)</span>
<span class="gi">+        if origin == self.origin(request_url):</span>
<span class="gi">+            return self.stripped_referrer(response_url)</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.tls_protected(response_url)</span>
<span class="gi">+            and self.potentially_trustworthy(request_url)</span>
<span class="gi">+            or not self.tls_protected(response_url)</span>
<span class="gi">+        ):</span>
<span class="gi">+            return self.origin_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class UnsafeUrlPolicy(ReferrerPolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -165,8 +249,12 @@ class UnsafeUrlPolicy(ReferrerPolicy):</span>
<span class="w"> </span>    to insecure origins.
<span class="w"> </span>    Carefully consider the impact of setting such a policy for potentially sensitive documents.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    name: str = POLICY_UNSAFE_URL

<span class="gi">+    def referrer(self, response_url, request_url):</span>
<span class="gi">+        return self.stripped_referrer(response_url)</span>
<span class="gi">+</span>

<span class="w"> </span>class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -174,15 +262,28 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):</span>
<span class="w"> </span>    with the addition that &quot;Referer&quot; is not sent if the parent request was
<span class="w"> </span>    using ``file://`` or ``s3://`` scheme.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (&#39;file&#39;, &#39;s3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (&quot;file&quot;, &quot;s3&quot;)</span>
<span class="w"> </span>    name: str = POLICY_SCRAPY_DEFAULT


<span class="gd">-_policy_classes = {p.name: p for p in (NoReferrerPolicy,</span>
<span class="gd">-    NoReferrerWhenDowngradePolicy, SameOriginPolicy, OriginPolicy,</span>
<span class="gd">-    StrictOriginPolicy, OriginWhenCrossOriginPolicy,</span>
<span class="gd">-    StrictOriginWhenCrossOriginPolicy, UnsafeUrlPolicy, DefaultReferrerPolicy)}</span>
<span class="gd">-_policy_classes[&#39;&#39;] = NoReferrerWhenDowngradePolicy</span>
<span class="gi">+_policy_classes = {</span>
<span class="gi">+    p.name: p</span>
<span class="gi">+    for p in (</span>
<span class="gi">+        NoReferrerPolicy,</span>
<span class="gi">+        NoReferrerWhenDowngradePolicy,</span>
<span class="gi">+        SameOriginPolicy,</span>
<span class="gi">+        OriginPolicy,</span>
<span class="gi">+        StrictOriginPolicy,</span>
<span class="gi">+        OriginWhenCrossOriginPolicy,</span>
<span class="gi">+        StrictOriginWhenCrossOriginPolicy,</span>
<span class="gi">+        UnsafeUrlPolicy,</span>
<span class="gi">+        DefaultReferrerPolicy,</span>
<span class="gi">+    )</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+# Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string</span>
<span class="gi">+_policy_classes[&quot;&quot;] = NoReferrerWhenDowngradePolicy</span>


<span class="w"> </span>def _load_policy_class(policy, warning_only=False):
<span class="gu">@@ -191,16 +292,36 @@ def _load_policy_class(policy, warning_only=False):</span>
<span class="w"> </span>    otherwise try to interpret the string as a standard value
<span class="w"> </span>    from https://www.w3.org/TR/referrer-policy/#referrer-policies
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        return load_object(policy)</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        try:</span>
<span class="gi">+            return _policy_classes[policy.lower()]</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            msg = f&quot;Could not load referrer policy {policy!r}&quot;</span>
<span class="gi">+            if not warning_only:</span>
<span class="gi">+                raise RuntimeError(msg)</span>
<span class="gi">+            else:</span>
<span class="gi">+                warnings.warn(msg, RuntimeWarning)</span>
<span class="gi">+                return None</span>


<span class="w"> </span>class RefererMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, settings=None):
<span class="w"> </span>        self.default_policy = DefaultReferrerPolicy
<span class="w"> </span>        if settings is not None:
<span class="gd">-            self.default_policy = _load_policy_class(settings.get(</span>
<span class="gd">-                &#39;REFERRER_POLICY&#39;))</span>
<span class="gi">+            self.default_policy = _load_policy_class(settings.get(&quot;REFERRER_POLICY&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        if not crawler.settings.getbool(&quot;REFERER_ENABLED&quot;):</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        mw = cls(crawler.settings)</span>
<span class="gi">+</span>
<span class="gi">+        # Note: this hook is a bit of a hack to intercept redirections</span>
<span class="gi">+        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)</span>
<span class="gi">+</span>
<span class="gi">+        return mw</span>

<span class="w"> </span>    def policy(self, resp_or_url, request):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gu">@@ -215,4 +336,50 @@ class RefererMiddleware:</span>
<span class="w"> </span>          it is used if valid
<span class="w"> </span>        - otherwise, the policy from settings is used.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        policy_name = request.meta.get(&quot;referrer_policy&quot;)</span>
<span class="gi">+        if policy_name is None:</span>
<span class="gi">+            if isinstance(resp_or_url, Response):</span>
<span class="gi">+                policy_header = resp_or_url.headers.get(&quot;Referrer-Policy&quot;)</span>
<span class="gi">+                if policy_header is not None:</span>
<span class="gi">+                    policy_name = to_unicode(policy_header.decode(&quot;latin1&quot;))</span>
<span class="gi">+        if policy_name is None:</span>
<span class="gi">+            return self.default_policy()</span>
<span class="gi">+</span>
<span class="gi">+        cls = _load_policy_class(policy_name, warning_only=True)</span>
<span class="gi">+        return cls() if cls else self.default_policy()</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_output(self, response, result, spider):</span>
<span class="gi">+        return (self._set_referer(r, response) for r in result or ())</span>
<span class="gi">+</span>
<span class="gi">+    async def process_spider_output_async(self, response, result, spider):</span>
<span class="gi">+        async for r in result or ():</span>
<span class="gi">+            yield self._set_referer(r, response)</span>
<span class="gi">+</span>
<span class="gi">+    def _set_referer(self, r, response):</span>
<span class="gi">+        if isinstance(r, Request):</span>
<span class="gi">+            referrer = self.policy(response, r).referrer(response.url, r.url)</span>
<span class="gi">+            if referrer is not None:</span>
<span class="gi">+                r.headers.setdefault(&quot;Referer&quot;, referrer)</span>
<span class="gi">+        return r</span>
<span class="gi">+</span>
<span class="gi">+    def request_scheduled(self, request, spider):</span>
<span class="gi">+        # check redirected request to patch &quot;Referer&quot; header if necessary</span>
<span class="gi">+        redirected_urls = request.meta.get(&quot;redirect_urls&quot;, [])</span>
<span class="gi">+        if redirected_urls:</span>
<span class="gi">+            request_referrer = request.headers.get(&quot;Referer&quot;)</span>
<span class="gi">+            # we don&#39;t patch the referrer value if there is none</span>
<span class="gi">+            if request_referrer is not None:</span>
<span class="gi">+                # the request&#39;s referrer header value acts as a surrogate</span>
<span class="gi">+                # for the parent response URL</span>
<span class="gi">+                #</span>
<span class="gi">+                # Note: if the 3xx response contained a Referrer-Policy header,</span>
<span class="gi">+                #       the information is not available using this hook</span>
<span class="gi">+                parent_url = safe_url_string(request_referrer)</span>
<span class="gi">+                policy_referrer = self.policy(parent_url, request).referrer(</span>
<span class="gi">+                    parent_url, request.url</span>
<span class="gi">+                )</span>
<span class="gi">+                if policy_referrer != request_referrer:</span>
<span class="gi">+                    if policy_referrer is None:</span>
<span class="gi">+                        request.headers.pop(&quot;Referer&quot;)</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        request.headers[&quot;Referer&quot;] = policy_referrer</span>
<span class="gh">diff --git a/scrapy/spidermiddlewares/urllength.py b/scrapy/spidermiddlewares/urllength.py</span>
<span class="gh">index 1844c1465..f6d92e53a 100644</span>
<span class="gd">--- a/scrapy/spidermiddlewares/urllength.py</span>
<span class="gi">+++ b/scrapy/spidermiddlewares/urllength.py</span>
<span class="gu">@@ -3,13 +3,43 @@ Url Length Spider Middleware</span>

<span class="w"> </span>See documentation in docs/topics/spider-middleware.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.http import Request
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class UrlLengthMiddleware:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, maxlength):
<span class="w"> </span>        self.maxlength = maxlength
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_settings(cls, settings):</span>
<span class="gi">+        maxlength = settings.getint(&quot;URLLENGTH_LIMIT&quot;)</span>
<span class="gi">+        if not maxlength:</span>
<span class="gi">+            raise NotConfigured</span>
<span class="gi">+        return cls(maxlength)</span>
<span class="gi">+</span>
<span class="gi">+    def process_spider_output(self, response, result, spider):</span>
<span class="gi">+        return (r for r in result or () if self._filter(r, spider))</span>
<span class="gi">+</span>
<span class="gi">+    async def process_spider_output_async(self, response, result, spider):</span>
<span class="gi">+        async for r in result or ():</span>
<span class="gi">+            if self._filter(r, spider):</span>
<span class="gi">+                yield r</span>
<span class="gi">+</span>
<span class="gi">+    def _filter(self, request, spider):</span>
<span class="gi">+        if isinstance(request, Request) and len(request.url) &gt; self.maxlength:</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;Ignoring link (url length &gt; %(maxlength)d): %(url)s &quot;,</span>
<span class="gi">+                {&quot;maxlength&quot;: self.maxlength, &quot;url&quot;: request.url},</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+            spider.crawler.stats.inc_value(</span>
<span class="gi">+                &quot;urllength/request_ignored_count&quot;, spider=spider</span>
<span class="gi">+            )</span>
<span class="gi">+            return False</span>
<span class="gi">+        return True</span>
<span class="gh">diff --git a/scrapy/spiders/crawl.py b/scrapy/spiders/crawl.py</span>
<span class="gh">index 291994af6..31e845716 100644</span>
<span class="gd">--- a/scrapy/spiders/crawl.py</span>
<span class="gi">+++ b/scrapy/spiders/crawl.py</span>
<span class="gu">@@ -4,20 +4,46 @@ for scraping typical web sites that requires crawling pages.</span>

<span class="w"> </span>See documentation in docs/topics/spiders.rst
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import copy
<span class="w"> </span>from typing import AsyncIterable, Awaitable, Sequence
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import HtmlResponse, Request, Response
<span class="w"> </span>from scrapy.linkextractors import LinkExtractor
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.utils.asyncgen import collect_asyncgen
<span class="w"> </span>from scrapy.utils.spider import iterate_spider_output
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _identity(x):</span>
<span class="gi">+    return x</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _identity_process_request(request, response):</span>
<span class="gi">+    return request</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_method(method, spider):</span>
<span class="gi">+    if callable(method):</span>
<span class="gi">+        return method</span>
<span class="gi">+    if isinstance(method, str):</span>
<span class="gi">+        return getattr(spider, method, None)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>_default_link_extractor = LinkExtractor()


<span class="w"> </span>class Rule:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None,</span>
<span class="gd">-        follow=None, process_links=None, process_request=None, errback=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        link_extractor=None,</span>
<span class="gi">+        callback=None,</span>
<span class="gi">+        cb_kwargs=None,</span>
<span class="gi">+        follow=None,</span>
<span class="gi">+        process_links=None,</span>
<span class="gi">+        process_request=None,</span>
<span class="gi">+        errback=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.link_extractor = link_extractor or _default_link_extractor
<span class="w"> </span>        self.callback = callback
<span class="w"> </span>        self.errback = errback
<span class="gu">@@ -26,6 +52,12 @@ class Rule:</span>
<span class="w"> </span>        self.process_request = process_request or _identity_process_request
<span class="w"> </span>        self.follow = follow if follow is not None else not callback

<span class="gi">+    def _compile(self, spider):</span>
<span class="gi">+        self.callback = _get_method(self.callback, spider)</span>
<span class="gi">+        self.errback = _get_method(self.errback, spider)</span>
<span class="gi">+        self.process_links = _get_method(self.process_links, spider)</span>
<span class="gi">+        self.process_request = _get_method(self.process_request, spider)</span>
<span class="gi">+</span>

<span class="w"> </span>class CrawlSpider(Spider):
<span class="w"> </span>    rules: Sequence[Rule] = ()
<span class="gu">@@ -33,3 +65,85 @@ class CrawlSpider(Spider):</span>
<span class="w"> </span>    def __init__(self, *a, **kw):
<span class="w"> </span>        super().__init__(*a, **kw)
<span class="w"> </span>        self._compile_rules()
<span class="gi">+</span>
<span class="gi">+    def _parse(self, response, **kwargs):</span>
<span class="gi">+        return self._parse_response(</span>
<span class="gi">+            response=response,</span>
<span class="gi">+            callback=self.parse_start_url,</span>
<span class="gi">+            cb_kwargs=kwargs,</span>
<span class="gi">+            follow=True,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def parse_start_url(self, response, **kwargs):</span>
<span class="gi">+        return []</span>
<span class="gi">+</span>
<span class="gi">+    def process_results(self, response: Response, results: list):</span>
<span class="gi">+        return results</span>
<span class="gi">+</span>
<span class="gi">+    def _build_request(self, rule_index, link):</span>
<span class="gi">+        return Request(</span>
<span class="gi">+            url=link.url,</span>
<span class="gi">+            callback=self._callback,</span>
<span class="gi">+            errback=self._errback,</span>
<span class="gi">+            meta=dict(rule=rule_index, link_text=link.text),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _requests_to_follow(self, response):</span>
<span class="gi">+        if not isinstance(response, HtmlResponse):</span>
<span class="gi">+            return</span>
<span class="gi">+        seen = set()</span>
<span class="gi">+        for rule_index, rule in enumerate(self._rules):</span>
<span class="gi">+            links = [</span>
<span class="gi">+                lnk</span>
<span class="gi">+                for lnk in rule.link_extractor.extract_links(response)</span>
<span class="gi">+                if lnk not in seen</span>
<span class="gi">+            ]</span>
<span class="gi">+            for link in rule.process_links(links):</span>
<span class="gi">+                seen.add(link)</span>
<span class="gi">+                request = self._build_request(rule_index, link)</span>
<span class="gi">+                yield rule.process_request(request, response)</span>
<span class="gi">+</span>
<span class="gi">+    def _callback(self, response, **cb_kwargs):</span>
<span class="gi">+        rule = self._rules[response.meta[&quot;rule&quot;]]</span>
<span class="gi">+        return self._parse_response(</span>
<span class="gi">+            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _errback(self, failure):</span>
<span class="gi">+        rule = self._rules[failure.request.meta[&quot;rule&quot;]]</span>
<span class="gi">+        return self._handle_failure(failure, rule.errback)</span>
<span class="gi">+</span>
<span class="gi">+    async def _parse_response(self, response, callback, cb_kwargs, follow=True):</span>
<span class="gi">+        if callback:</span>
<span class="gi">+            cb_res = callback(response, **cb_kwargs) or ()</span>
<span class="gi">+            if isinstance(cb_res, AsyncIterable):</span>
<span class="gi">+                cb_res = await collect_asyncgen(cb_res)</span>
<span class="gi">+            elif isinstance(cb_res, Awaitable):</span>
<span class="gi">+                cb_res = await cb_res</span>
<span class="gi">+            cb_res = self.process_results(response, cb_res)</span>
<span class="gi">+            for request_or_item in iterate_spider_output(cb_res):</span>
<span class="gi">+                yield request_or_item</span>
<span class="gi">+</span>
<span class="gi">+        if follow and self._follow_links:</span>
<span class="gi">+            for request_or_item in self._requests_to_follow(response):</span>
<span class="gi">+                yield request_or_item</span>
<span class="gi">+</span>
<span class="gi">+    def _handle_failure(self, failure, errback):</span>
<span class="gi">+        if errback:</span>
<span class="gi">+            results = errback(failure) or ()</span>
<span class="gi">+            for request_or_item in iterate_spider_output(results):</span>
<span class="gi">+                yield request_or_item</span>
<span class="gi">+</span>
<span class="gi">+    def _compile_rules(self):</span>
<span class="gi">+        self._rules = []</span>
<span class="gi">+        for rule in self.rules:</span>
<span class="gi">+            self._rules.append(copy.copy(rule))</span>
<span class="gi">+            self._rules[-1]._compile(self)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler, *args, **kwargs):</span>
<span class="gi">+        spider = super().from_crawler(crawler, *args, **kwargs)</span>
<span class="gi">+        spider._follow_links = crawler.settings.getbool(</span>
<span class="gi">+            &quot;CRAWLSPIDER_FOLLOW_LINKS&quot;, True</span>
<span class="gi">+        )</span>
<span class="gi">+        return spider</span>
<span class="gh">diff --git a/scrapy/spiders/feed.py b/scrapy/spiders/feed.py</span>
<span class="gh">index 4c0801928..42675c76a 100644</span>
<span class="gd">--- a/scrapy/spiders/feed.py</span>
<span class="gi">+++ b/scrapy/spiders/feed.py</span>
<span class="gu">@@ -20,8 +20,9 @@ class XMLFeedSpider(Spider):</span>
<span class="w"> </span>    &#39;xml&#39; selector, or an &#39;html&#39; selector.  In most cases, it&#39;s convenient to
<span class="w"> </span>    use iternodes, since it&#39;s a faster and cleaner.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    iterator = &#39;iternodes&#39;</span>
<span class="gd">-    itertag = &#39;item&#39;</span>
<span class="gi">+</span>
<span class="gi">+    iterator = &quot;iternodes&quot;</span>
<span class="gi">+    itertag = &quot;item&quot;</span>
<span class="w"> </span>    namespaces = ()

<span class="w"> </span>    def process_results(self, response, results):
<span class="gu">@@ -32,18 +33,20 @@ class XMLFeedSpider(Spider):</span>
<span class="w"> </span>        the response which originated that results. It must return a list of
<span class="w"> </span>        results (items or requests).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return results</span>

<span class="w"> </span>    def adapt_response(self, response):
<span class="w"> </span>        &quot;&quot;&quot;You can override this function in order to make any changes you want
<span class="w"> </span>        to into the feed before parsing it. This function must return a
<span class="w"> </span>        response.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return response</span>

<span class="w"> </span>    def parse_node(self, response, selector):
<span class="w"> </span>        &quot;&quot;&quot;This method must be overridden with your custom spider functionality&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(self, &quot;parse_item&quot;):  # backward compatibility</span>
<span class="gi">+            return self.parse_item(response, selector)</span>
<span class="gi">+        raise NotImplementedError</span>

<span class="w"> </span>    def parse_nodes(self, response, nodes):
<span class="w"> </span>        &quot;&quot;&quot;This method is called for the nodes matching the provided tag name
<span class="gu">@@ -52,7 +55,42 @@ class XMLFeedSpider(Spider):</span>
<span class="w"> </span>        This method must return either an item, a request, or a list
<span class="w"> </span>        containing any of them.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        for selector in nodes:</span>
<span class="gi">+            ret = iterate_spider_output(self.parse_node(response, selector))</span>
<span class="gi">+            for result_item in self.process_results(response, ret):</span>
<span class="gi">+                yield result_item</span>
<span class="gi">+</span>
<span class="gi">+    def _parse(self, response, **kwargs):</span>
<span class="gi">+        if not hasattr(self, &quot;parse_node&quot;):</span>
<span class="gi">+            raise NotConfigured(</span>
<span class="gi">+                &quot;You must define parse_node method in order to scrape this XML feed&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        response = self.adapt_response(response)</span>
<span class="gi">+        if self.iterator == &quot;iternodes&quot;:</span>
<span class="gi">+            nodes = self._iternodes(response)</span>
<span class="gi">+        elif self.iterator == &quot;xml&quot;:</span>
<span class="gi">+            selector = Selector(response, type=&quot;xml&quot;)</span>
<span class="gi">+            self._register_namespaces(selector)</span>
<span class="gi">+            nodes = selector.xpath(f&quot;//{self.itertag}&quot;)</span>
<span class="gi">+        elif self.iterator == &quot;html&quot;:</span>
<span class="gi">+            selector = Selector(response, type=&quot;html&quot;)</span>
<span class="gi">+            self._register_namespaces(selector)</span>
<span class="gi">+            nodes = selector.xpath(f&quot;//{self.itertag}&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise NotSupported(&quot;Unsupported node iterator&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        return self.parse_nodes(response, nodes)</span>
<span class="gi">+</span>
<span class="gi">+    def _iternodes(self, response):</span>
<span class="gi">+        for node in xmliter_lxml(response, self.itertag):</span>
<span class="gi">+            self._register_namespaces(node)</span>
<span class="gi">+            yield node</span>
<span class="gi">+</span>
<span class="gi">+    def _register_namespaces(self, selector):</span>
<span class="gi">+        for prefix, uri in self.namespaces:</span>
<span class="gi">+            selector.register_namespace(prefix, uri)</span>


<span class="w"> </span>class CSVFeedSpider(Spider):
<span class="gu">@@ -63,21 +101,26 @@ class CSVFeedSpider(Spider):</span>
<span class="w"> </span>    You can set some options regarding the CSV file, such as the delimiter, quotechar
<span class="w"> </span>    and the file&#39;s headers.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    delimiter = None</span>
<span class="gd">-    quotechar = None</span>
<span class="gi">+</span>
<span class="gi">+    delimiter = (</span>
<span class="gi">+        None  # When this is None, python&#39;s csv module&#39;s default delimiter is used</span>
<span class="gi">+    )</span>
<span class="gi">+    quotechar = (</span>
<span class="gi">+        None  # When this is None, python&#39;s csv module&#39;s default quotechar is used</span>
<span class="gi">+    )</span>
<span class="w"> </span>    headers = None

<span class="w"> </span>    def process_results(self, response, results):
<span class="w"> </span>        &quot;&quot;&quot;This method has the same purpose as the one in XMLFeedSpider&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return results</span>

<span class="w"> </span>    def adapt_response(self, response):
<span class="w"> </span>        &quot;&quot;&quot;This method has the same purpose as the one in XMLFeedSpider&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return response</span>

<span class="w"> </span>    def parse_row(self, response, row):
<span class="w"> </span>        &quot;&quot;&quot;This method must be overridden with your custom spider functionality&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        raise NotImplementedError</span>

<span class="w"> </span>    def parse_rows(self, response):
<span class="w"> </span>        &quot;&quot;&quot;Receives a response and a dict (representing each row) with a key for
<span class="gu">@@ -85,4 +128,18 @@ class CSVFeedSpider(Spider):</span>
<span class="w"> </span>        gives the opportunity to override adapt_response and
<span class="w"> </span>        process_results methods for pre and post-processing purposes.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        for row in csviter(</span>
<span class="gi">+            response, self.delimiter, self.headers, quotechar=self.quotechar</span>
<span class="gi">+        ):</span>
<span class="gi">+            ret = iterate_spider_output(self.parse_row(response, row))</span>
<span class="gi">+            for result_item in self.process_results(response, ret):</span>
<span class="gi">+                yield result_item</span>
<span class="gi">+</span>
<span class="gi">+    def _parse(self, response, **kwargs):</span>
<span class="gi">+        if not hasattr(self, &quot;parse_row&quot;):</span>
<span class="gi">+            raise NotConfigured(</span>
<span class="gi">+                &quot;You must define parse_row method in order to scrape this CSV feed&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        response = self.adapt_response(response)</span>
<span class="gi">+        return self.parse_rows(response)</span>
<span class="gh">diff --git a/scrapy/spiders/init.py b/scrapy/spiders/init.py</span>
<span class="gh">index 7f6f7eefe..3cb215b0f 100644</span>
<span class="gd">--- a/scrapy/spiders/init.py</span>
<span class="gi">+++ b/scrapy/spiders/init.py</span>
<span class="gu">@@ -5,11 +5,15 @@ from scrapy.utils.spider import iterate_spider_output</span>
<span class="w"> </span>class InitSpider(Spider):
<span class="w"> </span>    &quot;&quot;&quot;Base Spider with initialization facilities&quot;&quot;&quot;

<span class="gi">+    def start_requests(self):</span>
<span class="gi">+        self._postinit_reqs = super().start_requests()</span>
<span class="gi">+        return iterate_spider_output(self.init_request())</span>
<span class="gi">+</span>
<span class="w"> </span>    def initialized(self, response=None):
<span class="w"> </span>        &quot;&quot;&quot;This method must be set as the callback of your last initialization
<span class="w"> </span>        request. See self.init_request() docstring for more info.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.__dict__.pop(&quot;_postinit_reqs&quot;)</span>

<span class="w"> </span>    def init_request(self):
<span class="w"> </span>        &quot;&quot;&quot;This function should return one initialization request, with the
<span class="gu">@@ -24,4 +28,4 @@ class InitSpider(Spider):</span>
<span class="w"> </span>        overridden only when you need to perform requests to initialize your
<span class="w"> </span>        spider
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.initialized()</span>
<span class="gh">diff --git a/scrapy/spiders/sitemap.py b/scrapy/spiders/sitemap.py</span>
<span class="gh">index 97a4c2aed..386aa6a6e 100644</span>
<span class="gd">--- a/scrapy/spiders/sitemap.py</span>
<span class="gi">+++ b/scrapy/spiders/sitemap.py</span>
<span class="gu">@@ -1,25 +1,41 @@</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="w"> </span>from typing import TYPE_CHECKING, Any
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request, XmlResponse
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.utils._compression import _DecompressionMaxSizeExceeded
<span class="w"> </span>from scrapy.utils.gz import gunzip, gzip_magic_number
<span class="w"> </span>from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="w"> </span>class SitemapSpider(Spider):
<span class="w"> </span>    sitemap_urls = ()
<span class="gd">-    sitemap_rules = [(&#39;&#39;, &#39;parse&#39;)]</span>
<span class="gd">-    sitemap_follow = [&#39;&#39;]</span>
<span class="gi">+    sitemap_rules = [(&quot;&quot;, &quot;parse&quot;)]</span>
<span class="gi">+    sitemap_follow = [&quot;&quot;]</span>
<span class="w"> </span>    sitemap_alternate_links = False
<span class="w"> </span>    _max_size: int
<span class="w"> </span>    _warn_size: int

<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler: &quot;Crawler&quot;, *args: Any, **kwargs: Any) -&gt; &quot;Self&quot;:</span>
<span class="gi">+        spider = super().from_crawler(crawler, *args, **kwargs)</span>
<span class="gi">+        spider._max_size = getattr(</span>
<span class="gi">+            spider, &quot;download_maxsize&quot;, spider.settings.getint(&quot;DOWNLOAD_MAXSIZE&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+        spider._warn_size = getattr(</span>
<span class="gi">+            spider, &quot;download_warnsize&quot;, spider.settings.getint(&quot;DOWNLOAD_WARNSIZE&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+        return spider</span>
<span class="gi">+</span>
<span class="w"> </span>    def __init__(self, *a, **kw):
<span class="w"> </span>        super().__init__(*a, **kw)
<span class="w"> </span>        self._cbs = []
<span class="gu">@@ -29,15 +45,89 @@ class SitemapSpider(Spider):</span>
<span class="w"> </span>            self._cbs.append((regex(r), c))
<span class="w"> </span>        self._follow = [regex(x) for x in self.sitemap_follow]

<span class="gi">+    def start_requests(self):</span>
<span class="gi">+        for url in self.sitemap_urls:</span>
<span class="gi">+            yield Request(url, self._parse_sitemap)</span>
<span class="gi">+</span>
<span class="w"> </span>    def sitemap_filter(self, entries):
<span class="w"> </span>        &quot;&quot;&quot;This method can be used to filter sitemap entries by their
<span class="w"> </span>        attributes, for example, you can filter locs with lastmod greater
<span class="w"> </span>        than a given date (see docs).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for entry in entries:</span>
<span class="gi">+            yield entry</span>
<span class="gi">+</span>
<span class="gi">+    def _parse_sitemap(self, response):</span>
<span class="gi">+        if response.url.endswith(&quot;/robots.txt&quot;):</span>
<span class="gi">+            for url in sitemap_urls_from_robots(response.text, base_url=response.url):</span>
<span class="gi">+                yield Request(url, callback=self._parse_sitemap)</span>
<span class="gi">+        else:</span>
<span class="gi">+            body = self._get_sitemap_body(response)</span>
<span class="gi">+            if body is None:</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    &quot;Ignoring invalid sitemap: %(response)s&quot;,</span>
<span class="gi">+                    {&quot;response&quot;: response},</span>
<span class="gi">+                    extra={&quot;spider&quot;: self},</span>
<span class="gi">+                )</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            s = Sitemap(body)</span>
<span class="gi">+            it = self.sitemap_filter(s)</span>
<span class="gi">+</span>
<span class="gi">+            if s.type == &quot;sitemapindex&quot;:</span>
<span class="gi">+                for loc in iterloc(it, self.sitemap_alternate_links):</span>
<span class="gi">+                    if any(x.search(loc) for x in self._follow):</span>
<span class="gi">+                        yield Request(loc, callback=self._parse_sitemap)</span>
<span class="gi">+            elif s.type == &quot;urlset&quot;:</span>
<span class="gi">+                for loc in iterloc(it, self.sitemap_alternate_links):</span>
<span class="gi">+                    for r, c in self._cbs:</span>
<span class="gi">+                        if r.search(loc):</span>
<span class="gi">+                            yield Request(loc, callback=c)</span>
<span class="gi">+                            break</span>

<span class="w"> </span>    def _get_sitemap_body(self, response):
<span class="w"> </span>        &quot;&quot;&quot;Return the sitemap body contained in the given response,
<span class="w"> </span>        or None if the response is not a sitemap.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(response, XmlResponse):</span>
<span class="gi">+            return response.body</span>
<span class="gi">+        if gzip_magic_number(response):</span>
<span class="gi">+            uncompressed_size = len(response.body)</span>
<span class="gi">+            max_size = response.meta.get(&quot;download_maxsize&quot;, self._max_size)</span>
<span class="gi">+            warn_size = response.meta.get(&quot;download_warnsize&quot;, self._warn_size)</span>
<span class="gi">+            try:</span>
<span class="gi">+                body = gunzip(response.body, max_size=max_size)</span>
<span class="gi">+            except _DecompressionMaxSizeExceeded:</span>
<span class="gi">+                return None</span>
<span class="gi">+            if uncompressed_size &lt; warn_size &lt;= len(body):</span>
<span class="gi">+                logger.warning(</span>
<span class="gi">+                    f&quot;{response} body size after decompression ({len(body)} B) &quot;</span>
<span class="gi">+                    f&quot;is larger than the download warning size ({warn_size} B).&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return body</span>
<span class="gi">+        # actual gzipped sitemap files are decompressed above ;</span>
<span class="gi">+        # if we are here (response body is not gzipped)</span>
<span class="gi">+        # and have a response for .xml.gz,</span>
<span class="gi">+        # it usually means that it was already gunzipped</span>
<span class="gi">+        # by HttpCompression middleware,</span>
<span class="gi">+        # the HTTP response being sent with &quot;Content-Encoding: gzip&quot;</span>
<span class="gi">+        # without actually being a .xml.gz file in the first place,</span>
<span class="gi">+        # merely XML gzip-compressed on the fly,</span>
<span class="gi">+        # in other word, here, we have plain XML</span>
<span class="gi">+        if response.url.endswith(&quot;.xml&quot;) or response.url.endswith(&quot;.xml.gz&quot;):</span>
<span class="gi">+            return response.body</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def regex(x):</span>
<span class="gi">+    if isinstance(x, str):</span>
<span class="gi">+        return re.compile(x)</span>
<span class="gi">+    return x</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def iterloc(it, alt=False):</span>
<span class="gi">+    for d in it:</span>
<span class="gi">+        yield d[&quot;loc&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        # Also consider alternate URLs (xhtml:link rel=&quot;alternate&quot;)</span>
<span class="gi">+        if alt and &quot;alternate&quot; in d:</span>
<span class="gi">+            yield from d[&quot;alternate&quot;]</span>
<span class="gh">diff --git a/scrapy/squeues.py b/scrapy/squeues.py</span>
<span class="gh">index 612da342c..f665ad88c 100644</span>
<span class="gd">--- a/scrapy/squeues.py</span>
<span class="gi">+++ b/scrapy/squeues.py</span>
<span class="gu">@@ -1,28 +1,144 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Scheduler queues
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import marshal
<span class="w"> </span>import pickle
<span class="w"> </span>from os import PathLike
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import Union
<span class="gi">+</span>
<span class="w"> </span>from queuelib import queue
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.request import request_from_dict
<span class="gd">-_PickleFifoSerializationDiskQueue = _serializable_queue(_with_mkdir(queue.</span>
<span class="gd">-    FifoDiskQueue), _pickle_serialize, pickle.loads)</span>
<span class="gd">-_PickleLifoSerializationDiskQueue = _serializable_queue(_with_mkdir(queue.</span>
<span class="gd">-    LifoDiskQueue), _pickle_serialize, pickle.loads)</span>
<span class="gd">-_MarshalFifoSerializationDiskQueue = _serializable_queue(_with_mkdir(queue.</span>
<span class="gd">-    FifoDiskQueue), marshal.dumps, marshal.loads)</span>
<span class="gd">-_MarshalLifoSerializationDiskQueue = _serializable_queue(_with_mkdir(queue.</span>
<span class="gd">-    LifoDiskQueue), marshal.dumps, marshal.loads)</span>
<span class="gd">-PickleFifoDiskQueue = _scrapy_serialization_queue(</span>
<span class="gd">-    _PickleFifoSerializationDiskQueue)</span>
<span class="gd">-PickleLifoDiskQueue = _scrapy_serialization_queue(</span>
<span class="gd">-    _PickleLifoSerializationDiskQueue)</span>
<span class="gd">-MarshalFifoDiskQueue = _scrapy_serialization_queue(</span>
<span class="gd">-    _MarshalFifoSerializationDiskQueue)</span>
<span class="gd">-MarshalLifoDiskQueue = _scrapy_serialization_queue(</span>
<span class="gd">-    _MarshalLifoSerializationDiskQueue)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _with_mkdir(queue_class):</span>
<span class="gi">+    class DirectoriesCreated(queue_class):</span>
<span class="gi">+        def __init__(self, path: Union[str, PathLike], *args, **kwargs):</span>
<span class="gi">+            dirname = Path(path).parent</span>
<span class="gi">+            if not dirname.exists():</span>
<span class="gi">+                dirname.mkdir(parents=True, exist_ok=True)</span>
<span class="gi">+            super().__init__(path, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    return DirectoriesCreated</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _serializable_queue(queue_class, serialize, deserialize):</span>
<span class="gi">+    class SerializableQueue(queue_class):</span>
<span class="gi">+        def push(self, obj):</span>
<span class="gi">+            s = serialize(obj)</span>
<span class="gi">+            super().push(s)</span>
<span class="gi">+</span>
<span class="gi">+        def pop(self):</span>
<span class="gi">+            s = super().pop()</span>
<span class="gi">+            if s:</span>
<span class="gi">+                return deserialize(s)</span>
<span class="gi">+</span>
<span class="gi">+        def peek(self):</span>
<span class="gi">+            &quot;&quot;&quot;Returns the next object to be returned by :meth:`pop`,</span>
<span class="gi">+            but without removing it from the queue.</span>
<span class="gi">+</span>
<span class="gi">+            Raises :exc:`NotImplementedError` if the underlying queue class does</span>
<span class="gi">+            not implement a ``peek`` method, which is optional for queues.</span>
<span class="gi">+            &quot;&quot;&quot;</span>
<span class="gi">+            try:</span>
<span class="gi">+                s = super().peek()</span>
<span class="gi">+            except AttributeError as ex:</span>
<span class="gi">+                raise NotImplementedError(</span>
<span class="gi">+                    &quot;The underlying queue class does not implement &#39;peek&#39;&quot;</span>
<span class="gi">+                ) from ex</span>
<span class="gi">+            if s:</span>
<span class="gi">+                return deserialize(s)</span>
<span class="gi">+</span>
<span class="gi">+    return SerializableQueue</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _scrapy_serialization_queue(queue_class):</span>
<span class="gi">+    class ScrapyRequestQueue(queue_class):</span>
<span class="gi">+        def __init__(self, crawler, key):</span>
<span class="gi">+            self.spider = crawler.spider</span>
<span class="gi">+            super().__init__(key)</span>
<span class="gi">+</span>
<span class="gi">+        @classmethod</span>
<span class="gi">+        def from_crawler(cls, crawler, key, *args, **kwargs):</span>
<span class="gi">+            return cls(crawler, key)</span>
<span class="gi">+</span>
<span class="gi">+        def push(self, request):</span>
<span class="gi">+            request = request.to_dict(spider=self.spider)</span>
<span class="gi">+            return super().push(request)</span>
<span class="gi">+</span>
<span class="gi">+        def pop(self):</span>
<span class="gi">+            request = super().pop()</span>
<span class="gi">+            if not request:</span>
<span class="gi">+                return None</span>
<span class="gi">+            return request_from_dict(request, spider=self.spider)</span>
<span class="gi">+</span>
<span class="gi">+        def peek(self):</span>
<span class="gi">+            &quot;&quot;&quot;Returns the next object to be returned by :meth:`pop`,</span>
<span class="gi">+            but without removing it from the queue.</span>
<span class="gi">+</span>
<span class="gi">+            Raises :exc:`NotImplementedError` if the underlying queue class does</span>
<span class="gi">+            not implement a ``peek`` method, which is optional for queues.</span>
<span class="gi">+            &quot;&quot;&quot;</span>
<span class="gi">+            request = super().peek()</span>
<span class="gi">+            if not request:</span>
<span class="gi">+                return None</span>
<span class="gi">+            return request_from_dict(request, spider=self.spider)</span>
<span class="gi">+</span>
<span class="gi">+    return ScrapyRequestQueue</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _scrapy_non_serialization_queue(queue_class):</span>
<span class="gi">+    class ScrapyRequestQueue(queue_class):</span>
<span class="gi">+        @classmethod</span>
<span class="gi">+        def from_crawler(cls, crawler, *args, **kwargs):</span>
<span class="gi">+            return cls()</span>
<span class="gi">+</span>
<span class="gi">+        def peek(self):</span>
<span class="gi">+            &quot;&quot;&quot;Returns the next object to be returned by :meth:`pop`,</span>
<span class="gi">+            but without removing it from the queue.</span>
<span class="gi">+</span>
<span class="gi">+            Raises :exc:`NotImplementedError` if the underlying queue class does</span>
<span class="gi">+            not implement a ``peek`` method, which is optional for queues.</span>
<span class="gi">+            &quot;&quot;&quot;</span>
<span class="gi">+            try:</span>
<span class="gi">+                s = super().peek()</span>
<span class="gi">+            except AttributeError as ex:</span>
<span class="gi">+                raise NotImplementedError(</span>
<span class="gi">+                    &quot;The underlying queue class does not implement &#39;peek&#39;&quot;</span>
<span class="gi">+                ) from ex</span>
<span class="gi">+            return s</span>
<span class="gi">+</span>
<span class="gi">+    return ScrapyRequestQueue</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _pickle_serialize(obj):</span>
<span class="gi">+    try:</span>
<span class="gi">+        return pickle.dumps(obj, protocol=4)</span>
<span class="gi">+    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)</span>
<span class="gi">+    # TypeError is raised from parsel.Selector</span>
<span class="gi">+    except (pickle.PicklingError, AttributeError, TypeError) as e:</span>
<span class="gi">+        raise ValueError(str(e)) from e</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_PickleFifoSerializationDiskQueue = _serializable_queue(</span>
<span class="gi">+    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads</span>
<span class="gi">+)</span>
<span class="gi">+_PickleLifoSerializationDiskQueue = _serializable_queue(</span>
<span class="gi">+    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads</span>
<span class="gi">+)</span>
<span class="gi">+_MarshalFifoSerializationDiskQueue = _serializable_queue(</span>
<span class="gi">+    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads</span>
<span class="gi">+)</span>
<span class="gi">+_MarshalLifoSerializationDiskQueue = _serializable_queue(</span>
<span class="gi">+    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+# public queue classes</span>
<span class="gi">+PickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)</span>
<span class="gi">+PickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)</span>
<span class="gi">+MarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)</span>
<span class="gi">+MarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)</span>
<span class="w"> </span>FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)
<span class="w"> </span>LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)
<span class="gh">diff --git a/scrapy/statscollectors.py b/scrapy/statscollectors.py</span>
<span class="gh">index dac77ae91..15193aac5 100644</span>
<span class="gd">--- a/scrapy/statscollectors.py</span>
<span class="gi">+++ b/scrapy/statscollectors.py</span>
<span class="gu">@@ -4,26 +4,95 @@ Scrapy extension for collecting scraping stats</span>
<span class="w"> </span>import logging
<span class="w"> </span>import pprint
<span class="w"> </span>from typing import TYPE_CHECKING, Any, Dict, Optional
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>StatsT = Dict[str, Any]


<span class="w"> </span>class StatsCollector:
<span class="gd">-</span>
<span class="gd">-    def __init__(self, crawler: &#39;Crawler&#39;):</span>
<span class="gd">-        self._dump: bool = crawler.settings.getbool(&#39;STATS_DUMP&#39;)</span>
<span class="gi">+    def __init__(self, crawler: &quot;Crawler&quot;):</span>
<span class="gi">+        self._dump: bool = crawler.settings.getbool(&quot;STATS_DUMP&quot;)</span>
<span class="w"> </span>        self._stats: StatsT = {}

<span class="gi">+    def get_value(</span>
<span class="gi">+        self, key: str, default: Any = None, spider: Optional[Spider] = None</span>
<span class="gi">+    ) -&gt; Any:</span>
<span class="gi">+        return self._stats.get(key, default)</span>

<span class="gd">-class MemoryStatsCollector(StatsCollector):</span>
<span class="gi">+    def get_stats(self, spider: Optional[Spider] = None) -&gt; StatsT:</span>
<span class="gi">+        return self._stats</span>
<span class="gi">+</span>
<span class="gi">+    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        self._stats[key] = value</span>
<span class="gi">+</span>
<span class="gi">+    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        self._stats = stats</span>
<span class="gi">+</span>
<span class="gi">+    def inc_value(</span>
<span class="gi">+        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        d = self._stats</span>
<span class="gi">+        d[key] = d.setdefault(key, start) + count</span>
<span class="gi">+</span>
<span class="gi">+    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        self._stats[key] = max(self._stats.setdefault(key, value), value)</span>
<span class="gi">+</span>
<span class="gi">+    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        self._stats[key] = min(self._stats.setdefault(key, value), value)</span>

<span class="gd">-    def __init__(self, crawler: &#39;Crawler&#39;):</span>
<span class="gi">+    def clear_stats(self, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        self._stats.clear()</span>
<span class="gi">+</span>
<span class="gi">+    def open_spider(self, spider: Spider) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def close_spider(self, spider: Spider, reason: str) -&gt; None:</span>
<span class="gi">+        if self._dump:</span>
<span class="gi">+            logger.info(</span>
<span class="gi">+                &quot;Dumping Scrapy stats:\n&quot; + pprint.pformat(self._stats),</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        self._persist_stats(self._stats, spider)</span>
<span class="gi">+</span>
<span class="gi">+    def _persist_stats(self, stats: StatsT, spider: Spider) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class MemoryStatsCollector(StatsCollector):</span>
<span class="gi">+    def __init__(self, crawler: &quot;Crawler&quot;):</span>
<span class="w"> </span>        super().__init__(crawler)
<span class="w"> </span>        self.spider_stats: Dict[str, StatsT] = {}

<span class="gi">+    def _persist_stats(self, stats: StatsT, spider: Spider) -&gt; None:</span>
<span class="gi">+        self.spider_stats[spider.name] = stats</span>
<span class="gi">+</span>

<span class="w"> </span>class DummyStatsCollector(StatsCollector):
<span class="gd">-    pass</span>
<span class="gi">+    def get_value(</span>
<span class="gi">+        self, key: str, default: Any = None, spider: Optional[Spider] = None</span>
<span class="gi">+    ) -&gt; Any:</span>
<span class="gi">+        return default</span>
<span class="gi">+</span>
<span class="gi">+    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def inc_value(</span>
<span class="gi">+        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -&gt; None:</span>
<span class="gi">+        pass</span>
<span class="gh">diff --git a/scrapy/utils/_compression.py b/scrapy/utils/_compression.py</span>
<span class="gh">index 106ea8c09..7c40d0a02 100644</span>
<span class="gd">--- a/scrapy/utils/_compression.py</span>
<span class="gi">+++ b/scrapy/utils/_compression.py</span>
<span class="gu">@@ -1,7 +1,9 @@</span>
<span class="w"> </span>import zlib
<span class="w"> </span>from io import BytesIO
<span class="w"> </span>from warnings import warn
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import brotli
<span class="w"> </span>except ImportError:
<span class="gu">@@ -11,14 +13,111 @@ else:</span>
<span class="w"> </span>        brotli.Decompressor.process
<span class="w"> </span>    except AttributeError:
<span class="w"> </span>        warn(
<span class="gd">-            &#39;You have brotlipy installed, and Scrapy will use it, but Scrapy support for brotlipy is deprecated and will stop working in a future version of Scrapy. brotlipy itself is deprecated, it has been superseded by brotlicffi (not currently supported by Scrapy). Please, uninstall brotlipy and install brotli instead. brotlipy has the same import name as brotli, so keeping both installed is strongly discouraged.&#39;</span>
<span class="gd">-            , ScrapyDeprecationWarning)</span>
<span class="gi">+            (</span>
<span class="gi">+                &quot;You have brotlipy installed, and Scrapy will use it, but &quot;</span>
<span class="gi">+                &quot;Scrapy support for brotlipy is deprecated and will stop &quot;</span>
<span class="gi">+                &quot;working in a future version of Scrapy. brotlipy itself is &quot;</span>
<span class="gi">+                &quot;deprecated, it has been superseded by brotlicffi (not &quot;</span>
<span class="gi">+                &quot;currently supported by Scrapy). Please, uninstall brotlipy &quot;</span>
<span class="gi">+                &quot;and install brotli instead. brotlipy has the same import &quot;</span>
<span class="gi">+                &quot;name as brotli, so keeping both installed is strongly &quot;</span>
<span class="gi">+                &quot;discouraged.&quot;</span>
<span class="gi">+            ),</span>
<span class="gi">+            ScrapyDeprecationWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        def _brotli_decompress(decompressor, data):</span>
<span class="gi">+            return decompressor.decompress(data)</span>
<span class="gi">+</span>
<span class="gi">+    else:</span>
<span class="gi">+</span>
<span class="gi">+        def _brotli_decompress(decompressor, data):</span>
<span class="gi">+            return decompressor.process(data)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import zstandard
<span class="w"> </span>except ImportError:
<span class="w"> </span>    pass
<span class="gd">-_CHUNK_SIZE = 65536</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_CHUNK_SIZE = 65536  # 64 KiB</span>


<span class="w"> </span>class _DecompressionMaxSizeExceeded(ValueError):
<span class="w"> </span>    pass
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _inflate(data: bytes, *, max_size: int = 0) -&gt; bytes:</span>
<span class="gi">+    decompressor = zlib.decompressobj()</span>
<span class="gi">+    raw_decompressor = zlib.decompressobj(wbits=-15)</span>
<span class="gi">+    input_stream = BytesIO(data)</span>
<span class="gi">+    output_stream = BytesIO()</span>
<span class="gi">+    output_chunk = b&quot;.&quot;</span>
<span class="gi">+    decompressed_size = 0</span>
<span class="gi">+    while output_chunk:</span>
<span class="gi">+        input_chunk = input_stream.read(_CHUNK_SIZE)</span>
<span class="gi">+        try:</span>
<span class="gi">+            output_chunk = decompressor.decompress(input_chunk)</span>
<span class="gi">+        except zlib.error:</span>
<span class="gi">+            if decompressor != raw_decompressor:</span>
<span class="gi">+                # ugly hack to work with raw deflate content that may</span>
<span class="gi">+                # be sent by microsoft servers. For more information, see:</span>
<span class="gi">+                # http://carsten.codimi.de/gzip.yaws/</span>
<span class="gi">+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx</span>
<span class="gi">+                # http://www.gzip.org/zlib/zlib_faq.html#faq38</span>
<span class="gi">+                decompressor = raw_decompressor</span>
<span class="gi">+                output_chunk = decompressor.decompress(input_chunk)</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise</span>
<span class="gi">+        decompressed_size += len(output_chunk)</span>
<span class="gi">+        if max_size and decompressed_size &gt; max_size:</span>
<span class="gi">+            raise _DecompressionMaxSizeExceeded(</span>
<span class="gi">+                f&quot;The number of bytes decompressed so far &quot;</span>
<span class="gi">+                f&quot;({decompressed_size} B) exceed the specified maximum &quot;</span>
<span class="gi">+                f&quot;({max_size} B).&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        output_stream.write(output_chunk)</span>
<span class="gi">+    output_stream.seek(0)</span>
<span class="gi">+    return output_stream.read()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _unbrotli(data: bytes, *, max_size: int = 0) -&gt; bytes:</span>
<span class="gi">+    decompressor = brotli.Decompressor()</span>
<span class="gi">+    input_stream = BytesIO(data)</span>
<span class="gi">+    output_stream = BytesIO()</span>
<span class="gi">+    output_chunk = b&quot;.&quot;</span>
<span class="gi">+    decompressed_size = 0</span>
<span class="gi">+    while output_chunk:</span>
<span class="gi">+        input_chunk = input_stream.read(_CHUNK_SIZE)</span>
<span class="gi">+        output_chunk = _brotli_decompress(decompressor, input_chunk)</span>
<span class="gi">+        decompressed_size += len(output_chunk)</span>
<span class="gi">+        if max_size and decompressed_size &gt; max_size:</span>
<span class="gi">+            raise _DecompressionMaxSizeExceeded(</span>
<span class="gi">+                f&quot;The number of bytes decompressed so far &quot;</span>
<span class="gi">+                f&quot;({decompressed_size} B) exceed the specified maximum &quot;</span>
<span class="gi">+                f&quot;({max_size} B).&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        output_stream.write(output_chunk)</span>
<span class="gi">+    output_stream.seek(0)</span>
<span class="gi">+    return output_stream.read()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _unzstd(data: bytes, *, max_size: int = 0) -&gt; bytes:</span>
<span class="gi">+    decompressor = zstandard.ZstdDecompressor()</span>
<span class="gi">+    stream_reader = decompressor.stream_reader(BytesIO(data))</span>
<span class="gi">+    output_stream = BytesIO()</span>
<span class="gi">+    output_chunk = b&quot;.&quot;</span>
<span class="gi">+    decompressed_size = 0</span>
<span class="gi">+    while output_chunk:</span>
<span class="gi">+        output_chunk = stream_reader.read(_CHUNK_SIZE)</span>
<span class="gi">+        decompressed_size += len(output_chunk)</span>
<span class="gi">+        if max_size and decompressed_size &gt; max_size:</span>
<span class="gi">+            raise _DecompressionMaxSizeExceeded(</span>
<span class="gi">+                f&quot;The number of bytes decompressed so far &quot;</span>
<span class="gi">+                f&quot;({decompressed_size} B) exceed the specified maximum &quot;</span>
<span class="gi">+                f&quot;({max_size} B).&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        output_stream.write(output_chunk)</span>
<span class="gi">+    output_stream.seek(0)</span>
<span class="gi">+    return output_stream.read()</span>
<span class="gh">diff --git a/scrapy/utils/asyncgen.py b/scrapy/utils/asyncgen.py</span>
<span class="gh">index bcdb5eb14..0505db343 100644</span>
<span class="gd">--- a/scrapy/utils/asyncgen.py</span>
<span class="gi">+++ b/scrapy/utils/asyncgen.py</span>
<span class="gu">@@ -1,7 +1,18 @@</span>
<span class="w"> </span>from typing import AsyncGenerator, AsyncIterable, Iterable, Union


<span class="gd">-async def as_async_generator(it: Union[Iterable, AsyncIterable]</span>
<span class="gd">-    ) -&gt;AsyncGenerator:</span>
<span class="gi">+async def collect_asyncgen(result: AsyncIterable) -&gt; list:</span>
<span class="gi">+    results = []</span>
<span class="gi">+    async for x in result:</span>
<span class="gi">+        results.append(x)</span>
<span class="gi">+    return results</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+async def as_async_generator(it: Union[Iterable, AsyncIterable]) -&gt; AsyncGenerator:</span>
<span class="w"> </span>    &quot;&quot;&quot;Wraps an iterable (sync or async) into an async generator.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(it, AsyncIterable):</span>
<span class="gi">+        async for r in it:</span>
<span class="gi">+            yield r</span>
<span class="gi">+    else:</span>
<span class="gi">+        for r in it:</span>
<span class="gi">+            yield r</span>
<span class="gh">diff --git a/scrapy/utils/benchserver.py b/scrapy/utils/benchserver.py</span>
<span class="gh">index fd25c425c..38884a9f0 100644</span>
<span class="gd">--- a/scrapy/utils/benchserver.py</span>
<span class="gi">+++ b/scrapy/utils/benchserver.py</span>
<span class="gu">@@ -1,5 +1,6 @@</span>
<span class="w"> </span>import random
<span class="w"> </span>from urllib.parse import urlencode
<span class="gi">+</span>
<span class="w"> </span>from twisted.web.resource import Resource
<span class="w"> </span>from twisted.web.server import Site

<span class="gu">@@ -7,11 +8,39 @@ from twisted.web.server import Site</span>
<span class="w"> </span>class Root(Resource):
<span class="w"> </span>    isLeaf = True

<span class="gi">+    def getChild(self, name, request):</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def render(self, request):</span>
<span class="gi">+        total = _getarg(request, b&quot;total&quot;, 100, int)</span>
<span class="gi">+        show = _getarg(request, b&quot;show&quot;, 10, int)</span>
<span class="gi">+        nlist = [random.randint(1, total) for _ in range(show)]</span>
<span class="gi">+        request.write(b&quot;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&quot;)</span>
<span class="gi">+        args = request.args.copy()</span>
<span class="gi">+        for nl in nlist:</span>
<span class="gi">+            args[&quot;n&quot;] = nl</span>
<span class="gi">+            argstr = urlencode(args, doseq=True)</span>
<span class="gi">+            request.write(</span>
<span class="gi">+                f&quot;&lt;a href=&#39;/follow?{argstr}&#39;&gt;follow {nl}&lt;/a&gt;&lt;br&gt;&quot;.encode(&quot;utf8&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+        request.write(b&quot;&lt;/body&gt;&lt;/html&gt;&quot;)</span>
<span class="gi">+        return b&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _getarg(request, name, default=None, type=str):</span>
<span class="gi">+    return type(request.args[name][0]) if name in request.args else default</span>

<span class="gd">-if __name__ == &#39;__main__&#39;:</span>
<span class="gi">+</span>
<span class="gi">+if __name__ == &quot;__main__&quot;:</span>
<span class="w"> </span>    from twisted.internet import reactor
<span class="gi">+</span>
<span class="w"> </span>    root = Root()
<span class="w"> </span>    factory = Site(root)
<span class="w"> </span>    httpPort = reactor.listenTCP(8998, Site(root))
<span class="gi">+</span>
<span class="gi">+    def _print_listening():</span>
<span class="gi">+        httpHost = httpPort.getHost()</span>
<span class="gi">+        print(f&quot;Bench server at http://{httpHost.host}:{httpHost.port}&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>    reactor.callWhenRunning(_print_listening)
<span class="w"> </span>    reactor.run()
<span class="gh">diff --git a/scrapy/utils/boto.py b/scrapy/utils/boto.py</span>
<span class="gh">index 94a28dddb..53cfeddd0 100644</span>
<span class="gd">--- a/scrapy/utils/boto.py</span>
<span class="gi">+++ b/scrapy/utils/boto.py</span>
<span class="gu">@@ -1 +1,10 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Boto/botocore helpers&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def is_botocore_available() -&gt; bool:</span>
<span class="gi">+    try:</span>
<span class="gi">+        import botocore  # noqa: F401</span>
<span class="gi">+</span>
<span class="gi">+        return True</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        return False</span>
<span class="gh">diff --git a/scrapy/utils/conf.py b/scrapy/utils/conf.py</span>
<span class="gh">index d81cbfbe5..641dfa4a2 100644</span>
<span class="gd">--- a/scrapy/utils/conf.py</span>
<span class="gi">+++ b/scrapy/utils/conf.py</span>
<span class="gu">@@ -5,53 +5,240 @@ import warnings</span>
<span class="w"> </span>from configparser import ConfigParser
<span class="w"> </span>from operator import itemgetter
<span class="w"> </span>from pathlib import Path
<span class="gd">-from typing import Any, Callable, Collection, Dict, Iterable, List, Mapping, MutableMapping, Optional, Union</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Collection,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Mapping,</span>
<span class="gi">+    MutableMapping,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Union,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning, UsageError
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="w"> </span>from scrapy.utils.deprecate import update_classpath
<span class="w"> </span>from scrapy.utils.python import without_none_values


<span class="gd">-def build_component_list(compdict: MutableMapping[Any, Any], custom: Any=</span>
<span class="gd">-    None, convert: Callable[[Any], Any]=update_classpath) -&gt;List[Any]:</span>
<span class="gi">+def build_component_list(</span>
<span class="gi">+    compdict: MutableMapping[Any, Any],</span>
<span class="gi">+    custom: Any = None,</span>
<span class="gi">+    convert: Callable[[Any], Any] = update_classpath,</span>
<span class="gi">+) -&gt; List[Any]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Compose a component list from a { class: order } dictionary.&quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    def _check_components(complist: Collection[Any]) -&gt; None:</span>
<span class="gi">+        if len({convert(c) for c in complist}) != len(complist):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;Some paths in {complist!r} convert to the same object, &quot;</span>
<span class="gi">+                &quot;please update your settings&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _map_keys(compdict: Mapping[Any, Any]) -&gt; Union[BaseSettings, Dict[Any, Any]]:</span>
<span class="gi">+        if isinstance(compdict, BaseSettings):</span>
<span class="gi">+            compbs = BaseSettings()</span>
<span class="gi">+            for k, v in compdict.items():</span>
<span class="gi">+                prio = compdict.getpriority(k)</span>
<span class="gi">+                assert prio is not None</span>
<span class="gi">+                if compbs.getpriority(convert(k)) == prio:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;Some paths in {list(compdict.keys())!r} &quot;</span>
<span class="gi">+                        &quot;convert to the same &quot;</span>
<span class="gi">+                        &quot;object, please update your settings&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    compbs.set(convert(k), v, priority=prio)</span>
<span class="gi">+            return compbs</span>
<span class="gi">+        _check_components(compdict)</span>
<span class="gi">+        return {convert(k): v for k, v in compdict.items()}</span>
<span class="gi">+</span>
<span class="gi">+    def _validate_values(compdict: Mapping[Any, Any]) -&gt; None:</span>
<span class="gi">+        &quot;&quot;&quot;Fail if a value in the components dict is not a real number or None.&quot;&quot;&quot;</span>
<span class="gi">+        for name, value in compdict.items():</span>
<span class="gi">+            if value is not None and not isinstance(value, numbers.Real):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;Invalid value {value} for component {name}, &quot;</span>
<span class="gi">+                    &quot;please provide a real number or None instead&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    if custom is not None:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;The &#39;custom&#39; attribute of build_component_list() is deprecated. &quot;</span>
<span class="gi">+            &quot;Please merge its value into &#39;compdict&#39; manually or change your &quot;</span>
<span class="gi">+            &quot;code to use Settings.getwithbase().&quot;,</span>
<span class="gi">+            category=ScrapyDeprecationWarning,</span>
<span class="gi">+            stacklevel=2,</span>
<span class="gi">+        )</span>
<span class="gi">+        if isinstance(custom, (list, tuple)):</span>
<span class="gi">+            _check_components(custom)</span>
<span class="gi">+            return type(custom)(convert(c) for c in custom)  # type: ignore[return-value]</span>
<span class="gi">+        compdict.update(custom)</span>

<span class="gd">-def arglist_to_dict(arglist: List[str]) -&gt;Dict[str, str]:</span>
<span class="gi">+    _validate_values(compdict)</span>
<span class="gi">+    compdict = without_none_values(_map_keys(compdict))</span>
<span class="gi">+    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def arglist_to_dict(arglist: List[str]) -&gt; Dict[str, str]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Convert a list of arguments like [&#39;arg1=val1&#39;, &#39;arg2=val2&#39;, ...] to a
<span class="w"> </span>    dict
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return dict(x.split(&quot;=&quot;, 1) for x in arglist)</span>


<span class="gd">-def closest_scrapy_cfg(path: Union[str, os.PathLike]=&#39;.&#39;, prevpath:</span>
<span class="gd">-    Optional[Union[str, os.PathLike]]=None) -&gt;str:</span>
<span class="gi">+def closest_scrapy_cfg(</span>
<span class="gi">+    path: Union[str, os.PathLike] = &quot;.&quot;,</span>
<span class="gi">+    prevpath: Optional[Union[str, os.PathLike]] = None,</span>
<span class="gi">+) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the path to the closest scrapy.cfg file by traversing the current
<span class="w"> </span>    directory and its parents
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if prevpath is not None and str(path) == str(prevpath):</span>
<span class="gi">+        return &quot;&quot;</span>
<span class="gi">+    path = Path(path).resolve()</span>
<span class="gi">+    cfgfile = path / &quot;scrapy.cfg&quot;</span>
<span class="gi">+    if cfgfile.exists():</span>
<span class="gi">+        return str(cfgfile)</span>
<span class="gi">+    return closest_scrapy_cfg(path.parent, path)</span>


<span class="gd">-def init_env(project: str=&#39;default&#39;, set_syspath: bool=True) -&gt;None:</span>
<span class="gi">+def init_env(project: str = &quot;default&quot;, set_syspath: bool = True) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Initialize environment to use command-line tool from inside a project
<span class="w"> </span>    dir. This sets the Scrapy settings module and modifies the Python path to
<span class="w"> </span>    be able to locate the project module.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cfg = get_config()</span>
<span class="gi">+    if cfg.has_option(&quot;settings&quot;, project):</span>
<span class="gi">+        os.environ[&quot;SCRAPY_SETTINGS_MODULE&quot;] = cfg.get(&quot;settings&quot;, project)</span>
<span class="gi">+    closest = closest_scrapy_cfg()</span>
<span class="gi">+    if closest:</span>
<span class="gi">+        projdir = str(Path(closest).parent)</span>
<span class="gi">+        if set_syspath and projdir not in sys.path:</span>
<span class="gi">+            sys.path.append(projdir)</span>


<span class="gd">-def get_config(use_closest: bool=True) -&gt;ConfigParser:</span>
<span class="gi">+def get_config(use_closest: bool = True) -&gt; ConfigParser:</span>
<span class="w"> </span>    &quot;&quot;&quot;Get Scrapy config file as a ConfigParser&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    sources = get_sources(use_closest)</span>
<span class="gi">+    cfg = ConfigParser()</span>
<span class="gi">+    cfg.read(sources)</span>
<span class="gi">+    return cfg</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_sources(use_closest: bool = True) -&gt; List[str]:</span>
<span class="gi">+    xdg_config_home = (</span>
<span class="gi">+        os.environ.get(&quot;XDG_CONFIG_HOME&quot;) or Path(&quot;~/.config&quot;).expanduser()</span>
<span class="gi">+    )</span>
<span class="gi">+    sources = [</span>
<span class="gi">+        &quot;/etc/scrapy.cfg&quot;,</span>
<span class="gi">+        r&quot;c:\scrapy\scrapy.cfg&quot;,</span>
<span class="gi">+        str(Path(xdg_config_home) / &quot;scrapy.cfg&quot;),</span>
<span class="gi">+        str(Path(&quot;~/.scrapy.cfg&quot;).expanduser()),</span>
<span class="gi">+    ]</span>
<span class="gi">+    if use_closest:</span>
<span class="gi">+        sources.append(closest_scrapy_cfg())</span>
<span class="gi">+    return sources</span>


<span class="gd">-def feed_process_params_from_cli(settings: BaseSettings, output: List[str],</span>
<span class="gd">-    output_format: Optional[str]=None, overwrite_output: Optional[List[str]</span>
<span class="gd">-    ]=None) -&gt;Dict[str, Dict[str, Any]]:</span>
<span class="gi">+def feed_complete_default_values_from_settings(</span>
<span class="gi">+    feed: Dict[str, Any], settings: BaseSettings</span>
<span class="gi">+) -&gt; Dict[str, Any]:</span>
<span class="gi">+    out = feed.copy()</span>
<span class="gi">+    out.setdefault(&quot;batch_item_count&quot;, settings.getint(&quot;FEED_EXPORT_BATCH_ITEM_COUNT&quot;))</span>
<span class="gi">+    out.setdefault(&quot;encoding&quot;, settings[&quot;FEED_EXPORT_ENCODING&quot;])</span>
<span class="gi">+    out.setdefault(&quot;fields&quot;, settings.getdictorlist(&quot;FEED_EXPORT_FIELDS&quot;) or None)</span>
<span class="gi">+    out.setdefault(&quot;store_empty&quot;, settings.getbool(&quot;FEED_STORE_EMPTY&quot;))</span>
<span class="gi">+    out.setdefault(&quot;uri_params&quot;, settings[&quot;FEED_URI_PARAMS&quot;])</span>
<span class="gi">+    out.setdefault(&quot;item_export_kwargs&quot;, {})</span>
<span class="gi">+    if settings[&quot;FEED_EXPORT_INDENT&quot;] is None:</span>
<span class="gi">+        out.setdefault(&quot;indent&quot;, None)</span>
<span class="gi">+    else:</span>
<span class="gi">+        out.setdefault(&quot;indent&quot;, settings.getint(&quot;FEED_EXPORT_INDENT&quot;))</span>
<span class="gi">+    return out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def feed_process_params_from_cli(</span>
<span class="gi">+    settings: BaseSettings,</span>
<span class="gi">+    output: List[str],</span>
<span class="gi">+    output_format: Optional[str] = None,</span>
<span class="gi">+    overwrite_output: Optional[List[str]] = None,</span>
<span class="gi">+) -&gt; Dict[str, Dict[str, Any]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Receives feed export params (from the &#39;crawl&#39; or &#39;runspider&#39; commands),
<span class="w"> </span>    checks for inconsistencies in their quantities and returns a dictionary
<span class="w"> </span>    suitable to be used as the FEEDS setting.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    valid_output_formats: Iterable[str] = without_none_values(</span>
<span class="gi">+        settings.getwithbase(&quot;FEED_EXPORTERS&quot;)</span>
<span class="gi">+    ).keys()</span>
<span class="gi">+</span>
<span class="gi">+    def check_valid_format(output_format: str) -&gt; None:</span>
<span class="gi">+        if output_format not in valid_output_formats:</span>
<span class="gi">+            raise UsageError(</span>
<span class="gi">+                f&quot;Unrecognized output format &#39;{output_format}&#39;. &quot;</span>
<span class="gi">+                f&quot;Set a supported one ({tuple(valid_output_formats)}) &quot;</span>
<span class="gi">+                &quot;after a colon at the end of the output URI (i.e. -o/-O &quot;</span>
<span class="gi">+                &quot;&lt;URI&gt;:&lt;FORMAT&gt;) or as a file extension.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    overwrite = False</span>
<span class="gi">+    if overwrite_output:</span>
<span class="gi">+        if output:</span>
<span class="gi">+            raise UsageError(</span>
<span class="gi">+                &quot;Please use only one of -o/--output and -O/--overwrite-output&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        if output_format:</span>
<span class="gi">+            raise UsageError(</span>
<span class="gi">+                &quot;-t/--output-format is a deprecated command line option&quot;</span>
<span class="gi">+                &quot; and does not work in combination with -O/--overwrite-output.&quot;</span>
<span class="gi">+                &quot; To specify a format please specify it after a colon at the end of the&quot;</span>
<span class="gi">+                &quot; output URI (i.e. -O &lt;URI&gt;:&lt;FORMAT&gt;).&quot;</span>
<span class="gi">+                &quot; Example working in the tutorial: &quot;</span>
<span class="gi">+                &quot;scrapy crawl quotes -O quotes.json:json&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        output = overwrite_output</span>
<span class="gi">+        overwrite = True</span>
<span class="gi">+</span>
<span class="gi">+    if output_format:</span>
<span class="gi">+        if len(output) == 1:</span>
<span class="gi">+            check_valid_format(output_format)</span>
<span class="gi">+            message = (</span>
<span class="gi">+                &quot;The -t/--output-format command line option is deprecated in favor of &quot;</span>
<span class="gi">+                &quot;specifying the output format within the output URI using the -o/--output or the&quot;</span>
<span class="gi">+                &quot; -O/--overwrite-output option (i.e. -o/-O &lt;URI&gt;:&lt;FORMAT&gt;). See the documentation&quot;</span>
<span class="gi">+                &quot; of the -o or -O option or the following examples for more information. &quot;</span>
<span class="gi">+                &quot;Examples working in the tutorial: &quot;</span>
<span class="gi">+                &quot;scrapy crawl quotes -o quotes.csv:csv   or   &quot;</span>
<span class="gi">+                &quot;scrapy crawl quotes -O quotes.json:json&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gi">+            return {output[0]: {&quot;format&quot;: output_format}}</span>
<span class="gi">+        raise UsageError(</span>
<span class="gi">+            &quot;The -t command-line option cannot be used if multiple output &quot;</span>
<span class="gi">+            &quot;URIs are specified&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    result: Dict[str, Dict[str, Any]] = {}</span>
<span class="gi">+    for element in output:</span>
<span class="gi">+        try:</span>
<span class="gi">+            feed_uri, feed_format = element.rsplit(&quot;:&quot;, 1)</span>
<span class="gi">+            check_valid_format(feed_format)</span>
<span class="gi">+        except (ValueError, UsageError):</span>
<span class="gi">+            feed_uri = element</span>
<span class="gi">+            feed_format = Path(element).suffix.replace(&quot;.&quot;, &quot;&quot;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            if feed_uri == &quot;-&quot;:</span>
<span class="gi">+                feed_uri = &quot;stdout:&quot;</span>
<span class="gi">+        check_valid_format(feed_format)</span>
<span class="gi">+        result[feed_uri] = {&quot;format&quot;: feed_format}</span>
<span class="gi">+        if overwrite:</span>
<span class="gi">+            result[feed_uri][&quot;overwrite&quot;] = True</span>
<span class="gi">+</span>
<span class="gi">+    # FEEDS setting should take precedence over the matching CLI options</span>
<span class="gi">+    result.update(settings.getdict(&quot;FEEDS&quot;))</span>
<span class="gi">+</span>
<span class="gi">+    return result</span>
<span class="gh">diff --git a/scrapy/utils/console.py b/scrapy/utils/console.py</span>
<span class="gh">index b5bd733b2..100f040bb 100644</span>
<span class="gd">--- a/scrapy/utils/console.py</span>
<span class="gi">+++ b/scrapy/utils/console.py</span>
<span class="gu">@@ -1,40 +1,110 @@</span>
<span class="w"> </span>from functools import wraps


<span class="gd">-def _embed_ipython_shell(namespace={}, banner=&#39;&#39;):</span>
<span class="gi">+def _embed_ipython_shell(namespace={}, banner=&quot;&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot;Start an IPython Shell&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        from IPython.terminal.embed import InteractiveShellEmbed</span>
<span class="gi">+        from IPython.terminal.ipapp import load_default_config</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        from IPython.frontend.terminal.embed import InteractiveShellEmbed</span>
<span class="gi">+        from IPython.frontend.terminal.ipapp import load_default_config</span>

<span class="gi">+    @wraps(_embed_ipython_shell)</span>
<span class="gi">+    def wrapper(namespace=namespace, banner=&quot;&quot;):</span>
<span class="gi">+        config = load_default_config()</span>
<span class="gi">+        # Always use .instance() to ensure _instance propagation to all parents</span>
<span class="gi">+        # this is needed for &lt;TAB&gt; completion works well for new imports</span>
<span class="gi">+        # and clear the instance to always have the fresh env</span>
<span class="gi">+        # on repeated breaks like with inspect_response()</span>
<span class="gi">+        InteractiveShellEmbed.clear_instance()</span>
<span class="gi">+        shell = InteractiveShellEmbed.instance(</span>
<span class="gi">+            banner1=banner, user_ns=namespace, config=config</span>
<span class="gi">+        )</span>
<span class="gi">+        shell()</span>

<span class="gd">-def _embed_bpython_shell(namespace={}, banner=&#39;&#39;):</span>
<span class="gi">+    return wrapper</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _embed_bpython_shell(namespace={}, banner=&quot;&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot;Start a bpython shell&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import bpython</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(_embed_bpython_shell)</span>
<span class="gi">+    def wrapper(namespace=namespace, banner=&quot;&quot;):</span>
<span class="gi">+        bpython.embed(locals_=namespace, banner=banner)</span>

<span class="gi">+    return wrapper</span>

<span class="gd">-def _embed_ptpython_shell(namespace={}, banner=&#39;&#39;):</span>
<span class="gi">+</span>
<span class="gi">+def _embed_ptpython_shell(namespace={}, banner=&quot;&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot;Start a ptpython shell&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import ptpython.repl</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(_embed_ptpython_shell)</span>
<span class="gi">+    def wrapper(namespace=namespace, banner=&quot;&quot;):</span>
<span class="gi">+        print(banner)</span>
<span class="gi">+        ptpython.repl.embed(locals=namespace)</span>
<span class="gi">+</span>
<span class="gi">+    return wrapper</span>


<span class="gd">-def _embed_standard_shell(namespace={}, banner=&#39;&#39;):</span>
<span class="gi">+def _embed_standard_shell(namespace={}, banner=&quot;&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot;Start a standard python shell&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import code</span>

<span class="gi">+    try:  # readline module is only available on unix systems</span>
<span class="gi">+        import readline</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        pass</span>
<span class="gi">+    else:</span>
<span class="gi">+        import rlcompleter  # noqa: F401</span>

<span class="gd">-DEFAULT_PYTHON_SHELLS = {&#39;ptpython&#39;: _embed_ptpython_shell, &#39;ipython&#39;:</span>
<span class="gd">-    _embed_ipython_shell, &#39;bpython&#39;: _embed_bpython_shell, &#39;python&#39;:</span>
<span class="gd">-    _embed_standard_shell}</span>
<span class="gi">+        readline.parse_and_bind(&quot;tab:complete&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(_embed_standard_shell)</span>
<span class="gi">+    def wrapper(namespace=namespace, banner=&quot;&quot;):</span>
<span class="gi">+        code.interact(banner=banner, local=namespace)</span>
<span class="gi">+</span>
<span class="gi">+    return wrapper</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+DEFAULT_PYTHON_SHELLS = {</span>
<span class="gi">+    &quot;ptpython&quot;: _embed_ptpython_shell,</span>
<span class="gi">+    &quot;ipython&quot;: _embed_ipython_shell,</span>
<span class="gi">+    &quot;bpython&quot;: _embed_bpython_shell,</span>
<span class="gi">+    &quot;python&quot;: _embed_standard_shell,</span>
<span class="gi">+}</span>


<span class="w"> </span>def get_shell_embed_func(shells=None, known_shells=None):
<span class="w"> </span>    &quot;&quot;&quot;Return the first acceptable shell-embed function
<span class="w"> </span>    from a given list of shell names.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if shells is None:  # list, preference order of shells</span>
<span class="gi">+        shells = DEFAULT_PYTHON_SHELLS.keys()</span>
<span class="gi">+    if known_shells is None:  # available embeddable shells</span>
<span class="gi">+        known_shells = DEFAULT_PYTHON_SHELLS.copy()</span>
<span class="gi">+    for shell in shells:</span>
<span class="gi">+        if shell in known_shells:</span>
<span class="gi">+            try:</span>
<span class="gi">+                # function test: run all setup code (imports),</span>
<span class="gi">+                # but dont fall into the shell</span>
<span class="gi">+                return known_shells[shell]()</span>
<span class="gi">+            except ImportError:</span>
<span class="gi">+                continue</span>


<span class="gd">-def start_python_console(namespace=None, banner=&#39;&#39;, shells=None):</span>
<span class="gi">+def start_python_console(namespace=None, banner=&quot;&quot;, shells=None):</span>
<span class="w"> </span>    &quot;&quot;&quot;Start Python console bound to the given namespace.
<span class="w"> </span>    Readline support and tab completion will be used on Unix, if available.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if namespace is None:</span>
<span class="gi">+        namespace = {}</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        shell = get_shell_embed_func(shells)</span>
<span class="gi">+        if shell is not None:</span>
<span class="gi">+            shell(namespace=namespace, banner=banner)</span>
<span class="gi">+    except SystemExit:  # raised when using exit() in python code.interact</span>
<span class="gi">+        pass</span>
<span class="gh">diff --git a/scrapy/utils/curl.py b/scrapy/utils/curl.py</span>
<span class="gh">index fec4cdf21..f5dbbd64e 100644</span>
<span class="gd">--- a/scrapy/utils/curl.py</span>
<span class="gi">+++ b/scrapy/utils/curl.py</span>
<span class="gu">@@ -3,37 +3,68 @@ import warnings</span>
<span class="w"> </span>from http.cookies import SimpleCookie
<span class="w"> </span>from shlex import split
<span class="w"> </span>from urllib.parse import urlparse
<span class="gi">+</span>
<span class="w"> </span>from w3lib.http import basic_auth_header


<span class="w"> </span>class DataAction(argparse.Action):
<span class="gd">-</span>
<span class="w"> </span>    def __call__(self, parser, namespace, values, option_string=None):
<span class="w"> </span>        value = str(values)
<span class="gd">-        if value.startswith(&#39;$&#39;):</span>
<span class="gi">+        if value.startswith(&quot;$&quot;):</span>
<span class="w"> </span>            value = value[1:]
<span class="w"> </span>        setattr(namespace, self.dest, value)


<span class="w"> </span>class CurlParser(argparse.ArgumentParser):
<span class="gd">-    pass</span>
<span class="gi">+    def error(self, message):</span>
<span class="gi">+        error_msg = f&quot;There was an error parsing the curl command: {message}&quot;</span>
<span class="gi">+        raise ValueError(error_msg)</span>


<span class="w"> </span>curl_parser = CurlParser()
<span class="gd">-curl_parser.add_argument(&#39;url&#39;)</span>
<span class="gd">-curl_parser.add_argument(&#39;-H&#39;, &#39;--header&#39;, dest=&#39;headers&#39;, action=&#39;append&#39;)</span>
<span class="gd">-curl_parser.add_argument(&#39;-X&#39;, &#39;--request&#39;, dest=&#39;method&#39;)</span>
<span class="gd">-curl_parser.add_argument(&#39;-d&#39;, &#39;--data&#39;, &#39;--data-raw&#39;, dest=&#39;data&#39;, action=</span>
<span class="gd">-    DataAction)</span>
<span class="gd">-curl_parser.add_argument(&#39;-u&#39;, &#39;--user&#39;, dest=&#39;auth&#39;)</span>
<span class="gd">-safe_to_ignore_arguments = [[&#39;--compressed&#39;], [&#39;-s&#39;, &#39;--silent&#39;], [&#39;-v&#39;,</span>
<span class="gd">-    &#39;--verbose&#39;], [&#39;-#&#39;, &#39;--progress-bar&#39;]]</span>
<span class="gi">+curl_parser.add_argument(&quot;url&quot;)</span>
<span class="gi">+curl_parser.add_argument(&quot;-H&quot;, &quot;--header&quot;, dest=&quot;headers&quot;, action=&quot;append&quot;)</span>
<span class="gi">+curl_parser.add_argument(&quot;-X&quot;, &quot;--request&quot;, dest=&quot;method&quot;)</span>
<span class="gi">+curl_parser.add_argument(&quot;-d&quot;, &quot;--data&quot;, &quot;--data-raw&quot;, dest=&quot;data&quot;, action=DataAction)</span>
<span class="gi">+curl_parser.add_argument(&quot;-u&quot;, &quot;--user&quot;, dest=&quot;auth&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+safe_to_ignore_arguments = [</span>
<span class="gi">+    [&quot;--compressed&quot;],</span>
<span class="gi">+    # `--compressed` argument is not safe to ignore, but it&#39;s included here</span>
<span class="gi">+    # because the `HttpCompressionMiddleware` is enabled by default</span>
<span class="gi">+    [&quot;-s&quot;, &quot;--silent&quot;],</span>
<span class="gi">+    [&quot;-v&quot;, &quot;--verbose&quot;],</span>
<span class="gi">+    [&quot;-#&quot;, &quot;--progress-bar&quot;],</span>
<span class="gi">+]</span>
<span class="gi">+</span>
<span class="w"> </span>for argument in safe_to_ignore_arguments:
<span class="gd">-    curl_parser.add_argument(*argument, action=&#39;store_true&#39;)</span>
<span class="gi">+    curl_parser.add_argument(*argument, action=&quot;store_true&quot;)</span>


<span class="gd">-def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool=True</span>
<span class="gd">-    ) -&gt;dict:</span>
<span class="gi">+def _parse_headers_and_cookies(parsed_args):</span>
<span class="gi">+    headers = []</span>
<span class="gi">+    cookies = {}</span>
<span class="gi">+    for header in parsed_args.headers or ():</span>
<span class="gi">+        name, val = header.split(&quot;:&quot;, 1)</span>
<span class="gi">+        name = name.strip()</span>
<span class="gi">+        val = val.strip()</span>
<span class="gi">+        if name.title() == &quot;Cookie&quot;:</span>
<span class="gi">+            for name, morsel in SimpleCookie(val).items():</span>
<span class="gi">+                cookies[name] = morsel.value</span>
<span class="gi">+        else:</span>
<span class="gi">+            headers.append((name, val))</span>
<span class="gi">+</span>
<span class="gi">+    if parsed_args.auth:</span>
<span class="gi">+        user, password = parsed_args.auth.split(&quot;:&quot;, 1)</span>
<span class="gi">+        headers.append((&quot;Authorization&quot;, basic_auth_header(user, password)))</span>
<span class="gi">+</span>
<span class="gi">+    return headers, cookies</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def curl_to_request_kwargs(</span>
<span class="gi">+    curl_command: str, ignore_unknown_options: bool = True</span>
<span class="gi">+) -&gt; dict:</span>
<span class="w"> </span>    &quot;&quot;&quot;Convert a cURL command syntax to Request kwargs.

<span class="w"> </span>    :param str curl_command: string containing the curl command
<span class="gu">@@ -42,4 +73,44 @@ def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool=True</span>
<span class="w"> </span>                                        raises an error. (default: True)
<span class="w"> </span>    :return: dictionary of Request kwargs
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    curl_args = split(curl_command)</span>
<span class="gi">+</span>
<span class="gi">+    if curl_args[0] != &quot;curl&quot;:</span>
<span class="gi">+        raise ValueError(&#39;A curl command must start with &quot;curl&quot;&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])</span>
<span class="gi">+</span>
<span class="gi">+    if argv:</span>
<span class="gi">+        msg = f&#39;Unrecognized options: {&quot;, &quot;.join(argv)}&#39;</span>
<span class="gi">+        if ignore_unknown_options:</span>
<span class="gi">+            warnings.warn(msg)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(msg)</span>
<span class="gi">+</span>
<span class="gi">+    url = parsed_args.url</span>
<span class="gi">+</span>
<span class="gi">+    # curl automatically prepends &#39;http&#39; if the scheme is missing, but Request</span>
<span class="gi">+    # needs the scheme to work</span>
<span class="gi">+    parsed_url = urlparse(url)</span>
<span class="gi">+    if not parsed_url.scheme:</span>
<span class="gi">+        url = &quot;http://&quot; + url</span>
<span class="gi">+</span>
<span class="gi">+    method = parsed_args.method or &quot;GET&quot;</span>
<span class="gi">+</span>
<span class="gi">+    result = {&quot;method&quot;: method.upper(), &quot;url&quot;: url}</span>
<span class="gi">+</span>
<span class="gi">+    headers, cookies = _parse_headers_and_cookies(parsed_args)</span>
<span class="gi">+</span>
<span class="gi">+    if headers:</span>
<span class="gi">+        result[&quot;headers&quot;] = headers</span>
<span class="gi">+    if cookies:</span>
<span class="gi">+        result[&quot;cookies&quot;] = cookies</span>
<span class="gi">+    if parsed_args.data:</span>
<span class="gi">+        result[&quot;body&quot;] = parsed_args.data</span>
<span class="gi">+        if not parsed_args.method:</span>
<span class="gi">+            # if the &quot;data&quot; is specified but the &quot;method&quot; is not specified,</span>
<span class="gi">+            # the default method is &#39;POST&#39;</span>
<span class="gi">+            result[&quot;method&quot;] = &quot;POST&quot;</span>
<span class="gi">+</span>
<span class="gi">+    return result</span>
<span class="gh">diff --git a/scrapy/utils/datatypes.py b/scrapy/utils/datatypes.py</span>
<span class="gh">index 3ec4ef789..d5b9544cc 100644</span>
<span class="gd">--- a/scrapy/utils/datatypes.py</span>
<span class="gi">+++ b/scrapy/utils/datatypes.py</span>
<span class="gu">@@ -4,14 +4,17 @@ Python Standard Library.</span>

<span class="w"> </span>This module must not depend on any module outside the Standard Library.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import collections
<span class="w"> </span>import warnings
<span class="w"> </span>import weakref
<span class="w"> </span>from collections.abc import Mapping
<span class="w"> </span>from typing import Any, AnyStr, Optional, OrderedDict, Sequence, TypeVar
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="gd">-_KT = TypeVar(&#39;_KT&#39;)</span>
<span class="gd">-_VT = TypeVar(&#39;_VT&#39;)</span>
<span class="gi">+</span>
<span class="gi">+_KT = TypeVar(&quot;_KT&quot;)</span>
<span class="gi">+_VT = TypeVar(&quot;_VT&quot;)</span>


<span class="w"> </span>class CaselessDict(dict):
<span class="gu">@@ -19,10 +22,14 @@ class CaselessDict(dict):</span>

<span class="w"> </span>    def __new__(cls, *args, **kwargs):
<span class="w"> </span>        from scrapy.http.headers import Headers
<span class="gi">+</span>
<span class="w"> </span>        if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):
<span class="w"> </span>            warnings.warn(
<span class="gd">-                &#39;scrapy.utils.datatypes.CaselessDict is deprecated, please use scrapy.utils.datatypes.CaseInsensitiveDict instead&#39;</span>
<span class="gd">-                , category=ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gi">+                &quot;scrapy.utils.datatypes.CaselessDict is deprecated,&quot;</span>
<span class="gi">+                &quot; please use scrapy.utils.datatypes.CaseInsensitiveDict instead&quot;,</span>
<span class="gi">+                category=ScrapyDeprecationWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="w"> </span>        return super().__new__(cls, *args, **kwargs)

<span class="w"> </span>    def __init__(self, seq=None):
<span class="gu">@@ -41,19 +48,39 @@ class CaselessDict(dict):</span>

<span class="w"> </span>    def __contains__(self, key):
<span class="w"> </span>        return dict.__contains__(self, self.normkey(key))
<span class="gi">+</span>
<span class="w"> </span>    has_key = __contains__

<span class="w"> </span>    def __copy__(self):
<span class="w"> </span>        return self.__class__(self)
<span class="gi">+</span>
<span class="w"> </span>    copy = __copy__

<span class="w"> </span>    def normkey(self, key):
<span class="w"> </span>        &quot;&quot;&quot;Method to normalize dictionary key access&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return key.lower()</span>

<span class="w"> </span>    def normvalue(self, value):
<span class="w"> </span>        &quot;&quot;&quot;Method to normalize values prior to be set&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return value</span>
<span class="gi">+</span>
<span class="gi">+    def get(self, key, def_val=None):</span>
<span class="gi">+        return dict.get(self, self.normkey(key), self.normvalue(def_val))</span>
<span class="gi">+</span>
<span class="gi">+    def setdefault(self, key, def_val=None):</span>
<span class="gi">+        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))</span>
<span class="gi">+</span>
<span class="gi">+    def update(self, seq):</span>
<span class="gi">+        seq = seq.items() if isinstance(seq, Mapping) else seq</span>
<span class="gi">+        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)</span>
<span class="gi">+        super().update(iseq)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def fromkeys(cls, keys, value=None):</span>
<span class="gi">+        return cls((k, value) for k in keys)</span>
<span class="gi">+</span>
<span class="gi">+    def pop(self, key, *args):</span>
<span class="gi">+        return dict.pop(self, self.normkey(key), *args)</span>


<span class="w"> </span>class CaseInsensitiveDict(collections.UserDict):
<span class="gu">@@ -61,15 +88,15 @@ class CaseInsensitiveDict(collections.UserDict):</span>
<span class="w"> </span>    as keys and allows case-insensitive lookups.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, *args, **kwargs) -&gt;None:</span>
<span class="gi">+    def __init__(self, *args, **kwargs) -&gt; None:</span>
<span class="w"> </span>        self._keys: dict = {}
<span class="w"> </span>        super().__init__(*args, **kwargs)

<span class="gd">-    def __getitem__(self, key: AnyStr) -&gt;Any:</span>
<span class="gi">+    def __getitem__(self, key: AnyStr) -&gt; Any:</span>
<span class="w"> </span>        normalized_key = self._normkey(key)
<span class="w"> </span>        return super().__getitem__(self._keys[normalized_key.lower()])

<span class="gd">-    def __setitem__(self, key: AnyStr, value: Any) -&gt;None:</span>
<span class="gi">+    def __setitem__(self, key: AnyStr, value: Any) -&gt; None:</span>
<span class="w"> </span>        normalized_key = self._normkey(key)
<span class="w"> </span>        try:
<span class="w"> </span>            lower_key = self._keys[normalized_key.lower()]
<span class="gu">@@ -79,17 +106,23 @@ class CaseInsensitiveDict(collections.UserDict):</span>
<span class="w"> </span>        super().__setitem__(normalized_key, self._normvalue(value))
<span class="w"> </span>        self._keys[normalized_key.lower()] = normalized_key

<span class="gd">-    def __delitem__(self, key: AnyStr) -&gt;None:</span>
<span class="gi">+    def __delitem__(self, key: AnyStr) -&gt; None:</span>
<span class="w"> </span>        normalized_key = self._normkey(key)
<span class="w"> </span>        stored_key = self._keys.pop(normalized_key.lower())
<span class="w"> </span>        super().__delitem__(stored_key)

<span class="gd">-    def __contains__(self, key: AnyStr) -&gt;bool:</span>
<span class="gi">+    def __contains__(self, key: AnyStr) -&gt; bool:  # type: ignore[override]</span>
<span class="w"> </span>        normalized_key = self._normkey(key)
<span class="w"> </span>        return normalized_key.lower() in self._keys

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}: {super().__repr__()}&gt;&#39;</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}: {super().__repr__()}&gt;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def _normkey(self, key: AnyStr) -&gt; AnyStr:</span>
<span class="gi">+        return key</span>
<span class="gi">+</span>
<span class="gi">+    def _normvalue(self, value: Any) -&gt; Any:</span>
<span class="gi">+        return value</span>


<span class="w"> </span>class LocalCache(OrderedDict[_KT, _VT]):
<span class="gu">@@ -98,11 +131,11 @@ class LocalCache(OrderedDict[_KT, _VT]):</span>
<span class="w"> </span>    Older items expires first.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, limit: Optional[int]=None):</span>
<span class="gi">+    def __init__(self, limit: Optional[int] = None):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.limit: Optional[int] = limit

<span class="gd">-    def __setitem__(self, key: _KT, value: _VT) -&gt;None:</span>
<span class="gi">+    def __setitem__(self, key: _KT, value: _VT) -&gt; None:</span>
<span class="w"> </span>        if self.limit:
<span class="w"> </span>            while len(self) &gt;= self.limit:
<span class="w"> </span>                self.popitem(last=False)
<span class="gu">@@ -121,21 +154,21 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):</span>
<span class="w"> </span>    it cannot be instantiated with an initial dictionary.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, limit: Optional[int]=None):</span>
<span class="gi">+    def __init__(self, limit: Optional[int] = None):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.data: LocalCache = LocalCache(limit=limit)

<span class="gd">-    def __setitem__(self, key: _KT, value: _VT) -&gt;None:</span>
<span class="gi">+    def __setitem__(self, key: _KT, value: _VT) -&gt; None:</span>
<span class="w"> </span>        try:
<span class="w"> </span>            super().__setitem__(key, value)
<span class="w"> </span>        except TypeError:
<span class="gd">-            pass</span>
<span class="gi">+            pass  # key is not weak-referenceable, skip caching</span>

<span class="gd">-    def __getitem__(self, key: _KT) -&gt;Optional[_VT]:</span>
<span class="gi">+    def __getitem__(self, key: _KT) -&gt; Optional[_VT]:  # type: ignore[override]</span>
<span class="w"> </span>        try:
<span class="w"> </span>            return super().__getitem__(key)
<span class="w"> </span>        except (TypeError, KeyError):
<span class="gd">-            return None</span>
<span class="gi">+            return None  # key is either not weak-referenceable or not cached</span>


<span class="w"> </span>class SequenceExclude:
<span class="gu">@@ -144,5 +177,5 @@ class SequenceExclude:</span>
<span class="w"> </span>    def __init__(self, seq: Sequence):
<span class="w"> </span>        self.seq: Sequence = seq

<span class="gd">-    def __contains__(self, item: Any) -&gt;bool:</span>
<span class="gi">+    def __contains__(self, item: Any) -&gt; bool:</span>
<span class="w"> </span>        return item not in self.seq
<span class="gh">diff --git a/scrapy/utils/decorators.py b/scrapy/utils/decorators.py</span>
<span class="gh">index b441b495f..04186559f 100644</span>
<span class="gd">--- a/scrapy/utils/decorators.py</span>
<span class="gi">+++ b/scrapy/utils/decorators.py</span>
<span class="gu">@@ -1,25 +1,52 @@</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from functools import wraps
<span class="w"> </span>from typing import Any, Callable
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer, threads
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning


<span class="gd">-def deprecated(use_instead: Any=None) -&gt;Callable:</span>
<span class="gi">+def deprecated(use_instead: Any = None) -&gt; Callable:</span>
<span class="w"> </span>    &quot;&quot;&quot;This is a decorator which can be used to mark functions
<span class="w"> </span>    as deprecated. It will result in a warning being emitted
<span class="w"> </span>    when the function is used.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def deco(func: Callable) -&gt; Callable:</span>
<span class="gi">+        @wraps(func)</span>
<span class="gi">+        def wrapped(*args: Any, **kwargs: Any) -&gt; Any:</span>
<span class="gi">+            message = f&quot;Call to deprecated function {func.__name__}.&quot;</span>
<span class="gi">+            if use_instead:</span>
<span class="gi">+                message += f&quot; Use {use_instead} instead.&quot;</span>
<span class="gi">+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gi">+            return func(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+        return wrapped</span>
<span class="gi">+</span>
<span class="gi">+    if callable(use_instead):</span>
<span class="gi">+        deco = deco(use_instead)</span>
<span class="gi">+        use_instead = None</span>
<span class="gi">+    return deco</span>


<span class="gd">-def defers(func: Callable) -&gt;Callable[..., Deferred]:</span>
<span class="gi">+def defers(func: Callable) -&gt; Callable[..., Deferred]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Decorator to make sure a function always returns a deferred&quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    @wraps(func)</span>
<span class="gi">+    def wrapped(*a: Any, **kw: Any) -&gt; Deferred:</span>
<span class="gi">+        return defer.maybeDeferred(func, *a, **kw)</span>

<span class="gd">-def inthread(func: Callable) -&gt;Callable[..., Deferred]:</span>
<span class="gi">+    return wrapped</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def inthread(func: Callable) -&gt; Callable[..., Deferred]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Decorator to call a function in a thread and return a deferred with the
<span class="w"> </span>    result
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(func)</span>
<span class="gi">+    def wrapped(*a: Any, **kw: Any) -&gt; Deferred:</span>
<span class="gi">+        return threads.deferToThread(func, *a, **kw)</span>
<span class="gi">+</span>
<span class="gi">+    return wrapped</span>
<span class="gh">diff --git a/scrapy/utils/defer.py b/scrapy/utils/defer.py</span>
<span class="gh">index aff03f895..bf3c5ef5b 100644</span>
<span class="gd">--- a/scrapy/utils/defer.py</span>
<span class="gi">+++ b/scrapy/utils/defer.py</span>
<span class="gu">@@ -6,51 +6,101 @@ import inspect</span>
<span class="w"> </span>from asyncio import Future
<span class="w"> </span>from functools import wraps
<span class="w"> </span>from types import CoroutineType
<span class="gd">-from typing import Any, AsyncGenerator, AsyncIterable, AsyncIterator, Awaitable, Callable, Coroutine, Dict, Generator, Iterable, Iterator, List, Optional, Tuple, TypeVar, Union, cast, overload</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    Any,</span>
<span class="gi">+    AsyncGenerator,</span>
<span class="gi">+    AsyncIterable,</span>
<span class="gi">+    AsyncIterator,</span>
<span class="gi">+    Awaitable,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Coroutine,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    Iterator,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    TypeVar,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+    overload,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import defer
<span class="w"> </span>from twisted.internet.defer import Deferred, DeferredList, ensureDeferred
<span class="w"> </span>from twisted.internet.task import Cooperator
<span class="w"> </span>from twisted.python import failure
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import IgnoreRequest
<span class="w"> </span>from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed


<span class="gd">-def defer_fail(_failure: Failure) -&gt;Deferred:</span>
<span class="gi">+def defer_fail(_failure: Failure) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Same as twisted.internet.defer.fail but delay calling errback until
<span class="w"> </span>    next reactor loop

<span class="w"> </span>    It delays by 100ms so reactor has a chance to go through readers and writers
<span class="w"> </span>    before attending pending delayed calls, so do not set delay to zero.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    d: Deferred = Deferred()</span>
<span class="gi">+    reactor.callLater(0.1, d.errback, _failure)</span>
<span class="gi">+    return d</span>


<span class="gd">-def defer_succeed(result: Any) -&gt;Deferred:</span>
<span class="gi">+def defer_succeed(result: Any) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Same as twisted.internet.defer.succeed but delay calling callback until
<span class="w"> </span>    next reactor loop

<span class="w"> </span>    It delays by 100ms so reactor has a chance to go through readers and writers
<span class="w"> </span>    before attending pending delayed calls, so do not set delay to zero.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    d: Deferred = Deferred()</span>
<span class="gi">+    reactor.callLater(0.1, d.callback, result)</span>
<span class="gi">+    return d</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def defer_result(result: Any) -&gt; Deferred:</span>
<span class="gi">+    if isinstance(result, Deferred):</span>
<span class="gi">+        return result</span>
<span class="gi">+    if isinstance(result, failure.Failure):</span>
<span class="gi">+        return defer_fail(result)</span>
<span class="gi">+    return defer_succeed(result)</span>


<span class="gd">-def mustbe_deferred(f: Callable, *args: Any, **kw: Any) -&gt;Deferred:</span>
<span class="gi">+def mustbe_deferred(f: Callable, *args: Any, **kw: Any) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Same as twisted.internet.defer.maybeDeferred, but delay calling
<span class="w"> </span>    callback/errback to next reactor loop
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def parallel(iterable: Iterable, count: int, callable: Callable, *args: Any,</span>
<span class="gd">-    **named: Any) -&gt;Deferred:</span>
<span class="gi">+    try:</span>
<span class="gi">+        result = f(*args, **kw)</span>
<span class="gi">+    # FIXME: Hack to avoid introspecting tracebacks. This to speed up</span>
<span class="gi">+    # processing of IgnoreRequest errors which are, by far, the most common</span>
<span class="gi">+    # exception in Scrapy - see #125</span>
<span class="gi">+    except IgnoreRequest as e:</span>
<span class="gi">+        return defer_fail(failure.Failure(e))</span>
<span class="gi">+    except Exception:</span>
<span class="gi">+        return defer_fail(failure.Failure())</span>
<span class="gi">+    else:</span>
<span class="gi">+        return defer_result(result)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def parallel(</span>
<span class="gi">+    iterable: Iterable, count: int, callable: Callable, *args: Any, **named: Any</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Execute a callable over the objects in the given iterable, in parallel,
<span class="w"> </span>    using no more than ``count`` concurrent calls.

<span class="w"> </span>    Taken from: https://jcalderone.livejournal.com/24285.html
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    coop = Cooperator()</span>
<span class="gi">+    work = (callable(elem, *args, **named) for elem in iterable)</span>
<span class="gi">+    return DeferredList([coop.coiterate(work) for _ in range(count)])</span>


<span class="w"> </span>class _AsyncCooperatorAdapter(Iterator):
<span class="gu">@@ -99,8 +149,13 @@ class _AsyncCooperatorAdapter(Iterator):</span>
<span class="w"> </span>    goal.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, aiterable: AsyncIterable, callable: Callable, *</span>
<span class="gd">-        callable_args: Any, **callable_kwargs: Any):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        aiterable: AsyncIterable,</span>
<span class="gi">+        callable: Callable,</span>
<span class="gi">+        *callable_args: Any,</span>
<span class="gi">+        **callable_kwargs: Any,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.aiterator: AsyncIterator = aiterable.__aiter__()
<span class="w"> </span>        self.callable: Callable = callable
<span class="w"> </span>        self.callable_args: Tuple[Any, ...] = callable_args
<span class="gu">@@ -109,7 +164,38 @@ class _AsyncCooperatorAdapter(Iterator):</span>
<span class="w"> </span>        self.waiting_deferreds: List[Deferred] = []
<span class="w"> </span>        self.anext_deferred: Optional[Deferred] = None

<span class="gd">-    def __next__(self) -&gt;Deferred:</span>
<span class="gi">+    def _callback(self, result: Any) -&gt; None:</span>
<span class="gi">+        # This gets called when the result from aiterator.__anext__() is available.</span>
<span class="gi">+        # It calls the callable on it and sends the result to the oldest waiting Deferred</span>
<span class="gi">+        # (by chaining if the result is a Deferred too or by firing if not).</span>
<span class="gi">+        self.anext_deferred = None</span>
<span class="gi">+        result = self.callable(result, *self.callable_args, **self.callable_kwargs)</span>
<span class="gi">+        d = self.waiting_deferreds.pop(0)</span>
<span class="gi">+        if isinstance(result, Deferred):</span>
<span class="gi">+            result.chainDeferred(d)</span>
<span class="gi">+        else:</span>
<span class="gi">+            d.callback(None)</span>
<span class="gi">+        if self.waiting_deferreds:</span>
<span class="gi">+            self._call_anext()</span>
<span class="gi">+</span>
<span class="gi">+    def _errback(self, failure: Failure) -&gt; None:</span>
<span class="gi">+        # This gets called on any exceptions in aiterator.__anext__().</span>
<span class="gi">+        # It handles StopAsyncIteration by stopping the iteration and reraises all others.</span>
<span class="gi">+        self.anext_deferred = None</span>
<span class="gi">+        failure.trap(StopAsyncIteration)</span>
<span class="gi">+        self.finished = True</span>
<span class="gi">+        for d in self.waiting_deferreds:</span>
<span class="gi">+            d.callback(None)</span>
<span class="gi">+</span>
<span class="gi">+    def _call_anext(self) -&gt; None:</span>
<span class="gi">+        # This starts waiting for the next result from aiterator.</span>
<span class="gi">+        # If aiterator is exhausted, _errback will be called.</span>
<span class="gi">+        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())</span>
<span class="gi">+        self.anext_deferred.addCallbacks(self._callback, self._errback)</span>
<span class="gi">+</span>
<span class="gi">+    def __next__(self) -&gt; Deferred:</span>
<span class="gi">+        # This puts a new Deferred into self.waiting_deferreds and returns it.</span>
<span class="gi">+        # It also calls __anext__() if needed.</span>
<span class="w"> </span>        if self.finished:
<span class="w"> </span>            raise StopIteration
<span class="w"> </span>        d: Deferred = Deferred()
<span class="gu">@@ -119,72 +205,160 @@ class _AsyncCooperatorAdapter(Iterator):</span>
<span class="w"> </span>        return d


<span class="gd">-def parallel_async(async_iterable: AsyncIterable, count: int, callable:</span>
<span class="gd">-    Callable, *args: Any, **named: Any) -&gt;Deferred:</span>
<span class="gi">+def parallel_async(</span>
<span class="gi">+    async_iterable: AsyncIterable,</span>
<span class="gi">+    count: int,</span>
<span class="gi">+    callable: Callable,</span>
<span class="gi">+    *args: Any,</span>
<span class="gi">+    **named: Any,</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Like parallel but for async iterators&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    coop = Cooperator()</span>
<span class="gi">+    work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)</span>
<span class="gi">+    dl: Deferred = DeferredList([coop.coiterate(work) for _ in range(count)])</span>
<span class="gi">+    return dl</span>


<span class="gd">-def process_chain(callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any</span>
<span class="gd">-    ) -&gt;Deferred:</span>
<span class="gi">+def process_chain(</span>
<span class="gi">+    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a Deferred built by chaining the given callbacks&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[</span>
<span class="gd">-    Callable], input: Any, *a: Any, **kw: Any) -&gt;Deferred:</span>
<span class="gi">+    d: Deferred = Deferred()</span>
<span class="gi">+    for x in callbacks:</span>
<span class="gi">+        d.addCallback(x, *a, **kw)</span>
<span class="gi">+    d.callback(input)</span>
<span class="gi">+    return d</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def process_chain_both(</span>
<span class="gi">+    callbacks: Iterable[Callable],</span>
<span class="gi">+    errbacks: Iterable[Callable],</span>
<span class="gi">+    input: Any,</span>
<span class="gi">+    *a: Any,</span>
<span class="gi">+    **kw: Any,</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a Deferred built by chaining the given callbacks and errbacks&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def process_parallel(callbacks: Iterable[Callable], input: Any, *a: Any, **</span>
<span class="gd">-    kw: Any) -&gt;Deferred:</span>
<span class="gi">+    d: Deferred = Deferred()</span>
<span class="gi">+    for cb, eb in zip(callbacks, errbacks):</span>
<span class="gi">+        d.addCallbacks(</span>
<span class="gi">+            callback=cb,</span>
<span class="gi">+            errback=eb,</span>
<span class="gi">+            callbackArgs=a,</span>
<span class="gi">+            callbackKeywords=kw,</span>
<span class="gi">+            errbackArgs=a,</span>
<span class="gi">+            errbackKeywords=kw,</span>
<span class="gi">+        )</span>
<span class="gi">+    if isinstance(input, failure.Failure):</span>
<span class="gi">+        d.errback(input)</span>
<span class="gi">+    else:</span>
<span class="gi">+        d.callback(input)</span>
<span class="gi">+    return d</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def process_parallel(</span>
<span class="gi">+    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a Deferred with the output of all successful calls to the given
<span class="w"> </span>    callbacks
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]</span>
<span class="gi">+    d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)</span>
<span class="gi">+    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)</span>
<span class="gi">+    return d</span>


<span class="gd">-def iter_errback(iterable: Iterable, errback: Callable, *a: Any, **kw: Any</span>
<span class="gd">-    ) -&gt;Generator:</span>
<span class="gi">+def iter_errback(</span>
<span class="gi">+    iterable: Iterable, errback: Callable, *a: Any, **kw: Any</span>
<span class="gi">+) -&gt; Generator:</span>
<span class="w"> </span>    &quot;&quot;&quot;Wraps an iterable calling an errback if an error is caught while
<span class="w"> </span>    iterating it.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-async def aiter_errback(aiterable: AsyncIterable, errback: Callable, *a:</span>
<span class="gd">-    Any, **kw: Any) -&gt;AsyncGenerator:</span>
<span class="gi">+    it = iter(iterable)</span>
<span class="gi">+    while True:</span>
<span class="gi">+        try:</span>
<span class="gi">+            yield next(it)</span>
<span class="gi">+        except StopIteration:</span>
<span class="gi">+            break</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            errback(failure.Failure(), *a, **kw)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+async def aiter_errback(</span>
<span class="gi">+    aiterable: AsyncIterable, errback: Callable, *a: Any, **kw: Any</span>
<span class="gi">+) -&gt; AsyncGenerator:</span>
<span class="w"> </span>    &quot;&quot;&quot;Wraps an async iterable calling an errback if an error is caught while
<span class="w"> </span>    iterating it. Similar to scrapy.utils.defer.iter_errback()
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    it = aiterable.__aiter__()</span>
<span class="gi">+    while True:</span>
<span class="gi">+        try:</span>
<span class="gi">+            yield await it.__anext__()</span>
<span class="gi">+        except StopAsyncIteration:</span>
<span class="gi">+            break</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            errback(failure.Failure(), *a, **kw)</span>


<span class="gd">-_CT = TypeVar(&#39;_CT&#39;, bound=Union[Awaitable, CoroutineType, Future])</span>
<span class="gd">-_T = TypeVar(&#39;_T&#39;)</span>
<span class="gi">+_CT = TypeVar(&quot;_CT&quot;, bound=Union[Awaitable, CoroutineType, Future])</span>
<span class="gi">+_T = TypeVar(&quot;_T&quot;)</span>


<span class="gd">-def deferred_from_coro(o: _T) -&gt;Union[Deferred, _T]:</span>
<span class="gd">-    &quot;&quot;&quot;Converts a coroutine into a Deferred, or returns the object as is if it isn&#39;t a coroutine&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+@overload</span>
<span class="gi">+def deferred_from_coro(o: _CT) -&gt; Deferred:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>

<span class="gi">+@overload</span>
<span class="gi">+def deferred_from_coro(o: _T) -&gt; _T:</span>
<span class="gi">+    ...</span>

<span class="gd">-def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -&gt;Callable:</span>
<span class="gi">+</span>
<span class="gi">+def deferred_from_coro(o: _T) -&gt; Union[Deferred, _T]:</span>
<span class="gi">+    &quot;&quot;&quot;Converts a coroutine into a Deferred, or returns the object as is if it isn&#39;t a coroutine&quot;&quot;&quot;</span>
<span class="gi">+    if isinstance(o, Deferred):</span>
<span class="gi">+        return o</span>
<span class="gi">+    if asyncio.isfuture(o) or inspect.isawaitable(o):</span>
<span class="gi">+        if not is_asyncio_reactor_installed():</span>
<span class="gi">+            # wrapping the coroutine directly into a Deferred, this doesn&#39;t work correctly with coroutines</span>
<span class="gi">+            # that use asyncio, e.g. &quot;await asyncio.sleep(1)&quot;</span>
<span class="gi">+            return ensureDeferred(cast(Coroutine[Deferred, Any, Any], o))</span>
<span class="gi">+        # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor</span>
<span class="gi">+        event_loop = _get_asyncio_event_loop()</span>
<span class="gi">+        return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))</span>
<span class="gi">+    return o</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -&gt; Callable:</span>
<span class="w"> </span>    &quot;&quot;&quot;Converts a coroutine function into a function that returns a Deferred.

<span class="w"> </span>    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.
<span class="w"> </span>    This is useful for callback chains, as callback functions are called with the previous callback result.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    @wraps(coro_f)</span>
<span class="gi">+    def f(*coro_args: Any, **coro_kwargs: Any) -&gt; Any:</span>
<span class="gi">+        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))</span>
<span class="gi">+</span>
<span class="gi">+    return f</span>

<span class="gd">-def maybeDeferred_coro(f: Callable, *args: Any, **kw: Any) -&gt;Deferred:</span>
<span class="gi">+</span>
<span class="gi">+def maybeDeferred_coro(f: Callable, *args: Any, **kw: Any) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Copy of defer.maybeDeferred that also converts coroutines to Deferreds.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        result = f(*args, **kw)</span>
<span class="gi">+    except:  # noqa: E722</span>
<span class="gi">+        return defer.fail(failure.Failure(captureVars=Deferred.debug))</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(result, Deferred):</span>
<span class="gi">+        return result</span>
<span class="gi">+    if asyncio.isfuture(result) or inspect.isawaitable(result):</span>
<span class="gi">+        return deferred_from_coro(result)</span>
<span class="gi">+    if isinstance(result, failure.Failure):</span>
<span class="gi">+        return defer.fail(result)</span>
<span class="gi">+    return defer.succeed(result)</span>


<span class="gd">-def deferred_to_future(d: Deferred) -&gt;Future:</span>
<span class="gi">+def deferred_to_future(d: Deferred) -&gt; Future:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    .. versionadded:: 2.6.0

<span class="gu">@@ -203,10 +377,10 @@ def deferred_to_future(d: Deferred) -&gt;Future:</span>
<span class="w"> </span>                deferred = self.crawler.engine.download(additional_request)
<span class="w"> </span>                additional_response = await deferred_to_future(deferred)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return d.asFuture(_get_asyncio_event_loop())</span>


<span class="gd">-def maybe_deferred_to_future(d: Deferred) -&gt;Union[Deferred, Future]:</span>
<span class="gi">+def maybe_deferred_to_future(d: Deferred) -&gt; Union[Deferred, Future]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    .. versionadded:: 2.6.0

<span class="gu">@@ -232,4 +406,6 @@ def maybe_deferred_to_future(d: Deferred) -&gt;Union[Deferred, Future]:</span>
<span class="w"> </span>                deferred = self.crawler.engine.download(additional_request)
<span class="w"> </span>                additional_response = await maybe_deferred_to_future(deferred)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not is_asyncio_reactor_installed():</span>
<span class="gi">+        return d</span>
<span class="gi">+    return deferred_to_future(d)</span>
<span class="gh">diff --git a/scrapy/utils/deprecate.py b/scrapy/utils/deprecate.py</span>
<span class="gh">index 42dcda1fa..ea577c44a 100644</span>
<span class="gd">--- a/scrapy/utils/deprecate.py</span>
<span class="gi">+++ b/scrapy/utils/deprecate.py</span>
<span class="gu">@@ -1,18 +1,33 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Some helpers for deprecation messages&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import inspect
<span class="w"> </span>import warnings
<span class="w"> </span>from typing import Any, Dict, List, Optional, Tuple, Type, overload
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning


<span class="gd">-def create_deprecated_class(name: str, new_class: type, clsdict: Optional[</span>
<span class="gd">-    Dict[str, Any]]=None, warn_category: Type[Warning]=</span>
<span class="gd">-    ScrapyDeprecationWarning, warn_once: bool=True, old_class_path:</span>
<span class="gd">-    Optional[str]=None, new_class_path: Optional[str]=None,</span>
<span class="gd">-    subclass_warn_message: str=</span>
<span class="gd">-    &#39;{cls} inherits from deprecated class {old}, please inherit from {new}.&#39;,</span>
<span class="gd">-    instance_warn_message: str=</span>
<span class="gd">-    &#39;{cls} is deprecated, instantiate {new} instead.&#39;) -&gt;type:</span>
<span class="gi">+def attribute(obj: Any, oldattr: str, newattr: str, version: str = &quot;0.12&quot;) -&gt; None:</span>
<span class="gi">+    cname = obj.__class__.__name__</span>
<span class="gi">+    warnings.warn(</span>
<span class="gi">+        f&quot;{cname}.{oldattr} attribute is deprecated and will be no longer supported &quot;</span>
<span class="gi">+        f&quot;in Scrapy {version}, use {cname}.{newattr} attribute instead&quot;,</span>
<span class="gi">+        ScrapyDeprecationWarning,</span>
<span class="gi">+        stacklevel=3,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def create_deprecated_class(</span>
<span class="gi">+    name: str,</span>
<span class="gi">+    new_class: type,</span>
<span class="gi">+    clsdict: Optional[Dict[str, Any]] = None,</span>
<span class="gi">+    warn_category: Type[Warning] = ScrapyDeprecationWarning,</span>
<span class="gi">+    warn_once: bool = True,</span>
<span class="gi">+    old_class_path: Optional[str] = None,</span>
<span class="gi">+    new_class_path: Optional[str] = None,</span>
<span class="gi">+    subclass_warn_message: str = &quot;{cls} inherits from deprecated class {old}, please inherit from {new}.&quot;,</span>
<span class="gi">+    instance_warn_message: str = &quot;{cls} is deprecated, instantiate {new} instead.&quot;,</span>
<span class="gi">+) -&gt; type:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return a &quot;deprecated&quot; class that causes its subclasses to issue a warning.
<span class="w"> </span>    Subclasses of ``new_class`` are considered subclasses of this class.
<span class="gu">@@ -37,19 +52,115 @@ def create_deprecated_class(name: str, new_class: type, clsdict: Optional[</span>
<span class="w"> </span>    checks they&#39;ll still return True if sub is a subclass of NewName instead of
<span class="w"> </span>    OldName.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    # https://github.com/python/mypy/issues/4177</span>
<span class="gi">+    class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]</span>
<span class="gi">+        deprecated_class: Optional[type] = None</span>
<span class="gi">+        warned_on_subclass: bool = False</span>
<span class="gi">+</span>
<span class="gi">+        def __new__(</span>
<span class="gi">+            metacls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]</span>
<span class="gi">+        ) -&gt; type:</span>
<span class="gi">+            cls = super().__new__(metacls, name, bases, clsdict_)</span>
<span class="gi">+            if metacls.deprecated_class is None:</span>
<span class="gi">+                metacls.deprecated_class = cls</span>
<span class="gi">+            return cls</span>
<span class="gi">+</span>
<span class="gi">+        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]):</span>
<span class="gi">+            meta = cls.__class__</span>
<span class="gi">+            old = meta.deprecated_class</span>
<span class="gi">+            if old in bases and not (warn_once and meta.warned_on_subclass):</span>
<span class="gi">+                meta.warned_on_subclass = True</span>
<span class="gi">+                msg = subclass_warn_message.format(</span>
<span class="gi">+                    cls=_clspath(cls),</span>
<span class="gi">+                    old=_clspath(old, old_class_path),</span>
<span class="gi">+                    new=_clspath(new_class, new_class_path),</span>
<span class="gi">+                )</span>
<span class="gi">+                if warn_once:</span>
<span class="gi">+                    msg += &quot; (warning only on first subclass, there may be others)&quot;</span>
<span class="gi">+                warnings.warn(msg, warn_category, stacklevel=2)</span>
<span class="gi">+            super().__init__(name, bases, clsdict_)</span>
<span class="gi">+</span>
<span class="gi">+        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass</span>
<span class="gi">+        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks</span>
<span class="gi">+        # for implementation details</span>
<span class="gi">+        def __instancecheck__(cls, inst: Any) -&gt; bool:</span>
<span class="gi">+            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))</span>
<span class="gi">+</span>
<span class="gi">+        def __subclasscheck__(cls, sub: type) -&gt; bool:</span>
<span class="gi">+            if cls is not DeprecatedClass.deprecated_class:</span>
<span class="gi">+                # we should do the magic only if second `issubclass` argument</span>
<span class="gi">+                # is the deprecated class itself - subclasses of the</span>
<span class="gi">+                # deprecated class should not use custom `__subclasscheck__`</span>
<span class="gi">+                # method.</span>
<span class="gi">+                return super().__subclasscheck__(sub)</span>
<span class="gi">+</span>
<span class="gi">+            if not inspect.isclass(sub):</span>
<span class="gi">+                raise TypeError(&quot;issubclass() arg 1 must be a class&quot;)</span>
<span class="gi">+</span>
<span class="gi">+            mro = getattr(sub, &quot;__mro__&quot;, ())</span>
<span class="gi">+            return any(c in {cls, new_class} for c in mro)</span>
<span class="gi">+</span>
<span class="gi">+        def __call__(cls, *args: Any, **kwargs: Any) -&gt; Any:</span>
<span class="gi">+            old = DeprecatedClass.deprecated_class</span>
<span class="gi">+            if cls is old:</span>
<span class="gi">+                msg = instance_warn_message.format(</span>
<span class="gi">+                    cls=_clspath(cls, old_class_path),</span>
<span class="gi">+                    new=_clspath(new_class, new_class_path),</span>
<span class="gi">+                )</span>
<span class="gi">+                warnings.warn(msg, warn_category, stacklevel=2)</span>
<span class="gi">+            return super().__call__(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        frm = inspect.stack()[1]</span>
<span class="gi">+        parent_module = inspect.getmodule(frm[0])</span>
<span class="gi">+        if parent_module is not None:</span>
<span class="gi">+            deprecated_cls.__module__ = parent_module.__name__</span>
<span class="gi">+    except Exception as e:</span>
<span class="gi">+        # Sometimes inspect.stack() fails (e.g. when the first import of</span>
<span class="gi">+        # deprecated class is in jinja2 template). __module__ attribute is not</span>
<span class="gi">+        # important enough to raise an exception as users may be unable</span>
<span class="gi">+        # to fix inspect.stack() errors.</span>
<span class="gi">+        warnings.warn(f&quot;Error detecting parent module: {e!r}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return deprecated_cls</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _clspath(cls: type, forced: Optional[str] = None) -&gt; str:</span>
<span class="gi">+    if forced is not None:</span>
<span class="gi">+        return forced</span>
<span class="gi">+    return f&quot;{cls.__module__}.{cls.__name__}&quot;</span>


<span class="w"> </span>DEPRECATION_RULES: List[Tuple[str, str]] = []


<span class="gd">-def update_classpath(path: Any) -&gt;Any:</span>
<span class="gi">+@overload</span>
<span class="gi">+def update_classpath(path: str) -&gt; str:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def update_classpath(path: Any) -&gt; Any:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def update_classpath(path: Any) -&gt; Any:</span>
<span class="w"> </span>    &quot;&quot;&quot;Update a deprecated path from an object with its new location&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for prefix, replacement in DEPRECATION_RULES:</span>
<span class="gi">+        if isinstance(path, str) and path.startswith(prefix):</span>
<span class="gi">+            new_path = path.replace(prefix, replacement, 1)</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;`{path}` class is deprecated, use `{new_path}` instead&quot;,</span>
<span class="gi">+                ScrapyDeprecationWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            return new_path</span>
<span class="gi">+    return path</span>


<span class="gd">-def method_is_overridden(subclass: type, base_class: type, method_name: str</span>
<span class="gd">-    ) -&gt;bool:</span>
<span class="gi">+def method_is_overridden(subclass: type, base_class: type, method_name: str) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return True if a method named ``method_name`` of a ``base_class``
<span class="w"> </span>    is overridden in a ``subclass``.
<span class="gu">@@ -76,4 +187,6 @@ def method_is_overridden(subclass: type, base_class: type, method_name: str</span>
<span class="w"> </span>    &gt;&gt;&gt; method_is_overridden(Sub4, Base, &#39;foo&#39;)
<span class="w"> </span>    True
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    base_method = getattr(base_class, method_name)</span>
<span class="gi">+    sub_method = getattr(subclass, method_name)</span>
<span class="gi">+    return base_method.__code__ is not sub_method.__code__</span>
<span class="gh">diff --git a/scrapy/utils/display.py b/scrapy/utils/display.py</span>
<span class="gh">index 887a1f2c6..596cf89e4 100644</span>
<span class="gd">--- a/scrapy/utils/display.py</span>
<span class="gi">+++ b/scrapy/utils/display.py</span>
<span class="gu">@@ -1,9 +1,51 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>pprint and pformat wrappers with colorization support
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import ctypes
<span class="w"> </span>import platform
<span class="w"> </span>import sys
<span class="w"> </span>from pprint import pformat as pformat_
<span class="w"> </span>from typing import Any
<span class="gi">+</span>
<span class="w"> </span>from packaging.version import Version as parse_version
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _enable_windows_terminal_processing() -&gt; bool:</span>
<span class="gi">+    # https://stackoverflow.com/a/36760881</span>
<span class="gi">+    kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]</span>
<span class="gi">+    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _tty_supports_color() -&gt; bool:</span>
<span class="gi">+    if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    if parse_version(platform.version()) &lt; parse_version(&quot;10.0.14393&quot;):</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    # Windows &gt;= 10.0.14393 interprets ANSI escape sequences providing terminal</span>
<span class="gi">+    # processing is enabled.</span>
<span class="gi">+    return _enable_windows_terminal_processing()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _colorize(text: str, colorize: bool = True) -&gt; str:</span>
<span class="gi">+    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():</span>
<span class="gi">+        return text</span>
<span class="gi">+    try:</span>
<span class="gi">+        from pygments import highlight</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        return text</span>
<span class="gi">+    else:</span>
<span class="gi">+        from pygments.formatters import TerminalFormatter</span>
<span class="gi">+        from pygments.lexers import PythonLexer</span>
<span class="gi">+</span>
<span class="gi">+        return highlight(text, PythonLexer(), TerminalFormatter())</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def pformat(obj: Any, *args: Any, **kwargs: Any) -&gt; str:</span>
<span class="gi">+    return _colorize(pformat_(obj), kwargs.pop(&quot;colorize&quot;, True))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def pprint(obj: Any, *args: Any, **kwargs: Any) -&gt; None:</span>
<span class="gi">+    print(pformat(obj, *args, **kwargs))</span>
<span class="gh">diff --git a/scrapy/utils/engine.py b/scrapy/utils/engine.py</span>
<span class="gh">index 2861f1a69..a5f2a8c6e 100644</span>
<span class="gd">--- a/scrapy/utils/engine.py</span>
<span class="gi">+++ b/scrapy/utils/engine.py</span>
<span class="gu">@@ -1,10 +1,51 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Some debugging functions for working with the Scrapy engine&quot;&quot;&quot;
<span class="gd">-from time import time</span>
<span class="gi">+</span>
<span class="gi">+# used in global tests code</span>
<span class="gi">+from time import time  # noqa: F401</span>
<span class="w"> </span>from typing import TYPE_CHECKING, Any, List, Tuple
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.core.engine import ExecutionEngine


<span class="gd">-def get_engine_status(engine: &#39;ExecutionEngine&#39;) -&gt;List[Tuple[str, Any]]:</span>
<span class="gi">+def get_engine_status(engine: &quot;ExecutionEngine&quot;) -&gt; List[Tuple[str, Any]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a report of the current engine status&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    tests = [</span>
<span class="gi">+        &quot;time()-engine.start_time&quot;,</span>
<span class="gi">+        &quot;len(engine.downloader.active)&quot;,</span>
<span class="gi">+        &quot;engine.scraper.is_idle()&quot;,</span>
<span class="gi">+        &quot;engine.spider.name&quot;,</span>
<span class="gi">+        &quot;engine.spider_is_idle()&quot;,</span>
<span class="gi">+        &quot;engine.slot.closing&quot;,</span>
<span class="gi">+        &quot;len(engine.slot.inprogress)&quot;,</span>
<span class="gi">+        &quot;len(engine.slot.scheduler.dqs or [])&quot;,</span>
<span class="gi">+        &quot;len(engine.slot.scheduler.mqs)&quot;,</span>
<span class="gi">+        &quot;len(engine.scraper.slot.queue)&quot;,</span>
<span class="gi">+        &quot;len(engine.scraper.slot.active)&quot;,</span>
<span class="gi">+        &quot;engine.scraper.slot.active_size&quot;,</span>
<span class="gi">+        &quot;engine.scraper.slot.itemproc_size&quot;,</span>
<span class="gi">+        &quot;engine.scraper.slot.needs_backout()&quot;,</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    checks: List[Tuple[str, Any]] = []</span>
<span class="gi">+    for test in tests:</span>
<span class="gi">+        try:</span>
<span class="gi">+            checks += [(test, eval(test))]</span>
<span class="gi">+        except Exception as e:</span>
<span class="gi">+            checks += [(test, f&quot;{type(e).__name__} (exception)&quot;)]</span>
<span class="gi">+</span>
<span class="gi">+    return checks</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def format_engine_status(engine: &quot;ExecutionEngine&quot;) -&gt; str:</span>
<span class="gi">+    checks = get_engine_status(engine)</span>
<span class="gi">+    s = &quot;Execution engine status\n\n&quot;</span>
<span class="gi">+    for test, result in checks:</span>
<span class="gi">+        s += f&quot;{test:&lt;47} : {result}\n&quot;</span>
<span class="gi">+    s += &quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    return s</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def print_engine_status(engine: &quot;ExecutionEngine&quot;) -&gt; None:</span>
<span class="gi">+    print(format_engine_status(engine))</span>
<span class="gh">diff --git a/scrapy/utils/ftp.py b/scrapy/utils/ftp.py</span>
<span class="gh">index 6acf0ecb9..c77681a53 100644</span>
<span class="gd">--- a/scrapy/utils/ftp.py</span>
<span class="gi">+++ b/scrapy/utils/ftp.py</span>
<span class="gu">@@ -4,18 +4,42 @@ from posixpath import dirname</span>
<span class="w"> </span>from typing import IO


<span class="gd">-def ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool=True) -&gt;None:</span>
<span class="gi">+def ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Set the current directory of the FTP connection given in the ``ftp``
<span class="w"> </span>    argument (as a ftplib.FTP object), creating all parent directories if they
<span class="w"> </span>    don&#39;t exist. The ftplib.FTP object must be already connected and logged in.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        ftp.cwd(path)</span>
<span class="gi">+    except error_perm:</span>
<span class="gi">+        ftp_makedirs_cwd(ftp, dirname(path), False)</span>
<span class="gi">+        ftp.mkd(path)</span>
<span class="gi">+        if first_call:</span>
<span class="gi">+            ftp.cwd(path)</span>


<span class="gd">-def ftp_store_file(*, path: str, file: IO, host: str, port: int, username:</span>
<span class="gd">-    str, password: str, use_active_mode: bool=False, overwrite: bool=True</span>
<span class="gd">-    ) -&gt;None:</span>
<span class="gi">+def ftp_store_file(</span>
<span class="gi">+    *,</span>
<span class="gi">+    path: str,</span>
<span class="gi">+    file: IO,</span>
<span class="gi">+    host: str,</span>
<span class="gi">+    port: int,</span>
<span class="gi">+    username: str,</span>
<span class="gi">+    password: str,</span>
<span class="gi">+    use_active_mode: bool = False,</span>
<span class="gi">+    overwrite: bool = True,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Opens a FTP connection with passed credentials,sets current directory
<span class="w"> </span>    to the directory extracted from given path, then uploads the file to server
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with FTP() as ftp:</span>
<span class="gi">+        ftp.connect(host, port)</span>
<span class="gi">+        ftp.login(username, password)</span>
<span class="gi">+        if use_active_mode:</span>
<span class="gi">+            ftp.set_pasv(False)</span>
<span class="gi">+        file.seek(0)</span>
<span class="gi">+        dirname, filename = posixpath.split(path)</span>
<span class="gi">+        ftp_makedirs_cwd(ftp, dirname)</span>
<span class="gi">+        command = &quot;STOR&quot; if overwrite else &quot;APPE&quot;</span>
<span class="gi">+        ftp.storbinary(f&quot;{command} {filename}&quot;, file)</span>
<span class="gi">+        file.close()</span>
<span class="gh">diff --git a/scrapy/utils/gz.py b/scrapy/utils/gz.py</span>
<span class="gh">index 69751b6b3..2e487d88b 100644</span>
<span class="gd">--- a/scrapy/utils/gz.py</span>
<span class="gi">+++ b/scrapy/utils/gz.py</span>
<span class="gu">@@ -1,13 +1,42 @@</span>
<span class="w"> </span>import struct
<span class="w"> </span>from gzip import GzipFile
<span class="w"> </span>from io import BytesIO
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Response
<span class="gi">+</span>
<span class="w"> </span>from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded


<span class="gd">-def gunzip(data: bytes, *, max_size: int=0) -&gt;bytes:</span>
<span class="gi">+def gunzip(data: bytes, *, max_size: int = 0) -&gt; bytes:</span>
<span class="w"> </span>    &quot;&quot;&quot;Gunzip the given data and return as much data as possible.

<span class="w"> </span>    This is resilient to CRC checksum errors.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    f = GzipFile(fileobj=BytesIO(data))</span>
<span class="gi">+    output_stream = BytesIO()</span>
<span class="gi">+    chunk = b&quot;.&quot;</span>
<span class="gi">+    decompressed_size = 0</span>
<span class="gi">+    while chunk:</span>
<span class="gi">+        try:</span>
<span class="gi">+            chunk = f.read1(_CHUNK_SIZE)</span>
<span class="gi">+        except (OSError, EOFError, struct.error):</span>
<span class="gi">+            # complete only if there is some data, otherwise re-raise</span>
<span class="gi">+            # see issue 87 about catching struct.error</span>
<span class="gi">+            # some pages are quite small so output_stream is empty</span>
<span class="gi">+            if output_stream.getbuffer().nbytes &gt; 0:</span>
<span class="gi">+                break</span>
<span class="gi">+            raise</span>
<span class="gi">+        decompressed_size += len(chunk)</span>
<span class="gi">+        if max_size and decompressed_size &gt; max_size:</span>
<span class="gi">+            raise _DecompressionMaxSizeExceeded(</span>
<span class="gi">+                f&quot;The number of bytes decompressed so far &quot;</span>
<span class="gi">+                f&quot;({decompressed_size} B) exceed the specified maximum &quot;</span>
<span class="gi">+                f&quot;({max_size} B).&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        output_stream.write(chunk)</span>
<span class="gi">+    output_stream.seek(0)</span>
<span class="gi">+    return output_stream.read()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def gzip_magic_number(response: Response) -&gt; bool:</span>
<span class="gi">+    return response.body[:3] == b&quot;\x1f\x8b\x08&quot;</span>
<span class="gh">diff --git a/scrapy/utils/httpobj.py b/scrapy/utils/httpobj.py</span>
<span class="gh">index 8ecd2b938..d502e8910 100644</span>
<span class="gd">--- a/scrapy/utils/httpobj.py</span>
<span class="gi">+++ b/scrapy/utils/httpobj.py</span>
<span class="gu">@@ -1,15 +1,20 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Helper functions for scrapy.http objects (Request, Response)&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from typing import Union
<span class="w"> </span>from urllib.parse import ParseResult, urlparse
<span class="w"> </span>from weakref import WeakKeyDictionary
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request, Response
<span class="gd">-(_urlparse_cache: &#39;WeakKeyDictionary[Union[Request, Response], ParseResult]&#39;</span>
<span class="gd">-    ) = WeakKeyDictionary()</span>
<span class="gi">+</span>
<span class="gi">+_urlparse_cache: &quot;WeakKeyDictionary[Union[Request, Response], ParseResult]&quot; = (</span>
<span class="gi">+    WeakKeyDictionary()</span>
<span class="gi">+)</span>


<span class="gd">-def urlparse_cached(request_or_response: Union[Request, Response]</span>
<span class="gd">-    ) -&gt;ParseResult:</span>
<span class="gi">+def urlparse_cached(request_or_response: Union[Request, Response]) -&gt; ParseResult:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return urlparse.urlparse caching the result, where the argument can be a
<span class="w"> </span>    Request or Response object
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if request_or_response not in _urlparse_cache:</span>
<span class="gi">+        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)</span>
<span class="gi">+    return _urlparse_cache[request_or_response]</span>
<span class="gh">diff --git a/scrapy/utils/iterators.py b/scrapy/utils/iterators.py</span>
<span class="gh">index 8b4ca3eac..db86af2c3 100644</span>
<span class="gd">--- a/scrapy/utils/iterators.py</span>
<span class="gi">+++ b/scrapy/utils/iterators.py</span>
<span class="gu">@@ -2,20 +2,38 @@ import csv</span>
<span class="w"> </span>import logging
<span class="w"> </span>import re
<span class="w"> </span>from io import StringIO
<span class="gd">-from typing import TYPE_CHECKING, Any, Callable, Dict, Generator, Iterable, List, Literal, Optional, Union, cast, overload</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Literal,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+    overload,</span>
<span class="gi">+)</span>
<span class="w"> </span>from warnings import warn
<span class="gd">-from lxml import etree</span>
<span class="gi">+</span>
<span class="gi">+from lxml import etree  # nosec</span>
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.http import Response, TextResponse
<span class="w"> </span>from scrapy.selector import Selector
<span class="w"> </span>from scrapy.utils.python import re_rsearch, to_unicode
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gd">-    from lxml._types import SupportsReadClose</span>
<span class="gi">+    from lxml._types import SupportsReadClose  # nosec</span>
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gd">-def xmliter(obj: Union[Response, str, bytes], nodename: str) -&gt;Generator[</span>
<span class="gd">-    Selector, Any, None]:</span>
<span class="gi">+def xmliter(</span>
<span class="gi">+    obj: Union[Response, str, bytes], nodename: str</span>
<span class="gi">+) -&gt; Generator[Selector, Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a iterator of Selector&#39;s over all nodes of a XML document,
<span class="w"> </span>       given the name of the node to iterate. Useful for parsing XML feeds.

<span class="gu">@@ -24,27 +42,137 @@ def xmliter(obj: Union[Response, str, bytes], nodename: str) -&gt;Generator[</span>
<span class="w"> </span>    - a unicode string
<span class="w"> </span>    - a string encoded as utf-8
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    warn(</span>
<span class="gi">+        (</span>
<span class="gi">+            &quot;xmliter is deprecated and its use strongly discouraged because &quot;</span>
<span class="gi">+            &quot;it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See &quot;</span>
<span class="gi">+            &quot;https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9&quot;</span>
<span class="gi">+        ),</span>
<span class="gi">+        ScrapyDeprecationWarning,</span>
<span class="gi">+        stacklevel=2,</span>
<span class="gi">+    )</span>

<span class="gi">+    nodename_patt = re.escape(nodename)</span>
<span class="gi">+</span>
<span class="gi">+    DOCUMENT_HEADER_RE = re.compile(r&quot;&lt;\?xml[^&gt;]+&gt;\s*&quot;, re.S)</span>
<span class="gi">+    HEADER_END_RE = re.compile(rf&quot;&lt;\s*/{nodename_patt}\s*&gt;&quot;, re.S)</span>
<span class="gi">+    END_TAG_RE = re.compile(r&quot;&lt;\s*/([^\s&gt;]+)\s*&gt;&quot;, re.S)</span>
<span class="gi">+    NAMESPACE_RE = re.compile(r&quot;((xmlns[:A-Za-z]*)=[^&gt;\s]+)&quot;, re.S)</span>
<span class="gi">+    text = _body_or_str(obj)</span>
<span class="gi">+</span>
<span class="gi">+    document_header_match = re.search(DOCUMENT_HEADER_RE, text)</span>
<span class="gi">+    document_header = (</span>
<span class="gi">+        document_header_match.group().strip() if document_header_match else &quot;&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    header_end_idx = re_rsearch(HEADER_END_RE, text)</span>
<span class="gi">+    header_end = text[header_end_idx[1] :].strip() if header_end_idx else &quot;&quot;</span>
<span class="gi">+    namespaces: Dict[str, str] = {}</span>
<span class="gi">+    if header_end:</span>
<span class="gi">+        for tagname in reversed(re.findall(END_TAG_RE, header_end)):</span>
<span class="gi">+            assert header_end_idx</span>
<span class="gi">+            tag = re.search(</span>
<span class="gi">+                rf&quot;&lt;\s*{tagname}.*?xmlns[:=][^&gt;]*&gt;&quot;, text[: header_end_idx[1]], re.S</span>
<span class="gi">+            )</span>
<span class="gi">+            if tag:</span>
<span class="gi">+                for x in re.findall(NAMESPACE_RE, tag.group()):</span>
<span class="gi">+                    namespaces[x[1]] = x[0]</span>
<span class="gi">+</span>
<span class="gi">+    r = re.compile(rf&quot;&lt;{nodename_patt}[\s&gt;].*?&lt;/{nodename_patt}&gt;&quot;, re.DOTALL)</span>
<span class="gi">+    for match in r.finditer(text):</span>
<span class="gi">+        nodetext = (</span>
<span class="gi">+            document_header</span>
<span class="gi">+            + match.group().replace(</span>
<span class="gi">+                nodename, f&#39;{nodename} {&quot; &quot;.join(namespaces.values())}&#39;, 1</span>
<span class="gi">+            )</span>
<span class="gi">+            + header_end</span>
<span class="gi">+        )</span>
<span class="gi">+        yield Selector(text=nodetext, type=&quot;xml&quot;)</span>

<span class="gd">-class _StreamReader:</span>

<span class="gi">+def xmliter_lxml(</span>
<span class="gi">+    obj: Union[Response, str, bytes],</span>
<span class="gi">+    nodename: str,</span>
<span class="gi">+    namespace: Optional[str] = None,</span>
<span class="gi">+    prefix: str = &quot;x&quot;,</span>
<span class="gi">+) -&gt; Generator[Selector, Any, None]:</span>
<span class="gi">+    reader = _StreamReader(obj)</span>
<span class="gi">+    tag = f&quot;{{{namespace}}}{nodename}&quot; if namespace else nodename</span>
<span class="gi">+    iterable = etree.iterparse(</span>
<span class="gi">+        cast(&quot;SupportsReadClose[bytes]&quot;, reader),</span>
<span class="gi">+        encoding=reader.encoding,</span>
<span class="gi">+        events=(&quot;end&quot;, &quot;start-ns&quot;),</span>
<span class="gi">+        resolve_entities=False,</span>
<span class="gi">+        huge_tree=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    selxpath = &quot;//&quot; + (f&quot;{prefix}:{nodename}&quot; if namespace else nodename)</span>
<span class="gi">+    needs_namespace_resolution = not namespace and &quot;:&quot; in nodename</span>
<span class="gi">+    if needs_namespace_resolution:</span>
<span class="gi">+        prefix, nodename = nodename.split(&quot;:&quot;, maxsplit=1)</span>
<span class="gi">+    for event, data in iterable:</span>
<span class="gi">+        if event == &quot;start-ns&quot;:</span>
<span class="gi">+            assert isinstance(data, tuple)</span>
<span class="gi">+            if needs_namespace_resolution:</span>
<span class="gi">+                _prefix, _namespace = data</span>
<span class="gi">+                if _prefix != prefix:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                namespace = _namespace</span>
<span class="gi">+                needs_namespace_resolution = False</span>
<span class="gi">+                selxpath = f&quot;//{prefix}:{nodename}&quot;</span>
<span class="gi">+                tag = f&quot;{{{namespace}}}{nodename}&quot;</span>
<span class="gi">+            continue</span>
<span class="gi">+        assert isinstance(data, etree._Element)</span>
<span class="gi">+        node = data</span>
<span class="gi">+        if node.tag != tag:</span>
<span class="gi">+            continue</span>
<span class="gi">+        nodetext = etree.tostring(node, encoding=&quot;unicode&quot;)</span>
<span class="gi">+        node.clear()</span>
<span class="gi">+        xs = Selector(text=nodetext, type=&quot;xml&quot;)</span>
<span class="gi">+        if namespace:</span>
<span class="gi">+            xs.register_namespace(prefix, namespace)</span>
<span class="gi">+        yield xs.xpath(selxpath)[0]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class _StreamReader:</span>
<span class="w"> </span>    def __init__(self, obj: Union[Response, str, bytes]):
<span class="w"> </span>        self._ptr: int = 0
<span class="w"> </span>        self._text: Union[str, bytes]
<span class="w"> </span>        if isinstance(obj, TextResponse):
<span class="w"> </span>            self._text, self.encoding = obj.body, obj.encoding
<span class="w"> </span>        elif isinstance(obj, Response):
<span class="gd">-            self._text, self.encoding = obj.body, &#39;utf-8&#39;</span>
<span class="gi">+            self._text, self.encoding = obj.body, &quot;utf-8&quot;</span>
<span class="w"> </span>        else:
<span class="gd">-            self._text, self.encoding = obj, &#39;utf-8&#39;</span>
<span class="gi">+            self._text, self.encoding = obj, &quot;utf-8&quot;</span>
<span class="w"> </span>        self._is_unicode: bool = isinstance(self._text, str)
<span class="w"> </span>        self._is_first_read: bool = True

<span class="gi">+    def read(self, n: int = 65535) -&gt; bytes:</span>
<span class="gi">+        method: Callable[[int], bytes] = (</span>
<span class="gi">+            self._read_unicode if self._is_unicode else self._read_string</span>
<span class="gi">+        )</span>
<span class="gi">+        result = method(n)</span>
<span class="gi">+        if self._is_first_read:</span>
<span class="gi">+            self._is_first_read = False</span>
<span class="gi">+            result = result.lstrip()</span>
<span class="gi">+        return result</span>
<span class="gi">+</span>
<span class="gi">+    def _read_string(self, n: int = 65535) -&gt; bytes:</span>
<span class="gi">+        s, e = self._ptr, self._ptr + n</span>
<span class="gi">+        self._ptr = e</span>
<span class="gi">+        return cast(bytes, self._text)[s:e]</span>

<span class="gd">-def csviter(obj: Union[Response, str, bytes], delimiter: Optional[str]=None,</span>
<span class="gd">-    headers: Optional[List[str]]=None, encoding: Optional[str]=None,</span>
<span class="gd">-    quotechar: Optional[str]=None) -&gt;Generator[Dict[str, str], Any, None]:</span>
<span class="gi">+    def _read_unicode(self, n: int = 65535) -&gt; bytes:</span>
<span class="gi">+        s, e = self._ptr, self._ptr + n</span>
<span class="gi">+        self._ptr = e</span>
<span class="gi">+        return cast(str, self._text)[s:e].encode(&quot;utf-8&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def csviter(</span>
<span class="gi">+    obj: Union[Response, str, bytes],</span>
<span class="gi">+    delimiter: Optional[str] = None,</span>
<span class="gi">+    headers: Optional[List[str]] = None,</span>
<span class="gi">+    encoding: Optional[str] = None,</span>
<span class="gi">+    quotechar: Optional[str] = None,</span>
<span class="gi">+) -&gt; Generator[Dict[str, str], Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Returns an iterator of dictionaries from the given csv object

<span class="w"> </span>    obj can be:
<span class="gu">@@ -59,4 +187,74 @@ def csviter(obj: Union[Response, str, bytes], delimiter: Optional[str]=None,</span>

<span class="w"> </span>    quotechar is the character used to enclosure fields on the given obj.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or &quot;utf-8&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def row_to_unicode(row_: Iterable) -&gt; List[str]:</span>
<span class="gi">+        return [to_unicode(field, encoding) for field in row_]</span>
<span class="gi">+</span>
<span class="gi">+    lines = StringIO(_body_or_str(obj, unicode=True))</span>
<span class="gi">+</span>
<span class="gi">+    kwargs: Dict[str, Any] = {}</span>
<span class="gi">+    if delimiter:</span>
<span class="gi">+        kwargs[&quot;delimiter&quot;] = delimiter</span>
<span class="gi">+    if quotechar:</span>
<span class="gi">+        kwargs[&quot;quotechar&quot;] = quotechar</span>
<span class="gi">+    csv_r = csv.reader(lines, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    if not headers:</span>
<span class="gi">+        try:</span>
<span class="gi">+            row = next(csv_r)</span>
<span class="gi">+        except StopIteration:</span>
<span class="gi">+            return</span>
<span class="gi">+        headers = row_to_unicode(row)</span>
<span class="gi">+</span>
<span class="gi">+    for row in csv_r:</span>
<span class="gi">+        row = row_to_unicode(row)</span>
<span class="gi">+        if len(row) != len(headers):</span>
<span class="gi">+            logger.warning(</span>
<span class="gi">+                &quot;ignoring row %(csvlnum)d (length: %(csvrow)d, &quot;</span>
<span class="gi">+                &quot;should be: %(csvheader)d)&quot;,</span>
<span class="gi">+                {</span>
<span class="gi">+                    &quot;csvlnum&quot;: csv_r.line_num,</span>
<span class="gi">+                    &quot;csvrow&quot;: len(row),</span>
<span class="gi">+                    &quot;csvheader&quot;: len(headers),</span>
<span class="gi">+                },</span>
<span class="gi">+            )</span>
<span class="gi">+            continue</span>
<span class="gi">+        yield dict(zip(headers, row))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def _body_or_str(obj: Union[Response, str, bytes]) -&gt; str:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -&gt; str:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[False]) -&gt; bytes:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _body_or_str(</span>
<span class="gi">+    obj: Union[Response, str, bytes], unicode: bool = True</span>
<span class="gi">+) -&gt; Union[str, bytes]:</span>
<span class="gi">+    expected_types = (Response, str, bytes)</span>
<span class="gi">+    if not isinstance(obj, expected_types):</span>
<span class="gi">+        expected_types_str = &quot; or &quot;.join(t.__name__ for t in expected_types)</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            f&quot;Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if isinstance(obj, Response):</span>
<span class="gi">+        if not unicode:</span>
<span class="gi">+            return cast(bytes, obj.body)</span>
<span class="gi">+        if isinstance(obj, TextResponse):</span>
<span class="gi">+            return obj.text</span>
<span class="gi">+        return cast(bytes, obj.body).decode(&quot;utf-8&quot;)</span>
<span class="gi">+    if isinstance(obj, str):</span>
<span class="gi">+        return obj if unicode else obj.encode(&quot;utf-8&quot;)</span>
<span class="gi">+    return obj.decode(&quot;utf-8&quot;) if unicode else obj</span>
<span class="gh">diff --git a/scrapy/utils/job.py b/scrapy/utils/job.py</span>
<span class="gh">index 1149db8f5..e230e4235 100644</span>
<span class="gd">--- a/scrapy/utils/job.py</span>
<span class="gi">+++ b/scrapy/utils/job.py</span>
<span class="gu">@@ -1,3 +1,13 @@</span>
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import Optional
<span class="gi">+</span>
<span class="w"> </span>from scrapy.settings import BaseSettings
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def job_dir(settings: BaseSettings) -&gt; Optional[str]:</span>
<span class="gi">+    path: Optional[str] = settings[&quot;JOBDIR&quot;]</span>
<span class="gi">+    if not path:</span>
<span class="gi">+        return None</span>
<span class="gi">+    if not Path(path).exists():</span>
<span class="gi">+        Path(path).mkdir(parents=True)</span>
<span class="gi">+    return path</span>
<span class="gh">diff --git a/scrapy/utils/log.py b/scrapy/utils/log.py</span>
<span class="gh">index cbe04ec83..e85082963 100644</span>
<span class="gd">--- a/scrapy/utils/log.py</span>
<span class="gi">+++ b/scrapy/utils/log.py</span>
<span class="gu">@@ -1,25 +1,49 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import logging
<span class="w"> </span>import sys
<span class="w"> </span>import warnings
<span class="w"> </span>from logging.config import dictConfig
<span class="w"> </span>from types import TracebackType
<span class="gd">-from typing import TYPE_CHECKING, Any, List, MutableMapping, Optional, Tuple, Type, Union, cast</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    List,</span>
<span class="gi">+    MutableMapping,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Type,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.python import log as twisted_log
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.versions import scrapy_components_versions
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gd">-def failure_to_exc_info(failure: Failure) -&gt;Optional[Tuple[Type[</span>
<span class="gd">-    BaseException], BaseException, Optional[TracebackType]]]:</span>
<span class="gi">+def failure_to_exc_info(</span>
<span class="gi">+    failure: Failure,</span>
<span class="gi">+) -&gt; Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Extract exc_info from Failure instances&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(failure, Failure):</span>
<span class="gi">+        assert failure.type</span>
<span class="gi">+        assert failure.value</span>
<span class="gi">+        return (</span>
<span class="gi">+            failure.type,</span>
<span class="gi">+            failure.value,</span>
<span class="gi">+            cast(Optional[TracebackType], failure.getTracebackObject()),</span>
<span class="gi">+        )</span>
<span class="gi">+    return None</span>


<span class="w"> </span>class TopLevelFormatter(logging.Filter):
<span class="gu">@@ -34,17 +58,38 @@ class TopLevelFormatter(logging.Filter):</span>
<span class="w"> </span>    ``loggers`` list where it should act.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, loggers: Optional[List[str]]=None):</span>
<span class="gi">+    def __init__(self, loggers: Optional[List[str]] = None):</span>
<span class="w"> </span>        self.loggers: List[str] = loggers or []

<span class="gd">-</span>
<span class="gd">-DEFAULT_LOGGING = {&#39;version&#39;: 1, &#39;disable_existing_loggers&#39;: False,</span>
<span class="gd">-    &#39;loggers&#39;: {&#39;filelock&#39;: {&#39;level&#39;: &#39;ERROR&#39;}, &#39;hpack&#39;: {&#39;level&#39;: &#39;ERROR&#39;},</span>
<span class="gd">-    &#39;scrapy&#39;: {&#39;level&#39;: &#39;DEBUG&#39;}, &#39;twisted&#39;: {&#39;level&#39;: &#39;ERROR&#39;}}}</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def configure_logging(settings: Union[Settings, dict, None]=None,</span>
<span class="gd">-    install_root_handler: bool=True) -&gt;None:</span>
<span class="gi">+    def filter(self, record: logging.LogRecord) -&gt; bool:</span>
<span class="gi">+        if any(record.name.startswith(logger + &quot;.&quot;) for logger in self.loggers):</span>
<span class="gi">+            record.name = record.name.split(&quot;.&quot;, 1)[0]</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+DEFAULT_LOGGING = {</span>
<span class="gi">+    &quot;version&quot;: 1,</span>
<span class="gi">+    &quot;disable_existing_loggers&quot;: False,</span>
<span class="gi">+    &quot;loggers&quot;: {</span>
<span class="gi">+        &quot;filelock&quot;: {</span>
<span class="gi">+            &quot;level&quot;: &quot;ERROR&quot;,</span>
<span class="gi">+        },</span>
<span class="gi">+        &quot;hpack&quot;: {</span>
<span class="gi">+            &quot;level&quot;: &quot;ERROR&quot;,</span>
<span class="gi">+        },</span>
<span class="gi">+        &quot;scrapy&quot;: {</span>
<span class="gi">+            &quot;level&quot;: &quot;DEBUG&quot;,</span>
<span class="gi">+        },</span>
<span class="gi">+        &quot;twisted&quot;: {</span>
<span class="gi">+            &quot;level&quot;: &quot;ERROR&quot;,</span>
<span class="gi">+        },</span>
<span class="gi">+    },</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def configure_logging(</span>
<span class="gi">+    settings: Union[Settings, dict, None] = None, install_root_handler: bool = True</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Initialize logging defaults for Scrapy.

<span class="gu">@@ -68,15 +113,93 @@ def configure_logging(settings: Union[Settings, dict, None]=None,</span>
<span class="w"> </span>    using ``settings`` argument. When ``settings`` is empty or None, defaults
<span class="w"> </span>    are used.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not sys.warnoptions:</span>
<span class="gi">+        # Route warnings through python logging</span>
<span class="gi">+        logging.captureWarnings(True)</span>
<span class="gi">+</span>
<span class="gi">+    observer = twisted_log.PythonLoggingObserver(&quot;twisted&quot;)</span>
<span class="gi">+    observer.start()</span>
<span class="gi">+</span>
<span class="gi">+    dictConfig(DEFAULT_LOGGING)</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(settings, dict) or settings is None:</span>
<span class="gi">+        settings = Settings(settings)</span>
<span class="gi">+</span>
<span class="gi">+    if settings.getbool(&quot;LOG_STDOUT&quot;):</span>
<span class="gi">+        sys.stdout = StreamLogger(logging.getLogger(&quot;stdout&quot;))  # type: ignore[assignment]</span>
<span class="gi">+</span>
<span class="gi">+    if install_root_handler:</span>
<span class="gi">+        install_scrapy_root_handler(settings)</span>


<span class="w"> </span>_scrapy_root_handler: Optional[logging.Handler] = None


<span class="gd">-def _get_handler(settings: Settings) -&gt;logging.Handler:</span>
<span class="gi">+def install_scrapy_root_handler(settings: Settings) -&gt; None:</span>
<span class="gi">+    global _scrapy_root_handler</span>
<span class="gi">+</span>
<span class="gi">+    if (</span>
<span class="gi">+        _scrapy_root_handler is not None</span>
<span class="gi">+        and _scrapy_root_handler in logging.root.handlers</span>
<span class="gi">+    ):</span>
<span class="gi">+        logging.root.removeHandler(_scrapy_root_handler)</span>
<span class="gi">+    logging.root.setLevel(logging.NOTSET)</span>
<span class="gi">+    _scrapy_root_handler = _get_handler(settings)</span>
<span class="gi">+    logging.root.addHandler(_scrapy_root_handler)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_scrapy_root_handler() -&gt; Optional[logging.Handler]:</span>
<span class="gi">+    return _scrapy_root_handler</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_handler(settings: Settings) -&gt; logging.Handler:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a log handler object according to settings&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    filename = settings.get(&quot;LOG_FILE&quot;)</span>
<span class="gi">+    handler: logging.Handler</span>
<span class="gi">+    if filename:</span>
<span class="gi">+        mode = &quot;a&quot; if settings.getbool(&quot;LOG_FILE_APPEND&quot;) else &quot;w&quot;</span>
<span class="gi">+        encoding = settings.get(&quot;LOG_ENCODING&quot;)</span>
<span class="gi">+        handler = logging.FileHandler(filename, mode=mode, encoding=encoding)</span>
<span class="gi">+    elif settings.getbool(&quot;LOG_ENABLED&quot;):</span>
<span class="gi">+        handler = logging.StreamHandler()</span>
<span class="gi">+    else:</span>
<span class="gi">+        handler = logging.NullHandler()</span>
<span class="gi">+</span>
<span class="gi">+    formatter = logging.Formatter(</span>
<span class="gi">+        fmt=settings.get(&quot;LOG_FORMAT&quot;), datefmt=settings.get(&quot;LOG_DATEFORMAT&quot;)</span>
<span class="gi">+    )</span>
<span class="gi">+    handler.setFormatter(formatter)</span>
<span class="gi">+    handler.setLevel(settings.get(&quot;LOG_LEVEL&quot;))</span>
<span class="gi">+    if settings.getbool(&quot;LOG_SHORT_NAMES&quot;):</span>
<span class="gi">+        handler.addFilter(TopLevelFormatter([&quot;scrapy&quot;]))</span>
<span class="gi">+    return handler</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def log_scrapy_info(settings: Settings) -&gt; None:</span>
<span class="gi">+    logger.info(</span>
<span class="gi">+        &quot;Scrapy %(version)s started (bot: %(bot)s)&quot;,</span>
<span class="gi">+        {&quot;version&quot;: scrapy.__version__, &quot;bot&quot;: settings[&quot;BOT_NAME&quot;]},</span>
<span class="gi">+    )</span>
<span class="gi">+    versions = [</span>
<span class="gi">+        f&quot;{name} {version}&quot;</span>
<span class="gi">+        for name, version in scrapy_components_versions()</span>
<span class="gi">+        if name != &quot;Scrapy&quot;</span>
<span class="gi">+    ]</span>
<span class="gi">+    logger.info(&quot;Versions: %(versions)s&quot;, {&quot;versions&quot;: &quot;, &quot;.join(versions)})</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def log_reactor_info() -&gt; None:</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    logger.debug(&quot;Using reactor: %s.%s&quot;, reactor.__module__, reactor.__class__.__name__)</span>
<span class="gi">+    from twisted.internet import asyncioreactor</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):</span>
<span class="gi">+        logger.debug(</span>
<span class="gi">+            &quot;Using asyncio event loop: %s.%s&quot;,</span>
<span class="gi">+            reactor._asyncioEventloop.__module__,</span>
<span class="gi">+            reactor._asyncioEventloop.__class__.__name__,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class StreamLogger:
<span class="gu">@@ -86,10 +209,18 @@ class StreamLogger:</span>
<span class="w"> </span>        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, logger: logging.Logger, log_level: int=logging.INFO):</span>
<span class="gi">+    def __init__(self, logger: logging.Logger, log_level: int = logging.INFO):</span>
<span class="w"> </span>        self.logger: logging.Logger = logger
<span class="w"> </span>        self.log_level: int = log_level
<span class="gd">-        self.linebuf: str = &#39;&#39;</span>
<span class="gi">+        self.linebuf: str = &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def write(self, buf: str) -&gt; None:</span>
<span class="gi">+        for line in buf.rstrip().splitlines():</span>
<span class="gi">+            self.logger.log(self.log_level, line.rstrip())</span>
<span class="gi">+</span>
<span class="gi">+    def flush(self) -&gt; None:</span>
<span class="gi">+        for h in self.logger.handlers:</span>
<span class="gi">+            h.flush()</span>


<span class="w"> </span>class LogCounterHandler(logging.Handler):
<span class="gu">@@ -99,19 +230,45 @@ class LogCounterHandler(logging.Handler):</span>
<span class="w"> </span>        super().__init__(*args, **kwargs)
<span class="w"> </span>        self.crawler: Crawler = crawler

<span class="gi">+    def emit(self, record: logging.LogRecord) -&gt; None:</span>
<span class="gi">+        sname = f&quot;log_count/{record.levelname}&quot;</span>
<span class="gi">+        assert self.crawler.stats</span>
<span class="gi">+        self.crawler.stats.inc_value(sname)</span>

<span class="gd">-def logformatter_adapter(logkws: dict) -&gt;Tuple[int, str, dict]:</span>
<span class="gi">+</span>
<span class="gi">+def logformatter_adapter(logkws: dict) -&gt; Tuple[int, str, dict]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Helper that takes the dictionary output from the methods in LogFormatter
<span class="w"> </span>    and adapts it into a tuple of positional arguments for logger.log calls,
<span class="w"> </span>    handling backward compatibility as well.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not {&quot;level&quot;, &quot;msg&quot;, &quot;args&quot;} &lt;= set(logkws):</span>
<span class="gi">+        warnings.warn(&quot;Missing keys in LogFormatter method&quot;, ScrapyDeprecationWarning)</span>

<span class="gi">+    if &quot;format&quot; in logkws:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`format` key in LogFormatter methods has been &quot;</span>
<span class="gi">+            &quot;deprecated, use `msg` instead&quot;,</span>
<span class="gi">+            ScrapyDeprecationWarning,</span>
<span class="gi">+        )</span>

<span class="gd">-class SpiderLoggerAdapter(logging.LoggerAdapter):</span>
<span class="gi">+    level = logkws.get(&quot;level&quot;, logging.INFO)</span>
<span class="gi">+    message = logkws.get(&quot;format&quot;, logkws.get(&quot;msg&quot;))</span>
<span class="gi">+    # NOTE: This also handles &#39;args&#39; being an empty dict, that case doesn&#39;t</span>
<span class="gi">+    # play well in logger.log calls</span>
<span class="gi">+    args = logkws if not logkws.get(&quot;args&quot;) else logkws[&quot;args&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    return (level, message, args)</span>

<span class="gd">-    def process(self, msg: str, kwargs: MutableMapping[str, Any]) -&gt;Tuple[</span>
<span class="gd">-        str, MutableMapping[str, Any]]:</span>
<span class="gi">+</span>
<span class="gi">+class SpiderLoggerAdapter(logging.LoggerAdapter):</span>
<span class="gi">+    def process(</span>
<span class="gi">+        self, msg: str, kwargs: MutableMapping[str, Any]</span>
<span class="gi">+    ) -&gt; Tuple[str, MutableMapping[str, Any]]:</span>
<span class="w"> </span>        &quot;&quot;&quot;Method that augments logging with additional &#39;extra&#39; data&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(kwargs.get(&quot;extra&quot;), MutableMapping):</span>
<span class="gi">+            kwargs[&quot;extra&quot;].update(self.extra)</span>
<span class="gi">+        else:</span>
<span class="gi">+            kwargs[&quot;extra&quot;] = self.extra</span>
<span class="gi">+</span>
<span class="gi">+        return msg, kwargs</span>
<span class="gh">diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py</span>
<span class="gh">index 6e45b6a2b..b3c28da92 100644</span>
<span class="gd">--- a/scrapy/utils/misc.py</span>
<span class="gi">+++ b/scrapy/utils/misc.py</span>
<span class="gu">@@ -11,27 +11,49 @@ from functools import partial</span>
<span class="w"> </span>from importlib import import_module
<span class="w"> </span>from pkgutil import iter_modules
<span class="w"> </span>from types import ModuleType
<span class="gd">-from typing import IO, TYPE_CHECKING, Any, Callable, Deque, Generator, Iterable, List, Optional, Pattern, Union, cast</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    IO,</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Deque,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Pattern,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    cast,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from w3lib.html import replace_entities
<span class="gi">+</span>
<span class="w"> </span>from scrapy.item import Item
<span class="w"> </span>from scrapy.utils.datatypes import LocalWeakReferencedCache
<span class="w"> </span>from scrapy.utils.deprecate import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.python import flatten, to_unicode
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy import Spider
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes


<span class="gd">-def arg_to_iter(arg: Any) -&gt;Iterable[Any]:</span>
<span class="gi">+def arg_to_iter(arg: Any) -&gt; Iterable[Any]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Convert an argument to an iterable. The argument can be a None, single
<span class="w"> </span>    value, or an iterable.

<span class="w"> </span>    Exception: if arg is a dict, [arg] will be returned
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if arg is None:</span>
<span class="gi">+        return []</span>
<span class="gi">+    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, &quot;__iter__&quot;):</span>
<span class="gi">+        return cast(Iterable[Any], arg)</span>
<span class="gi">+    return [arg]</span>


<span class="gd">-def load_object(path: Union[str, Callable]) -&gt;Any:</span>
<span class="gi">+def load_object(path: Union[str, Callable]) -&gt; Any:</span>
<span class="w"> </span>    &quot;&quot;&quot;Load an object given its absolute object path, and return it.

<span class="w"> </span>    The object can be the import path of a class, function, variable or an
<span class="gu">@@ -40,31 +62,86 @@ def load_object(path: Union[str, Callable]) -&gt;Any:</span>
<span class="w"> </span>    If ``path`` is not a string, but is a callable object, such as a class or
<span class="w"> </span>    a function, then return it as is.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    if not isinstance(path, str):</span>
<span class="gi">+        if callable(path):</span>
<span class="gi">+            return path</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            f&quot;Unexpected argument type, expected string or object, got: {type(path)}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        dot = path.rindex(&quot;.&quot;)</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        raise ValueError(f&quot;Error loading object &#39;{path}&#39;: not a full path&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    module, name = path[:dot], path[dot + 1 :]</span>
<span class="gi">+    mod = import_module(module)</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        obj = getattr(mod, name)</span>
<span class="gi">+    except AttributeError:</span>
<span class="gi">+        raise NameError(f&quot;Module &#39;{module}&#39; doesn&#39;t define any object named &#39;{name}&#39;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return obj</span>


<span class="gd">-def walk_modules(path: str) -&gt;List[ModuleType]:</span>
<span class="gi">+def walk_modules(path: str) -&gt; List[ModuleType]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Loads a module and all its submodules from the given module path and
<span class="w"> </span>    returns them. If *any* module throws an exception while importing, that
<span class="w"> </span>    exception is thrown back.

<span class="w"> </span>    For example: walk_modules(&#39;scrapy.utils&#39;)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gd">-</span>
<span class="gd">-def extract_regex(regex: Union[str, Pattern], text: str, encoding: str=&#39;utf-8&#39;</span>
<span class="gd">-    ) -&gt;List[str]:</span>
<span class="gi">+    mods: List[ModuleType] = []</span>
<span class="gi">+    mod = import_module(path)</span>
<span class="gi">+    mods.append(mod)</span>
<span class="gi">+    if hasattr(mod, &quot;__path__&quot;):</span>
<span class="gi">+        for _, subpath, ispkg in iter_modules(mod.__path__):</span>
<span class="gi">+            fullpath = path + &quot;.&quot; + subpath</span>
<span class="gi">+            if ispkg:</span>
<span class="gi">+                mods += walk_modules(fullpath)</span>
<span class="gi">+            else:</span>
<span class="gi">+                submod = import_module(fullpath)</span>
<span class="gi">+                mods.append(submod)</span>
<span class="gi">+    return mods</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def extract_regex(</span>
<span class="gi">+    regex: Union[str, Pattern], text: str, encoding: str = &quot;utf-8&quot;</span>
<span class="gi">+) -&gt; List[str]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Extract a list of unicode strings from the given text/encoding using the following policies:

<span class="w"> </span>    * if the regex contains a named group called &quot;extract&quot; that will be returned
<span class="w"> </span>    * if the regex contains multiple numbered groups, all those will be returned (flattened)
<span class="w"> </span>    * if the regex doesn&#39;t contain any group the entire regex matching is returned
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def md5sum(file: IO) -&gt;str:</span>
<span class="gi">+    warnings.warn(</span>
<span class="gi">+        &quot;scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.&quot;,</span>
<span class="gi">+        ScrapyDeprecationWarning,</span>
<span class="gi">+        stacklevel=2,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(regex, str):</span>
<span class="gi">+        regex = re.compile(regex, re.UNICODE)</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        # named group</span>
<span class="gi">+        strings = [regex.search(text).group(&quot;extract&quot;)]  # type: ignore[union-attr]</span>
<span class="gi">+    except Exception:</span>
<span class="gi">+        # full regex or numbered groups</span>
<span class="gi">+        strings = regex.findall(text)</span>
<span class="gi">+    strings = flatten(strings)</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(text, str):</span>
<span class="gi">+        return [replace_entities(s, keep=[&quot;lt&quot;, &quot;amp&quot;]) for s in strings]</span>
<span class="gi">+    return [</span>
<span class="gi">+        replace_entities(to_unicode(s, encoding), keep=[&quot;lt&quot;, &quot;amp&quot;]) for s in strings</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def md5sum(file: IO) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Calculate the md5 checksum of a file-like object without reading its
<span class="w"> </span>    whole content in memory.

<span class="gu">@@ -72,12 +149,18 @@ def md5sum(file: IO) -&gt;str:</span>
<span class="w"> </span>    &gt;&gt;&gt; md5sum(BytesIO(b&#39;file content to hash&#39;))
<span class="w"> </span>    &#39;784406af91dd5a54fbb9c84c2236595a&#39;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    m = hashlib.md5()</span>
<span class="gi">+    while True:</span>
<span class="gi">+        d = file.read(8096)</span>
<span class="gi">+        if not d:</span>
<span class="gi">+            break</span>
<span class="gi">+        m.update(d)</span>
<span class="gi">+    return m.hexdigest()</span>


<span class="gd">-def rel_has_nofollow(rel: Optional[str]) -&gt;bool:</span>
<span class="gi">+def rel_has_nofollow(rel: Optional[str]) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return True if link rel attribute has nofollow type&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return rel is not None and &quot;nofollow&quot; in rel.replace(&quot;,&quot;, &quot; &quot;).split()</span>


<span class="w"> </span>def create_instance(objcls, settings, crawler, *args, **kwargs):
<span class="gu">@@ -97,39 +180,120 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):</span>
<span class="w"> </span>       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
<span class="w"> </span>       extension has not been implemented correctly).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if settings is None:</span>
<span class="gi">+        if crawler is None:</span>
<span class="gi">+            raise ValueError(&quot;Specify at least one of settings and crawler.&quot;)</span>
<span class="gi">+        settings = crawler.settings</span>
<span class="gi">+    if crawler and hasattr(objcls, &quot;from_crawler&quot;):</span>
<span class="gi">+        instance = objcls.from_crawler(crawler, *args, **kwargs)</span>
<span class="gi">+        method_name = &quot;from_crawler&quot;</span>
<span class="gi">+    elif hasattr(objcls, &quot;from_settings&quot;):</span>
<span class="gi">+        instance = objcls.from_settings(settings, *args, **kwargs)</span>
<span class="gi">+        method_name = &quot;from_settings&quot;</span>
<span class="gi">+    else:</span>
<span class="gi">+        instance = objcls(*args, **kwargs)</span>
<span class="gi">+        method_name = &quot;__new__&quot;</span>
<span class="gi">+    if instance is None:</span>
<span class="gi">+        raise TypeError(f&quot;{objcls.__qualname__}.{method_name} returned None&quot;)</span>
<span class="gi">+    return instance</span>


<span class="w"> </span>@contextmanager
<span class="gd">-def set_environ(**kwargs: str) -&gt;Generator[None, Any, None]:</span>
<span class="gi">+def set_environ(**kwargs: str) -&gt; Generator[None, Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Temporarily set environment variables inside the context manager and
<span class="w"> </span>    fully restore previous environment afterwards
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    original_env = {k: os.environ.get(k) for k in kwargs}</span>
<span class="gi">+    os.environ.update(kwargs)</span>
<span class="gi">+    try:</span>
<span class="gi">+        yield</span>
<span class="gi">+    finally:</span>
<span class="gi">+        for k, v in original_env.items():</span>
<span class="gi">+            if v is None:</span>
<span class="gi">+                del os.environ[k]</span>
<span class="gi">+            else:</span>
<span class="gi">+                os.environ[k] = v</span>

<span class="gd">-def walk_callable(node: ast.AST) -&gt;Generator[ast.AST, Any, None]:</span>
<span class="gi">+</span>
<span class="gi">+def walk_callable(node: ast.AST) -&gt; Generator[ast.AST, Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Similar to ``ast.walk``, but walks only function body and skips nested
<span class="w"> </span>    functions defined within the node.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    todo: Deque[ast.AST] = deque([node])</span>
<span class="gi">+    walked_func_def = False</span>
<span class="gi">+    while todo:</span>
<span class="gi">+        node = todo.popleft()</span>
<span class="gi">+        if isinstance(node, ast.FunctionDef):</span>
<span class="gi">+            if walked_func_def:</span>
<span class="gi">+                continue</span>
<span class="gi">+            walked_func_def = True</span>
<span class="gi">+        todo.extend(ast.iter_child_nodes(node))</span>
<span class="gi">+        yield node</span>


<span class="w"> </span>_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)


<span class="gd">-def is_generator_with_return_value(callable: Callable) -&gt;bool:</span>
<span class="gi">+def is_generator_with_return_value(callable: Callable) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Returns True if a callable is a generator function which includes a
<span class="w"> </span>    &#39;return&#39; statement with a value different than None, False otherwise
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if callable in _generator_callbacks_cache:</span>
<span class="gi">+        return bool(_generator_callbacks_cache[callable])</span>
<span class="gi">+</span>
<span class="gi">+    def returns_none(return_node: ast.Return) -&gt; bool:</span>
<span class="gi">+        value = return_node.value</span>
<span class="gi">+        return (</span>
<span class="gi">+            value is None or isinstance(value, ast.NameConstant) and value.value is None</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if inspect.isgeneratorfunction(callable):</span>
<span class="gi">+        func = callable</span>
<span class="gi">+        while isinstance(func, partial):</span>
<span class="gi">+            func = func.func</span>
<span class="gi">+</span>
<span class="gi">+        src = inspect.getsource(func)</span>
<span class="gi">+        pattern = re.compile(r&quot;(^[\t ]+)&quot;)</span>
<span class="gi">+        code = pattern.sub(&quot;&quot;, src)</span>
<span class="gi">+</span>
<span class="gi">+        match = pattern.match(src)  # finds indentation</span>
<span class="gi">+        if match:</span>
<span class="gi">+            code = re.sub(f&quot;\n{match.group(0)}&quot;, &quot;\n&quot;, code)  # remove indentation</span>
<span class="gi">+</span>
<span class="gi">+        tree = ast.parse(code)</span>
<span class="gi">+        for node in walk_callable(tree):</span>
<span class="gi">+            if isinstance(node, ast.Return) and not returns_none(node):</span>
<span class="gi">+                _generator_callbacks_cache[callable] = True</span>
<span class="gi">+                return bool(_generator_callbacks_cache[callable])</span>
<span class="gi">+</span>
<span class="gi">+    _generator_callbacks_cache[callable] = False</span>
<span class="gi">+    return bool(_generator_callbacks_cache[callable])</span>


<span class="gd">-def warn_on_generator_with_return_value(spider: &#39;Spider&#39;, callable: Callable</span>
<span class="gd">-    ) -&gt;None:</span>
<span class="gi">+def warn_on_generator_with_return_value(spider: &quot;Spider&quot;, callable: Callable) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Logs a warning if a callable is a generator function and includes
<span class="w"> </span>    a &#39;return&#39; statement with a value different than None
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        if is_generator_with_return_value(callable):</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&#39;The &quot;{spider.__class__.__name__}.{callable.__name__}&quot; method is &#39;</span>
<span class="gi">+                &#39;a generator and includes a &quot;return&quot; statement with a value &#39;</span>
<span class="gi">+                &quot;different than None. This could lead to unexpected behaviour. Please see &quot;</span>
<span class="gi">+                &quot;https://docs.python.org/3/reference/simple_stmts.html#the-return-statement &quot;</span>
<span class="gi">+                &#39;for details about the semantics of the &quot;return&quot; statement within generators&#39;,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+    except IndentationError:</span>
<span class="gi">+        callable_name = spider.__class__.__name__ + &quot;.&quot; + callable.__name__</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            f&#39;Unable to determine whether or not &quot;{callable_name}&quot; is a generator with a return value. &#39;</span>
<span class="gi">+            &quot;This will not prevent your code from working, but it prevents Scrapy from detecting &quot;</span>
<span class="gi">+            f&#39;potential issues in your implementation of &quot;{callable_name}&quot;. Please, report this in the &#39;</span>
<span class="gi">+            &quot;Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), &quot;</span>
<span class="gi">+            f&#39;including the code of &quot;{callable_name}&quot;&#39;,</span>
<span class="gi">+            stacklevel=2,</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/scrapy/utils/ossignal.py b/scrapy/utils/ossignal.py</span>
<span class="gh">index 012a5cf9a..db9a71273 100644</span>
<span class="gd">--- a/scrapy/utils/ossignal.py</span>
<span class="gi">+++ b/scrapy/utils/ossignal.py</span>
<span class="gu">@@ -1,21 +1,31 @@</span>
<span class="w"> </span>import signal
<span class="w"> </span>from types import FrameType
<span class="w"> </span>from typing import Any, Callable, Dict, Optional, Union
<span class="gd">-SignalHandlerT = Union[Callable[[int, Optional[FrameType]], Any], int,</span>
<span class="gd">-    signal.Handlers, None]</span>
<span class="gi">+</span>
<span class="gi">+# copy of _HANDLER from typeshed/stdlib/signal.pyi</span>
<span class="gi">+SignalHandlerT = Union[</span>
<span class="gi">+    Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None</span>
<span class="gi">+]</span>
<span class="gi">+</span>
<span class="w"> </span>signal_names: Dict[int, str] = {}
<span class="w"> </span>for signame in dir(signal):
<span class="gd">-    if signame.startswith(&#39;SIG&#39;) and not signame.startswith(&#39;SIG_&#39;):</span>
<span class="gi">+    if signame.startswith(&quot;SIG&quot;) and not signame.startswith(&quot;SIG_&quot;):</span>
<span class="w"> </span>        signum = getattr(signal, signame)
<span class="w"> </span>        if isinstance(signum, int):
<span class="w"> </span>            signal_names[signum] = signame


<span class="gd">-def install_shutdown_handlers(function: SignalHandlerT, override_sigint:</span>
<span class="gd">-    bool=True) -&gt;None:</span>
<span class="gi">+def install_shutdown_handlers(</span>
<span class="gi">+    function: SignalHandlerT, override_sigint: bool = True</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Install the given function as a signal handler for all common shutdown
<span class="w"> </span>    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the
<span class="w"> </span>    SIGINT handler won&#39;t be installed if there is already a handler in place
<span class="w"> </span>    (e.g. Pdb)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    signal.signal(signal.SIGTERM, function)</span>
<span class="gi">+    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:</span>
<span class="gi">+        signal.signal(signal.SIGINT, function)</span>
<span class="gi">+    # Catch Ctrl-Break in windows</span>
<span class="gi">+    if hasattr(signal, &quot;SIGBREAK&quot;):</span>
<span class="gi">+        signal.signal(signal.SIGBREAK, function)</span>
<span class="gh">diff --git a/scrapy/utils/project.py b/scrapy/utils/project.py</span>
<span class="gh">index ce406428a..a2c224b90 100644</span>
<span class="gd">--- a/scrapy/utils/project.py</span>
<span class="gi">+++ b/scrapy/utils/project.py</span>
<span class="gu">@@ -2,21 +2,87 @@ import os</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from importlib import import_module
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import NotConfigured
<span class="w"> </span>from scrapy.settings import Settings
<span class="w"> </span>from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env
<span class="gd">-ENVVAR = &#39;SCRAPY_SETTINGS_MODULE&#39;</span>
<span class="gd">-DATADIR_CFG_SECTION = &#39;datadir&#39;</span>

<span class="gi">+ENVVAR = &quot;SCRAPY_SETTINGS_MODULE&quot;</span>
<span class="gi">+DATADIR_CFG_SECTION = &quot;datadir&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def inside_project() -&gt; bool:</span>
<span class="gi">+    scrapy_module = os.environ.get(ENVVAR)</span>
<span class="gi">+    if scrapy_module:</span>
<span class="gi">+        try:</span>
<span class="gi">+            import_module(scrapy_module)</span>
<span class="gi">+        except ImportError as exc:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;Cannot import scrapy settings module {scrapy_module}: {exc}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            return True</span>
<span class="gi">+    return bool(closest_scrapy_cfg())</span>

<span class="gd">-def project_data_dir(project: str=&#39;default&#39;) -&gt;str:</span>
<span class="gi">+</span>
<span class="gi">+def project_data_dir(project: str = &quot;default&quot;) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the current project data dir, creating it if it doesn&#39;t exist&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not inside_project():</span>
<span class="gi">+        raise NotConfigured(&quot;Not inside a project&quot;)</span>
<span class="gi">+    cfg = get_config()</span>
<span class="gi">+    if cfg.has_option(DATADIR_CFG_SECTION, project):</span>
<span class="gi">+        d = Path(cfg.get(DATADIR_CFG_SECTION, project))</span>
<span class="gi">+    else:</span>
<span class="gi">+        scrapy_cfg = closest_scrapy_cfg()</span>
<span class="gi">+        if not scrapy_cfg:</span>
<span class="gi">+            raise NotConfigured(</span>
<span class="gi">+                &quot;Unable to find scrapy.cfg file to infer project data dir&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        d = (Path(scrapy_cfg).parent / &quot;.scrapy&quot;).resolve()</span>
<span class="gi">+    if not d.exists():</span>
<span class="gi">+        d.mkdir(parents=True)</span>
<span class="gi">+    return str(d)</span>


<span class="gd">-def data_path(path: str, createdir: bool=False) -&gt;str:</span>
<span class="gi">+def data_path(path: str, createdir: bool = False) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return the given path joined with the .scrapy data directory.
<span class="w"> </span>    If given an absolute path, return it unmodified.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    path_obj = Path(path)</span>
<span class="gi">+    if not path_obj.is_absolute():</span>
<span class="gi">+        if inside_project():</span>
<span class="gi">+            path_obj = Path(project_data_dir(), path)</span>
<span class="gi">+        else:</span>
<span class="gi">+            path_obj = Path(&quot;.scrapy&quot;, path)</span>
<span class="gi">+    if createdir and not path_obj.exists():</span>
<span class="gi">+        path_obj.mkdir(parents=True)</span>
<span class="gi">+    return str(path_obj)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_project_settings() -&gt; Settings:</span>
<span class="gi">+    if ENVVAR not in os.environ:</span>
<span class="gi">+        project = os.environ.get(&quot;SCRAPY_PROJECT&quot;, &quot;default&quot;)</span>
<span class="gi">+        init_env(project)</span>
<span class="gi">+</span>
<span class="gi">+    settings = Settings()</span>
<span class="gi">+    settings_module_path = os.environ.get(ENVVAR)</span>
<span class="gi">+    if settings_module_path:</span>
<span class="gi">+        settings.setmodule(settings_module_path, priority=&quot;project&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    valid_envvars = {</span>
<span class="gi">+        &quot;CHECK&quot;,</span>
<span class="gi">+        &quot;PROJECT&quot;,</span>
<span class="gi">+        &quot;PYTHON_SHELL&quot;,</span>
<span class="gi">+        &quot;SETTINGS_MODULE&quot;,</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    scrapy_envvars = {</span>
<span class="gi">+        k[7:]: v</span>
<span class="gi">+        for k, v in os.environ.items()</span>
<span class="gi">+        if k.startswith(&quot;SCRAPY_&quot;) and k.replace(&quot;SCRAPY_&quot;, &quot;&quot;) in valid_envvars</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    settings.setdict(scrapy_envvars, priority=&quot;project&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return settings</span>
<span class="gh">diff --git a/scrapy/utils/python.py b/scrapy/utils/python.py</span>
<span class="gh">index fa47a6995..20305a75e 100644</span>
<span class="gd">--- a/scrapy/utils/python.py</span>
<span class="gi">+++ b/scrapy/utils/python.py</span>
<span class="gu">@@ -9,11 +9,29 @@ import sys</span>
<span class="w"> </span>import weakref
<span class="w"> </span>from functools import partial, wraps
<span class="w"> </span>from itertools import chain
<span class="gd">-from typing import Any, AsyncGenerator, AsyncIterable, AsyncIterator, Callable, Dict, Generator, Iterable, Iterator, List, Mapping, Optional, Pattern, Tuple, Union, overload</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    Any,</span>
<span class="gi">+    AsyncGenerator,</span>
<span class="gi">+    AsyncIterable,</span>
<span class="gi">+    AsyncIterator,</span>
<span class="gi">+    Callable,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    Iterator,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Mapping,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Pattern,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    overload,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.asyncgen import as_async_generator


<span class="gd">-def flatten(x: Iterable) -&gt;list:</span>
<span class="gi">+def flatten(x: Iterable) -&gt; list:</span>
<span class="w"> </span>    &quot;&quot;&quot;flatten(sequence) -&gt; list

<span class="w"> </span>    Returns a single, flat list which contains all elements retrieved
<span class="gu">@@ -30,17 +48,22 @@ def flatten(x: Iterable) -&gt;list:</span>
<span class="w"> </span>    &gt;&gt;&gt; flatten([&quot;foo&quot;, [&quot;baz&quot;, 42], &quot;bar&quot;])
<span class="w"> </span>    [&#39;foo&#39;, &#39;baz&#39;, 42, &#39;bar&#39;]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return list(iflatten(x))</span>


<span class="gd">-def iflatten(x: Iterable) -&gt;Iterable:</span>
<span class="gi">+def iflatten(x: Iterable) -&gt; Iterable:</span>
<span class="w"> </span>    &quot;&quot;&quot;iflatten(sequence) -&gt; iterator

<span class="w"> </span>    Similar to ``.flatten()``, but returns iterator instead&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for el in x:</span>
<span class="gi">+        if is_listlike(el):</span>
<span class="gi">+            for el_ in iflatten(el):</span>
<span class="gi">+                yield el_</span>
<span class="gi">+        else:</span>
<span class="gi">+            yield el</span>


<span class="gd">-def is_listlike(x: Any) -&gt;bool:</span>
<span class="gi">+def is_listlike(x: Any) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    &gt;&gt;&gt; is_listlike(&quot;foo&quot;)
<span class="w"> </span>    False
<span class="gu">@@ -61,30 +84,58 @@ def is_listlike(x: Any) -&gt;bool:</span>
<span class="w"> </span>    &gt;&gt;&gt; is_listlike(range(5))
<span class="w"> </span>    True
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return hasattr(x, &quot;__iter__&quot;) and not isinstance(x, (str, bytes))</span>


<span class="gd">-def unique(list_: Iterable, key: Callable[[Any], Any]=lambda x: x) -&gt;list:</span>
<span class="gi">+def unique(list_: Iterable, key: Callable[[Any], Any] = lambda x: x) -&gt; list:</span>
<span class="w"> </span>    &quot;&quot;&quot;efficient function to uniquify a list preserving item order&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def to_unicode(text: Union[str, bytes], encoding: Optional[str]=None,</span>
<span class="gd">-    errors: str=&#39;strict&#39;) -&gt;str:</span>
<span class="gi">+    seen = set()</span>
<span class="gi">+    result = []</span>
<span class="gi">+    for item in list_:</span>
<span class="gi">+        seenkey = key(item)</span>
<span class="gi">+        if seenkey in seen:</span>
<span class="gi">+            continue</span>
<span class="gi">+        seen.add(seenkey)</span>
<span class="gi">+        result.append(item)</span>
<span class="gi">+    return result</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def to_unicode(</span>
<span class="gi">+    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = &quot;strict&quot;</span>
<span class="gi">+) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the unicode representation of a bytes object ``text``. If
<span class="w"> </span>    ``text`` is already an unicode object, return it as-is.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def to_bytes(text: Union[str, bytes], encoding: Optional[str]=None, errors:</span>
<span class="gd">-    str=&#39;strict&#39;) -&gt;bytes:</span>
<span class="gi">+    if isinstance(text, str):</span>
<span class="gi">+        return text</span>
<span class="gi">+    if not isinstance(text, (bytes, str)):</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            &quot;to_unicode must receive a bytes or str &quot;</span>
<span class="gi">+            f&quot;object, got {type(text).__name__}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if encoding is None:</span>
<span class="gi">+        encoding = &quot;utf-8&quot;</span>
<span class="gi">+    return text.decode(encoding, errors)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def to_bytes(</span>
<span class="gi">+    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = &quot;strict&quot;</span>
<span class="gi">+) -&gt; bytes:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the binary representation of ``text``. If ``text``
<span class="w"> </span>    is already a bytes object, return it as-is.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def re_rsearch(pattern: Union[str, Pattern], text: str, chunk_size: int=1024</span>
<span class="gd">-    ) -&gt;Optional[Tuple[int, int]]:</span>
<span class="gi">+    if isinstance(text, bytes):</span>
<span class="gi">+        return text</span>
<span class="gi">+    if not isinstance(text, str):</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            &quot;to_bytes must receive a str or bytes &quot; f&quot;object, got {type(text).__name__}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if encoding is None:</span>
<span class="gi">+        encoding = &quot;utf-8&quot;</span>
<span class="gi">+    return text.encode(encoding, errors)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def re_rsearch(</span>
<span class="gi">+    pattern: Union[str, Pattern], text: str, chunk_size: int = 1024</span>
<span class="gi">+) -&gt; Optional[Tuple[int, int]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    This function does a reverse search in a text using a regular expression
<span class="w"> </span>    given in the attribute &#39;pattern&#39;.
<span class="gu">@@ -97,33 +148,87 @@ def re_rsearch(pattern: Union[str, Pattern], text: str, chunk_size: int=1024</span>
<span class="w"> </span>    In case the pattern wasn&#39;t found, None is returned, otherwise it returns a tuple containing
<span class="w"> </span>    the start position of the match, and the ending (regarding the entire text).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    def _chunk_iter() -&gt; Generator[Tuple[str, int], Any, None]:</span>
<span class="gi">+        offset = len(text)</span>
<span class="gi">+        while True:</span>
<span class="gi">+            offset -= chunk_size * 1024</span>
<span class="gi">+            if offset &lt;= 0:</span>
<span class="gi">+                break</span>
<span class="gi">+            yield (text[offset:], offset)</span>
<span class="gi">+        yield (text, 0)</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(pattern, str):</span>
<span class="gi">+        pattern = re.compile(pattern)</span>

<span class="gd">-def memoizemethod_noargs(method: Callable) -&gt;Callable:</span>
<span class="gi">+    for chunk, offset in _chunk_iter():</span>
<span class="gi">+        matches = [match for match in pattern.finditer(chunk)]</span>
<span class="gi">+        if matches:</span>
<span class="gi">+            start, end = matches[-1].span()</span>
<span class="gi">+            return offset + start, offset + end</span>
<span class="gi">+    return None</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def memoizemethod_noargs(method: Callable) -&gt; Callable:</span>
<span class="w"> </span>    &quot;&quot;&quot;Decorator to cache the result of a method (without arguments) using a
<span class="w"> </span>    weak reference to its object
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cache: weakref.WeakKeyDictionary[Any, Any] = weakref.WeakKeyDictionary()</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(method)</span>
<span class="gi">+    def new_method(self: Any, *args: Any, **kwargs: Any) -&gt; Any:</span>
<span class="gi">+        if self not in cache:</span>
<span class="gi">+            cache[self] = method(self, *args, **kwargs)</span>
<span class="gi">+        return cache[self]</span>
<span class="gi">+</span>
<span class="gi">+    return new_method</span>


<span class="gd">-_BINARYCHARS = {i for i in range(32) if to_bytes(chr(i)) not in {b&#39;\x00&#39;,</span>
<span class="gd">-    b&#39;\t&#39;, b&#39;\n&#39;, b&#39;\r&#39;}}</span>
<span class="gi">+_BINARYCHARS = {</span>
<span class="gi">+    i for i in range(32) if to_bytes(chr(i)) not in {b&quot;\0&quot;, b&quot;\t&quot;, b&quot;\n&quot;, b&quot;\r&quot;}</span>
<span class="gi">+}</span>


<span class="gd">-def binary_is_text(data: bytes) -&gt;bool:</span>
<span class="gi">+def binary_is_text(data: bytes) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
<span class="w"> </span>    does not contain unprintable control characters.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not isinstance(data, bytes):</span>
<span class="gi">+        raise TypeError(f&quot;data must be bytes, got &#39;{type(data).__name__}&#39;&quot;)</span>
<span class="gi">+    return all(c not in _BINARYCHARS for c in data)</span>


<span class="gd">-def get_func_args(func: Callable, stripself: bool=False) -&gt;List[str]:</span>
<span class="gi">+def get_func_args(func: Callable, stripself: bool = False) -&gt; List[str]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the argument name list of a callable object&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def get_spec(func: Callable) -&gt;Tuple[List[str], Dict[str, Any]]:</span>
<span class="gi">+    if not callable(func):</span>
<span class="gi">+        raise TypeError(f&quot;func must be callable, got &#39;{type(func).__name__}&#39;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    args: List[str] = []</span>
<span class="gi">+    try:</span>
<span class="gi">+        sig = inspect.signature(func)</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        return args</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(func, partial):</span>
<span class="gi">+        partial_args = func.args</span>
<span class="gi">+        partial_kw = func.keywords</span>
<span class="gi">+</span>
<span class="gi">+        for name, param in sig.parameters.items():</span>
<span class="gi">+            if param.name in partial_args:</span>
<span class="gi">+                continue</span>
<span class="gi">+            if partial_kw and param.name in partial_kw:</span>
<span class="gi">+                continue</span>
<span class="gi">+            args.append(name)</span>
<span class="gi">+    else:</span>
<span class="gi">+        for name in sig.parameters.keys():</span>
<span class="gi">+            args.append(name)</span>
<span class="gi">+</span>
<span class="gi">+    if stripself and args and args[0] == &quot;self&quot;:</span>
<span class="gi">+        args = args[1:]</span>
<span class="gi">+    return args</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_spec(func: Callable) -&gt; Tuple[List[str], Dict[str, Any]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Returns (args, kwargs) tuple for a function
<span class="w"> </span>    &gt;&gt;&gt; import re
<span class="w"> </span>    &gt;&gt;&gt; get_spec(re.match)
<span class="gu">@@ -144,26 +249,66 @@ def get_spec(func: Callable) -&gt;Tuple[List[str], Dict[str, Any]]:</span>
<span class="w"> </span>    &gt;&gt;&gt; get_spec(Test().method)
<span class="w"> </span>    ([&#39;self&#39;, &#39;val&#39;], {&#39;flags&#39;: 0})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    if inspect.isfunction(func) or inspect.ismethod(func):</span>
<span class="gi">+        spec = inspect.getfullargspec(func)</span>
<span class="gi">+    elif hasattr(func, &quot;__call__&quot;):</span>
<span class="gi">+        spec = inspect.getfullargspec(func.__call__)</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise TypeError(f&quot;{type(func)} is not callable&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    defaults: Tuple[Any, ...] = spec.defaults or ()</span>

<span class="gd">-def equal_attributes(obj1: Any, obj2: Any, attributes: Optional[List[Union[</span>
<span class="gd">-    str, Callable]]]) -&gt;bool:</span>
<span class="gi">+    firstdefault = len(spec.args) - len(defaults)</span>
<span class="gi">+    args = spec.args[:firstdefault]</span>
<span class="gi">+    kwargs = dict(zip(spec.args[firstdefault:], defaults))</span>
<span class="gi">+    return args, kwargs</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def equal_attributes(</span>
<span class="gi">+    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable]]]</span>
<span class="gi">+) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Compare two objects attributes&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # not attributes given return False by default</span>
<span class="gi">+    if not attributes:</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    temp1, temp2 = object(), object()</span>
<span class="gi">+    for attr in attributes:</span>
<span class="gi">+        # support callables like itemgetter</span>
<span class="gi">+        if callable(attr):</span>
<span class="gi">+            if attr(obj1) != attr(obj2):</span>
<span class="gi">+                return False</span>
<span class="gi">+        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):</span>
<span class="gi">+            return False</span>
<span class="gi">+    # all attributes equal</span>
<span class="gi">+    return True</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def without_none_values(iterable: Mapping) -&gt; dict:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def without_none_values(iterable: Iterable) -&gt; Iterable:</span>
<span class="gi">+    ...</span>


<span class="gd">-def without_none_values(iterable: Union[Mapping, Iterable]) -&gt;Union[dict,</span>
<span class="gd">-    Iterable]:</span>
<span class="gi">+def without_none_values(iterable: Union[Mapping, Iterable]) -&gt; Union[dict, Iterable]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a copy of ``iterable`` with all ``None`` entries removed.

<span class="w"> </span>    If ``iterable`` is a mapping, return a dictionary where all pairs that have
<span class="w"> </span>    value ``None`` have been removed.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(iterable, collections.abc.Mapping):</span>
<span class="gi">+        return {k: v for k, v in iterable.items() if v is not None}</span>
<span class="gi">+    else:</span>
<span class="gi">+        # the iterable __init__ must take another iterable</span>
<span class="gi">+        return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]</span>


<span class="gd">-def global_object_name(obj: Any) -&gt;str:</span>
<span class="gi">+def global_object_name(obj: Any) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return full name of a global object.

<span class="gu">@@ -171,10 +316,20 @@ def global_object_name(obj: Any) -&gt;str:</span>
<span class="w"> </span>    &gt;&gt;&gt; global_object_name(Request)
<span class="w"> </span>    &#39;scrapy.http.request.Request&#39;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return f&quot;{obj.__module__}.{obj.__qualname__}&quot;</span>


<span class="gd">-if hasattr(sys, &#39;pypy_version_info&#39;):</span>
<span class="gi">+if hasattr(sys, &quot;pypy_version_info&quot;):</span>
<span class="gi">+</span>
<span class="gi">+    def garbage_collect() -&gt; None:</span>
<span class="gi">+        # Collecting weakreferences can take two collections on PyPy.</span>
<span class="gi">+        gc.collect()</span>
<span class="gi">+        gc.collect()</span>
<span class="gi">+</span>
<span class="gi">+else:</span>
<span class="gi">+</span>
<span class="gi">+    def garbage_collect() -&gt; None:</span>
<span class="gi">+        gc.collect()</span>


<span class="w"> </span>class MutableChain(Iterable):
<span class="gu">@@ -185,13 +340,22 @@ class MutableChain(Iterable):</span>
<span class="w"> </span>    def __init__(self, *args: Iterable):
<span class="w"> </span>        self.data = chain.from_iterable(args)

<span class="gd">-    def __iter__(self) -&gt;Iterator:</span>
<span class="gi">+    def extend(self, *iterables: Iterable) -&gt; None:</span>
<span class="gi">+        self.data = chain(self.data, chain.from_iterable(iterables))</span>
<span class="gi">+</span>
<span class="gi">+    def __iter__(self) -&gt; Iterator:</span>
<span class="w"> </span>        return self

<span class="gd">-    def __next__(self) -&gt;Any:</span>
<span class="gi">+    def __next__(self) -&gt; Any:</span>
<span class="w"> </span>        return next(self.data)


<span class="gi">+async def _async_chain(*iterables: Union[Iterable, AsyncIterable]) -&gt; AsyncGenerator:</span>
<span class="gi">+    for it in iterables:</span>
<span class="gi">+        async for o in as_async_generator(it):</span>
<span class="gi">+            yield o</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class MutableAsyncChain(AsyncIterable):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Similar to MutableChain but for async iterables
<span class="gu">@@ -200,8 +364,11 @@ class MutableAsyncChain(AsyncIterable):</span>
<span class="w"> </span>    def __init__(self, *args: Union[Iterable, AsyncIterable]):
<span class="w"> </span>        self.data = _async_chain(*args)

<span class="gd">-    def __aiter__(self) -&gt;AsyncIterator:</span>
<span class="gi">+    def extend(self, *iterables: Union[Iterable, AsyncIterable]) -&gt; None:</span>
<span class="gi">+        self.data = _async_chain(self.data, _async_chain(*iterables))</span>
<span class="gi">+</span>
<span class="gi">+    def __aiter__(self) -&gt; AsyncIterator:</span>
<span class="w"> </span>        return self

<span class="gd">-    async def __anext__(self) -&gt;Any:</span>
<span class="gi">+    async def __anext__(self) -&gt; Any:</span>
<span class="w"> </span>        return await self.data.__anext__()
<span class="gh">diff --git a/scrapy/utils/reactor.py b/scrapy/utils/reactor.py</span>
<span class="gh">index 5f7bcdfbd..ad3d1d8bc 100644</span>
<span class="gd">--- a/scrapy/utils/reactor.py</span>
<span class="gi">+++ b/scrapy/utils/reactor.py</span>
<span class="gu">@@ -4,15 +4,32 @@ from asyncio import AbstractEventLoop, AbstractEventLoopPolicy</span>
<span class="w"> </span>from contextlib import suppress
<span class="w"> </span>from typing import Any, Callable, Dict, Optional, Sequence, Type
<span class="w"> </span>from warnings import catch_warnings, filterwarnings, warn
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet import asyncioreactor, error
<span class="w"> </span>from twisted.internet.base import DelayedCall
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.misc import load_object


<span class="w"> </span>def listen_tcp(portrange, host, factory):
<span class="w"> </span>    &quot;&quot;&quot;Like reactor.listenTCP but tries different ports in a range.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    if len(portrange) &gt; 2:</span>
<span class="gi">+        raise ValueError(f&quot;invalid portrange: {portrange}&quot;)</span>
<span class="gi">+    if not portrange:</span>
<span class="gi">+        return reactor.listenTCP(0, factory, interface=host)</span>
<span class="gi">+    if not hasattr(portrange, &quot;__iter__&quot;):</span>
<span class="gi">+        return reactor.listenTCP(portrange, factory, interface=host)</span>
<span class="gi">+    if len(portrange) == 1:</span>
<span class="gi">+        return reactor.listenTCP(portrange[0], factory, interface=host)</span>
<span class="gi">+    for x in range(portrange[0], portrange[1] + 1):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return reactor.listenTCP(x, factory, interface=host)</span>
<span class="gi">+        except error.CannotListenError:</span>
<span class="gi">+            if x == portrange[1]:</span>
<span class="gi">+                raise</span>


<span class="w"> </span>class CallLaterOnce:
<span class="gu">@@ -26,34 +43,146 @@ class CallLaterOnce:</span>
<span class="w"> </span>        self._kw: Dict[str, Any] = kw
<span class="w"> </span>        self._call: Optional[DelayedCall] = None

<span class="gd">-    def __call__(self) -&gt;Any:</span>
<span class="gi">+    def schedule(self, delay: float = 0) -&gt; None:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        if self._call is None:</span>
<span class="gi">+            self._call = reactor.callLater(delay, self)</span>
<span class="gi">+</span>
<span class="gi">+    def cancel(self) -&gt; None:</span>
<span class="gi">+        if self._call:</span>
<span class="gi">+            self._call.cancel()</span>
<span class="gi">+</span>
<span class="gi">+    def __call__(self) -&gt; Any:</span>
<span class="w"> </span>        self._call = None
<span class="w"> </span>        return self._func(*self._a, **self._kw)


<span class="gd">-def set_asyncio_event_loop_policy() -&gt;None:</span>
<span class="gi">+def set_asyncio_event_loop_policy() -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;The policy functions from asyncio often behave unexpectedly,
<span class="w"> </span>    so we restrict their use to the absolutely essential case.
<span class="w"> </span>    This should only be used to install the reactor.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    _get_asyncio_event_loop_policy()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_asyncio_event_loop_policy() -&gt; AbstractEventLoopPolicy:</span>
<span class="gi">+    warn(</span>
<span class="gi">+        &quot;Call to deprecated function &quot;</span>
<span class="gi">+        &quot;scrapy.utils.reactor.get_asyncio_event_loop_policy().\n&quot;</span>
<span class="gi">+        &quot;\n&quot;</span>
<span class="gi">+        &quot;Please use get_event_loop, new_event_loop and set_event_loop&quot;</span>
<span class="gi">+        &quot; from asyncio instead, as the corresponding policy methods may lead&quot;</span>
<span class="gi">+        &quot; to unexpected behaviour.\n&quot;</span>
<span class="gi">+        &quot;This function is replaced by set_asyncio_event_loop_policy and&quot;</span>
<span class="gi">+        &quot; is meant to be used only when the reactor is being installed.&quot;,</span>
<span class="gi">+        category=ScrapyDeprecationWarning,</span>
<span class="gi">+        stacklevel=2,</span>
<span class="gi">+    )</span>
<span class="gi">+    return _get_asyncio_event_loop_policy()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_asyncio_event_loop_policy() -&gt; AbstractEventLoopPolicy:</span>
<span class="gi">+    policy = asyncio.get_event_loop_policy()</span>
<span class="gi">+    if (</span>
<span class="gi">+        sys.version_info &gt;= (3, 8)</span>
<span class="gi">+        and sys.platform == &quot;win32&quot;</span>
<span class="gi">+        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)</span>
<span class="gi">+    ):</span>
<span class="gi">+        policy = asyncio.WindowsSelectorEventLoopPolicy()</span>
<span class="gi">+        asyncio.set_event_loop_policy(policy)</span>
<span class="gi">+    return policy</span>


<span class="gd">-def install_reactor(reactor_path: str, event_loop_path: Optional[str]=None</span>
<span class="gd">-    ) -&gt;None:</span>
<span class="gi">+def install_reactor(reactor_path: str, event_loop_path: Optional[str] = None) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Installs the :mod:`~twisted.internet.reactor` with the specified
<span class="w"> </span>    import path. Also installs the asyncio event loop with the specified import
<span class="w"> </span>    path if the asyncio reactor is enabled&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    reactor_class = load_object(reactor_path)</span>
<span class="gi">+    if reactor_class is asyncioreactor.AsyncioSelectorReactor:</span>
<span class="gi">+        set_asyncio_event_loop_policy()</span>
<span class="gi">+        with suppress(error.ReactorAlreadyInstalledError):</span>
<span class="gi">+            event_loop = set_asyncio_event_loop(event_loop_path)</span>
<span class="gi">+            asyncioreactor.install(eventloop=event_loop)</span>
<span class="gi">+    else:</span>
<span class="gi">+        *module, _ = reactor_path.split(&quot;.&quot;)</span>
<span class="gi">+        installer_path = module + [&quot;install&quot;]</span>
<span class="gi">+        installer = load_object(&quot;.&quot;.join(installer_path))</span>
<span class="gi">+        with suppress(error.ReactorAlreadyInstalledError):</span>
<span class="gi">+            installer()</span>


<span class="gd">-def set_asyncio_event_loop(event_loop_path: Optional[str]) -&gt;AbstractEventLoop:</span>
<span class="gi">+def _get_asyncio_event_loop() -&gt; AbstractEventLoop:</span>
<span class="gi">+    return set_asyncio_event_loop(None)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def set_asyncio_event_loop(event_loop_path: Optional[str]) -&gt; AbstractEventLoop:</span>
<span class="w"> </span>    &quot;&quot;&quot;Sets and returns the event loop with specified import path.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if event_loop_path is not None:</span>
<span class="gi">+        event_loop_class: Type[AbstractEventLoop] = load_object(event_loop_path)</span>
<span class="gi">+        event_loop = event_loop_class()</span>
<span class="gi">+        asyncio.set_event_loop(event_loop)</span>
<span class="gi">+    else:</span>
<span class="gi">+        try:</span>
<span class="gi">+            with catch_warnings():</span>
<span class="gi">+                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning</span>
<span class="gi">+                # is emitted about the lack of a current event loop, because in</span>
<span class="gi">+                # Python 3.14 and later `get_event_loop` will raise a</span>
<span class="gi">+                # RuntimeError in that event. Because our code is already</span>
<span class="gi">+                # prepared for that future behavior, we ignore the deprecation</span>
<span class="gi">+                # warning.</span>
<span class="gi">+                filterwarnings(</span>
<span class="gi">+                    &quot;ignore&quot;,</span>
<span class="gi">+                    message=&quot;There is no current event loop&quot;,</span>
<span class="gi">+                    category=DeprecationWarning,</span>
<span class="gi">+                )</span>
<span class="gi">+                event_loop = asyncio.get_event_loop()</span>
<span class="gi">+        except RuntimeError:</span>
<span class="gi">+            # `get_event_loop` raises RuntimeError when called with no asyncio</span>
<span class="gi">+            # event loop yet installed in the following scenarios:</span>
<span class="gi">+            # - Previsibly on Python 3.14 and later.</span>
<span class="gi">+            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902</span>
<span class="gi">+            event_loop = asyncio.new_event_loop()</span>
<span class="gi">+            asyncio.set_event_loop(event_loop)</span>
<span class="gi">+    return event_loop</span>


<span class="gd">-def verify_installed_reactor(reactor_path: str) -&gt;None:</span>
<span class="gi">+def verify_installed_reactor(reactor_path: str) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Raises :exc:`Exception` if the installed
<span class="w"> </span>    :mod:`~twisted.internet.reactor` does not match the specified import
<span class="w"> </span>    path.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    reactor_class = load_object(reactor_path)</span>
<span class="gi">+    if not reactor.__class__ == reactor_class:</span>
<span class="gi">+        msg = (</span>
<span class="gi">+            &quot;The installed reactor &quot;</span>
<span class="gi">+            f&quot;({reactor.__module__}.{reactor.__class__.__name__}) does not &quot;</span>
<span class="gi">+            f&quot;match the requested one ({reactor_path})&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        raise Exception(msg)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def verify_installed_asyncio_event_loop(loop_path: str) -&gt; None:</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    loop_class = load_object(loop_path)</span>
<span class="gi">+    if isinstance(reactor._asyncioEventloop, loop_class):</span>
<span class="gi">+        return</span>
<span class="gi">+    installed = (</span>
<span class="gi">+        f&quot;{reactor._asyncioEventloop.__class__.__module__}&quot;</span>
<span class="gi">+        f&quot;.{reactor._asyncioEventloop.__class__.__qualname__}&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    specified = f&quot;{loop_class.__module__}.{loop_class.__qualname__}&quot;</span>
<span class="gi">+    raise Exception(</span>
<span class="gi">+        &quot;Scrapy found an asyncio Twisted reactor already &quot;</span>
<span class="gi">+        f&quot;installed, and its event loop class ({installed}) does &quot;</span>
<span class="gi">+        &quot;not match the one specified in the ASYNCIO_EVENT_LOOP &quot;</span>
<span class="gi">+        f&quot;setting ({specified})&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def is_asyncio_reactor_installed() -&gt; bool:</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)</span>
<span class="gh">diff --git a/scrapy/utils/request.py b/scrapy/utils/request.py</span>
<span class="gh">index 9514d35a1..24fcbd85e 100644</span>
<span class="gd">--- a/scrapy/utils/request.py</span>
<span class="gi">+++ b/scrapy/utils/request.py</span>
<span class="gu">@@ -2,27 +2,57 @@</span>
<span class="w"> </span>This module provides some useful functions for working with
<span class="w"> </span>scrapy.http.Request objects
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import hashlib
<span class="w"> </span>import json
<span class="w"> </span>import warnings
<span class="gd">-from typing import TYPE_CHECKING, Any, Dict, Generator, Iterable, List, Optional, Protocol, Tuple, Type, Union</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    Dict,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    List,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Protocol,</span>
<span class="gi">+    Tuple,</span>
<span class="gi">+    Type,</span>
<span class="gi">+    Union,</span>
<span class="gi">+)</span>
<span class="w"> </span>from urllib.parse import urlunparse
<span class="w"> </span>from weakref import WeakKeyDictionary
<span class="gi">+</span>
<span class="w"> </span>from w3lib.http import basic_auth_header
<span class="w"> </span>from w3lib.url import canonicalize_url
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request, Spider
<span class="w"> </span>from scrapy.exceptions import ScrapyDeprecationWarning
<span class="w"> </span>from scrapy.utils.httpobj import urlparse_cached
<span class="w"> </span>from scrapy.utils.misc import load_object
<span class="w"> </span>from scrapy.utils.python import to_bytes, to_unicode
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.crawler import Crawler
<span class="gd">-_deprecated_fingerprint_cache: &#39;WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]&#39;</span>
<span class="gi">+</span>
<span class="gi">+_deprecated_fingerprint_cache: &quot;WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]&quot;</span>
<span class="w"> </span>_deprecated_fingerprint_cache = WeakKeyDictionary()


<span class="gd">-def request_fingerprint(request: Request, include_headers: Optional[</span>
<span class="gd">-    Iterable[Union[bytes, str]]]=None, keep_fragments: bool=False) -&gt;str:</span>
<span class="gi">+def _serialize_headers(</span>
<span class="gi">+    headers: Iterable[bytes], request: Request</span>
<span class="gi">+) -&gt; Generator[bytes, Any, None]:</span>
<span class="gi">+    for header in headers:</span>
<span class="gi">+        if header in request.headers:</span>
<span class="gi">+            yield header</span>
<span class="gi">+            for value in request.headers.getlist(header):</span>
<span class="gi">+                yield value</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def request_fingerprint(</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    include_headers: Optional[Iterable[Union[bytes, str]]] = None,</span>
<span class="gi">+    keep_fragments: bool = False,</span>
<span class="gi">+) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return the request fingerprint as an hexadecimal string.

<span class="gu">@@ -53,15 +83,95 @@ def request_fingerprint(request: Request, include_headers: Optional[</span>
<span class="w"> </span>    If you want to include them, set the keep_fragments argument to True
<span class="w"> </span>    (for instance when handling requests with a headless browser).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-_fingerprint_cache: &#39;WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]&#39;</span>
<span class="gi">+    if include_headers or keep_fragments:</span>
<span class="gi">+        message = (</span>
<span class="gi">+            &quot;Call to deprecated function &quot;</span>
<span class="gi">+            &quot;scrapy.utils.request.request_fingerprint().\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;If you are using this function in a Scrapy component because you &quot;</span>
<span class="gi">+            &quot;need a non-default fingerprinting algorithm, and you are OK &quot;</span>
<span class="gi">+            &quot;with that non-default fingerprinting algorithm being used by &quot;</span>
<span class="gi">+            &quot;all Scrapy components and not just the one calling this &quot;</span>
<span class="gi">+            &quot;function, use crawler.request_fingerprinter.fingerprint() &quot;</span>
<span class="gi">+            &quot;instead in your Scrapy component (you can get the crawler &quot;</span>
<span class="gi">+            &quot;object from the &#39;from_crawler&#39; class method), and use the &quot;</span>
<span class="gi">+            &quot;&#39;REQUEST_FINGERPRINTER_CLASS&#39; setting to configure your &quot;</span>
<span class="gi">+            &quot;non-default fingerprinting algorithm.\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;Otherwise, consider using the &quot;</span>
<span class="gi">+            &quot;scrapy.utils.request.fingerprint() function instead.\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;If you switch to &#39;fingerprint()&#39;, or assign the &quot;</span>
<span class="gi">+            &quot;&#39;REQUEST_FINGERPRINTER_CLASS&#39; setting a class that uses &quot;</span>
<span class="gi">+            &quot;&#39;fingerprint()&#39;, the generated fingerprints will not only be &quot;</span>
<span class="gi">+            &quot;bytes instead of a string, but they will also be different from &quot;</span>
<span class="gi">+            &quot;those generated by &#39;request_fingerprint()&#39;. Before you switch, &quot;</span>
<span class="gi">+            &quot;make sure that you understand the consequences of this (e.g. &quot;</span>
<span class="gi">+            &quot;cache invalidation) and are OK with them; otherwise, consider &quot;</span>
<span class="gi">+            &quot;implementing your own function which returns the same &quot;</span>
<span class="gi">+            &quot;fingerprints as the deprecated &#39;request_fingerprint()&#39; function.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        message = (</span>
<span class="gi">+            &quot;Call to deprecated function &quot;</span>
<span class="gi">+            &quot;scrapy.utils.request.request_fingerprint().\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;If you are using this function in a Scrapy component, and you &quot;</span>
<span class="gi">+            &quot;are OK with users of your component changing the fingerprinting &quot;</span>
<span class="gi">+            &quot;algorithm through settings, use &quot;</span>
<span class="gi">+            &quot;crawler.request_fingerprinter.fingerprint() instead in your &quot;</span>
<span class="gi">+            &quot;Scrapy component (you can get the crawler object from the &quot;</span>
<span class="gi">+            &quot;&#39;from_crawler&#39; class method).\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;Otherwise, consider using the &quot;</span>
<span class="gi">+            &quot;scrapy.utils.request.fingerprint() function instead.\n&quot;</span>
<span class="gi">+            &quot;\n&quot;</span>
<span class="gi">+            &quot;Either way, the resulting fingerprints will be returned as &quot;</span>
<span class="gi">+            &quot;bytes, not as a string, and they will also be different from &quot;</span>
<span class="gi">+            &quot;those generated by &#39;request_fingerprint()&#39;. Before you switch, &quot;</span>
<span class="gi">+            &quot;make sure that you understand the consequences of this (e.g. &quot;</span>
<span class="gi">+            &quot;cache invalidation) and are OK with them; otherwise, consider &quot;</span>
<span class="gi">+            &quot;implementing your own function which returns the same &quot;</span>
<span class="gi">+            &quot;fingerprints as the deprecated &#39;request_fingerprint()&#39; function.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="gi">+    processed_include_headers: Optional[Tuple[bytes, ...]] = None</span>
<span class="gi">+    if include_headers:</span>
<span class="gi">+        processed_include_headers = tuple(</span>
<span class="gi">+            to_bytes(h.lower()) for h in sorted(include_headers)</span>
<span class="gi">+        )</span>
<span class="gi">+    cache = _deprecated_fingerprint_cache.setdefault(request, {})</span>
<span class="gi">+    cache_key = (processed_include_headers, keep_fragments)</span>
<span class="gi">+    if cache_key not in cache:</span>
<span class="gi">+        fp = hashlib.sha1()</span>
<span class="gi">+        fp.update(to_bytes(request.method))</span>
<span class="gi">+        fp.update(</span>
<span class="gi">+            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))</span>
<span class="gi">+        )</span>
<span class="gi">+        fp.update(request.body or b&quot;&quot;)</span>
<span class="gi">+        if processed_include_headers:</span>
<span class="gi">+            for part in _serialize_headers(processed_include_headers, request):</span>
<span class="gi">+                fp.update(part)</span>
<span class="gi">+        cache[cache_key] = fp.hexdigest()</span>
<span class="gi">+    return cache[cache_key]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _request_fingerprint_as_bytes(*args: Any, **kwargs: Any) -&gt; bytes:</span>
<span class="gi">+    with warnings.catch_warnings():</span>
<span class="gi">+        warnings.simplefilter(&quot;ignore&quot;)</span>
<span class="gi">+        return bytes.fromhex(request_fingerprint(*args, **kwargs))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_fingerprint_cache: &quot;WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]&quot;</span>
<span class="w"> </span>_fingerprint_cache = WeakKeyDictionary()


<span class="gd">-def fingerprint(request: Request, *, include_headers: Optional[Iterable[</span>
<span class="gd">-    Union[bytes, str]]]=None, keep_fragments: bool=False) -&gt;bytes:</span>
<span class="gi">+def fingerprint(</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    *,</span>
<span class="gi">+    include_headers: Optional[Iterable[Union[bytes, str]]] = None,</span>
<span class="gi">+    keep_fragments: bool = False,</span>
<span class="gi">+) -&gt; bytes:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return the request fingerprint.

<span class="gu">@@ -92,11 +202,38 @@ def fingerprint(request: Request, *, include_headers: Optional[Iterable[</span>
<span class="w"> </span>    If you want to include them, set the keep_fragments argument to True
<span class="w"> </span>    (for instance when handling requests with a headless browser).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    processed_include_headers: Optional[Tuple[bytes, ...]] = None</span>
<span class="gi">+    if include_headers:</span>
<span class="gi">+        processed_include_headers = tuple(</span>
<span class="gi">+            to_bytes(h.lower()) for h in sorted(include_headers)</span>
<span class="gi">+        )</span>
<span class="gi">+    cache = _fingerprint_cache.setdefault(request, {})</span>
<span class="gi">+    cache_key = (processed_include_headers, keep_fragments)</span>
<span class="gi">+    if cache_key not in cache:</span>
<span class="gi">+        # To decode bytes reliably (JSON does not support bytes), regardless of</span>
<span class="gi">+        # character encoding, we use bytes.hex()</span>
<span class="gi">+        headers: Dict[str, List[str]] = {}</span>
<span class="gi">+        if processed_include_headers:</span>
<span class="gi">+            for header in processed_include_headers:</span>
<span class="gi">+                if header in request.headers:</span>
<span class="gi">+                    headers[header.hex()] = [</span>
<span class="gi">+                        header_value.hex()</span>
<span class="gi">+                        for header_value in request.headers.getlist(header)</span>
<span class="gi">+                    ]</span>
<span class="gi">+        fingerprint_data = {</span>
<span class="gi">+            &quot;method&quot;: to_unicode(request.method),</span>
<span class="gi">+            &quot;url&quot;: canonicalize_url(request.url, keep_fragments=keep_fragments),</span>
<span class="gi">+            &quot;body&quot;: (request.body or b&quot;&quot;).hex(),</span>
<span class="gi">+            &quot;headers&quot;: headers,</span>
<span class="gi">+        }</span>
<span class="gi">+        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)</span>
<span class="gi">+        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()</span>
<span class="gi">+    return cache[cache_key]</span>


<span class="w"> </span>class RequestFingerprinterProtocol(Protocol):
<span class="gd">-    pass</span>
<span class="gi">+    def fingerprint(self, request: Request) -&gt; bytes:</span>
<span class="gi">+        ...</span>


<span class="w"> </span>class RequestFingerprinter:
<span class="gu">@@ -112,70 +249,134 @@ class RequestFingerprinter:</span>
<span class="w"> </span>    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, crawler: Optional[&#39;Crawler&#39;]=None):</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def from_crawler(cls, crawler):</span>
<span class="gi">+        return cls(crawler)</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, crawler: Optional[&quot;Crawler&quot;] = None):</span>
<span class="w"> </span>        if crawler:
<span class="w"> </span>            implementation = crawler.settings.get(
<span class="gd">-                &#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39;)</span>
<span class="gi">+                &quot;REQUEST_FINGERPRINTER_IMPLEMENTATION&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>        else:
<span class="gd">-            implementation = &#39;2.6&#39;</span>
<span class="gd">-        if implementation == &#39;2.6&#39;:</span>
<span class="gd">-            message = &quot;&quot;&quot;&#39;2.6&#39; is a deprecated value for the &#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting.</span>
<span class="gd">-</span>
<span class="gd">-It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the &#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.</span>
<span class="gd">-</span>
<span class="gd">-See the documentation of the &#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting for information on how to handle this deprecation.&quot;&quot;&quot;</span>
<span class="gd">-            warnings.warn(message, category=ScrapyDeprecationWarning,</span>
<span class="gd">-                stacklevel=2)</span>
<span class="gi">+            implementation = &quot;2.6&quot;</span>
<span class="gi">+        if implementation == &quot;2.6&quot;:</span>
<span class="gi">+            message = (</span>
<span class="gi">+                &quot;&#39;2.6&#39; is a deprecated value for the &quot;</span>
<span class="gi">+                &quot;&#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting.\n&quot;</span>
<span class="gi">+                &quot;\n&quot;</span>
<span class="gi">+                &quot;It is also the default value. In other words, it is normal &quot;</span>
<span class="gi">+                &quot;to get this warning if you have not defined a value for the &quot;</span>
<span class="gi">+                &quot;&#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting. This is so &quot;</span>
<span class="gi">+                &quot;for backward compatibility reasons, but it will change in a &quot;</span>
<span class="gi">+                &quot;future version of Scrapy.\n&quot;</span>
<span class="gi">+                &quot;\n&quot;</span>
<span class="gi">+                &quot;See the documentation of the &quot;</span>
<span class="gi">+                &quot;&#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39; setting for &quot;</span>
<span class="gi">+                &quot;information on how to handle this deprecation.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)</span>
<span class="w"> </span>            self._fingerprint = _request_fingerprint_as_bytes
<span class="gd">-        elif implementation == &#39;2.7&#39;:</span>
<span class="gi">+        elif implementation == &quot;2.7&quot;:</span>
<span class="w"> </span>            self._fingerprint = fingerprint
<span class="w"> </span>        else:
<span class="w"> </span>            raise ValueError(
<span class="gd">-                f&quot;Got an invalid value on setting &#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39;: {implementation!r}. Valid values are &#39;2.6&#39; (deprecated) and &#39;2.7&#39;.&quot;</span>
<span class="gd">-                )</span>
<span class="gi">+                f&quot;Got an invalid value on setting &quot;</span>
<span class="gi">+                f&quot;&#39;REQUEST_FINGERPRINTER_IMPLEMENTATION&#39;: &quot;</span>
<span class="gi">+                f&quot;{implementation!r}. Valid values are &#39;2.6&#39; (deprecated) &quot;</span>
<span class="gi">+                f&quot;and &#39;2.7&#39;.&quot;</span>
<span class="gi">+            )</span>

<span class="gi">+    def fingerprint(self, request: Request) -&gt; bytes:</span>
<span class="gi">+        return self._fingerprint(request)</span>

<span class="gd">-def request_authenticate(request: Request, username: str, password: str</span>
<span class="gd">-    ) -&gt;None:</span>
<span class="gi">+</span>
<span class="gi">+def request_authenticate(</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    username: str,</span>
<span class="gi">+    password: str,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Authenticate the given request (in place) using the HTTP basic access
<span class="w"> </span>    authentication mechanism (RFC 2617) and the given username and password
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    request.headers[&quot;Authorization&quot;] = basic_auth_header(username, password)</span>


<span class="gd">-def request_httprepr(request: Request) -&gt;bytes:</span>
<span class="gi">+def request_httprepr(request: Request) -&gt; bytes:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the raw HTTP representation (as bytes) of the given request.
<span class="w"> </span>    This is provided only for reference since it&#39;s not the actual stream of
<span class="w"> </span>    bytes that will be send when performing the request (that&#39;s controlled
<span class="w"> </span>    by Twisted).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def referer_str(request: Request) -&gt;Optional[str]:</span>
<span class="gi">+    parsed = urlparse_cached(request)</span>
<span class="gi">+    path = urlunparse((&quot;&quot;, &quot;&quot;, parsed.path or &quot;/&quot;, parsed.params, parsed.query, &quot;&quot;))</span>
<span class="gi">+    s = to_bytes(request.method) + b&quot; &quot; + to_bytes(path) + b&quot; HTTP/1.1\r\n&quot;</span>
<span class="gi">+    s += b&quot;Host: &quot; + to_bytes(parsed.hostname or b&quot;&quot;) + b&quot;\r\n&quot;</span>
<span class="gi">+    if request.headers:</span>
<span class="gi">+        s += request.headers.to_string() + b&quot;\r\n&quot;</span>
<span class="gi">+    s += b&quot;\r\n&quot;</span>
<span class="gi">+    s += request.body</span>
<span class="gi">+    return s</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def referer_str(request: Request) -&gt; Optional[str]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return Referer HTTP header suitable for logging.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    referrer = request.headers.get(&quot;Referer&quot;)</span>
<span class="gi">+    if referrer is None:</span>
<span class="gi">+        return referrer</span>
<span class="gi">+    return to_unicode(referrer, errors=&quot;replace&quot;)</span>


<span class="gd">-def request_from_dict(d: dict, *, spider: Optional[Spider]=None) -&gt;Request:</span>
<span class="gi">+def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -&gt; Request:</span>
<span class="w"> </span>    &quot;&quot;&quot;Create a :class:`~scrapy.Request` object from a dict.

<span class="w"> </span>    If a spider is given, it will try to resolve the callbacks looking at the
<span class="w"> </span>    spider for methods with the same name.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    request_cls: Type[Request] = load_object(d[&quot;_class&quot;]) if &quot;_class&quot; in d else Request</span>
<span class="gi">+    kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}</span>
<span class="gi">+    if d.get(&quot;callback&quot;) and spider:</span>
<span class="gi">+        kwargs[&quot;callback&quot;] = _get_method(spider, d[&quot;callback&quot;])</span>
<span class="gi">+    if d.get(&quot;errback&quot;) and spider:</span>
<span class="gi">+        kwargs[&quot;errback&quot;] = _get_method(spider, d[&quot;errback&quot;])</span>
<span class="gi">+    return request_cls(**kwargs)</span>


<span class="gd">-def _get_method(obj: Any, name: Any) -&gt;Any:</span>
<span class="gi">+def _get_method(obj: Any, name: Any) -&gt; Any:</span>
<span class="w"> </span>    &quot;&quot;&quot;Helper function for request_from_dict&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    name = str(name)</span>
<span class="gi">+    try:</span>
<span class="gi">+        return getattr(obj, name)</span>
<span class="gi">+    except AttributeError:</span>
<span class="gi">+        raise ValueError(f&quot;Method {name!r} not found in: {obj}&quot;)</span>


<span class="gd">-def request_to_curl(request: Request) -&gt;str:</span>
<span class="gi">+def request_to_curl(request: Request) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Converts a :class:`~scrapy.Request` object to a curl command.

<span class="w"> </span>    :param :class:`~scrapy.Request`: Request object to be converted
<span class="w"> </span>    :return: string containing the curl command
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    method = request.method</span>
<span class="gi">+</span>
<span class="gi">+    data = f&quot;--data-raw &#39;{request.body.decode(&#39;utf-8&#39;)}&#39;&quot; if request.body else &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    headers = &quot; &quot;.join(</span>
<span class="gi">+        f&quot;-H &#39;{k.decode()}: {v[0].decode()}&#39;&quot; for k, v in request.headers.items()</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    url = request.url</span>
<span class="gi">+    cookies = &quot;&quot;</span>
<span class="gi">+    if request.cookies:</span>
<span class="gi">+        if isinstance(request.cookies, dict):</span>
<span class="gi">+            cookie = &quot;; &quot;.join(f&quot;{k}={v}&quot; for k, v in request.cookies.items())</span>
<span class="gi">+            cookies = f&quot;--cookie &#39;{cookie}&#39;&quot;</span>
<span class="gi">+        elif isinstance(request.cookies, list):</span>
<span class="gi">+            cookie = &quot;; &quot;.join(</span>
<span class="gi">+                f&quot;{list(c.keys())[0]}={list(c.values())[0]}&quot; for c in request.cookies</span>
<span class="gi">+            )</span>
<span class="gi">+            cookies = f&quot;--cookie &#39;{cookie}&#39;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    curl_cmd = f&quot;curl -X {method} {url} {data} {headers} {cookies}&quot;.strip()</span>
<span class="gi">+    return &quot; &quot;.join(curl_cmd.split())</span>
<span class="gh">diff --git a/scrapy/utils/response.py b/scrapy/utils/response.py</span>
<span class="gh">index ce657fa90..fabfb1167 100644</span>
<span class="gd">--- a/scrapy/utils/response.py</span>
<span class="gi">+++ b/scrapy/utils/response.py</span>
<span class="gu">@@ -8,50 +8,91 @@ import tempfile</span>
<span class="w"> </span>import webbrowser
<span class="w"> </span>from typing import Any, Callable, Iterable, Tuple, Union
<span class="w"> </span>from weakref import WeakKeyDictionary
<span class="gi">+</span>
<span class="w"> </span>from twisted.web import http
<span class="w"> </span>from w3lib import html
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.http.response import Response
<span class="w"> </span>from scrapy.utils.decorators import deprecated
<span class="w"> </span>from scrapy.utils.python import to_bytes, to_unicode
<span class="gd">-_baseurl_cache: &#39;WeakKeyDictionary[Response, str]&#39; = WeakKeyDictionary()</span>
<span class="gi">+</span>
<span class="gi">+_baseurl_cache: &quot;WeakKeyDictionary[Response, str]&quot; = WeakKeyDictionary()</span>


<span class="gd">-def get_base_url(response: &#39;scrapy.http.response.text.TextResponse&#39;) -&gt;str:</span>
<span class="gi">+def get_base_url(response: &quot;scrapy.http.response.text.TextResponse&quot;) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the base url of the given response, joined with the response url&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if response not in _baseurl_cache:</span>
<span class="gi">+        text = response.text[0:4096]</span>
<span class="gi">+        _baseurl_cache[response] = html.get_base_url(</span>
<span class="gi">+            text, response.url, response.encoding</span>
<span class="gi">+        )</span>
<span class="gi">+    return _baseurl_cache[response]</span>


<span class="gd">-(_metaref_cache:</span>
<span class="gd">-    &#39;WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]&#39;</span>
<span class="gd">-    ) = WeakKeyDictionary()</span>
<span class="gi">+_metaref_cache: &quot;WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]&quot; = (</span>
<span class="gi">+    WeakKeyDictionary()</span>
<span class="gi">+)</span>


<span class="gd">-def get_meta_refresh(response: &#39;scrapy.http.response.text.TextResponse&#39;,</span>
<span class="gd">-    ignore_tags: Iterable[str]=(&#39;script&#39;, &#39;noscript&#39;)) -&gt;Union[Tuple[None,</span>
<span class="gd">-    None], Tuple[float, str]]:</span>
<span class="gi">+def get_meta_refresh(</span>
<span class="gi">+    response: &quot;scrapy.http.response.text.TextResponse&quot;,</span>
<span class="gi">+    ignore_tags: Iterable[str] = (&quot;script&quot;, &quot;noscript&quot;),</span>
<span class="gi">+) -&gt; Union[Tuple[None, None], Tuple[float, str]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Parse the http-equiv refresh parameter from the given response&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if response not in _metaref_cache:</span>
<span class="gi">+        text = response.text[0:4096]</span>
<span class="gi">+        _metaref_cache[response] = html.get_meta_refresh(</span>
<span class="gi">+            text, response.url, response.encoding, ignore_tags=ignore_tags</span>
<span class="gi">+        )</span>
<span class="gi">+    return _metaref_cache[response]</span>


<span class="gd">-def response_status_message(status: Union[bytes, float, int, str]) -&gt;str:</span>
<span class="gi">+def response_status_message(status: Union[bytes, float, int, str]) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return status code plus status text descriptive message&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    status_int = int(status)</span>
<span class="gi">+    message = http.RESPONSES.get(status_int, &quot;Unknown Status&quot;)</span>
<span class="gi">+    return f&quot;{status_int} {to_unicode(message)}&quot;</span>


<span class="w"> </span>@deprecated
<span class="gd">-def response_httprepr(response: Response) -&gt;bytes:</span>
<span class="gi">+def response_httprepr(response: Response) -&gt; bytes:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return raw HTTP representation (as bytes) of the given response. This
<span class="w"> </span>    is provided only for reference, since it&#39;s not the exact stream of bytes
<span class="w"> </span>    that was received (that&#39;s not exposed by Twisted).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def open_in_browser(response: Union[</span>
<span class="gd">-    &#39;scrapy.http.response.html.HtmlResponse&#39;,</span>
<span class="gd">-    &#39;scrapy.http.response.text.TextResponse&#39;], _openfunc: Callable[[str],</span>
<span class="gd">-    Any]=webbrowser.open) -&gt;Any:</span>
<span class="gi">+    values = [</span>
<span class="gi">+        b&quot;HTTP/1.1 &quot;,</span>
<span class="gi">+        to_bytes(str(response.status)),</span>
<span class="gi">+        b&quot; &quot;,</span>
<span class="gi">+        to_bytes(http.RESPONSES.get(response.status, b&quot;&quot;)),</span>
<span class="gi">+        b&quot;\r\n&quot;,</span>
<span class="gi">+    ]</span>
<span class="gi">+    if response.headers:</span>
<span class="gi">+        values.extend([response.headers.to_string(), b&quot;\r\n&quot;])</span>
<span class="gi">+    values.extend([b&quot;\r\n&quot;, response.body])</span>
<span class="gi">+    return b&quot;&quot;.join(values)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _remove_html_comments(body):</span>
<span class="gi">+    start = body.find(b&quot;&lt;!--&quot;)</span>
<span class="gi">+    while start != -1:</span>
<span class="gi">+        end = body.find(b&quot;--&gt;&quot;, start + 1)</span>
<span class="gi">+        if end == -1:</span>
<span class="gi">+            return body[:start]</span>
<span class="gi">+        else:</span>
<span class="gi">+            body = body[:start] + body[end + 3 :]</span>
<span class="gi">+            start = body.find(b&quot;&lt;!--&quot;)</span>
<span class="gi">+    return body</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def open_in_browser(</span>
<span class="gi">+    response: Union[</span>
<span class="gi">+        &quot;scrapy.http.response.html.HtmlResponse&quot;,</span>
<span class="gi">+        &quot;scrapy.http.response.text.TextResponse&quot;,</span>
<span class="gi">+    ],</span>
<span class="gi">+    _openfunc: Callable[[str], Any] = webbrowser.open,</span>
<span class="gi">+) -&gt; Any:</span>
<span class="w"> </span>    &quot;&quot;&quot;Open *response* in a local web browser, adjusting the `base tag`_ for
<span class="w"> </span>    external links to work, e.g. so that images and styles are displayed.

<span class="gu">@@ -68,4 +109,21 @@ def open_in_browser(response: Union[</span>
<span class="w"> </span>            if &quot;item name&quot; not in response.body:
<span class="w"> </span>                open_in_browser(response)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from scrapy.http import HtmlResponse, TextResponse</span>
<span class="gi">+</span>
<span class="gi">+    # XXX: this implementation is a bit dirty and could be improved</span>
<span class="gi">+    body = response.body</span>
<span class="gi">+    if isinstance(response, HtmlResponse):</span>
<span class="gi">+        if b&quot;&lt;base&quot; not in body:</span>
<span class="gi">+            _remove_html_comments(body)</span>
<span class="gi">+            repl = rf&#39;\0&lt;base href=&quot;{response.url}&quot;&gt;&#39;</span>
<span class="gi">+            body = re.sub(rb&quot;&lt;head(?:[^&lt;&gt;]*?&gt;)&quot;, to_bytes(repl), body, count=1)</span>
<span class="gi">+        ext = &quot;.html&quot;</span>
<span class="gi">+    elif isinstance(response, TextResponse):</span>
<span class="gi">+        ext = &quot;.txt&quot;</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise TypeError(&quot;Unsupported response type: &quot; f&quot;{response.__class__.__name__}&quot;)</span>
<span class="gi">+    fd, fname = tempfile.mkstemp(ext)</span>
<span class="gi">+    os.write(fd, body)</span>
<span class="gi">+    os.close(fd)</span>
<span class="gi">+    return _openfunc(f&quot;file://{fname}&quot;)</span>
<span class="gh">diff --git a/scrapy/utils/serialize.py b/scrapy/utils/serialize.py</span>
<span class="gh">index 349bc19f4..3b4f67f00 100644</span>
<span class="gd">--- a/scrapy/utils/serialize.py</span>
<span class="gi">+++ b/scrapy/utils/serialize.py</span>
<span class="gu">@@ -2,14 +2,37 @@ import datetime</span>
<span class="w"> </span>import decimal
<span class="w"> </span>import json
<span class="w"> </span>from typing import Any
<span class="gi">+</span>
<span class="w"> </span>from itemadapter import ItemAdapter, is_item
<span class="w"> </span>from twisted.internet import defer
<span class="gi">+</span>
<span class="w"> </span>from scrapy.http import Request, Response


<span class="w"> </span>class ScrapyJSONEncoder(json.JSONEncoder):
<span class="gd">-    DATE_FORMAT = &#39;%Y-%m-%d&#39;</span>
<span class="gd">-    TIME_FORMAT = &#39;%H:%M:%S&#39;</span>
<span class="gi">+    DATE_FORMAT = &quot;%Y-%m-%d&quot;</span>
<span class="gi">+    TIME_FORMAT = &quot;%H:%M:%S&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def default(self, o: Any) -&gt; Any:</span>
<span class="gi">+        if isinstance(o, set):</span>
<span class="gi">+            return list(o)</span>
<span class="gi">+        if isinstance(o, datetime.datetime):</span>
<span class="gi">+            return o.strftime(f&quot;{self.DATE_FORMAT} {self.TIME_FORMAT}&quot;)</span>
<span class="gi">+        if isinstance(o, datetime.date):</span>
<span class="gi">+            return o.strftime(self.DATE_FORMAT)</span>
<span class="gi">+        if isinstance(o, datetime.time):</span>
<span class="gi">+            return o.strftime(self.TIME_FORMAT)</span>
<span class="gi">+        if isinstance(o, decimal.Decimal):</span>
<span class="gi">+            return str(o)</span>
<span class="gi">+        if isinstance(o, defer.Deferred):</span>
<span class="gi">+            return str(o)</span>
<span class="gi">+        if is_item(o):</span>
<span class="gi">+            return ItemAdapter(o).asdict()</span>
<span class="gi">+        if isinstance(o, Request):</span>
<span class="gi">+            return f&quot;&lt;{type(o).__name__} {o.method} {o.url}&gt;&quot;</span>
<span class="gi">+        if isinstance(o, Response):</span>
<span class="gi">+            return f&quot;&lt;{type(o).__name__} {o.status} {o.url}&gt;&quot;</span>
<span class="gi">+        return super().default(o)</span>


<span class="w"> </span>class ScrapyJSONDecoder(json.JSONDecoder):
<span class="gh">diff --git a/scrapy/utils/signal.py b/scrapy/utils/signal.py</span>
<span class="gh">index ba9aa6bf8..21a12a19e 100644</span>
<span class="gd">--- a/scrapy/utils/signal.py</span>
<span class="gi">+++ b/scrapy/utils/signal.py</span>
<span class="gu">@@ -3,36 +3,110 @@ import collections.abc</span>
<span class="w"> </span>import logging
<span class="w"> </span>from typing import Any as TypingAny
<span class="w"> </span>from typing import List, Tuple
<span class="gd">-from pydispatch.dispatcher import Anonymous, Any, disconnect, getAllReceivers, liveReceivers</span>
<span class="gi">+</span>
<span class="gi">+from pydispatch.dispatcher import (</span>
<span class="gi">+    Anonymous,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    disconnect,</span>
<span class="gi">+    getAllReceivers,</span>
<span class="gi">+    liveReceivers,</span>
<span class="gi">+)</span>
<span class="w"> </span>from pydispatch.robustapply import robustApply
<span class="w"> </span>from twisted.internet.defer import Deferred, DeferredList
<span class="w"> </span>from twisted.python.failure import Failure
<span class="gi">+</span>
<span class="w"> </span>from scrapy.exceptions import StopDownload
<span class="w"> </span>from scrapy.utils.defer import maybeDeferred_coro
<span class="w"> </span>from scrapy.utils.log import failure_to_exc_info
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)


<span class="gd">-def send_catch_log(signal: TypingAny=Any, sender: TypingAny=Anonymous, *</span>
<span class="gd">-    arguments: TypingAny, **named: TypingAny) -&gt;List[Tuple[TypingAny,</span>
<span class="gd">-    TypingAny]]:</span>
<span class="gi">+def send_catch_log(</span>
<span class="gi">+    signal: TypingAny = Any,</span>
<span class="gi">+    sender: TypingAny = Anonymous,</span>
<span class="gi">+    *arguments: TypingAny,</span>
<span class="gi">+    **named: TypingAny</span>
<span class="gi">+) -&gt; List[Tuple[TypingAny, TypingAny]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Like pydispatcher.robust.sendRobust but it also logs errors and returns
<span class="w"> </span>    Failures instead of exceptions.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    dont_log = named.pop(&quot;dont_log&quot;, ())</span>
<span class="gi">+    dont_log = (</span>
<span class="gi">+        tuple(dont_log)</span>
<span class="gi">+        if isinstance(dont_log, collections.abc.Sequence)</span>
<span class="gi">+        else (dont_log,)</span>
<span class="gi">+    )</span>
<span class="gi">+    dont_log += (StopDownload,)</span>
<span class="gi">+    spider = named.get(&quot;spider&quot;, None)</span>
<span class="gi">+    responses: List[Tuple[TypingAny, TypingAny]] = []</span>
<span class="gi">+    for receiver in liveReceivers(getAllReceivers(sender, signal)):</span>
<span class="gi">+        result: TypingAny</span>
<span class="gi">+        try:</span>
<span class="gi">+            response = robustApply(</span>
<span class="gi">+                receiver, signal=signal, sender=sender, *arguments, **named</span>
<span class="gi">+            )</span>
<span class="gi">+            if isinstance(response, Deferred):</span>
<span class="gi">+                logger.error(</span>
<span class="gi">+                    &quot;Cannot return deferreds from signal handler: %(receiver)s&quot;,</span>
<span class="gi">+                    {&quot;receiver&quot;: receiver},</span>
<span class="gi">+                    extra={&quot;spider&quot;: spider},</span>
<span class="gi">+                )</span>
<span class="gi">+        except dont_log:</span>
<span class="gi">+            result = Failure()</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            result = Failure()</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;Error caught on signal handler: %(receiver)s&quot;,</span>
<span class="gi">+                {&quot;receiver&quot;: receiver},</span>
<span class="gi">+                exc_info=True,</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            result = response</span>
<span class="gi">+        responses.append((receiver, result))</span>
<span class="gi">+    return responses</span>


<span class="gd">-def send_catch_log_deferred(signal: TypingAny=Any, sender: TypingAny=</span>
<span class="gd">-    Anonymous, *arguments: TypingAny, **named: TypingAny) -&gt;Deferred:</span>
<span class="gi">+def send_catch_log_deferred(</span>
<span class="gi">+    signal: TypingAny = Any,</span>
<span class="gi">+    sender: TypingAny = Anonymous,</span>
<span class="gi">+    *arguments: TypingAny,</span>
<span class="gi">+    **named: TypingAny</span>
<span class="gi">+) -&gt; Deferred:</span>
<span class="w"> </span>    &quot;&quot;&quot;Like send_catch_log but supports returning deferreds on signal handlers.
<span class="w"> </span>    Returns a deferred that gets fired once all signal handlers deferreds were
<span class="w"> </span>    fired.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def logerror(failure: Failure, recv: Any) -&gt; Failure:</span>
<span class="gi">+        if dont_log is None or not isinstance(failure.value, dont_log):</span>
<span class="gi">+            logger.error(</span>
<span class="gi">+                &quot;Error caught on signal handler: %(receiver)s&quot;,</span>
<span class="gi">+                {&quot;receiver&quot;: recv},</span>
<span class="gi">+                exc_info=failure_to_exc_info(failure),</span>
<span class="gi">+                extra={&quot;spider&quot;: spider},</span>
<span class="gi">+            )</span>
<span class="gi">+        return failure</span>
<span class="gi">+</span>
<span class="gi">+    dont_log = named.pop(&quot;dont_log&quot;, None)</span>
<span class="gi">+    spider = named.get(&quot;spider&quot;, None)</span>
<span class="gi">+    dfds = []</span>
<span class="gi">+    for receiver in liveReceivers(getAllReceivers(sender, signal)):</span>
<span class="gi">+        d = maybeDeferred_coro(</span>
<span class="gi">+            robustApply, receiver, signal=signal, sender=sender, *arguments, **named</span>
<span class="gi">+        )</span>
<span class="gi">+        d.addErrback(logerror, receiver)</span>
<span class="gi">+        d.addBoth(lambda result: (receiver, result))</span>
<span class="gi">+        dfds.append(d)</span>
<span class="gi">+    d = DeferredList(dfds)</span>
<span class="gi">+    d.addCallback(lambda out: [x[1] for x in out])</span>
<span class="gi">+    return d</span>


<span class="gd">-def disconnect_all(signal: TypingAny=Any, sender: TypingAny=Any) -&gt;None:</span>
<span class="gi">+def disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Disconnect all signal handlers. Useful for cleaning up after running
<span class="w"> </span>    tests
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for receiver in liveReceivers(getAllReceivers(sender, signal)):</span>
<span class="gi">+        disconnect(receiver, signal=signal, sender=sender)</span>
<span class="gh">diff --git a/scrapy/utils/sitemap.py b/scrapy/utils/sitemap.py</span>
<span class="gh">index 2ee154109..759b1c1a9 100644</span>
<span class="gd">--- a/scrapy/utils/sitemap.py</span>
<span class="gi">+++ b/scrapy/utils/sitemap.py</span>
<span class="gu">@@ -6,7 +6,8 @@ SitemapSpider, its API is subject to change without notice.</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from typing import Any, Dict, Generator, Iterator, Optional
<span class="w"> </span>from urllib.parse import urljoin
<span class="gd">-import lxml.etree</span>
<span class="gi">+</span>
<span class="gi">+import lxml.etree  # nosec</span>


<span class="w"> </span>class Sitemap:
<span class="gu">@@ -14,30 +15,37 @@ class Sitemap:</span>
<span class="w"> </span>    (type=sitemapindex) files&quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, xmltext: str):
<span class="gd">-        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True,</span>
<span class="gd">-            resolve_entities=False)</span>
<span class="gd">-        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)</span>
<span class="gi">+        xmlp = lxml.etree.XMLParser(</span>
<span class="gi">+            recover=True, remove_comments=True, resolve_entities=False</span>
<span class="gi">+        )</span>
<span class="gi">+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # nosec</span>
<span class="w"> </span>        rt = self._root.tag
<span class="gd">-        self.type = self._root.tag.split(&#39;}&#39;, 1)[1] if &#39;}&#39; in rt else rt</span>
<span class="gi">+        self.type = self._root.tag.split(&quot;}&quot;, 1)[1] if &quot;}&quot; in rt else rt</span>

<span class="gd">-    def __iter__(self) -&gt;Iterator[Dict[str, Any]]:</span>
<span class="gi">+    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:</span>
<span class="w"> </span>        for elem in self._root.getchildren():
<span class="w"> </span>            d: Dict[str, Any] = {}
<span class="w"> </span>            for el in elem.getchildren():
<span class="w"> </span>                tag = el.tag
<span class="gd">-                name = tag.split(&#39;}&#39;, 1)[1] if &#39;}&#39; in tag else tag</span>
<span class="gd">-                if name == &#39;link&#39;:</span>
<span class="gd">-                    if &#39;href&#39; in el.attrib:</span>
<span class="gd">-                        d.setdefault(&#39;alternate&#39;, []).append(el.get(&#39;href&#39;))</span>
<span class="gi">+                name = tag.split(&quot;}&quot;, 1)[1] if &quot;}&quot; in tag else tag</span>
<span class="gi">+</span>
<span class="gi">+                if name == &quot;link&quot;:</span>
<span class="gi">+                    if &quot;href&quot; in el.attrib:</span>
<span class="gi">+                        d.setdefault(&quot;alternate&quot;, []).append(el.get(&quot;href&quot;))</span>
<span class="w"> </span>                else:
<span class="gd">-                    d[name] = el.text.strip() if el.text else &#39;&#39;</span>
<span class="gd">-            if &#39;loc&#39; in d:</span>
<span class="gi">+                    d[name] = el.text.strip() if el.text else &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+            if &quot;loc&quot; in d:</span>
<span class="w"> </span>                yield d


<span class="gd">-def sitemap_urls_from_robots(robots_text: str, base_url: Optional[str]=None</span>
<span class="gd">-    ) -&gt;Generator[str, Any, None]:</span>
<span class="gi">+def sitemap_urls_from_robots(</span>
<span class="gi">+    robots_text: str, base_url: Optional[str] = None</span>
<span class="gi">+) -&gt; Generator[str, Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return an iterator over all sitemap urls contained in the given
<span class="w"> </span>    robots.txt file
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for line in robots_text.splitlines():</span>
<span class="gi">+        if line.lstrip().lower().startswith(&quot;sitemap:&quot;):</span>
<span class="gi">+            url = line.split(&quot;:&quot;, 1)[1].strip()</span>
<span class="gi">+            yield urljoin(base_url or &quot;&quot;, url)</span>
<span class="gh">diff --git a/scrapy/utils/spider.py b/scrapy/utils/spider.py</span>
<span class="gh">index b93bbbad9..704df8657 100644</span>
<span class="gd">--- a/scrapy/utils/spider.py</span>
<span class="gi">+++ b/scrapy/utils/spider.py</span>
<span class="gu">@@ -1,30 +1,121 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import inspect
<span class="w"> </span>import logging
<span class="w"> </span>from types import CoroutineType, ModuleType
<span class="gd">-from typing import TYPE_CHECKING, Any, AsyncGenerator, Generator, Iterable, Literal, Optional, Type, TypeVar, Union, overload</span>
<span class="gi">+from typing import (</span>
<span class="gi">+    TYPE_CHECKING,</span>
<span class="gi">+    Any,</span>
<span class="gi">+    AsyncGenerator,</span>
<span class="gi">+    Generator,</span>
<span class="gi">+    Iterable,</span>
<span class="gi">+    Literal,</span>
<span class="gi">+    Optional,</span>
<span class="gi">+    Type,</span>
<span class="gi">+    TypeVar,</span>
<span class="gi">+    Union,</span>
<span class="gi">+    overload,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Request
<span class="w"> </span>from scrapy.spiders import Spider
<span class="w"> </span>from scrapy.utils.defer import deferred_from_coro
<span class="w"> </span>from scrapy.utils.misc import arg_to_iter
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy.spiderloader import SpiderLoader
<span class="gi">+</span>
<span class="w"> </span>logger = logging.getLogger(__name__)
<span class="gd">-_T = TypeVar(&#39;_T&#39;)</span>
<span class="gi">+</span>
<span class="gi">+_T = TypeVar(&quot;_T&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# https://stackoverflow.com/questions/60222982</span>
<span class="gi">+@overload</span>
<span class="gi">+def iterate_spider_output(result: AsyncGenerator) -&gt; AsyncGenerator:  # type: ignore[misc]</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def iterate_spider_output(result: CoroutineType) -&gt; Deferred:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def iterate_spider_output(result: _T) -&gt; Iterable:</span>
<span class="gi">+    ...</span>


<span class="gd">-def iter_spider_classes(module: ModuleType) -&gt;Generator[Type[Spider], Any, None</span>
<span class="gd">-    ]:</span>
<span class="gi">+def iterate_spider_output(result: Any) -&gt; Union[Iterable, AsyncGenerator, Deferred]:</span>
<span class="gi">+    if inspect.isasyncgen(result):</span>
<span class="gi">+        return result</span>
<span class="gi">+    if inspect.iscoroutine(result):</span>
<span class="gi">+        d = deferred_from_coro(result)</span>
<span class="gi">+        d.addCallback(iterate_spider_output)</span>
<span class="gi">+        return d</span>
<span class="gi">+    return arg_to_iter(deferred_from_coro(result))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def iter_spider_classes(module: ModuleType) -&gt; Generator[Type[Spider], Any, None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return an iterator over all spider classes defined in the given module
<span class="w"> </span>    that can be instantiated (i.e. which have name)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # this needs to be imported here until get rid of the spider manager</span>
<span class="gi">+    # singleton in scrapy.spider.spiders</span>
<span class="gi">+    from scrapy.spiders import Spider</span>
<span class="gi">+</span>
<span class="gi">+    for obj in vars(module).values():</span>
<span class="gi">+        if (</span>
<span class="gi">+            inspect.isclass(obj)</span>
<span class="gi">+            and issubclass(obj, Spider)</span>
<span class="gi">+            and obj.__module__ == module.__name__</span>
<span class="gi">+            and getattr(obj, &quot;name&quot;, None)</span>
<span class="gi">+        ):</span>
<span class="gi">+            yield obj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def spidercls_for_request(</span>
<span class="gi">+    spider_loader: SpiderLoader,</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    default_spidercls: Type[Spider],</span>
<span class="gi">+    log_none: bool = ...,</span>
<span class="gi">+    log_multiple: bool = ...,</span>
<span class="gi">+) -&gt; Type[Spider]:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>

<span class="gi">+@overload</span>
<span class="gi">+def spidercls_for_request(</span>
<span class="gi">+    spider_loader: SpiderLoader,</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    default_spidercls: Literal[None],</span>
<span class="gi">+    log_none: bool = ...,</span>
<span class="gi">+    log_multiple: bool = ...,</span>
<span class="gi">+) -&gt; Optional[Type[Spider]]:</span>
<span class="gi">+    ...</span>

<span class="gd">-def spidercls_for_request(spider_loader: SpiderLoader, request: Request,</span>
<span class="gd">-    default_spidercls: Optional[Type[Spider]]=None, log_none: bool=False,</span>
<span class="gd">-    log_multiple: bool=False) -&gt;Optional[Type[Spider]]:</span>
<span class="gi">+</span>
<span class="gi">+@overload</span>
<span class="gi">+def spidercls_for_request(</span>
<span class="gi">+    spider_loader: SpiderLoader,</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    *,</span>
<span class="gi">+    log_none: bool = ...,</span>
<span class="gi">+    log_multiple: bool = ...,</span>
<span class="gi">+) -&gt; Optional[Type[Spider]]:</span>
<span class="gi">+    ...</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def spidercls_for_request(</span>
<span class="gi">+    spider_loader: SpiderLoader,</span>
<span class="gi">+    request: Request,</span>
<span class="gi">+    default_spidercls: Optional[Type[Spider]] = None,</span>
<span class="gi">+    log_none: bool = False,</span>
<span class="gi">+    log_multiple: bool = False,</span>
<span class="gi">+) -&gt; Optional[Type[Spider]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a spider class that handles the given Request.

<span class="w"> </span>    This will look for the spiders that can handle the given request (using
<span class="gu">@@ -35,8 +126,23 @@ def spidercls_for_request(spider_loader: SpiderLoader, request: Request,</span>
<span class="w"> </span>    default_spidercls passed. It can optionally log if multiple or no spiders
<span class="w"> </span>    are found.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    snames = spider_loader.find_by_request(request)</span>
<span class="gi">+    if len(snames) == 1:</span>
<span class="gi">+        return spider_loader.load(snames[0])</span>
<span class="gi">+</span>
<span class="gi">+    if len(snames) &gt; 1 and log_multiple:</span>
<span class="gi">+        logger.error(</span>
<span class="gi">+            &quot;More than one spider can handle: %(request)s - %(snames)s&quot;,</span>
<span class="gi">+            {&quot;request&quot;: request, &quot;snames&quot;: &quot;, &quot;.join(snames)},</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if len(snames) == 0 and log_none:</span>
<span class="gi">+        logger.error(</span>
<span class="gi">+            &quot;Unable to find spider that handles: %(request)s&quot;, {&quot;request&quot;: request}</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    return default_spidercls</span>


<span class="w"> </span>class DefaultSpider(Spider):
<span class="gd">-    name = &#39;default&#39;</span>
<span class="gi">+    name = &quot;default&quot;</span>
<span class="gh">diff --git a/scrapy/utils/ssl.py b/scrapy/utils/ssl.py</span>
<span class="gh">index 1588eed7a..d520ef809 100644</span>
<span class="gd">--- a/scrapy/utils/ssl.py</span>
<span class="gi">+++ b/scrapy/utils/ssl.py</span>
<span class="gu">@@ -1,6 +1,63 @@</span>
<span class="w"> </span>from typing import Any, Optional
<span class="gi">+</span>
<span class="w"> </span>import OpenSSL._util as pyOpenSSLutil
<span class="w"> </span>import OpenSSL.SSL
<span class="w"> </span>import OpenSSL.version
<span class="w"> </span>from OpenSSL.crypto import X509Name
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def ffi_buf_to_string(buf: Any) -&gt; str:</span>
<span class="gi">+    return to_unicode(pyOpenSSLutil.ffi.string(buf))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def x509name_to_string(x509name: X509Name) -&gt; str:</span>
<span class="gi">+    # from OpenSSL.crypto.X509Name.__repr__</span>
<span class="gi">+    result_buffer: Any = pyOpenSSLutil.ffi.new(&quot;char[]&quot;, 512)</span>
<span class="gi">+    pyOpenSSLutil.lib.X509_NAME_oneline(</span>
<span class="gi">+        x509name._name, result_buffer, len(result_buffer)  # type: ignore[attr-defined]</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    return ffi_buf_to_string(result_buffer)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_temp_key_info(ssl_object: Any) -&gt; Optional[str]:</span>
<span class="gi">+    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()</span>
<span class="gi">+    if not hasattr(pyOpenSSLutil.lib, &quot;SSL_get_server_tmp_key&quot;):</span>
<span class="gi">+        # removed in cryptography 40.0.0</span>
<span class="gi">+        return None</span>
<span class="gi">+    temp_key_p = pyOpenSSLutil.ffi.new(&quot;EVP_PKEY **&quot;)</span>
<span class="gi">+    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):</span>
<span class="gi">+        return None</span>
<span class="gi">+    temp_key = temp_key_p[0]</span>
<span class="gi">+    if temp_key == pyOpenSSLutil.ffi.NULL:</span>
<span class="gi">+        return None</span>
<span class="gi">+    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)</span>
<span class="gi">+    key_info = []</span>
<span class="gi">+    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)</span>
<span class="gi">+    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:</span>
<span class="gi">+        key_info.append(&quot;RSA&quot;)</span>
<span class="gi">+    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:</span>
<span class="gi">+        key_info.append(&quot;DH&quot;)</span>
<span class="gi">+    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:</span>
<span class="gi">+        key_info.append(&quot;ECDH&quot;)</span>
<span class="gi">+        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)</span>
<span class="gi">+        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)</span>
<span class="gi">+        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(</span>
<span class="gi">+            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)</span>
<span class="gi">+        )</span>
<span class="gi">+        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)</span>
<span class="gi">+        if cname == pyOpenSSLutil.ffi.NULL:</span>
<span class="gi">+            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)</span>
<span class="gi">+        key_info.append(ffi_buf_to_string(cname))</span>
<span class="gi">+    else:</span>
<span class="gi">+        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))</span>
<span class="gi">+    key_info.append(f&quot;{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits&quot;)</span>
<span class="gi">+    return &quot;, &quot;.join(key_info)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_openssl_version() -&gt; str:</span>
<span class="gi">+    system_openssl_bytes = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)</span>
<span class="gi">+    system_openssl = system_openssl_bytes.decode(&quot;ascii&quot;, errors=&quot;replace&quot;)</span>
<span class="gi">+    return f&quot;{OpenSSL.version.__version__} ({system_openssl})&quot;</span>
<span class="gh">diff --git a/scrapy/utils/template.py b/scrapy/utils/template.py</span>
<span class="gh">index 705073b43..6b22f3bfa 100644</span>
<span class="gd">--- a/scrapy/utils/template.py</span>
<span class="gi">+++ b/scrapy/utils/template.py</span>
<span class="gu">@@ -1,13 +1,30 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Helper functions for working with templates&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import re
<span class="w"> </span>import string
<span class="w"> </span>from os import PathLike
<span class="w"> </span>from pathlib import Path
<span class="w"> </span>from typing import Any, Union
<span class="gd">-CAMELCASE_INVALID_CHARS = re.compile(&#39;[^a-zA-Z\\d]&#39;)</span>


<span class="gd">-def string_camelcase(string: str) -&gt;str:</span>
<span class="gi">+def render_templatefile(path: Union[str, PathLike], **kwargs: Any) -&gt; None:</span>
<span class="gi">+    path_obj = Path(path)</span>
<span class="gi">+    raw = path_obj.read_text(&quot;utf8&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    content = string.Template(raw).substitute(**kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    render_path = path_obj.with_suffix(&quot;&quot;) if path_obj.suffix == &quot;.tmpl&quot; else path_obj</span>
<span class="gi">+</span>
<span class="gi">+    if path_obj.suffix == &quot;.tmpl&quot;:</span>
<span class="gi">+        path_obj.rename(render_path)</span>
<span class="gi">+</span>
<span class="gi">+    render_path.write_text(content, &quot;utf8&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+CAMELCASE_INVALID_CHARS = re.compile(r&quot;[^a-zA-Z\d]&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def string_camelcase(string: str) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Convert a word  to its CamelCase version and remove invalid chars

<span class="w"> </span>    &gt;&gt;&gt; string_camelcase(&#39;lost-pound&#39;)
<span class="gu">@@ -17,4 +34,4 @@ def string_camelcase(string: str) -&gt;str:</span>
<span class="w"> </span>    &#39;MissingImages&#39;

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return CAMELCASE_INVALID_CHARS.sub(&quot;&quot;, string.title())</span>
<span class="gh">diff --git a/scrapy/utils/test.py b/scrapy/utils/test.py</span>
<span class="gh">index 88505e748..709e0b00d 100644</span>
<span class="gd">--- a/scrapy/utils/test.py</span>
<span class="gi">+++ b/scrapy/utils/test.py</span>
<span class="gu">@@ -1,6 +1,7 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>This module contains some assorted functions used in tests
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import asyncio
<span class="w"> </span>import os
<span class="w"> </span>from importlib import import_module
<span class="gu">@@ -8,49 +9,144 @@ from pathlib import Path</span>
<span class="w"> </span>from posixpath import split
<span class="w"> </span>from typing import Any, Coroutine, Dict, List, Optional, Tuple, Type
<span class="w"> </span>from unittest import TestCase, mock
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="w"> </span>from twisted.trial.unittest import SkipTest
<span class="gi">+</span>
<span class="w"> </span>from scrapy import Spider
<span class="w"> </span>from scrapy.crawler import Crawler
<span class="w"> </span>from scrapy.utils.boto import is_botocore_available


<span class="gi">+def assert_gcs_environ() -&gt; None:</span>
<span class="gi">+    if &quot;GCS_PROJECT_ID&quot; not in os.environ:</span>
<span class="gi">+        raise SkipTest(&quot;GCS_PROJECT_ID not found&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def skip_if_no_boto() -&gt; None:</span>
<span class="gi">+    if not is_botocore_available():</span>
<span class="gi">+        raise SkipTest(&quot;missing botocore library&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_gcs_content_and_delete(</span>
<span class="gi">+    bucket: Any, path: str</span>
<span class="gi">+) -&gt; Tuple[bytes, List[Dict[str, str]], Any]:</span>
<span class="gi">+    from google.cloud import storage</span>
<span class="gi">+</span>
<span class="gi">+    client = storage.Client(project=os.environ.get(&quot;GCS_PROJECT_ID&quot;))</span>
<span class="gi">+    bucket = client.get_bucket(bucket)</span>
<span class="gi">+    blob = bucket.get_blob(path)</span>
<span class="gi">+    content = blob.download_as_string()</span>
<span class="gi">+    acl = list(blob.acl)  # loads acl before it will be deleted</span>
<span class="gi">+    bucket.delete_blob(path)</span>
<span class="gi">+    return content, acl, blob</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_ftp_content_and_delete(</span>
<span class="gi">+    path: str,</span>
<span class="gi">+    host: str,</span>
<span class="gi">+    port: int,</span>
<span class="gi">+    username: str,</span>
<span class="gi">+    password: str,</span>
<span class="gi">+    use_active_mode: bool = False,</span>
<span class="gi">+) -&gt; bytes:</span>
<span class="gi">+    from ftplib import FTP</span>
<span class="gi">+</span>
<span class="gi">+    ftp = FTP()</span>
<span class="gi">+    ftp.connect(host, port)</span>
<span class="gi">+    ftp.login(username, password)</span>
<span class="gi">+    if use_active_mode:</span>
<span class="gi">+        ftp.set_pasv(False)</span>
<span class="gi">+    ftp_data: List[bytes] = []</span>
<span class="gi">+</span>
<span class="gi">+    def buffer_data(data: bytes) -&gt; None:</span>
<span class="gi">+        ftp_data.append(data)</span>
<span class="gi">+</span>
<span class="gi">+    ftp.retrbinary(f&quot;RETR {path}&quot;, buffer_data)</span>
<span class="gi">+    dirname, filename = split(path)</span>
<span class="gi">+    ftp.cwd(dirname)</span>
<span class="gi">+    ftp.delete(filename)</span>
<span class="gi">+    return b&quot;&quot;.join(ftp_data)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class TestSpider(Spider):
<span class="gd">-    name = &#39;test&#39;</span>
<span class="gi">+    name = &quot;test&quot;</span>


<span class="gd">-def get_crawler(spidercls: Optional[Type[Spider]]=None, settings_dict:</span>
<span class="gd">-    Optional[Dict[str, Any]]=None, prevent_warnings: bool=True) -&gt;Crawler:</span>
<span class="gi">+def get_crawler(</span>
<span class="gi">+    spidercls: Optional[Type[Spider]] = None,</span>
<span class="gi">+    settings_dict: Optional[Dict[str, Any]] = None,</span>
<span class="gi">+    prevent_warnings: bool = True,</span>
<span class="gi">+) -&gt; Crawler:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return an unconfigured Crawler object. If settings_dict is given, it
<span class="w"> </span>    will be used to populate the crawler settings with a project level
<span class="w"> </span>    priority.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from scrapy.crawler import CrawlerRunner</span>
<span class="gi">+</span>
<span class="gi">+    # Set by default settings that prevent deprecation warnings.</span>
<span class="gi">+    settings: Dict[str, Any] = {}</span>
<span class="gi">+    if prevent_warnings:</span>
<span class="gi">+        settings[&quot;REQUEST_FINGERPRINTER_IMPLEMENTATION&quot;] = &quot;2.7&quot;</span>
<span class="gi">+    settings.update(settings_dict or {})</span>
<span class="gi">+    runner = CrawlerRunner(settings)</span>
<span class="gi">+    crawler = runner.create_crawler(spidercls or TestSpider)</span>
<span class="gi">+    crawler._apply_settings()</span>
<span class="gi">+    return crawler</span>


<span class="gd">-def get_pythonpath() -&gt;str:</span>
<span class="gi">+def get_pythonpath() -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a PYTHONPATH suitable to use in processes so that they find this
<span class="w"> </span>    installation of Scrapy&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    scrapy_path = import_module(&quot;scrapy&quot;).__path__[0]</span>
<span class="gi">+    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(&quot;PYTHONPATH&quot;, &quot;&quot;)</span>


<span class="gd">-def get_testenv() -&gt;Dict[str, str]:</span>
<span class="gi">+def get_testenv() -&gt; Dict[str, str]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a OS environment dict suitable to fork processes that need to import
<span class="w"> </span>    this installation of Scrapy, instead of a system installed one.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    env = os.environ.copy()</span>
<span class="gi">+    env[&quot;PYTHONPATH&quot;] = get_pythonpath()</span>
<span class="gi">+    return env</span>


<span class="gd">-def assert_samelines(testcase: TestCase, text1: str, text2: str, msg:</span>
<span class="gd">-    Optional[str]=None) -&gt;None:</span>
<span class="gi">+def assert_samelines(</span>
<span class="gi">+    testcase: TestCase, text1: str, text2: str, msg: Optional[str] = None</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Asserts text1 and text2 have the same lines, ignoring differences in
<span class="w"> </span>    line endings between platforms
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)</span>


<span class="gd">-def mock_google_cloud_storage() -&gt;Tuple[Any, Any, Any]:</span>
<span class="gi">+def get_from_asyncio_queue(value: Any) -&gt; Coroutine:</span>
<span class="gi">+    q: asyncio.Queue = asyncio.Queue()</span>
<span class="gi">+    getter = q.get()</span>
<span class="gi">+    q.put_nowait(value)</span>
<span class="gi">+    return getter</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def mock_google_cloud_storage() -&gt; Tuple[Any, Any, Any]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Creates autospec mocks for google-cloud-storage Client, Bucket and Blob
<span class="w"> </span>    classes and set their proper return values.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from google.cloud.storage import Blob, Bucket, Client</span>
<span class="gi">+</span>
<span class="gi">+    client_mock = mock.create_autospec(Client)</span>
<span class="gi">+</span>
<span class="gi">+    bucket_mock = mock.create_autospec(Bucket)</span>
<span class="gi">+    client_mock.get_bucket.return_value = bucket_mock</span>
<span class="gi">+</span>
<span class="gi">+    blob_mock = mock.create_autospec(Blob)</span>
<span class="gi">+    bucket_mock.blob.return_value = blob_mock</span>
<span class="gi">+</span>
<span class="gi">+    return (client_mock, bucket_mock, blob_mock)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_web_client_agent_req(url: str) -&gt; Deferred:</span>
<span class="gi">+    from twisted.internet import reactor</span>
<span class="gi">+    from twisted.web.client import Agent  # imports twisted.internet.reactor</span>
<span class="gi">+</span>
<span class="gi">+    agent = Agent(reactor)</span>
<span class="gi">+    return agent.request(b&quot;GET&quot;, url.encode(&quot;utf-8&quot;))</span>
<span class="gh">diff --git a/scrapy/utils/testproc.py b/scrapy/utils/testproc.py</span>
<span class="gh">index 4e09c46cc..0688e014b 100644</span>
<span class="gd">--- a/scrapy/utils/testproc.py</span>
<span class="gi">+++ b/scrapy/utils/testproc.py</span>
<span class="gu">@@ -1,7 +1,9 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>from typing import Iterable, List, Optional, Tuple, cast
<span class="gi">+</span>
<span class="w"> </span>from twisted.internet.defer import Deferred
<span class="w"> </span>from twisted.internet.error import ProcessTerminated
<span class="w"> </span>from twisted.internet.protocol import ProcessProtocol
<span class="gu">@@ -10,14 +12,52 @@ from twisted.python.failure import Failure</span>

<span class="w"> </span>class ProcessTest:
<span class="w"> </span>    command = None
<span class="gd">-    prefix = [sys.executable, &#39;-m&#39;, &#39;scrapy.cmdline&#39;]</span>
<span class="gd">-    cwd = os.getcwd()</span>
<span class="gi">+    prefix = [sys.executable, &quot;-m&quot;, &quot;scrapy.cmdline&quot;]</span>
<span class="gi">+    cwd = os.getcwd()  # trial chdirs to temp dir</span>

<span class="gi">+    def execute(</span>
<span class="gi">+        self,</span>
<span class="gi">+        args: Iterable[str],</span>
<span class="gi">+        check_code: bool = True,</span>
<span class="gi">+        settings: Optional[str] = None,</span>
<span class="gi">+    ) -&gt; Deferred:</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        env = os.environ.copy()</span>
<span class="gi">+        if settings is not None:</span>
<span class="gi">+            env[&quot;SCRAPY_SETTINGS_MODULE&quot;] = settings</span>
<span class="gi">+        assert self.command</span>
<span class="gi">+        cmd = self.prefix + [self.command] + list(args)</span>
<span class="gi">+        pp = TestProcessProtocol()</span>
<span class="gi">+        pp.deferred.addCallback(self._process_finished, cmd, check_code)</span>
<span class="gi">+        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)</span>
<span class="gi">+        return pp.deferred</span>
<span class="gi">+</span>
<span class="gi">+    def _process_finished(</span>
<span class="gi">+        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool</span>
<span class="gi">+    ) -&gt; Tuple[int, bytes, bytes]:</span>
<span class="gi">+        if pp.exitcode and check_code:</span>
<span class="gi">+            msg = f&quot;process {cmd} exit with code {pp.exitcode}&quot;</span>
<span class="gi">+            msg += f&quot;\n&gt;&gt;&gt; stdout &lt;&lt;&lt;\n{pp.out.decode()}&quot;</span>
<span class="gi">+            msg += &quot;\n&quot;</span>
<span class="gi">+            msg += f&quot;\n&gt;&gt;&gt; stderr &lt;&lt;&lt;\n{pp.err.decode()}&quot;</span>
<span class="gi">+            raise RuntimeError(msg)</span>
<span class="gi">+        return cast(int, pp.exitcode), pp.out, pp.err</span>

<span class="gd">-class TestProcessProtocol(ProcessProtocol):</span>

<span class="gd">-    def __init__(self) -&gt;None:</span>
<span class="gi">+class TestProcessProtocol(ProcessProtocol):</span>
<span class="gi">+    def __init__(self) -&gt; None:</span>
<span class="w"> </span>        self.deferred: Deferred = Deferred()
<span class="gd">-        self.out: bytes = b&#39;&#39;</span>
<span class="gd">-        self.err: bytes = b&#39;&#39;</span>
<span class="gi">+        self.out: bytes = b&quot;&quot;</span>
<span class="gi">+        self.err: bytes = b&quot;&quot;</span>
<span class="w"> </span>        self.exitcode: Optional[int] = None
<span class="gi">+</span>
<span class="gi">+    def outReceived(self, data: bytes) -&gt; None:</span>
<span class="gi">+        self.out += data</span>
<span class="gi">+</span>
<span class="gi">+    def errReceived(self, data: bytes) -&gt; None:</span>
<span class="gi">+        self.err += data</span>
<span class="gi">+</span>
<span class="gi">+    def processEnded(self, status: Failure) -&gt; None:</span>
<span class="gi">+        self.exitcode = cast(ProcessTerminated, status.value).exitCode</span>
<span class="gi">+        self.deferred.callback(self)</span>
<span class="gh">diff --git a/scrapy/utils/testsite.py b/scrapy/utils/testsite.py</span>
<span class="gh">index c91969a13..de9ce992a 100644</span>
<span class="gd">--- a/scrapy/utils/testsite.py</span>
<span class="gi">+++ b/scrapy/utils/testsite.py</span>
<span class="gu">@@ -1,17 +1,55 @@</span>
<span class="w"> </span>from urllib.parse import urljoin
<span class="gi">+</span>
<span class="w"> </span>from twisted.web import resource, server, static, util


<span class="w"> </span>class SiteTest:
<span class="gd">-    pass</span>
<span class="gi">+    def setUp(self):</span>
<span class="gi">+        from twisted.internet import reactor</span>
<span class="gi">+</span>
<span class="gi">+        super().setUp()</span>
<span class="gi">+        self.site = reactor.listenTCP(0, test_site(), interface=&quot;127.0.0.1&quot;)</span>
<span class="gi">+        self.baseurl = f&quot;http://localhost:{self.site.getHost().port}/&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def tearDown(self):</span>
<span class="gi">+        super().tearDown()</span>
<span class="gi">+        self.site.stopListening()</span>
<span class="gi">+</span>
<span class="gi">+    def url(self, path):</span>
<span class="gi">+        return urljoin(self.baseurl, path)</span>


<span class="w"> </span>class NoMetaRefreshRedirect(util.Redirect):
<span class="gd">-    pass</span>
<span class="gi">+    def render(self, request):</span>
<span class="gi">+        content = util.Redirect.render(self, request)</span>
<span class="gi">+        return content.replace(</span>
<span class="gi">+            b&#39;http-equiv=&quot;refresh&quot;&#39;, b&#39;http-no-equiv=&quot;do-not-refresh-me&quot;&#39;</span>
<span class="gi">+        )</span>


<span class="gd">-if __name__ == &#39;__main__&#39;:</span>
<span class="gi">+def test_site():</span>
<span class="gi">+    r = resource.Resource()</span>
<span class="gi">+    r.putChild(b&quot;text&quot;, static.Data(b&quot;Works&quot;, &quot;text/plain&quot;))</span>
<span class="gi">+    r.putChild(</span>
<span class="gi">+        b&quot;html&quot;,</span>
<span class="gi">+        static.Data(</span>
<span class="gi">+            b&quot;&lt;body&gt;&lt;p class=&#39;one&#39;&gt;Works&lt;/p&gt;&lt;p class=&#39;two&#39;&gt;World&lt;/p&gt;&lt;/body&gt;&quot;,</span>
<span class="gi">+            &quot;text/html&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+    )</span>
<span class="gi">+    r.putChild(</span>
<span class="gi">+        b&quot;enc-gb18030&quot;,</span>
<span class="gi">+        static.Data(b&quot;&lt;p&gt;gb18030 encoding&lt;/p&gt;&quot;, &quot;text/html; charset=gb18030&quot;),</span>
<span class="gi">+    )</span>
<span class="gi">+    r.putChild(b&quot;redirect&quot;, util.Redirect(b&quot;/redirected&quot;))</span>
<span class="gi">+    r.putChild(b&quot;redirect-no-meta-refresh&quot;, NoMetaRefreshRedirect(b&quot;/redirected&quot;))</span>
<span class="gi">+    r.putChild(b&quot;redirected&quot;, static.Data(b&quot;Redirected here&quot;, &quot;text/plain&quot;))</span>
<span class="gi">+    return server.Site(r)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+if __name__ == &quot;__main__&quot;:</span>
<span class="w"> </span>    from twisted.internet import reactor
<span class="gd">-    port = reactor.listenTCP(0, test_site(), interface=&#39;127.0.0.1&#39;)</span>
<span class="gd">-    print(f&#39;http://localhost:{port.getHost().port}/&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    port = reactor.listenTCP(0, test_site(), interface=&quot;127.0.0.1&quot;)</span>
<span class="gi">+    print(f&quot;http://localhost:{port.getHost().port}/&quot;)</span>
<span class="w"> </span>    reactor.run()
<span class="gh">diff --git a/scrapy/utils/trackref.py b/scrapy/utils/trackref.py</span>
<span class="gh">index 42dd22dd7..9ff9a273f 100644</span>
<span class="gd">--- a/scrapy/utils/trackref.py</span>
<span class="gi">+++ b/scrapy/utils/trackref.py</span>
<span class="gu">@@ -8,43 +8,65 @@ About performance: This library has a minimal performance impact when enabled,</span>
<span class="w"> </span>and no performance penalty at all when disabled (as object_ref becomes just an
<span class="w"> </span>alias to object in that case).
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from collections import defaultdict
<span class="w"> </span>from operator import itemgetter
<span class="w"> </span>from time import time
<span class="w"> </span>from typing import TYPE_CHECKING, Any, DefaultDict, Iterable
<span class="w"> </span>from weakref import WeakKeyDictionary
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="gi">+    # typing.Self requires Python 3.11</span>
<span class="w"> </span>    from typing_extensions import Self
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>NoneType = type(None)
<span class="gd">-live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary</span>
<span class="gd">-    )</span>
<span class="gi">+live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)</span>


<span class="w"> </span>class object_ref:
<span class="w"> </span>    &quot;&quot;&quot;Inherit from this class to a keep a record of live instances&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    __slots__ = ()

<span class="gd">-    def __new__(cls, *args: Any, **kwargs: Any) -&gt;&#39;Self&#39;:</span>
<span class="gi">+    def __new__(cls, *args: Any, **kwargs: Any) -&gt; &quot;Self&quot;:</span>
<span class="w"> </span>        obj = object.__new__(cls)
<span class="w"> </span>        live_refs[cls][obj] = time()
<span class="w"> </span>        return obj


<span class="gd">-def format_live_refs(ignore: Any=NoneType) -&gt;str:</span>
<span class="gi">+# using Any as it&#39;s hard to type type(None)</span>
<span class="gi">+def format_live_refs(ignore: Any = NoneType) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return a tabular representation of tracked objects&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    s = &quot;Live References\n\n&quot;</span>
<span class="gi">+    now = time()</span>
<span class="gi">+    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):</span>
<span class="gi">+        if not wdict:</span>
<span class="gi">+            continue</span>
<span class="gi">+        if issubclass(cls, ignore):</span>
<span class="gi">+            continue</span>
<span class="gi">+        oldest = min(wdict.values())</span>
<span class="gi">+        s += f&quot;{cls.__name__:&lt;30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\n&quot;</span>
<span class="gi">+    return s</span>


<span class="gd">-def print_live_refs(*a: Any, **kw: Any) -&gt;None:</span>
<span class="gi">+def print_live_refs(*a: Any, **kw: Any) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;Print tracked objects&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    print(format_live_refs(*a, **kw))</span>


<span class="gd">-def get_oldest(class_name: str) -&gt;Any:</span>
<span class="gi">+def get_oldest(class_name: str) -&gt; Any:</span>
<span class="w"> </span>    &quot;&quot;&quot;Get the oldest object for a specific class name&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for cls, wdict in live_refs.items():</span>
<span class="gi">+        if cls.__name__ == class_name:</span>
<span class="gi">+            if not wdict:</span>
<span class="gi">+                break</span>
<span class="gi">+            return min(wdict.items(), key=itemgetter(1))[0]</span>


<span class="gd">-def iter_all(class_name: str) -&gt;Iterable[Any]:</span>
<span class="gi">+def iter_all(class_name: str) -&gt; Iterable[Any]:</span>
<span class="w"> </span>    &quot;&quot;&quot;Iterate over all objects of the same class by its class name&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for cls, wdict in live_refs.items():</span>
<span class="gi">+        if cls.__name__ == class_name:</span>
<span class="gi">+            return wdict.keys()</span>
<span class="gi">+    return []</span>
<span class="gh">diff --git a/scrapy/utils/url.py b/scrapy/utils/url.py</span>
<span class="gh">index 31325513d..22b4197f9 100644</span>
<span class="gd">--- a/scrapy/utils/url.py</span>
<span class="gi">+++ b/scrapy/utils/url.py</span>
<span class="gu">@@ -8,37 +8,53 @@ to the w3lib.url module. Always import those from there instead.</span>
<span class="w"> </span>import re
<span class="w"> </span>from typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast
<span class="w"> </span>from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse
<span class="gi">+</span>
<span class="gi">+# scrapy.utils.url was moved to w3lib.url and import * ensures this</span>
<span class="gi">+# move doesn&#39;t break old code</span>
<span class="w"> </span>from w3lib.url import *
<span class="gd">-from w3lib.url import _safe_chars, _unquotepath</span>
<span class="gi">+from w3lib.url import _safe_chars, _unquotepath  # noqa: F401</span>
<span class="gi">+</span>
<span class="w"> </span>from scrapy.utils.python import to_unicode
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from scrapy import Spider
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>UrlT = Union[str, bytes, ParseResult]


<span class="gd">-def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -&gt;bool:</span>
<span class="gi">+def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return True if the url belongs to any of the given domains&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    host = parse_url(url).netloc.lower()</span>
<span class="gi">+    if not host:</span>
<span class="gi">+        return False</span>
<span class="gi">+    domains = [d.lower() for d in domains]</span>
<span class="gi">+    return any((host == d) or (host.endswith(f&quot;.{d}&quot;)) for d in domains)</span>


<span class="gd">-def url_is_from_spider(url: UrlT, spider: Type[&#39;Spider&#39;]) -&gt;bool:</span>
<span class="gi">+def url_is_from_spider(url: UrlT, spider: Type[&quot;Spider&quot;]) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return True if the url belongs to the given spider&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return url_is_from_any_domain(</span>
<span class="gi">+        url, [spider.name] + list(getattr(spider, &quot;allowed_domains&quot;, []))</span>
<span class="gi">+    )</span>


<span class="gd">-def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -&gt;bool:</span>
<span class="gi">+def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -&gt; bool:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return True if the url ends with one of the extensions provided&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    lowercase_path = parse_url(url).path.lower()</span>
<span class="gi">+    return any(lowercase_path.endswith(ext) for ext in extensions)</span>


<span class="gd">-def parse_url(url: UrlT, encoding: Optional[str]=None) -&gt;ParseResult:</span>
<span class="gi">+def parse_url(url: UrlT, encoding: Optional[str] = None) -&gt; ParseResult:</span>
<span class="w"> </span>    &quot;&quot;&quot;Return urlparsed url from the given argument (which could be an already
<span class="w"> </span>    parsed url)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(url, ParseResult):</span>
<span class="gi">+        return url</span>
<span class="gi">+    return cast(ParseResult, urlparse(to_unicode(url, encoding)))</span>


<span class="gd">-def escape_ajax(url: str) -&gt;str:</span>
<span class="gi">+def escape_ajax(url: str) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Return the crawlable url according to:
<span class="w"> </span>    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started
<span class="gu">@@ -61,22 +77,80 @@ def escape_ajax(url: str) -&gt;str:</span>
<span class="w"> </span>    &gt;&gt;&gt; escape_ajax(&quot;www.example.com/ajax.html&quot;)
<span class="w"> </span>    &#39;www.example.com/ajax.html&#39;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    defrag, frag = urldefrag(url)</span>
<span class="gi">+    if not frag.startswith(&quot;!&quot;):</span>
<span class="gi">+        return url</span>
<span class="gi">+    return add_or_replace_parameter(defrag, &quot;_escaped_fragment_&quot;, frag[1:])</span>


<span class="gd">-def add_http_if_no_scheme(url: str) -&gt;str:</span>
<span class="gi">+def add_http_if_no_scheme(url: str) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Add http as the default scheme if it is missing from the url.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def guess_scheme(url: str) -&gt;str:</span>
<span class="gi">+    match = re.match(r&quot;^\w+://&quot;, url, flags=re.I)</span>
<span class="gi">+    if not match:</span>
<span class="gi">+        parts = urlparse(url)</span>
<span class="gi">+        scheme = &quot;http:&quot; if parts.netloc else &quot;http://&quot;</span>
<span class="gi">+        url = scheme + url</span>
<span class="gi">+</span>
<span class="gi">+    return url</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _is_posix_path(string: str) -&gt; bool:</span>
<span class="gi">+    return bool(</span>
<span class="gi">+        re.match(</span>
<span class="gi">+            r&quot;&quot;&quot;</span>
<span class="gi">+            ^                   # start with...</span>
<span class="gi">+            (</span>
<span class="gi">+                \.              # ...a single dot,</span>
<span class="gi">+                (</span>
<span class="gi">+                    \. | [^/\.]+  # optionally followed by</span>
<span class="gi">+                )?                # either a second dot or some characters</span>
<span class="gi">+                |</span>
<span class="gi">+                ~   # $HOME</span>
<span class="gi">+            )?      # optional match of &quot;.&quot;, &quot;..&quot; or &quot;.blabla&quot;</span>
<span class="gi">+            /       # at least one &quot;/&quot; for a file path,</span>
<span class="gi">+            .       # and something after the &quot;/&quot;</span>
<span class="gi">+            &quot;&quot;&quot;,</span>
<span class="gi">+            string,</span>
<span class="gi">+            flags=re.VERBOSE,</span>
<span class="gi">+        )</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _is_windows_path(string: str) -&gt; bool:</span>
<span class="gi">+    return bool(</span>
<span class="gi">+        re.match(</span>
<span class="gi">+            r&quot;&quot;&quot;</span>
<span class="gi">+            ^</span>
<span class="gi">+            (</span>
<span class="gi">+                [a-z]:\\</span>
<span class="gi">+                | \\\\</span>
<span class="gi">+            )</span>
<span class="gi">+            &quot;&quot;&quot;,</span>
<span class="gi">+            string,</span>
<span class="gi">+            flags=re.IGNORECASE | re.VERBOSE,</span>
<span class="gi">+        )</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _is_filesystem_path(string: str) -&gt; bool:</span>
<span class="gi">+    return _is_posix_path(string) or _is_windows_path(string)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def guess_scheme(url: str) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Add an URL scheme if missing: file:// for filepath-like input or
<span class="w"> </span>    http:// otherwise.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def strip_url(url: str, strip_credentials: bool=True, strip_default_port:</span>
<span class="gd">-    bool=True, origin_only: bool=False, strip_fragment: bool=True) -&gt;str:</span>
<span class="gi">+    if _is_filesystem_path(url):</span>
<span class="gi">+        return any_to_uri(url)</span>
<span class="gi">+    return add_http_if_no_scheme(url)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def strip_url(</span>
<span class="gi">+    url: str,</span>
<span class="gi">+    strip_credentials: bool = True,</span>
<span class="gi">+    strip_default_port: bool = True,</span>
<span class="gi">+    origin_only: bool = False,</span>
<span class="gi">+    strip_fragment: bool = True,</span>
<span class="gi">+) -&gt; str:</span>
<span class="w"> </span>    &quot;&quot;&quot;Strip URL string from some of its components:

<span class="w"> </span>    - ``strip_credentials`` removes &quot;user:password@&quot;
<span class="gu">@@ -86,4 +160,27 @@ def strip_url(url: str, strip_credentials: bool=True, strip_default_port:</span>
<span class="w"> </span>      query and fragment components ; it also strips credentials
<span class="w"> </span>    - ``strip_fragment`` drops any #fragment component
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    parsed_url = urlparse(url)</span>
<span class="gi">+    netloc = parsed_url.netloc</span>
<span class="gi">+    if (strip_credentials or origin_only) and (</span>
<span class="gi">+        parsed_url.username or parsed_url.password</span>
<span class="gi">+    ):</span>
<span class="gi">+        netloc = netloc.split(&quot;@&quot;)[-1]</span>
<span class="gi">+    if strip_default_port and parsed_url.port:</span>
<span class="gi">+        if (parsed_url.scheme, parsed_url.port) in (</span>
<span class="gi">+            (&quot;http&quot;, 80),</span>
<span class="gi">+            (&quot;https&quot;, 443),</span>
<span class="gi">+            (&quot;ftp&quot;, 21),</span>
<span class="gi">+        ):</span>
<span class="gi">+            netloc = netloc.replace(f&quot;:{parsed_url.port}&quot;, &quot;&quot;)</span>
<span class="gi">+    return urlunparse(</span>
<span class="gi">+        (</span>
<span class="gi">+            parsed_url.scheme,</span>
<span class="gi">+            netloc,</span>
<span class="gi">+            &quot;/&quot; if origin_only else parsed_url.path,</span>
<span class="gi">+            &quot;&quot; if origin_only else parsed_url.params,</span>
<span class="gi">+            &quot;&quot; if origin_only else parsed_url.query,</span>
<span class="gi">+            &quot;&quot; if strip_fragment else parsed_url.fragment,</span>
<span class="gi">+        )</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/scrapy/utils/versions.py b/scrapy/utils/versions.py</span>
<span class="gh">index b49afb199..42e5e9be4 100644</span>
<span class="gd">--- a/scrapy/utils/versions.py</span>
<span class="gi">+++ b/scrapy/utils/versions.py</span>
<span class="gu">@@ -1,11 +1,32 @@</span>
<span class="w"> </span>import platform
<span class="w"> </span>import sys
<span class="w"> </span>from typing import List, Tuple
<span class="gi">+</span>
<span class="w"> </span>import cryptography
<span class="w"> </span>import cssselect
<span class="gd">-import lxml.etree</span>
<span class="gi">+import lxml.etree  # nosec</span>
<span class="w"> </span>import parsel
<span class="w"> </span>import twisted
<span class="w"> </span>import w3lib
<span class="gi">+</span>
<span class="w"> </span>import scrapy
<span class="w"> </span>from scrapy.utils.ssl import get_openssl_version
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def scrapy_components_versions() -&gt; List[Tuple[str, str]]:</span>
<span class="gi">+    lxml_version = &quot;.&quot;.join(map(str, lxml.etree.LXML_VERSION))</span>
<span class="gi">+    libxml2_version = &quot;.&quot;.join(map(str, lxml.etree.LIBXML_VERSION))</span>
<span class="gi">+</span>
<span class="gi">+    return [</span>
<span class="gi">+        (&quot;Scrapy&quot;, scrapy.__version__),</span>
<span class="gi">+        (&quot;lxml&quot;, lxml_version),</span>
<span class="gi">+        (&quot;libxml2&quot;, libxml2_version),</span>
<span class="gi">+        (&quot;cssselect&quot;, cssselect.__version__),</span>
<span class="gi">+        (&quot;parsel&quot;, parsel.__version__),</span>
<span class="gi">+        (&quot;w3lib&quot;, w3lib.__version__),</span>
<span class="gi">+        (&quot;Twisted&quot;, twisted.version.short()),</span>
<span class="gi">+        (&quot;Python&quot;, sys.version.replace(&quot;\n&quot;, &quot;- &quot;)),</span>
<span class="gi">+        (&quot;pyOpenSSL&quot;, get_openssl_version()),</span>
<span class="gi">+        (&quot;cryptography&quot;, cryptography.__version__),</span>
<span class="gi">+        (&quot;Platform&quot;, platform.platform()),</span>
<span class="gi">+    ]</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
        <script src="../javascripts/tablesort.number.js"></script>
      
    
  </body>
</html>