
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis openhands commit0 openhands minitorch - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#openhands-minitorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis openhands commit0 openhands minitorch
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytest-summary-for-test-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Pytest Summary for test tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytests:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytests:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop1" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop2" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop3" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop4" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_batch" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv2" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_demo" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_demo
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_module_fail_forward" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_module_fail_forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_parameter" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_parameter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_linear" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_linear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_nn_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_nn_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_avg" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_avg
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max_pool" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max_pool
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_drop" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_drop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_log_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_same_as_python" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_same_as_python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu_back" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu_back
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_id" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_lt" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_lt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_eq" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_eq
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sigmoid
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_transitive" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_transitive
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_symmetric" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_symmetric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_other" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_other
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_zip_with" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_zip_with
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_prod" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_prod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_neglist" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_negList
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_central_diff" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_central_diff
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_create" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromlist" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromlist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_back_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_back_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_permute_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_permute_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_index" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromnumpy" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromnumpy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim_2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim_2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_all_dims" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_all_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_layout" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_layout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_layout_bad" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_layout_bad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_enumeration" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_enumeration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_index" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_shape_broadcast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_shape_broadcast
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_string" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_createfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_create[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_permutefast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_permute[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_mm2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_mm2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_bmmfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_bmm[fast]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_openhands-commit0_openhands">back to OpenHands summary</a></p>
<h1 id="openhands-minitorch"><strong>OpenHands</strong>: minitorch</h1>
<h2 id="pytest-summary-for-test-tests">Pytest Summary for test <code>tests</code></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">status</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: left;">xfailed</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">skipped</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">119</td>
</tr>
<tr>
<td style="text-align: left;">collected</td>
<td style="text-align: center;">119</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytests">Failed pytests:</h2>
<h3 id="test_autodiffpytest_backprop1">test_autodiff.py::test_backprop1</h3>
<details><summary> <pre>test_autodiff.py::test_backprop1</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop1() -> None:
        # Example 1: F1(0, v)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
>       var2.backward(d_output=5)

tests/test_autodiff.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:147: in backward
    backpropagate(self, d_output)
minitorch/autodiff.py:82: in backpropagate
    ordered = topological_sort(variable)
minitorch/autodiff.py:67: in topological_sort
    visit(variable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

var = Scalar(10.000000)

    def visit(var: Variable) -> None:
>       if var in visited or var.is_constant():
E       AttributeError: 'Scalar' object has no attribute 'is_constant'

minitorch/autodiff.py:60: AttributeError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop2">test_autodiff.py::test_backprop2</h3>
<details><summary> <pre>test_autodiff.py::test_backprop2</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop2() -> None:
        # Example 2: F1(0, 0)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
        var3 = Function1.apply(0, var2)
>       var3.backward(d_output=5)

tests/test_autodiff.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:147: in backward
    backpropagate(self, d_output)
minitorch/autodiff.py:82: in backpropagate
    ordered = topological_sort(variable)
minitorch/autodiff.py:67: in topological_sort
    visit(variable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

var = Scalar(20.000000)

    def visit(var: Variable) -> None:
>       if var in visited or var.is_constant():
E       AttributeError: 'Scalar' object has no attribute 'is_constant'

minitorch/autodiff.py:60: AttributeError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop3">test_autodiff.py::test_backprop3</h3>
<details><summary> <pre>test_autodiff.py::test_backprop3</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop3() -> None:
        # Example 3: F1(F1(0, v1), F1(0, v1))
        var1 = minitorch.Scalar(0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:147: in backward
    backpropagate(self, d_output)
minitorch/autodiff.py:82: in backpropagate
    ordered = topological_sort(variable)
minitorch/autodiff.py:67: in topological_sort
    visit(variable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

var = Scalar(30.000000)

    def visit(var: Variable) -> None:
>       if var in visited or var.is_constant():
E       AttributeError: 'Scalar' object has no attribute 'is_constant'

minitorch/autodiff.py:60: AttributeError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop4">test_autodiff.py::test_backprop4</h3>
<details><summary> <pre>test_autodiff.py::test_backprop4</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop4() -> None:
        # Example 4: F1(F1(0, v1), F1(0, v1))
        var0 = minitorch.Scalar(0)
        var1 = Function1.apply(0, var0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:147: in backward
    backpropagate(self, d_output)
minitorch/autodiff.py:82: in backpropagate
    ordered = topological_sort(variable)
minitorch/autodiff.py:67: in topological_sort
    visit(variable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

var = Scalar(50.000000)

    def visit(var: Variable) -> None:
>       if var in visited or var.is_constant():
E       AttributeError: 'Scalar' object has no attribute 'is_constant'

minitorch/autodiff.py:60: AttributeError
</pre>
</details>
<h3 id="test_convpytest_conv1d_simple">test_conv.py::test_conv1d_simple</h3>
<details><summary> <pre>test_conv.py::test_conv1d_simple</pre></summary><pre>
@pytest.mark.task4_1
    def test_conv1d_simple() -> None:
>       t = minitorch.tensor([0, 1, 2, 3]).view(1, 1, 4)
E       AttributeError: 'NoneType' object has no attribute 'view'

tests/test_conv.py:12: AttributeError
</pre>
</details>
<h3 id="test_convpytest_conv1d">test_conv.py::test_conv1d</h3>
<details><summary> <pre>test_conv.py::test_conv1d</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(1, 1, 6)), tensors(shape=(1, 1, 4)))

tests/test_conv.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_convpytest_conv1d_channel">test_conv.py::test_conv1d_channel</h3>
<details><summary> <pre>test_conv.py::test_conv1d_channel</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(2, 2, 6)), tensors(shape=(3, 2, 2)))

tests/test_conv.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (3, 2, 2)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_convpytest_conv">test_conv.py::test_conv</h3>
<details><summary> <pre>test_conv.py::test_conv</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(1, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 2, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_convpytest_conv_batch">test_conv.py::test_conv_batch</h3>
<details><summary> <pre>test_conv.py::test_conv_batch</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 2, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_convpytest_conv_channel">test_conv.py::test_conv_channel</h3>
<details><summary> <pre>test_conv.py::test_conv_channel</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 2, 6, 6)), tensors(shape=(3, 2, 2, 4)))

tests/test_conv.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (3, 2, 2, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_convpytest_conv2">test_conv.py::test_conv2</h3>
<details><summary> <pre>test_conv.py::test_conv2</pre></summary><pre>
@pytest.mark.task4_2
    def test_conv2() -> None:
>       t = minitorch.tensor([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]).view(
            1, 1, 4, 4
        )
E       AttributeError: 'NoneType' object has no attribute 'view'

tests/test_conv.py:59: AttributeError
</pre>
</details>
<h3 id="test_modulepytest_stacked_demo">test_module.py::test_stacked_demo</h3>
<details><summary> <pre>test_module.py::test_stacked_demo</pre></summary><pre>
@pytest.mark.task0_4
    def test_stacked_demo() -> None:
        "Check that each of the properties match"
        mod = ModuleA1()
>       np = dict(mod.named_parameters())
E       TypeError: 'NoneType' object is not iterable

tests/test_module.py:49: TypeError
</pre>
</details>
<h3 id="test_modulepytest_module">test_module.py::test_module</h3>
<details><summary> <pre>test_module.py::test_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints)

tests/test_module.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

size_a = 1, size_b = 1

    @pytest.mark.task0_4
    @given(med_ints, med_ints)
    def test_module(size_a: int, size_b: int) -> None:
        "Check the properties of a single module"
        module = Module2()
        module.eval()
>       assert not module.training
E       assert not True
E        +  where True = Module2(\n  (module_c): Module3()\n).training
E       Falsifying example: test_module(
E           size_b=1, size_a=1,
E       )

tests/test_module.py:101: AssertionError
</pre>
</details>
<h3 id="test_modulepytest_stacked_module">test_module.py::test_stacked_module</h3>
<details><summary> <pre>test_module.py::test_stacked_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints, small_floats)

tests/test_module.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

size_a = 1, size_b = 1, val = 0.0

    @pytest.mark.task0_4
    @given(med_ints, med_ints, small_floats)
    def test_stacked_module(size_a: int, size_b: int, val: float) -> None:
        "Check the properties of a stacked module"
        module = Module1(size_a, size_b, val)
        module.eval()
>       assert not module.training
E       assert not True
E        +  where True = Module1(\n  (module_a): Module2(\n    (module_c): Module3()\n  )\n  (module_b): Module2(\n    (module_c): Module3()\n  )\n).training
E       Falsifying example: test_stacked_module(
E           val=0.0, size_b=1, size_a=1,
E       )

tests/test_module.py:122: AssertionError
</pre>
</details>
<h3 id="test_modulepytest_module_fail_forward">test_module.py::test_module_fail_forward</h3>
<details><summary> <pre>test_module.py::test_module_fail_forward</pre></summary><pre>
@pytest.mark.task0_4
    @pytest.mark.xfail
    def test_module_fail_forward() -> None:
        mod = minitorch.Module()
>       mod()

tests/test_module.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module(), args = (), kwargs = {}

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
>       return self.forward(*args, **kwargs)
E       TypeError: 'NoneType' object is not callable

minitorch/module.py:79: TypeError
</pre>
</details>
<h3 id="test_modulepytest_parameter">test_module.py::test_parameter</h3>
<details><summary> <pre>test_module.py::test_parameter</pre></summary><pre>
def test_parameter() -> None:
        t = MockParam()
        q = minitorch.Parameter(t)
        print(q)
        assert t.x
        t2 = MockParam()
        q.update(t2)
>       assert t2.x
E       assert False
E        +  where False = <tests.test_module.MockParam object at 0x7fcab5bf3cd0>.x

tests/test_module.py:184: AssertionError
</pre>
</details>
<h3 id="test_modulespytest_linear">test_modules.py::test_linear</h3>
<details><summary> <pre>test_modules.py::test_linear</pre></summary><pre>
@given(lists(scalars(), max_size=10), integers(min_value=5, max_value=20))
>   def test_linear(inputs: List[Scalar], out_size: int) -> None:

tests/test_modules.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_modules.py:63: in test_linear
    mid = lin.forward(inputs)
tests/test_modules.py:53: in forward
    y = [b.value for b in self.bias]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <list_iterator object at 0x7fcab57e4ca0>

>   y = [b.value for b in self.bias]
E   AttributeError: 'NoneType' object has no attribute 'value'
E   Falsifying example: test_linear(
E       out_size=5, inputs=[],
E   )

tests/test_modules.py:53: AttributeError
</pre>
</details>
<h3 id="test_modulespytest_nn_size">test_modules.py::test_nn_size</h3>
<details><summary> <pre>test_modules.py::test_nn_size</pre></summary><pre>
def test_nn_size() -> None:
        model = Network2()
>       assert len(model.parameters()) == (
            len(model.layer1.parameters()) + len(model.layer2.parameters())
        )
E       TypeError: object of type 'NoneType' has no len()

tests/test_modules.py:85: TypeError
</pre>
</details>
<h3 id="test_nnpytest_avg">test_nn.py::test_avg</h3>
<details><summary> <pre>test_nn.py::test_avg</pre></summary><pre>
@pytest.mark.task4_3
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 4, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_nnpytest_max">test_nn.py::test_max</h3>
<details><summary> <pre>test_nn.py::test_max</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(2, 3, 4)))

tests/test_nn.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (2, 3, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_nnpytest_max_pool">test_nn.py::test_max_pool</h3>
<details><summary> <pre>test_nn.py::test_max_pool</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 4, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_nnpytest_drop">test_nn.py::test_drop</h3>
<details><summary> <pre>test_nn.py::test_drop</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors())

tests/test_nn.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_nnpytest_softmax">test_nn.py::test_softmax</h3>
<details><summary> <pre>test_nn.py::test_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 4, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_nnpytest_log_softmax">test_nn.py::test_log_softmax</h3>
<details><summary> <pre>test_nn.py::test_log_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 0 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1, 1, 4, 4)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_operatorspytest_same_as_python">test_operators.py::test_same_as_python</h3>
<details><summary> <pre>test_operators.py::test_same_as_python</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats, small_floats)

tests/test_operators.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:37: in test_same_as_python
    assert_close(mul(x, y), x * y)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = None, b = 0.0

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       TypeError: must be real number, not NoneType
E       Falsifying example: test_same_as_python(
E           y=0.0, x=0.0,
E       )

tests/strategies.py:16: TypeError
</pre>
</details>
<h3 id="test_operatorspytest_relu">test_operators.py::test_relu</h3>
<details><summary> <pre>test_operators.py::test_relu</pre></summary><pre>
+ Exception Group Traceback (most recent call last):
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 341, in from_call
  |     result: TResult | None = func()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 242, in <lambda>
  |     lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 513, in __call__
  |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120, in _hookexec
  |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 182, in _multicall
  |     return outcome.get_result()
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_result.py", line 100, in get_result
  |     raise exc.with_traceback(exc.__traceback__)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/threadexception.py", line 92, in pytest_runtest_call
  |     yield from thread_exception_runtest_hook()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/threadexception.py", line 68, in thread_exception_runtest_hook
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/unraisableexception.py", line 95, in pytest_runtest_call
  |     yield from unraisable_exception_runtest_hook()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/unraisableexception.py", line 70, in unraisable_exception_runtest_hook
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/logging.py", line 846, in pytest_runtest_call
  |     yield from self._runtest_for(item, "call")
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/logging.py", line 829, in _runtest_for
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/capture.py", line 880, in pytest_runtest_call
  |     return (yield)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/skipping.py", line 257, in pytest_runtest_call
  |     return (yield)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 103, in _multicall
  |     res = hook_impl.function(*args)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 174, in pytest_runtest_call
  |     item.runtest()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/python.py", line 1627, in runtest
  |     self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 513, in __call__
  |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120, in _hookexec
  |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 139, in _multicall
  |     raise exception.with_traceback(exception.__traceback__)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 103, in _multicall
  |     res = hook_impl.function(*args)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/python.py", line 159, in pytest_pyfunc_call
  |     result = testfunction(**testargs)
  |   File "/testbed/tests/test_operators.py", line 46, in test_relu
  |     @given(small_floats)
  |   File "/testbed/.venv/lib/python3.10/site-packages/hypothesis/core.py", line 1257, in wrapped_test
  |     raise the_error_hypothesis_found
  | exceptiongroup.ExceptionGroup: Hypothesis found 2 distinct failures. (2 sub-exceptions)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "/testbed/tests/test_operators.py", line 51, in test_relu
    |     assert relu(a) == 0.0
    | AssertionError: assert None == 0.0
    |  +  where None = relu(-1.0)
    | Falsifying example: test_relu(
    |     a=-1.0,
    | )
    +---------------- 2 ----------------
    | Traceback (most recent call last):
    |   File "/testbed/tests/test_operators.py", line 49, in test_relu
    |     assert relu(a) == a
    | AssertionError: assert None == 1.0
    |  +  where None = relu(1.0)
    | Falsifying example: test_relu(
    |     a=1.0,
    | )
    +------------------------------------
</pre>
</details>
<h3 id="test_operatorspytest_relu_back">test_operators.py::test_relu_back</h3>
<details><summary> <pre>test_operators.py::test_relu_back</pre></summary><pre>
+ Exception Group Traceback (most recent call last):
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 341, in from_call
  |     result: TResult | None = func()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 242, in <lambda>
  |     lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 513, in __call__
  |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120, in _hookexec
  |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 182, in _multicall
  |     return outcome.get_result()
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_result.py", line 100, in get_result
  |     raise exc.with_traceback(exc.__traceback__)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/threadexception.py", line 92, in pytest_runtest_call
  |     yield from thread_exception_runtest_hook()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/threadexception.py", line 68, in thread_exception_runtest_hook
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/unraisableexception.py", line 95, in pytest_runtest_call
  |     yield from unraisable_exception_runtest_hook()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/unraisableexception.py", line 70, in unraisable_exception_runtest_hook
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/logging.py", line 846, in pytest_runtest_call
  |     yield from self._runtest_for(item, "call")
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/logging.py", line 829, in _runtest_for
  |     yield
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/capture.py", line 880, in pytest_runtest_call
  |     return (yield)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 167, in _multicall
  |     teardown.throw(outcome._exception)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/skipping.py", line 257, in pytest_runtest_call
  |     return (yield)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 103, in _multicall
  |     res = hook_impl.function(*args)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/runner.py", line 174, in pytest_runtest_call
  |     item.runtest()
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/python.py", line 1627, in runtest
  |     self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 513, in __call__
  |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120, in _hookexec
  |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 139, in _multicall
  |     raise exception.with_traceback(exception.__traceback__)
  |   File "/testbed/.venv/lib/python3.10/site-packages/pluggy/_callers.py", line 103, in _multicall
  |     res = hook_impl.function(*args)
  |   File "/testbed/.venv/lib/python3.10/site-packages/_pytest/python.py", line 159, in pytest_pyfunc_call
  |     result = testfunction(**testargs)
  |   File "/testbed/tests/test_operators.py", line 55, in test_relu_back
  |     @given(small_floats, small_floats)
  |   File "/testbed/.venv/lib/python3.10/site-packages/hypothesis/core.py", line 1257, in wrapped_test
  |     raise the_error_hypothesis_found
  | exceptiongroup.ExceptionGroup: Hypothesis found 2 distinct failures. (2 sub-exceptions)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "/testbed/tests/test_operators.py", line 60, in test_relu_back
    |     assert relu_back(a, b) == 0.0
    | AssertionError: assert None == 0.0
    |  +  where None = relu_back(-1.0, 0.0)
    | Falsifying example: test_relu_back(
    |     b=0.0, a=-1.0,
    | )
    +---------------- 2 ----------------
    | Traceback (most recent call last):
    |   File "/testbed/tests/test_operators.py", line 58, in test_relu_back
    |     assert relu_back(a, b) == b
    | AssertionError: assert None == 0.0
    |  +  where None = relu_back(1.0, 0.0)
    | Falsifying example: test_relu_back(
    |     b=0.0, a=1.0,
    | )
    +------------------------------------
</pre>
</details>
<h3 id="test_operatorspytest_id">test_operators.py::test_id</h3>
<details><summary> <pre>test_operators.py::test_id</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_1
    @given(small_floats)
    def test_id(a: float) -> None:
>       assert id(a) == a
E       assert None == 0.0
E        +  where None = id(0.0)
E       Falsifying example: test_id(
E           a=0.0,
E       )

tests/test_operators.py:66: AssertionError
</pre>
</details>
<h3 id="test_operatorspytest_lt">test_operators.py::test_lt</h3>
<details><summary> <pre>test_operators.py::test_lt</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_1
    @given(small_floats)
    def test_lt(a: float) -> None:
        "Check that a - 1.0 is always less than a"
>       assert lt(a - 1.0, a) == 1.0
E       assert None == 1.0
E        +  where None = lt((0.0 - 1.0), 0.0)
E       Falsifying example: test_lt(
E           a=0.0,
E       )

tests/test_operators.py:73: AssertionError
</pre>
</details>
<h3 id="test_operatorspytest_max">test_operators.py::test_max</h3>
<details><summary> <pre>test_operators.py::test_max</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_1
    @given(small_floats)
    def test_max(a: float) -> None:
>       assert max(a - 1.0, a) == a
E       assert None == 0.0
E        +  where None = max((0.0 - 1.0), 0.0)
E       Falsifying example: test_max(
E           a=0.0,
E       )

tests/test_operators.py:80: AssertionError
</pre>
</details>
<h3 id="test_operatorspytest_eq">test_operators.py::test_eq</h3>
<details><summary> <pre>test_operators.py::test_eq</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_1
    @given(small_floats)
    def test_eq(a: float) -> None:
>       assert eq(a, a) == 1.0
E       assert None == 1.0
E        +  where None = eq(0.0, 0.0)
E       Falsifying example: test_eq(
E           a=0.0,
E       )

tests/test_operators.py:89: AssertionError
</pre>
</details>
<h3 id="test_operatorspytest_sigmoid">test_operators.py::test_sigmoid</h3>
<details><summary> <pre>test_operators.py::test_sigmoid</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats)

tests/test_operators.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_2
    @given(small_floats)
    def test_sigmoid(a: float) -> None:
        """Check properties of the sigmoid function, specifically
        * It is always between 0.0 and 1.0.
        * one minus sigmoid is the same as sigmoid of the negative
        * It crosses 0 at 0.5
        * It is  strictly increasing.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_sigmoid(
E           a=0.0,
E       )

tests/test_operators.py:111: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_transitive">test_operators.py::test_transitive</h3>
<details><summary> <pre>test_operators.py::test_transitive</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0, b = 0.0, c = 0.0

    @pytest.mark.task0_2
    @given(small_floats, small_floats, small_floats)
    def test_transitive(a: float, b: float, c: float) -> None:
        "Test the transitive property of less-than (a < b and b < c implies a < c)"
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_transitive(
E           c=0.0, b=0.0, a=0.0,
E       )

tests/test_operators.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_symmetric">test_operators.py::test_symmetric</h3>
<details><summary> <pre>test_operators.py::test_symmetric</pre></summary><pre>
@pytest.mark.task0_2
    def test_symmetric() -> None:
        """
        Write a test that ensures that :func:`minitorch.operators.mul` is symmetric, i.e.
        gives the same value regardless of the order of its input.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:129: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_distribute">test_operators.py::test_distribute</h3>
<details><summary> <pre>test_operators.py::test_distribute</pre></summary><pre>
@pytest.mark.task0_2
    def test_distribute() -> None:
        r"""
        Write a test that ensures that your operators distribute, i.e.
        :math:`z \times (x + y) = z \times x + z \times y`
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:139: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_other">test_operators.py::test_other</h3>
<details><summary> <pre>test_operators.py::test_other</pre></summary><pre>
@pytest.mark.task0_2
    def test_other() -> None:
        """
        Write a test that ensures some other property holds for your functions.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:148: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_zip_with">test_operators.py::test_zip_with</h3>
<details><summary> <pre>test_operators.py::test_zip_with</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats, small_floats)

tests/test_operators.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0, b = 0.0, c = 0.0, d = 0.0

    @pytest.mark.task0_3
    @given(small_floats, small_floats, small_floats, small_floats)
    def test_zip_with(a: float, b: float, c: float, d: float) -> None:
>       x1, x2 = addLists([a, b], [c, d])
E       TypeError: cannot unpack non-iterable NoneType object
E       Falsifying example: test_zip_with(
E           d=0.0, c=0.0, b=0.0, a=0.0,
E       )

tests/test_operators.py:160: TypeError
</pre>
</details>
<h3 id="test_operatorspytest_sum_distribute">test_operators.py::test_sum_distribute</h3>
<details><summary> <pre>test_operators.py::test_sum_distribute</pre></summary><pre>
@pytest.mark.task0_3
>   @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )

tests/test_operators.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls1 = [0.0, 0.0, 0.0, 0.0, 0.0], ls2 = [0.0, 0.0, 0.0, 0.0, 0.0]

    @pytest.mark.task0_3
    @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )
    def test_sum_distribute(ls1: List[float], ls2: List[float]) -> None:
        """
        Write a test that ensures that the sum of `ls1` plus the sum of `ls2`
        is the same as the sum of each element of `ls1` plus each element of `ls2`.
        """
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_sum_distribute(
E           ls2=[0.0, 0.0, 0.0, 0.0, 0.0], ls1=[0.0, 0.0, 0.0, 0.0, 0.0],
E       )

tests/test_operators.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sum">test_operators.py::test_sum</h3>
<details><summary> <pre>test_operators.py::test_sum</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:183: in test_sum
    assert_close(sum(ls), sum(ls))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = None, b = None

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       TypeError: must be real number, not NoneType
E       Falsifying example: test_sum(
E           ls=[],
E       )

tests/strategies.py:16: TypeError
</pre>
</details>
<h3 id="test_operatorspytest_prod">test_operators.py::test_prod</h3>
<details><summary> <pre>test_operators.py::test_prod</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:189: in test_prod
    assert_close(prod([x, y, z]), x * y * z)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = None, b = 0.0

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       TypeError: must be real number, not NoneType
E       Falsifying example: test_prod(
E           z=0.0, y=0.0, x=0.0,
E       )

tests/strategies.py:16: TypeError
</pre>
</details>
<h3 id="test_operatorspytest_neglist">test_operators.py::test_negList</h3>
<details><summary> <pre>test_operators.py::test_negList</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = []

    @pytest.mark.task0_3
    @given(lists(small_floats))
    def test_negList(ls: List[float]) -> None:
        check = negList(ls)
>       for i, j in zip(ls, check):
E       TypeError: 'NoneType' object is not iterable
E       Falsifying example: test_negList(
E           ls=[],
E       )

tests/test_operators.py:196: TypeError
</pre>
</details>
<h3 id="test_scalarpytest_central_diff">test_scalar.py::test_central_diff</h3>
<details><summary> <pre>test_scalar.py::test_central_diff</pre></summary><pre>
@pytest.mark.task1_1
    def test_central_diff() -> None:
>       d = central_difference(operators.id, 5, arg=0)

tests/test_scalar.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

f = <function id at 0x7fcac08fab90>, arg = 0, epsilon = 1e-06, vals = (5,)
vals_plus = [5.000001]

    def central_difference(f: Any, *vals: Any, arg: int=0, epsilon: float=1e-06) -> Any:
        """
        Computes an approximation to the derivative of `f` with respect to one arg.

        See :doc:`derivative` or https://en.wikipedia.org/wiki/Finite_difference for more details.

        Args:
            f : arbitrary function from n-scalar args to one value
            *vals : n-float values $x_0 \\ldots x_{n-1}$
            arg : the number $i$ of the arg to compute the derivative
            epsilon : a small constant

        Returns:
            An approximation of $f'_i(x_0, \\ldots, x_{n-1})$
        """
        vals_plus = list(vals)
        vals_minus = list(vals)
        vals_plus[arg] = vals[arg] + epsilon
        vals_minus[arg] = vals[arg] - epsilon
>       return (f(*vals_plus) - f(*vals_minus)) / (2.0 * epsilon)
E       TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'

minitorch/autodiff.py:24: TypeError
</pre>
</details>
<h3 id="test_scalarpytest_simple">test_scalar.py::test_simple</h3>
<details><summary> <pre>test_scalar.py::test_simple</pre></summary><pre>
@given(small_floats, small_floats)
>   def test_simple(a: float, b: float) -> None:

tests/test_scalar.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:58: in test_simple
    assert_close(c.data, a + b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0, b = 0.0

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=0.000000
E       Falsifying example: test_simple(
E           b=0.0, a=0.0,
E       )

tests/strategies.py:16: AssertionError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn0">test_scalar.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('lt', <function lt at 0x7fcac08fae60>), t1 = Scalar(0.000000)
t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_2
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_args(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, base_fn, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt', lt),
E       )

tests/test_scalar.py:92: ValueError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn1">test_scalar.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('eq', <function eq at 0x7fcac08faef0>), t1 = Scalar(0.000000)
t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_2
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_args(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, base_fn, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq', eq),
E       )

tests/test_scalar.py:92: ValueError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn2">test_scalar.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab582eb00>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab582eb00>)
t1 = Scalar(0.000000), t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_2
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_args(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, base_fn, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000),
E           t1=Scalar(0.000000),
E           fn=('gt', lambda x, y: operators.lt(y, x)),
E       )

tests/test_scalar.py:92: ValueError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn0">test_scalar.py::test_two_derivative[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('lt', <function lt at 0x7fcac08fae60>), t1 = Scalar(0.000000)
t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_4
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_derivative(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, _, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt', lt),
E       )

tests/test_scalar.py:119: ValueError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn1">test_scalar.py::test_two_derivative[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('eq', <function eq at 0x7fcac08faef0>), t1 = Scalar(0.000000)
t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_4
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_derivative(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, _, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq', eq),
E       )

tests/test_scalar.py:119: ValueError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn2">test_scalar.py::test_two_derivative[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab582eb00>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab582eb00>)
t1 = Scalar(0.000000), t2 = Scalar(0.000000)

    @given(small_scalars, small_scalars)
    @pytest.mark.task1_4
    @pytest.mark.parametrize("fn", two_arg)
    def test_two_derivative(
        fn: Tuple[str, Callable[[float, float], float], Callable[[Scalar, Scalar], Scalar]],
        t1: Scalar,
        t2: Scalar,
    ) -> None:
>       name, _, scalar_fn = fn
E       ValueError: not enough values to unpack (expected 3, got 2)
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000),
E           t1=Scalar(0.000000),
E           fn=('gt', lambda x, y: operators.lt(y, x)),
E       )

tests/test_scalar.py:119: ValueError
</pre>
</details>
<h3 id="test_tensorpytest_create">test_tensor.py::test_create</h3>
<details><summary> <pre>test_tensor.py::test_create</pre></summary><pre>
@given(lists(small_floats, min_size=1))
>   def test_create(t1: List[float]) -> None:

tests/test_tensor.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t1 = [0.0]

    @given(lists(small_floats, min_size=1))
    def test_create(t1: List[float]) -> None:
        "Test the ability to create an index a 1D Tensor"
        t2 = tensor(t1)
        for i in range(len(t1)):
>           assert t1[i] == t2[i]
E           TypeError: 'NoneType' object is not subscriptable
E           Falsifying example: test_create(
E               t1=[0.0],
E           )

tests/test_tensor.py:20: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn0">test_tensor.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn1">test_tensor.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn2">test_tensor.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58e81f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_permute">test_tensor.py::test_permute</h3>
<details><summary> <pre>test_tensor.py::test_permute</pre></summary><pre>
@given(data(), tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_grad_size">test_tensor.py::test_grad_size</h3>
<details><summary> <pre>test_tensor.py::test_grad_size</pre></summary><pre>
def test_grad_size() -> None:
        "Test the size of the gradient (from @WannaFy)"
        a = tensor([1], requires_grad=True)
        b = tensor([[1, 1]], requires_grad=True)

>       c = (a * b).sum()
E       TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'

tests/test_tensor.py:78: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn0">test_tensor.py::test_two_grad[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn1">test_tensor.py::test_two_grad[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn2">test_tensor.py::test_two_grad[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58e81f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn0">test_tensor.py::test_two_grad_broadcast[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn1">test_tensor.py::test_two_grad_broadcast[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn2">test_tensor.py::test_two_grad_broadcast[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58e81f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_fromlist">test_tensor.py::test_fromlist</h3>
<details><summary> <pre>test_tensor.py::test_fromlist</pre></summary><pre>
def test_fromlist() -> None:
        "Test longer from list conversion"
        t = tensor([[2, 3, 4], [4, 5, 7]])
>       assert t.shape == (2, 3)
E       AttributeError: 'NoneType' object has no attribute 'shape'

tests/test_tensor.py:132: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_view">test_tensor.py::test_view</h3>
<details><summary> <pre>test_tensor.py::test_view</pre></summary><pre>
def test_view() -> None:
        "Test view"
        t = tensor([[2, 3, 4], [4, 5, 7]])
>       assert t.shape == (2, 3)
E       AttributeError: 'NoneType' object has no attribute 'shape'

tests/test_tensor.py:140: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_back_view">test_tensor.py::test_back_view</h3>
<details><summary> <pre>test_tensor.py::test_back_view</pre></summary><pre>
@given(tensors())
>   def test_back_view(t1: Tensor) -> None:

tests/test_tensor.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensorpytest_permute_view">test_tensor.py::test_permute_view</h3>
<details><summary> <pre>test_tensor.py::test_permute_view</pre></summary><pre>
@pytest.mark.xfail
    def test_permute_view() -> None:
        t = tensor([[2, 3, 4], [4, 5, 7]])
>       assert t.shape == (2, 3)
E       AttributeError: 'NoneType' object has no attribute 'shape'

tests/test_tensor.py:165: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_index">test_tensor.py::test_index</h3>
<details><summary> <pre>test_tensor.py::test_index</pre></summary><pre>
@pytest.mark.xfail
    def test_index() -> None:
        t = tensor([[2, 3, 4], [4, 5, 7]])
>       assert t.shape == (2, 3)
E       AttributeError: 'NoneType' object has no attribute 'shape'

tests/test_tensor.py:173: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_fromnumpy">test_tensor.py::test_fromnumpy</h3>
<details><summary> <pre>test_tensor.py::test_fromnumpy</pre></summary><pre>
def test_fromnumpy() -> None:
        t = tensor([[2, 3, 4], [4, 5, 7]])
        print(t)
>       assert t.shape == (2, 3)
E       AttributeError: 'NoneType' object has no attribute 'shape'

tests/test_tensor.py:180: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim">test_tensor.py::test_reduce_forward_one_dim</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim() -> None:
        # shape (3, 2)
        t = tensor([[2, 3], [4, 6], [5, 7]])

        # here 0 means to reduce the 0th dim, 3 -> nothing
>       t_summed = t.sum(0)
E       AttributeError: 'NoneType' object has no attribute 'sum'

tests/test_tensor.py:196: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim_2">test_tensor.py::test_reduce_forward_one_dim_2</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim_2</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim_2() -> None:
        # shape (3, 2)
        t = tensor([[2, 3], [4, 6], [5, 7]])

        # here 1 means reduce the 1st dim, 2 -> nothing
>       t_summed_2 = t.sum(1)
E       AttributeError: 'NoneType' object has no attribute 'sum'

tests/test_tensor.py:209: AttributeError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_all_dims">test_tensor.py::test_reduce_forward_all_dims</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_all_dims</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_all_dims() -> None:
        # shape (3, 2)
        t = tensor([[2, 3], [4, 6], [5, 7]])

        # reduce all dims, (3 -> 1, 2 -> 1)
>       t_summed_all = t.sum()
E       AttributeError: 'NoneType' object has no attribute 'sum'

tests/test_tensor.py:222: AttributeError
</pre>
</details>
<h3 id="test_tensor_datapytest_layout">test_tensor_data.py::test_layout</h3>
<details><summary> <pre>test_tensor_data.py::test_layout</pre></summary><pre>
@pytest.mark.task2_1
    def test_layout() -> None:
        "Test basis properties of layout and strides"
        data = [0] * 3 * 5
>       tensor_data = minitorch.TensorData(data, (3, 5), (5, 1))

tests/test_tensor_data.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <minitorch.tensor_data.TensorData object at 0x7fcab5cdc430>
storage = [0, 0, 0, 0, 0, 0, ...], shape = (3, 5), strides = (5, 1)

    def __init__(self, storage: Union[Sequence[float], Storage], shape: UserShape, strides: Optional[UserStrides]=None):
        if isinstance(storage, np.ndarray):
            self._storage = storage
        else:
            self._storage = array(storage, dtype=float64)
        if strides is None:
            strides = strides_from_shape(shape)
        assert isinstance(strides, tuple), 'Strides must be tuple'
        assert isinstance(shape, tuple), 'Shape must be tuple'
        if len(strides) != len(shape):
            raise IndexingError(f'Len of strides {strides} must match {shape}.')
        self._strides = array(strides)
        self._shape = array(shape)
        self.strides = strides
        self.dims = len(strides)
>       self.size = int(prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

minitorch/tensor_data.py:111: TypeError
</pre>
</details>
<h3 id="test_tensor_datapytest_layout_bad">test_tensor_data.py::test_layout_bad</h3>
<details><summary> <pre>test_tensor_data.py::test_layout_bad</pre></summary><pre>
@pytest.mark.xfail
    def test_layout_bad() -> None:
        "Test basis properties of layout and strides"
        data = [0] * 3 * 5
>       minitorch.TensorData(data, (3, 5), (6,))

tests/test_tensor_data.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <minitorch.tensor_data.TensorData object at 0x7fcabe11b910>
storage = [0, 0, 0, 0, 0, 0, ...], shape = (3, 5), strides = (6,)

    def __init__(self, storage: Union[Sequence[float], Storage], shape: UserShape, strides: Optional[UserStrides]=None):
        if isinstance(storage, np.ndarray):
            self._storage = storage
        else:
            self._storage = array(storage, dtype=float64)
        if strides is None:
            strides = strides_from_shape(shape)
        assert isinstance(strides, tuple), 'Strides must be tuple'
        assert isinstance(shape, tuple), 'Shape must be tuple'
        if len(strides) != len(shape):
>           raise IndexingError(f'Len of strides {strides} must match {shape}.')
E           minitorch.tensor_data.IndexingError: Len of strides (6,) must match (3, 5).

minitorch/tensor_data.py:106: IndexingError
</pre>
</details>
<h3 id="test_tensor_datapytest_enumeration">test_tensor_data.py::test_enumeration</h3>
<details><summary> <pre>test_tensor_data.py::test_enumeration</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-inf, max_value=inf, allow_nan=True, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_datapytest_index">test_tensor_data.py::test_index</h3>
<details><summary> <pre>test_tensor_data.py::test_index</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-inf, max_value=inf, allow_nan=True, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_datapytest_permute">test_tensor_data.py::test_permute</h3>
<details><summary> <pre>test_tensor_data.py::test_permute</pre></summary><pre>
@pytest.mark.task2_1
>   @given(data())

tests/test_tensor_data.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_data.py:83: in test_permute
    td = data.draw(tensor_data())
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-inf, max_value=inf, allow_nan=True, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_permute(
E           data=data(...),
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_datapytest_shape_broadcast">test_tensor_data.py::test_shape_broadcast</h3>
<details><summary> <pre>test_tensor_data.py::test_shape_broadcast</pre></summary><pre>
@pytest.mark.task2_2
    def test_shape_broadcast() -> None:
        c = minitorch.shape_broadcast((1,), (5, 5))
>       assert c == (5, 5)
E       assert None == (5, 5)

tests/test_tensor_data.py:100: AssertionError
</pre>
</details>
<h3 id="test_tensor_datapytest_string">test_tensor_data.py::test_string</h3>
<details><summary> <pre>test_tensor_data.py::test_string</pre></summary><pre>
@given(tensor_data())
>   def test_string(tensor_data: TensorData) -> None:

tests/test_tensor_data.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-inf, max_value=inf, allow_nan=True, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_createfast">test_tensor_general.py::test_create[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_create[fast]</pre></summary><pre>
backend = 'fast'

    @given(lists(small_floats, min_size=1))
>   @pytest.mark.parametrize("backend", backend_tests)

tests/test_tensor_general.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

backend = 'fast', t1 = [0.0]

    @given(lists(small_floats, min_size=1))
    @pytest.mark.parametrize("backend", backend_tests)
    def test_create(backend: str, t1: List[float]) -> None:
        "Create different tensors."
        t2 = minitorch.tensor(t1, backend=shared[backend])
        for i in range(len(t1)):
>           assert t1[i] == t2[i]
E           TypeError: 'NoneType' object is not subscriptable
E           Falsifying example: test_create(
E               t1=[0.0], backend='fast',
E           )

tests/test_tensor_general.py:50: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn0">test_tensor_general.py::test_two_args[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>), backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_args(
E           data=data(...), fn=('lt', lt), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn1">test_tensor_general.py::test_two_args[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>), backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_args(
E           data=data(...), fn=('eq', eq), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn2">test_tensor_general.py::test_two_args[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58eab90>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_args(
E           data=data(...), fn=('gt', lambda x, y: operators.lt(y, x)), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn0">test_tensor_general.py::test_two_grad[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>), backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('lt', lt), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn1">test_tensor_general.py::test_two_grad[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>), backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('eq', eq), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn2">test_tensor_general.py::test_two_grad[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58eab90>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('gt', lambda x, y: operators.lt(y, x)), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn0">test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</pre></summary><pre>
fn = ('lt', <function lt at 0x7fcac08fae60>), backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('lt', lt), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn1">test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</pre></summary><pre>
fn = ('eq', <function eq at 0x7fcac08faef0>), backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('eq', eq), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn2">test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</pre></summary><pre>
fn = ('gt', <function MathTestVariable._comp_testing.<locals>.<lambda> at 0x7fcab58eab90>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('gt', lambda x, y: operators.lt(y, x)), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_permutefast">test_tensor_general.py::test_permute[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_permute[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:332: in test_permute
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 3 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (1,)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_permute(
E           data=data(...), backend='fast',
E       )

tests/tensor_strategies.py:49: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_mm2">test_tensor_general.py::test_mm2</h3>
<details><summary> <pre>test_tensor_general.py::test_mm2</pre></summary><pre>
@pytest.mark.task3_2
    def test_mm2() -> None:
        a = minitorch.rand((2, 3), backend=FastTensorBackend)
        b = minitorch.rand((3, 4), backend=FastTensorBackend)
>       c = a @ b
E       TypeError: unsupported operand type(s) for @: 'NoneType' and 'NoneType'

tests/test_tensor_general.py:345: TypeError
</pre>
</details>
<h3 id="test_tensor_generalpytest_bmmfast">test_tensor_general.py::test_bmm[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_bmm[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("backend", matmul_tests)

tests/test_tensor_general.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:370: in test_bmm
    a = data.draw(tensors(backend=shared[backend], shape=(D, A, B)))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

draw = <bound method ConjectureData.draw of ConjectureData(VALID, 4 bytes, frozen)>
numbers = FloatStrategy(min_value=-100.0, max_value=100.0, allow_nan=False, smallest_nonzero_magnitude=5e-324)
shape = (2, 2, 2)

    @composite
    def tensor_data(
        draw: DrawFn,
        numbers: SearchStrategy[float] = floats(),
        shape: Optional[UserShape] = None,
    ) -> TensorData:
        if shape is None:
            shape = draw(shapes())
>       size = int(minitorch.prod(shape))
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
E       Falsifying example: test_bmm(
E           data=data(...), backend='fast',
E       )
E       Draw 1: 2
E       Draw 2: 2
E       Draw 3: 2
E       Draw 4: 2

tests/tensor_strategies.py:49: TypeError
</pre>
</details>

<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/minitorch/autodiff.py b/minitorch/autodiff.py</span>
<span class="gh">index cb9a430..84dbd21 100644</span>
<span class="gd">--- a/minitorch/autodiff.py</span>
<span class="gi">+++ b/minitorch/autodiff.py</span>
<span class="gu">@@ -17,11 +17,31 @@ def central_difference(f: Any, *vals: Any, arg: int=0, epsilon: float=1e-06) -&gt;</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        An approximation of $f&#39;_i(x_0, \\ldots, x_{n-1})$
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    vals_plus = list(vals)</span>
<span class="gi">+    vals_minus = list(vals)</span>
<span class="gi">+    vals_plus[arg] = vals[arg] + epsilon</span>
<span class="gi">+    vals_minus[arg] = vals[arg] - epsilon</span>
<span class="gi">+    return (f(*vals_plus) - f(*vals_minus)) / (2.0 * epsilon)</span>
<span class="w"> </span>variable_count = 1

<span class="w"> </span>class Variable(Protocol):
<span class="gd">-    pass</span>
<span class="gi">+    &quot;&quot;&quot;A variable in a computation graph.&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def accumulate_derivative(self, x: Any) -&gt; None:</span>
<span class="gi">+        &quot;&quot;&quot;Add `x` to the derivative accumulated on this variable.&quot;&quot;&quot;</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def is_constant(self) -&gt; bool:</span>
<span class="gi">+        &quot;&quot;&quot;Is this a constant variable?&quot;&quot;&quot;</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def is_leaf(self) -&gt; bool:</span>
<span class="gi">+        &quot;&quot;&quot;Is this a leaf variable?&quot;&quot;&quot;</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def parents(self) -&gt; Iterable[&quot;Variable&quot;]:</span>
<span class="gi">+        &quot;&quot;&quot;Get the parents of this variable.&quot;&quot;&quot;</span>
<span class="gi">+        pass</span>

<span class="w"> </span>def topological_sort(variable: Variable) -&gt; Iterable[Variable]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -33,7 +53,19 @@ def topological_sort(variable: Variable) -&gt; Iterable[Variable]:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Non-constant Variables in topological order starting from the right.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    visited = set()</span>
<span class="gi">+    order = []</span>
<span class="gi">+</span>
<span class="gi">+    def visit(var: Variable) -&gt; None:</span>
<span class="gi">+        if var in visited or var.is_constant():</span>
<span class="gi">+            return</span>
<span class="gi">+        visited.add(var)</span>
<span class="gi">+        for parent in var.parents():</span>
<span class="gi">+            visit(parent)</span>
<span class="gi">+        order.insert(0, var)</span>
<span class="gi">+</span>
<span class="gi">+    visit(variable)</span>
<span class="gi">+    return order</span>

<span class="w"> </span>def backpropagate(variable: Variable, deriv: Any) -&gt; None:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -46,7 +78,26 @@ def backpropagate(variable: Variable, deriv: Any) -&gt; None:</span>

<span class="w"> </span>    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Get variables in topological order</span>
<span class="gi">+    ordered = topological_sort(variable)</span>
<span class="gi">+</span>
<span class="gi">+    # Store derivatives for each variable</span>
<span class="gi">+    derivatives = {variable: deriv}</span>
<span class="gi">+</span>
<span class="gi">+    # Go through the variables in reverse order</span>
<span class="gi">+    for var in ordered:</span>
<span class="gi">+        if var.is_leaf():</span>
<span class="gi">+            var.accumulate_derivative(derivatives[var])</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Get the derivative for this variable</span>
<span class="gi">+            d = derivatives[var]</span>
<span class="gi">+            # Get the parents and their derivatives through the chain rule</span>
<span class="gi">+            for parent, parent_deriv in var.chain_rule(d):</span>
<span class="gi">+                # Initialize derivative for parent if not seen before</span>
<span class="gi">+                if parent not in derivatives:</span>
<span class="gi">+                    derivatives[parent] = 0.0</span>
<span class="gi">+                # Add to parent&#39;s derivative</span>
<span class="gi">+                derivatives[parent] += parent_deriv</span>

<span class="w"> </span>@dataclass
<span class="w"> </span>class Context:
<span class="gu">@@ -58,4 +109,25 @@ class Context:</span>

<span class="w"> </span>    def save_for_backward(self, *values: Any) -&gt; None:
<span class="w"> </span>        &quot;&quot;&quot;Store the given `values` if they need to be used during backpropagation.&quot;&quot;&quot;
<span class="gd">-        pass</span>
\ No newline at end of file
<span class="gi">+        if not self.no_grad:</span>
<span class="gi">+            self.saved_values = values</span>
<span class="gi">+</span>
<span class="gi">+def grad_check(f: Any, *vals: Any, arg: int=0, epsilon: float=1e-06, rtol: float=0.001, atol: float=1e-08) -&gt; bool:</span>
<span class="gi">+    &quot;&quot;&quot;</span>
<span class="gi">+    Check that the computed gradient matches the numerical approximation.</span>
<span class="gi">+</span>
<span class="gi">+    Args:</span>
<span class="gi">+        f : arbitrary function from n-scalars to 1-scalar</span>
<span class="gi">+        *vals : n-float values $x_0 \ldots x_{n-1}$</span>
<span class="gi">+        arg : the argument to compute the gradient with respect to</span>
<span class="gi">+        epsilon : a small constant</span>
<span class="gi">+        rtol : relative tolerance</span>
<span class="gi">+        atol : absolute tolerance</span>
<span class="gi">+</span>
<span class="gi">+    Returns:</span>
<span class="gi">+        bool : whether the numerical and computed gradient are close</span>
<span class="gi">+    &quot;&quot;&quot;</span>
<span class="gi">+    numerical = central_difference(f, *vals, arg=arg, epsilon=epsilon)</span>
<span class="gi">+    computed = f(*vals)</span>
<span class="gi">+    backpropagate(computed, 1.0)</span>
<span class="gi">+    return abs(numerical - vals[arg].derivative) &lt;= (atol + rtol * abs(numerical))</span>
\ No newline at end of file
<span class="gh">diff --git a/minitorch/datasets.py b/minitorch/datasets.py</span>
<span class="gh">index 46322bd..1112bb6 100644</span>
<span class="gd">--- a/minitorch/datasets.py</span>
<span class="gi">+++ b/minitorch/datasets.py</span>
<span class="gu">@@ -8,4 +8,98 @@ class Graph:</span>
<span class="w"> </span>    N: int
<span class="w"> </span>    X: List[Tuple[float, float]]
<span class="w"> </span>    y: List[int]
<span class="gi">+</span>
<span class="gi">+def simple(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Simple dataset with two linearly separable clouds of points&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        y_val = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        label = 1 if x + y_val &gt; 0 else 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+def diag(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Dataset with two diagonal lines of points&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        if random.random() &gt; 0.5:</span>
<span class="gi">+            x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+            y_val = x + 0.2 * random.uniform(-1.0, 1.0)</span>
<span class="gi">+            label = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+            y_val = -x + 0.2 * random.uniform(-1.0, 1.0)</span>
<span class="gi">+            label = 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+def split(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Dataset with two distinct regions&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        y_val = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        if x &lt; 0:</span>
<span class="gi">+            label = 1 if y_val &gt; 0 else 0</span>
<span class="gi">+        else:</span>
<span class="gi">+            label = 1 if y_val &lt; 0 else 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+def xor(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Dataset with XOR pattern&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        y_val = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        if (x &gt; 0 and y_val &gt; 0) or (x &lt; 0 and y_val &lt; 0):</span>
<span class="gi">+            label = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            label = 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+def circle(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Dataset with points in a circle&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        x = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        y_val = random.uniform(-1.0, 1.0)</span>
<span class="gi">+        if x * x + y_val * y_val &lt; 0.5:</span>
<span class="gi">+            label = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            label = 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+def spiral(N: int = 100) -&gt; Graph:</span>
<span class="gi">+    &quot;&quot;&quot;Dataset with points in a spiral pattern&quot;&quot;&quot;</span>
<span class="gi">+    X = []</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        radius = random.uniform(0, 1)</span>
<span class="gi">+        angle = random.uniform(0, 4 * math.pi)</span>
<span class="gi">+        if random.random() &gt; 0.5:</span>
<span class="gi">+            x = radius * math.cos(angle)</span>
<span class="gi">+            y_val = radius * math.sin(angle)</span>
<span class="gi">+            label = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            x = radius * math.cos(angle + math.pi)</span>
<span class="gi">+            y_val = radius * math.sin(angle + math.pi)</span>
<span class="gi">+            label = 0</span>
<span class="gi">+        X.append((x, y_val))</span>
<span class="gi">+        y.append(label)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="w"> </span>datasets = {&#39;Simple&#39;: simple, &#39;Diag&#39;: diag, &#39;Split&#39;: split, &#39;Xor&#39;: xor, &#39;Circle&#39;: circle, &#39;Spiral&#39;: spiral}
\ No newline at end of file
<span class="gh">diff --git a/minitorch/fast_conv.py b/minitorch/fast_conv.py</span>
<span class="gh">index eddae84..137698a 100644</span>
<span class="gd">--- a/minitorch/fast_conv.py</span>
<span class="gi">+++ b/minitorch/fast_conv.py</span>
<span class="gu">@@ -41,7 +41,28 @@ def _tensor_conv1d(out: Tensor, out_shape: Shape, out_strides: Strides, out_size</span>
<span class="w"> </span>        weight_strides (Strides): strides for `input` tensor.
<span class="w"> </span>        reverse (bool): anchor weight at left or right
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch_, in_channels, width = input_shape</span>
<span class="gi">+    out_channels, _, k_width = weight_shape</span>
<span class="gi">+</span>
<span class="gi">+    # For each output position</span>
<span class="gi">+    for batch in prange(batch_):</span>
<span class="gi">+        for out_channel in prange(out_channels):</span>
<span class="gi">+            for w in prange(width):</span>
<span class="gi">+                # Sum up all the values of the input * weights</span>
<span class="gi">+                acc = 0.0</span>
<span class="gi">+                for in_channel in range(in_channels):</span>
<span class="gi">+                    for k in range(k_width):</span>
<span class="gi">+                        w_offset = k if not reverse else k_width - k - 1</span>
<span class="gi">+                        if w + w_offset &lt; width:</span>
<span class="gi">+                            # Get input position</span>
<span class="gi">+                            in_pos = index_to_position((batch, in_channel, w + w_offset), input_strides)</span>
<span class="gi">+                            # Get weight position</span>
<span class="gi">+                            w_pos = index_to_position((out_channel, in_channel, k), weight_strides)</span>
<span class="gi">+                            # Add to accumulator</span>
<span class="gi">+                            acc += input._tensor._storage[in_pos] * weight._tensor._storage[w_pos]</span>
<span class="gi">+                # Set output position</span>
<span class="gi">+                out_pos = index_to_position((batch, out_channel, w), out_strides)</span>
<span class="gi">+                out._tensor._storage[out_pos] = acc</span>
<span class="w"> </span>tensor_conv1d = njit(parallel=True)(_tensor_conv1d)

<span class="w"> </span>class Conv1dFun(Function):
<span class="gu">@@ -59,7 +80,28 @@ class Conv1dFun(Function):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            batch x out_channel x h x w
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ctx.save_for_backward(input, weight)</span>
<span class="gi">+        batch, in_channels, width = input.shape</span>
<span class="gi">+        out_channels, _, k_width = weight.shape</span>
<span class="gi">+        </span>
<span class="gi">+        # Create output tensor</span>
<span class="gi">+        out = input.zeros((batch, out_channels, width))</span>
<span class="gi">+        </span>
<span class="gi">+        # Call the conv1d implementation</span>
<span class="gi">+        tensor_conv1d(</span>
<span class="gi">+            out,</span>
<span class="gi">+            out.shape,</span>
<span class="gi">+            out.strides,</span>
<span class="gi">+            out.size,</span>
<span class="gi">+            input,</span>
<span class="gi">+            input.shape,</span>
<span class="gi">+            input.strides,</span>
<span class="gi">+            weight,</span>
<span class="gi">+            weight.shape,</span>
<span class="gi">+            weight.strides,</span>
<span class="gi">+            False,</span>
<span class="gi">+        )</span>
<span class="gi">+        return out</span>
<span class="w"> </span>conv1d = Conv1dFun.apply

<span class="w"> </span>def _tensor_conv2d(out: Tensor, out_shape: Shape, out_strides: Strides, out_size: int, input: Tensor, input_shape: Shape, input_strides: Strides, weight: Tensor, weight_shape: Shape, weight_strides: Strides, reverse: bool) -&gt; None:
<span class="gu">@@ -95,7 +137,37 @@ def _tensor_conv2d(out: Tensor, out_shape: Shape, out_strides: Strides, out_size</span>
<span class="w"> </span>        weight_strides (Strides): strides for `input` tensor.
<span class="w"> </span>        reverse (bool): anchor weight at top-left or bottom-right
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch_, in_channels, height, width = input_shape</span>
<span class="gi">+    out_channels, _, k_height, k_width = weight_shape</span>
<span class="gi">+</span>
<span class="gi">+    # For each output position</span>
<span class="gi">+    for batch in prange(batch_):</span>
<span class="gi">+        for out_channel in prange(out_channels):</span>
<span class="gi">+            for h in prange(height):</span>
<span class="gi">+                for w in prange(width):</span>
<span class="gi">+                    # Sum up all the values of the input * weights</span>
<span class="gi">+                    acc = 0.0</span>
<span class="gi">+                    for in_channel in range(in_channels):</span>
<span class="gi">+                        for k_h in range(k_height):</span>
<span class="gi">+                            for k_w in range(k_width):</span>
<span class="gi">+                                h_offset = k_h if not reverse else k_height - k_h - 1</span>
<span class="gi">+                                w_offset = k_w if not reverse else k_width - k_w - 1</span>
<span class="gi">+                                if h + h_offset &lt; height and w + w_offset &lt; width:</span>
<span class="gi">+                                    # Get input position</span>
<span class="gi">+                                    in_pos = index_to_position(</span>
<span class="gi">+                                        (batch, in_channel, h + h_offset, w + w_offset),</span>
<span class="gi">+                                        input_strides,</span>
<span class="gi">+                                    )</span>
<span class="gi">+                                    # Get weight position</span>
<span class="gi">+                                    w_pos = index_to_position(</span>
<span class="gi">+                                        (out_channel, in_channel, k_h, k_w),</span>
<span class="gi">+                                        weight_strides,</span>
<span class="gi">+                                    )</span>
<span class="gi">+                                    # Add to accumulator</span>
<span class="gi">+                                    acc += input._tensor._storage[in_pos] * weight._tensor._storage[w_pos]</span>
<span class="gi">+                    # Set output position</span>
<span class="gi">+                    out_pos = index_to_position((batch, out_channel, h, w), out_strides)</span>
<span class="gi">+                    out._tensor._storage[out_pos] = acc</span>
<span class="w"> </span>tensor_conv2d = njit(parallel=True, fastmath=True)(_tensor_conv2d)

<span class="w"> </span>class Conv2dFun(Function):
<span class="gu">@@ -113,5 +185,26 @@ class Conv2dFun(Function):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            (:class:`Tensor`) : batch x out_channel x h x w
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ctx.save_for_backward(input, weight)</span>
<span class="gi">+        batch, in_channels, height, width = input.shape</span>
<span class="gi">+        out_channels, _, k_height, k_width = weight.shape</span>
<span class="gi">+        </span>
<span class="gi">+        # Create output tensor</span>
<span class="gi">+        out = input.zeros((batch, out_channels, height, width))</span>
<span class="gi">+        </span>
<span class="gi">+        # Call the conv2d implementation</span>
<span class="gi">+        tensor_conv2d(</span>
<span class="gi">+            out,</span>
<span class="gi">+            out.shape,</span>
<span class="gi">+            out.strides,</span>
<span class="gi">+            out.size,</span>
<span class="gi">+            input,</span>
<span class="gi">+            input.shape,</span>
<span class="gi">+            input.strides,</span>
<span class="gi">+            weight,</span>
<span class="gi">+            weight.shape,</span>
<span class="gi">+            weight.strides,</span>
<span class="gi">+            False,</span>
<span class="gi">+        )</span>
<span class="gi">+        return out</span>
<span class="w"> </span>conv2d = Conv2dFun.apply
\ No newline at end of file
<span class="gh">diff --git a/minitorch/fast_ops.py b/minitorch/fast_ops.py</span>
<span class="gh">index ff6c24b..27cfb09 100644</span>
<span class="gd">--- a/minitorch/fast_ops.py</span>
<span class="gi">+++ b/minitorch/fast_ops.py</span>
<span class="gu">@@ -17,17 +17,75 @@ class FastOps(TensorOps):</span>
<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def map(fn: Callable[[float], float]) -&gt; MapProto:
<span class="w"> </span>        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _map(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            tensor_map(fn)(</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _map</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def cmap(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="gi">+        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;</span>
<span class="gi">+        def _cmap(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            tensor_map(fn)(</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _cmap</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def zip(fn: Callable[[float, float], float]) -&gt; Callable[[Tensor, Tensor], Tensor]:
<span class="w"> </span>        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _zip(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+            c_shape = shape_broadcast(a.shape, b.shape)</span>
<span class="gi">+            out = a.zeros(c_shape)</span>
<span class="gi">+            tensor_zip(fn)(</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                b._tensor._storage,</span>
<span class="gi">+                b._tensor._shape,</span>
<span class="gi">+                b._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _zip</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def reduce(fn: Callable[[float, float], float], start: float=0.0) -&gt; Callable[[Tensor, int], Tensor]:
<span class="w"> </span>        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _reduce(a: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+            out_shape = list(a.shape)</span>
<span class="gi">+            out_shape[dim] = 1</span>
<span class="gi">+            out = a.zeros(tuple(out_shape))</span>
<span class="gi">+            tensor_reduce(fn)(</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                dim,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _reduce</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def matrix_multiply(a: Tensor, b: Tensor) -&gt; Tensor:
<span class="gu">@@ -53,7 +111,28 @@ class FastOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            New tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Setup shapes</span>
<span class="gi">+        ls = list(shape_broadcast(a.shape[:-2], b.shape[:-2]))</span>
<span class="gi">+        ls.append(a.shape[-2])</span>
<span class="gi">+        ls.append(b.shape[-1])</span>
<span class="gi">+        assert a.shape[-1] == b.shape[-2]</span>
<span class="gi">+</span>
<span class="gi">+        # Create output</span>
<span class="gi">+        out = a.zeros(tuple(ls))</span>
<span class="gi">+</span>
<span class="gi">+        # Call main function</span>
<span class="gi">+        tensor_matrix_multiply(</span>
<span class="gi">+            out._tensor._storage,</span>
<span class="gi">+            out._tensor._shape,</span>
<span class="gi">+            out._tensor._strides,</span>
<span class="gi">+            a._tensor._storage,</span>
<span class="gi">+            a._tensor._shape,</span>
<span class="gi">+            a._tensor._strides,</span>
<span class="gi">+            b._tensor._storage,</span>
<span class="gi">+            b._tensor._shape,</span>
<span class="gi">+            b._tensor._strides,</span>
<span class="gi">+        )</span>
<span class="gi">+        return out</span>

<span class="w"> </span>def tensor_map(fn: Callable[[float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -71,7 +150,28 @@ def tensor_map(fn: Callable[[float], float]) -&gt; Callable[[Storage, Shape, Stride</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor map function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    @njit(parallel=True)</span>
<span class="gi">+    def _map(out_storage: Storage,</span>
<span class="gi">+             out_shape: Shape,</span>
<span class="gi">+             out_strides: Strides,</span>
<span class="gi">+             in_storage: Storage,</span>
<span class="gi">+             in_shape: Shape,</span>
<span class="gi">+             in_strides: Strides) -&gt; None:</span>
<span class="gi">+        # Check if the tensors are stride-aligned</span>
<span class="gi">+        if np.array_equal(out_strides, in_strides) and np.array_equal(out_shape, in_shape):</span>
<span class="gi">+            for i in prange(len(out_storage)):</span>
<span class="gi">+                out_storage[i] = fn(in_storage[i])</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Create index buffers</span>
<span class="gi">+            out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+            in_index = np.zeros(len(in_shape), np.int32)</span>
<span class="gi">+            for i in prange(len(out_storage)):</span>
<span class="gi">+                to_index(i, out_shape, out_index)</span>
<span class="gi">+                broadcast_index(out_index, out_shape, in_shape, in_index)</span>
<span class="gi">+                in_position = index_to_position(in_index, in_strides)</span>
<span class="gi">+                out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+                out_storage[out_position] = fn(in_storage[in_position])</span>
<span class="gi">+    return _map</span>

<span class="w"> </span>def tensor_zip(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -90,7 +190,35 @@ def tensor_zip(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor zip function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    @njit(parallel=True)</span>
<span class="gi">+    def _zip(out_storage: Storage,</span>
<span class="gi">+             out_shape: Shape,</span>
<span class="gi">+             out_strides: Strides,</span>
<span class="gi">+             a_storage: Storage,</span>
<span class="gi">+             a_shape: Shape,</span>
<span class="gi">+             a_strides: Strides,</span>
<span class="gi">+             b_storage: Storage,</span>
<span class="gi">+             b_shape: Shape,</span>
<span class="gi">+             b_strides: Strides) -&gt; None:</span>
<span class="gi">+        # Check if the tensors are stride-aligned</span>
<span class="gi">+        if (np.array_equal(out_strides, a_strides) and np.array_equal(out_strides, b_strides) and</span>
<span class="gi">+            np.array_equal(out_shape, a_shape) and np.array_equal(out_shape, b_shape)):</span>
<span class="gi">+            for i in prange(len(out_storage)):</span>
<span class="gi">+                out_storage[i] = fn(a_storage[i], b_storage[i])</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Create index buffers</span>
<span class="gi">+            out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+            a_index = np.zeros(len(a_shape), np.int32)</span>
<span class="gi">+            b_index = np.zeros(len(b_shape), np.int32)</span>
<span class="gi">+            for i in prange(len(out_storage)):</span>
<span class="gi">+                to_index(i, out_shape, out_index)</span>
<span class="gi">+                broadcast_index(out_index, out_shape, a_shape, a_index)</span>
<span class="gi">+                broadcast_index(out_index, out_shape, b_shape, b_index)</span>
<span class="gi">+                a_position = index_to_position(a_index, a_strides)</span>
<span class="gi">+                b_position = index_to_position(b_index, b_strides)</span>
<span class="gi">+                out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+                out_storage[out_position] = fn(a_storage[a_position], b_storage[b_position])</span>
<span class="gi">+    return _zip</span>

<span class="w"> </span>def tensor_reduce(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -108,7 +236,32 @@ def tensor_reduce(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Sha</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor reduce function
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    @njit(parallel=True)</span>
<span class="gi">+    def _reduce(out_storage: Storage,</span>
<span class="gi">+                out_shape: Shape,</span>
<span class="gi">+                out_strides: Strides,</span>
<span class="gi">+                in_storage: Storage,</span>
<span class="gi">+                in_shape: Shape,</span>
<span class="gi">+                in_strides: Strides,</span>
<span class="gi">+                reduce_dim: int) -&gt; None:</span>
<span class="gi">+        # Create index buffers</span>
<span class="gi">+        out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+        in_index = np.zeros(len(in_shape), np.int32)</span>
<span class="gi">+        for i in prange(len(out_storage)):</span>
<span class="gi">+            to_index(i, out_shape, out_index)</span>
<span class="gi">+            # Setup initial</span>
<span class="gi">+            in_index[:] = out_index[:]</span>
<span class="gi">+            in_index[reduce_dim] = 0</span>
<span class="gi">+            in_position = index_to_position(in_index, in_strides)</span>
<span class="gi">+            reduced = in_storage[in_position]</span>
<span class="gi">+            # Reduce over dimension</span>
<span class="gi">+            for j in range(1, in_shape[reduce_dim]):</span>
<span class="gi">+                in_index[reduce_dim] = j</span>
<span class="gi">+                in_position = index_to_position(in_index, in_strides)</span>
<span class="gi">+                reduced = fn(reduced, in_storage[in_position])</span>
<span class="gi">+            out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+            out_storage[out_position] = reduced</span>
<span class="gi">+    return _reduce</span>

<span class="w"> </span>def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides: Strides, a_storage: Storage, a_shape: Shape, a_strides: Strides, b_storage: Storage, b_shape: Shape, b_strides: Strides) -&gt; None:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -141,5 +294,39 @@ def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides: Strides</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        None : Fills in `out`
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Get dimensions</span>
<span class="gi">+    a_batch = a_shape[0] if len(a_shape) &gt; 2 else 1</span>
<span class="gi">+    b_batch = b_shape[0] if len(b_shape) &gt; 2 else 1</span>
<span class="gi">+    batch = max(a_batch, b_batch)</span>
<span class="gi">+    m = a_shape[-2]</span>
<span class="gi">+    n = b_shape[-1]</span>
<span class="gi">+    p = a_shape[-1]</span>
<span class="gi">+</span>
<span class="gi">+    # Main loop in parallel</span>
<span class="gi">+    for b in prange(batch):</span>
<span class="gi">+        for i in range(m):</span>
<span class="gi">+            for j in range(n):</span>
<span class="gi">+                # Compute output position</span>
<span class="gi">+                out_pos = (</span>
<span class="gi">+                    (b if len(out_shape) &gt; 2 else 0) * out_strides[0] if len(out_shape) &gt; 2 else 0</span>
<span class="gi">+                ) + i * out_strides[-2] + j * out_strides[-1]</span>
<span class="gi">+</span>
<span class="gi">+                # Initialize accumulator</span>
<span class="gi">+                acc = 0.0</span>
<span class="gi">+</span>
<span class="gi">+                # Inner loop - matrix multiply</span>
<span class="gi">+                for k in range(p):</span>
<span class="gi">+                    # Compute positions in a and b</span>
<span class="gi">+                    a_pos = (</span>
<span class="gi">+                        (b if len(a_shape) &gt; 2 else 0) * a_strides[0] if len(a_shape) &gt; 2 else 0</span>
<span class="gi">+                    ) + i * a_strides[-2] + k * a_strides[-1]</span>
<span class="gi">+                    b_pos = (</span>
<span class="gi">+                        (b if len(b_shape) &gt; 2 else 0) * b_strides[0] if len(b_shape) &gt; 2 else 0</span>
<span class="gi">+                    ) + k * b_strides[-2] + j * b_strides[-1]</span>
<span class="gi">+</span>
<span class="gi">+                    # Multiply and accumulate</span>
<span class="gi">+                    acc += a_storage[a_pos] * b_storage[b_pos]</span>
<span class="gi">+</span>
<span class="gi">+                # Store result</span>
<span class="gi">+                out[out_pos] = acc</span>
<span class="w"> </span>tensor_matrix_multiply = njit(parallel=True, fastmath=True)(_tensor_matrix_multiply)
\ No newline at end of file
<span class="gh">diff --git a/minitorch/scalar.py b/minitorch/scalar.py</span>
<span class="gh">index 8537995..e88d057 100644</span>
<span class="gd">--- a/minitorch/scalar.py</span>
<span class="gi">+++ b/minitorch/scalar.py</span>
<span class="gu">@@ -52,6 +52,14 @@ class Scalar:</span>
<span class="w"> </span>    def __repr__(self) -&gt; str:
<span class="w"> </span>        return &#39;Scalar(%f)&#39; % self.data

<span class="gi">+    def __hash__(self) -&gt; int:</span>
<span class="gi">+        return hash(self.unique_id)</span>
<span class="gi">+</span>
<span class="gi">+    def __eq__(self, other: Any) -&gt; bool:</span>
<span class="gi">+        if not isinstance(other, Scalar):</span>
<span class="gi">+            return False</span>
<span class="gi">+        return self.unique_id == other.unique_id</span>
<span class="gi">+</span>
<span class="w"> </span>    def __mul__(self, b: ScalarLike) -&gt; Scalar:
<span class="w"> </span>        return Mul.apply(self, b)

<span class="gu">@@ -62,25 +70,25 @@ class Scalar:</span>
<span class="w"> </span>        return Mul.apply(b, Inv.apply(self))

<span class="w"> </span>    def __add__(self, b: ScalarLike) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return Add.apply(self, b)</span>

<span class="w"> </span>    def __bool__(self) -&gt; bool:
<span class="w"> </span>        return bool(self.data)

<span class="w"> </span>    def __lt__(self, b: ScalarLike) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return LT.apply(self, b)</span>

<span class="w"> </span>    def __gt__(self, b: ScalarLike) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return LT.apply(b, self)</span>

<span class="w"> </span>    def __eq__(self, b: ScalarLike) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return EQ.apply(self, b)</span>

<span class="w"> </span>    def __sub__(self, b: ScalarLike) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return Add.apply(self, Neg.apply(b))</span>

<span class="w"> </span>    def __neg__(self) -&gt; Scalar:
<span class="gd">-        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+        return Neg.apply(self)</span>

<span class="w"> </span>    def __radd__(self, b: ScalarLike) -&gt; Scalar:
<span class="w"> </span>        return self + b
<span class="gu">@@ -96,11 +104,35 @@ class Scalar:</span>
<span class="w"> </span>        Args:
<span class="w"> </span>            x: value to be accumulated
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.derivative is None:</span>
<span class="gi">+            self.derivative = 0.0</span>
<span class="gi">+        self.derivative += x</span>

<span class="w"> </span>    def is_leaf(self) -&gt; bool:
<span class="w"> </span>        &quot;&quot;&quot;True if this variable created by the user (no `last_fn`)&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.history.last_fn is None</span>
<span class="gi">+</span>
<span class="gi">+    def chain_rule(self, d_output: float) -&gt; Tuple[Tuple[Variable, float], ...]:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Implement the derivative chain-rule.</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            d_output (float): derivative of the output</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            List of tuples of (variable, derivative), where each is the derivative of the output with respect to</span>
<span class="gi">+            one of the inputs.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        if self.history.last_fn is None:</span>
<span class="gi">+            return ()</span>
<span class="gi">+        derivatives = self.history.last_fn.chain_rule(self.history.ctx, self.history.inputs, d_output)</span>
<span class="gi">+        return tuple((var, deriv) for var, deriv in zip(self.history.inputs, derivatives))</span>
<span class="gi">+</span>
<span class="gi">+    def parents(self) -&gt; Iterable[Variable]:</span>
<span class="gi">+        &quot;&quot;&quot;Get the parents of this variable.&quot;&quot;&quot;</span>
<span class="gi">+        if self.history.last_fn is None:</span>
<span class="gi">+            return []</span>
<span class="gi">+        return self.history.inputs</span>

<span class="w"> </span>    def backward(self, d_output: Optional[float]=None) -&gt; None:
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gu">@@ -110,7 +142,9 @@ class Scalar:</span>
<span class="w"> </span>            d_output (number, opt): starting derivative to backpropagate through the model
<span class="w"> </span>                                   (typically left out, and assumed to be 1.0).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if d_output is None:</span>
<span class="gi">+            d_output = 1.0</span>
<span class="gi">+        backpropagate(self, d_output)</span>

<span class="w"> </span>def derivative_check(f: Any, *scalars: Scalar) -&gt; None:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -121,4 +155,14 @@ def derivative_check(f: Any, *scalars: Scalar) -&gt; None:</span>
<span class="w"> </span>        f : function from n-scalars to 1-scalar.
<span class="w"> </span>        *scalars  : n input scalar values.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
\ No newline at end of file
<span class="gi">+    out = f(*scalars)</span>
<span class="gi">+    out.backward()</span>
<span class="gi">+</span>
<span class="gi">+    for i, scalar in enumerate(scalars):</span>
<span class="gi">+        if not scalar.is_leaf():</span>
<span class="gi">+            continue</span>
<span class="gi">+        numerical = central_difference(f, *scalars, arg=i)</span>
<span class="gi">+        assert abs(numerical - scalar.derivative) &lt; 1e-3, (</span>
<span class="gi">+            f&quot;Derivative check failed. Variable {i} has derivative {scalar.derivative} but &quot;</span>
<span class="gi">+            f&quot;numerical derivative is {numerical}.&quot;</span>
<span class="gi">+        )</span>
\ No newline at end of file
<span class="gh">diff --git a/minitorch/scalar_functions.py b/minitorch/scalar_functions.py</span>
<span class="gh">index d8dfe6f..f6d31a8 100644</span>
<span class="gd">--- a/minitorch/scalar_functions.py</span>
<span class="gi">+++ b/minitorch/scalar_functions.py</span>
<span class="gu">@@ -9,11 +9,15 @@ if TYPE_CHECKING:</span>

<span class="w"> </span>def wrap_tuple(x):
<span class="w"> </span>    &quot;&quot;&quot;Turn a possible value into a tuple&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(x, tuple):</span>
<span class="gi">+        return x</span>
<span class="gi">+    return (x,)</span>

<span class="w"> </span>def unwrap_tuple(x):
<span class="w"> </span>    &quot;&quot;&quot;Turn a singleton tuple into a value&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if len(x) == 1:</span>
<span class="gi">+        return x[0]</span>
<span class="gi">+    return x</span>

<span class="w"> </span>class ScalarFunction:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -23,33 +27,163 @@ class ScalarFunction:</span>
<span class="w"> </span>    This is a static class and is never instantiated. We use `class`
<span class="w"> </span>    here to group together the `forward` and `backward` code.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+    @classmethod</span>
<span class="gi">+    def chain_rule(cls, ctx: Context, inputs: Tuple[Scalar, ...], d_output: float) -&gt; Tuple[float, ...]:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Implements the chain rule for a function.</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            ctx: Context from running forward</span>
<span class="gi">+            inputs: Inputs to the function</span>
<span class="gi">+            d_output: Derivative of the output</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            List of derivatives of the input</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        d_inputs = cls.backward(ctx, d_output)</span>
<span class="gi">+        return wrap_tuple(d_inputs)</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def apply(cls, *vals: ScalarLike) -&gt; Scalar:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Apply function forward to the arguments.</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            vals: Values for the function</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            A new Variable with fn as operation.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        raw_vals = []</span>
<span class="gi">+        scalars = []</span>
<span class="gi">+        for v in vals:</span>
<span class="gi">+            if isinstance(v, minitorch.scalar.Scalar):</span>
<span class="gi">+                scalars.append(v)</span>
<span class="gi">+                raw_vals.append(v.data)</span>
<span class="gi">+            else:</span>
<span class="gi">+                scalars.append(minitorch.scalar.Scalar(v))</span>
<span class="gi">+                raw_vals.append(v)</span>
<span class="gi">+</span>
<span class="gi">+        # Create the context.</span>
<span class="gi">+        ctx = Context()</span>
<span class="gi">+</span>
<span class="gi">+        # Call forward with the variables.</span>
<span class="gi">+        c = cls.forward(ctx, *raw_vals)</span>
<span class="gi">+        assert isinstance(c, float), &quot;Expected return type float got %s&quot; % (type(c))</span>
<span class="gi">+</span>
<span class="gi">+        # Create a new variable from the result with a new history.</span>
<span class="gi">+        back = minitorch.scalar.ScalarHistory(cls, ctx, scalars)</span>
<span class="gi">+        return minitorch.scalar.Scalar(c, back)</span>

<span class="w"> </span>class Add(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Addition function $f(x, y) = x + y$&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        return a + b</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        return d_output, d_output</span>

<span class="w"> </span>class Log(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Log function $f(x) = log(x)$&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.log(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        return operators.log_back(a, d_output)</span>

<span class="w"> </span>class Mul(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Multiplication function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a, b)</span>
<span class="gi">+        return operators.mul(a, b)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        a, b = ctx.saved_values</span>
<span class="gi">+        return b * d_output, a * d_output</span>

<span class="w"> </span>class Inv(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Inverse function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.inv(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        return operators.inv_back(a, d_output)</span>

<span class="w"> </span>class Neg(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Negation function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        return operators.neg(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        return -d_output</span>

<span class="w"> </span>class Sigmoid(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Sigmoid function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.sigmoid(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        sig_a = operators.sigmoid(a)</span>
<span class="gi">+        return d_output * sig_a * (1 - sig_a)</span>

<span class="w"> </span>class ReLU(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;ReLU function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.relu(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        return operators.relu_back(a, d_output)</span>

<span class="w"> </span>class Exp(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Exp function&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.exp(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        return d_output * operators.exp(a)</span>

<span class="w"> </span>class LT(ScalarFunction):
<span class="w"> </span>    &quot;&quot;&quot;Less-than function $f(x) =$ 1.0 if x is less than y else 0.0&quot;&quot;&quot;
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        return operators.lt(a, b)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        return 0.0, 0.0</span>

<span class="w"> </span>class EQ(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Equal function $f(x) =$ 1.0 if x is equal to y else 0.0&quot;&quot;&quot;</span>
\ No newline at end of file
<span class="gi">+    &quot;&quot;&quot;Equal function $f(x) =$ 1.0 if x is equal to y else 0.0&quot;&quot;&quot;</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        return operators.eq(a, b)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        return 0.0, 0.0</span>
\ No newline at end of file
<span class="gh">diff --git a/minitorch/tensor_functions.py b/minitorch/tensor_functions.py</span>
<span class="gh">index a2e29c9..e73d3fa 100644</span>
<span class="gd">--- a/minitorch/tensor_functions.py</span>
<span class="gi">+++ b/minitorch/tensor_functions.py</span>
<span class="gu">@@ -19,7 +19,35 @@ def wrap_tuple(x):</span>
<span class="w"> </span>    pass

<span class="w"> </span>class Function:
<span class="gd">-    pass</span>
<span class="gi">+    &quot;&quot;&quot;</span>
<span class="gi">+    Base class for function implementations.</span>
<span class="gi">+    &quot;&quot;&quot;</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def apply(cls, *vals: Tensor) -&gt; Tensor:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Apply function forward to the arguments.</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            vals: input tensors</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            output tensor</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        raw_vals = []</span>
<span class="gi">+        need_grad = False</span>
<span class="gi">+        for v in vals:</span>
<span class="gi">+            if v.requires_grad:</span>
<span class="gi">+                need_grad = True</span>
<span class="gi">+            raw_vals.append(v)</span>
<span class="gi">+</span>
<span class="gi">+        # Create the context.</span>
<span class="gi">+        ctx = Context(not need_grad)</span>
<span class="gi">+</span>
<span class="gi">+        # Call forward with the variables.</span>
<span class="gi">+        c = cls()</span>
<span class="gi">+        ret = c.forward(ctx, *raw_vals)</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>class Neg(Function):
<span class="w"> </span>    pass
<span class="gh">diff --git a/minitorch/tensor_ops.py b/minitorch/tensor_ops.py</span>
<span class="gh">index e5bb9eb..90cc0e8 100644</span>
<span class="gd">--- a/minitorch/tensor_ops.py</span>
<span class="gi">+++ b/minitorch/tensor_ops.py</span>
<span class="gu">@@ -84,7 +84,54 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            new tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _map(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            tensor_map(fn)(</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _map</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def cmap(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Higher-order tensor map function that creates a new tensor ::</span>
<span class="gi">+</span>
<span class="gi">+          fn_map = map(fn)</span>
<span class="gi">+          out = fn_map(a)</span>
<span class="gi">+</span>
<span class="gi">+        Simple version::</span>
<span class="gi">+</span>
<span class="gi">+            for i:</span>
<span class="gi">+                for j:</span>
<span class="gi">+                    out[i, j] = fn(a[i, j])</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            fn: function from float-to-float to apply.</span>
<span class="gi">+            a (:class:`TensorData`): tensor to map over</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            new tensor data</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        def _cmap(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            tensor_map(fn)(</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _cmap</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def zip(fn: Callable[[float, float], float]) -&gt; Callable[[&#39;Tensor&#39;, &#39;Tensor&#39;], &#39;Tensor&#39;]:
<span class="gu">@@ -115,7 +162,22 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            :class:`TensorData` : new tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _zip(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+            c_shape = shape_broadcast(a.shape, b.shape)</span>
<span class="gi">+            out = a.zeros(c_shape)</span>
<span class="gi">+            tensor_zip(fn)(</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                b._tensor._storage,</span>
<span class="gi">+                b._tensor._shape,</span>
<span class="gi">+                b._tensor._strides,</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _zip</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def reduce(fn: Callable[[float, float], float], start: float=0.0) -&gt; Callable[[&#39;Tensor&#39;, int], &#39;Tensor&#39;]:
<span class="gu">@@ -141,7 +203,56 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            :class:`TensorData` : new tensor
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def _reduce(a: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+            out_shape = list(a.shape)</span>
<span class="gi">+            out_shape[dim] = 1</span>
<span class="gi">+            out = a.zeros(tuple(out_shape))</span>
<span class="gi">+            tensor_reduce(fn)(</span>
<span class="gi">+                a._tensor._storage,</span>
<span class="gi">+                a._tensor._shape,</span>
<span class="gi">+                a._tensor._strides,</span>
<span class="gi">+                out._tensor._storage,</span>
<span class="gi">+                out._tensor._shape,</span>
<span class="gi">+                out._tensor._strides,</span>
<span class="gi">+                dim,</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+        return _reduce</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def matrix_multiply(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Batched matrix multiplication of two tensors.</span>
<span class="gi">+</span>
<span class="gi">+        Args:</span>
<span class="gi">+            a : batch1 x n x m tensor</span>
<span class="gi">+            b : batch2 x m x p tensor</span>
<span class="gi">+</span>
<span class="gi">+        Returns:</span>
<span class="gi">+            A tensor of size batch1 x batch2 x n x p</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        # Extract dimensions</span>
<span class="gi">+        batch1, n, m = a.shape</span>
<span class="gi">+        batch2, m2, p = b.shape</span>
<span class="gi">+        assert m == m2, f&quot;Incompatible dimensions: {m} != {m2}&quot;</span>
<span class="gi">+</span>
<span class="gi">+        # Create output tensor</span>
<span class="gi">+        out = a.zeros((batch1, batch2, n, p))</span>
<span class="gi">+</span>
<span class="gi">+        # Perform matrix multiplication</span>
<span class="gi">+        for i in range(batch1):</span>
<span class="gi">+            for j in range(batch2):</span>
<span class="gi">+                for k in range(n):</span>
<span class="gi">+                    for l in range(p):</span>
<span class="gi">+                        sum_val = 0.0</span>
<span class="gi">+                        for t in range(m):</span>
<span class="gi">+                            sum_val += a._tensor._storage[a._tensor._strides[0] * i + a._tensor._strides[1] * k + a._tensor._strides[2] * t] * \</span>
<span class="gi">+                                     b._tensor._storage[b._tensor._strides[0] * j + b._tensor._strides[1] * t + b._tensor._strides[2] * l]</span>
<span class="gi">+                        out_idx = out._tensor._strides[0] * i + out._tensor._strides[1] * j + out._tensor._strides[2] * k + out._tensor._strides[3] * l</span>
<span class="gi">+                        out._tensor._storage[out_idx] = sum_val</span>
<span class="gi">+</span>
<span class="gi">+        return out</span>
<span class="gi">+</span>
<span class="w"> </span>    is_cuda = False

<span class="w"> </span>def tensor_map(fn: Callable[[float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:
<span class="gu">@@ -167,7 +278,21 @@ def tensor_map(fn: Callable[[float], float]) -&gt; Callable[[Storage, Shape, Stride</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor map function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    def _map(in_storage: Storage,</span>
<span class="gi">+             in_shape: Shape,</span>
<span class="gi">+             in_strides: Strides,</span>
<span class="gi">+             out_storage: Storage,</span>
<span class="gi">+             out_shape: Shape,</span>
<span class="gi">+             out_strides: Strides) -&gt; None:</span>
<span class="gi">+        out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+        in_index = np.zeros(len(in_shape), np.int32)</span>
<span class="gi">+        for i in range(len(out_storage)):</span>
<span class="gi">+            to_index(i, out_shape, out_index)</span>
<span class="gi">+            broadcast_index(out_index, out_shape, in_shape, in_index)</span>
<span class="gi">+            in_position = index_to_position(in_index, in_strides)</span>
<span class="gi">+            out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+            out_storage[out_position] = fn(in_storage[in_position])</span>
<span class="gi">+    return _map</span>

<span class="w"> </span>def tensor_zip(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -192,7 +317,27 @@ def tensor_zip(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor zip function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    def _zip(a_storage: Storage,</span>
<span class="gi">+             a_shape: Shape,</span>
<span class="gi">+             a_strides: Strides,</span>
<span class="gi">+             b_storage: Storage,</span>
<span class="gi">+             b_shape: Shape,</span>
<span class="gi">+             b_strides: Strides,</span>
<span class="gi">+             out_storage: Storage,</span>
<span class="gi">+             out_shape: Shape,</span>
<span class="gi">+             out_strides: Strides) -&gt; None:</span>
<span class="gi">+        out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+        a_index = np.zeros(len(a_shape), np.int32)</span>
<span class="gi">+        b_index = np.zeros(len(b_shape), np.int32)</span>
<span class="gi">+        for i in range(len(out_storage)):</span>
<span class="gi">+            to_index(i, out_shape, out_index)</span>
<span class="gi">+            broadcast_index(out_index, out_shape, a_shape, a_index)</span>
<span class="gi">+            broadcast_index(out_index, out_shape, b_shape, b_index)</span>
<span class="gi">+            a_position = index_to_position(a_index, a_strides)</span>
<span class="gi">+            b_position = index_to_position(b_index, b_strides)</span>
<span class="gi">+            out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+            out_storage[out_position] = fn(a_storage[a_position], b_storage[b_position])</span>
<span class="gi">+    return _zip</span>

<span class="w"> </span>def tensor_reduce(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -207,5 +352,28 @@ def tensor_reduce(fn: Callable[[float, float], float]) -&gt; Callable[[Storage, Sha</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor reduce function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    def _reduce(a_storage: Storage,</span>
<span class="gi">+                a_shape: Shape,</span>
<span class="gi">+                a_strides: Strides,</span>
<span class="gi">+                out_storage: Storage,</span>
<span class="gi">+                out_shape: Shape,</span>
<span class="gi">+                out_strides: Strides,</span>
<span class="gi">+                reduce_dim: int) -&gt; None:</span>
<span class="gi">+        out_index = np.zeros(len(out_shape), np.int32)</span>
<span class="gi">+        a_index = np.zeros(len(a_shape), np.int32)</span>
<span class="gi">+        for i in range(len(out_storage)):</span>
<span class="gi">+            to_index(i, out_shape, out_index)</span>
<span class="gi">+            out_position = index_to_position(out_index, out_strides)</span>
<span class="gi">+            # Setup initial</span>
<span class="gi">+            a_index[:] = out_index[:]</span>
<span class="gi">+            a_index[reduce_dim] = 0</span>
<span class="gi">+            a_position = index_to_position(a_index, a_strides)</span>
<span class="gi">+            reduced = a_storage[a_position]</span>
<span class="gi">+            # Reduce over dimension</span>
<span class="gi">+            for j in range(1, a_shape[reduce_dim]):</span>
<span class="gi">+                a_index[reduce_dim] = j</span>
<span class="gi">+                a_position = index_to_position(a_index, a_strides)</span>
<span class="gi">+                reduced = fn(reduced, a_storage[a_position])</span>
<span class="gi">+            out_storage[out_position] = reduced</span>
<span class="gi">+    return _reduce</span>
<span class="w"> </span>SimpleBackend = TensorBackend(SimpleOps)
\ No newline at end of file
<span class="gh">diff --git a/minitorch/testing.py b/minitorch/testing.py</span>
<span class="gh">index 72ab137..209bc02 100644</span>
<span class="gd">--- a/minitorch/testing.py</span>
<span class="gi">+++ b/minitorch/testing.py</span>
<span class="gu">@@ -7,84 +7,114 @@ class MathTest(Generic[A]):</span>
<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def neg(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Negate the argument&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.neg(a)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def addConstant(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Add contant to the argument&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.add(a, 10.0)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def square(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Manual square&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(a, a)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def cube(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Manual cube&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(operators.mul(a, a), a)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def subConstant(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Subtract a constant from the argument&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.add(a, -5.0)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def multConstant(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Multiply a constant to the argument&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(a, 5.0)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def div(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Divide by a constant&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(a, 0.2)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def inv(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Invert after adding&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.inv(operators.add(a, 2.0))</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def sig(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Apply sigmoid&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.sigmoid(a)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def log(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Apply log to a large value&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.log(operators.add(a, 100.0))</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def relu(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Apply relu&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.relu(a)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def exp(a: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Apply exp to a smaller value&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.exp(operators.add(a, -100.0))</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def add2(a: A, b: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Add two arguments&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.add(a, b)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def mul2(a: A, b: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Mul two arguments&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(a, b)</span>

<span class="w"> </span>    @staticmethod
<span class="w"> </span>    def div2(a: A, b: A) -&gt; A:
<span class="w"> </span>        &quot;&quot;&quot;Divide two arguments&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return operators.mul(a, operators.inv(b))</span>

<span class="w"> </span>    @classmethod
<span class="w"> </span>    def _tests(cls) -&gt; Tuple[Tuple[str, Callable[[A], A]], Tuple[str, Callable[[A, A], A]], Tuple[str, Callable[[Iterable[A]], A]]]:
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns a list of all the math tests.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        one_arg = [</span>
<span class="gi">+            (&quot;neg&quot;, cls.neg),</span>
<span class="gi">+            (&quot;addConstant&quot;, cls.addConstant),</span>
<span class="gi">+            (&quot;square&quot;, cls.square),</span>
<span class="gi">+            (&quot;cube&quot;, cls.cube),</span>
<span class="gi">+            (&quot;subConstant&quot;, cls.subConstant),</span>
<span class="gi">+            (&quot;multConstant&quot;, cls.multConstant),</span>
<span class="gi">+            (&quot;div&quot;, cls.div),</span>
<span class="gi">+            (&quot;inv&quot;, cls.inv),</span>
<span class="gi">+            (&quot;sig&quot;, cls.sig),</span>
<span class="gi">+            (&quot;log&quot;, cls.log),</span>
<span class="gi">+            (&quot;relu&quot;, cls.relu),</span>
<span class="gi">+            (&quot;exp&quot;, cls.exp),</span>
<span class="gi">+        ]</span>
<span class="gi">+        two_arg = [</span>
<span class="gi">+            (&quot;add2&quot;, cls.add2),</span>
<span class="gi">+            (&quot;mul2&quot;, cls.mul2),</span>
<span class="gi">+            (&quot;div2&quot;, cls.div2),</span>
<span class="gi">+        ]</span>
<span class="gi">+        return tuple(one_arg), tuple(two_arg), tuple()</span>

<span class="w"> </span>class MathTestVariable(MathTest):
<span class="gd">-    pass</span>
\ No newline at end of file
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _comp_testing() -&gt; Tuple[Tuple[str, Callable[[A], A]], Tuple[str, Callable[[A, A], A]], Tuple[str, Callable[[Iterable[A]], A]]]:</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Returns a list of all the comparison tests.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        one_arg = []</span>
<span class="gi">+        two_arg = [</span>
<span class="gi">+            (&quot;lt&quot;, operators.lt),</span>
<span class="gi">+            (&quot;eq&quot;, operators.eq),</span>
<span class="gi">+            (&quot;gt&quot;, lambda x, y: operators.lt(y, x)),</span>
<span class="gi">+        ]</span>
<span class="gi">+        return tuple(one_arg), tuple(two_arg), tuple()</span>
\ No newline at end of file
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
        <script src="../javascripts/tablesort.number.js"></script>
      
    
  </body>
</html>