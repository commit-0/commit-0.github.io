
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis commit0 all reference joblib - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reference-gold-joblib" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis commit0 all reference joblib
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytest-summary-for-test-test" class="md-nav__link">
    <span class="md-ellipsis">
      Pytest Summary for test test
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytests:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytests:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_memmappingpytest_child_raises_parent_exits_cleanlymultiprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      test_memmapping.py::test_child_raises_parent_exits_cleanly[multiprocessing]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_parallelpytest_main_thread_renamed_no_warningmultiprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      test_parallel.py::test_main_thread_renamed_no_warning[multiprocessing]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_parallelpytest_main_thread_renamed_no_warningbackend7" class="md-nav__link">
    <span class="md-ellipsis">
      test_parallel.py::test_main_thread_renamed_no_warning[backend7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_parallelpytest_nested_exception_dispatchmultiprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      test_parallel.py::test_nested_exception_dispatch[multiprocessing]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_parallelpytest_nested_exception_dispatchloky" class="md-nav__link">
    <span class="md-ellipsis">
      test_parallel.py::test_nested_exception_dispatch[loky]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_parallelpytest_nested_exception_dispatchthreading" class="md-nav__link">
    <span class="md-ellipsis">
      test_parallel.py::test_nested_exception_dispatch[threading]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_commit0_all_reference">back to Reference (Gold) summary</a></p>
<h1 id="reference-gold-joblib"><strong>Reference (Gold)</strong>: joblib</h1>
<h2 id="pytest-summary-for-test-test">Pytest Summary for test <code>test</code></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">status</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">1441</td>
</tr>
<tr>
<td style="text-align: left;">skipped</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: left;">xpassed</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">1482</td>
</tr>
<tr>
<td style="text-align: left;">collected</td>
<td style="text-align: center;">1482</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytests">Failed pytests:</h2>
<h3 id="test_memmappingpytest_child_raises_parent_exits_cleanlymultiprocessing">test_memmapping.py::test_child_raises_parent_exits_cleanly[multiprocessing]</h3>
<details><summary> <pre>test_memmapping.py::test_child_raises_parent_exits_cleanly[multiprocessing]</pre></summary><pre>

</pre>
</details>
<h3 id="test_parallelpytest_main_thread_renamed_no_warningmultiprocessing">test_parallel.py::test_main_thread_renamed_no_warning[multiprocessing]</h3>
<details><summary> <pre>test_parallel.py::test_main_thread_renamed_no_warning[multiprocessing]</pre></summary><pre>
backend = 'multiprocessing'
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc0ecd00fe0>

    @parametrize('backend', ALL_VALID_BACKENDS)
    def test_main_thread_renamed_no_warning(backend, monkeypatch):
        # Check that no default backend relies on the name of the main thread:
        # https://github.com/joblib/joblib/issues/180#issuecomment-253266247
        # Some programs use a different name for the main thread. This is the case
        # for uWSGI apps for instance.
        monkeypatch.setattr(target=threading.current_thread(), name='name',
                            value='some_new_name_for_the_main_thread')

        with warnings.catch_warnings(record=True) as warninfo:
            results = Parallel(n_jobs=2, backend=backend)(
                delayed(square)(x) for x in range(3))
            assert results == [0, 1, 4]

        # Due to the default parameters of LokyBackend, there is a chance that
        # warninfo catches Warnings from worker timeouts. We remove it if it exists
        warninfo = [w for w in warninfo if "worker timeout" not in str(w.message)]

        # The multiprocessing backend will raise a warning when detecting that is
        # started from the non-main thread. Let's check that there is no false
        # positive because of the name change.
>       assert len(warninfo) == 0
E       assert 2 == 0
E        +  where 2 = len([<warnings.WarningMessage object at 0x7fc0ecd02630>, <warnings.WarningMessage object at 0x7fc0ecd03800>])

joblib/test/test_parallel.py:199: AssertionError
</pre>
</details>
<h3 id="test_parallelpytest_main_thread_renamed_no_warningbackend7">test_parallel.py::test_main_thread_renamed_no_warning[backend7]</h3>
<details><summary> <pre>test_parallel.py::test_main_thread_renamed_no_warning[backend7]</pre></summary><pre>
backend = <joblib._parallel_backends.MultiprocessingBackend object at 0x7fc0ee7aa9f0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc0ecd57d40>

    @parametrize('backend', ALL_VALID_BACKENDS)
    def test_main_thread_renamed_no_warning(backend, monkeypatch):
        # Check that no default backend relies on the name of the main thread:
        # https://github.com/joblib/joblib/issues/180#issuecomment-253266247
        # Some programs use a different name for the main thread. This is the case
        # for uWSGI apps for instance.
        monkeypatch.setattr(target=threading.current_thread(), name='name',
                            value='some_new_name_for_the_main_thread')

        with warnings.catch_warnings(record=True) as warninfo:
            results = Parallel(n_jobs=2, backend=backend)(
                delayed(square)(x) for x in range(3))
            assert results == [0, 1, 4]

        # Due to the default parameters of LokyBackend, there is a chance that
        # warninfo catches Warnings from worker timeouts. We remove it if it exists
        warninfo = [w for w in warninfo if "worker timeout" not in str(w.message)]

        # The multiprocessing backend will raise a warning when detecting that is
        # started from the non-main thread. Let's check that there is no false
        # positive because of the name change.
>       assert len(warninfo) == 0
E       assert 2 == 0
E        +  where 2 = len([<warnings.WarningMessage object at 0x7fc0ecd01fd0>, <warnings.WarningMessage object at 0x7fc0ecd01940>])

joblib/test/test_parallel.py:199: AssertionError
</pre>
</details>
<h3 id="test_parallelpytest_nested_exception_dispatchmultiprocessing">test_parallel.py::test_nested_exception_dispatch[multiprocessing]</h3>
<details><summary> <pre>test_parallel.py::test_nested_exception_dispatch[multiprocessing]</pre></summary><pre>

</pre>
</details>
<h3 id="test_parallelpytest_nested_exception_dispatchloky">test_parallel.py::test_nested_exception_dispatch[loky]</h3>
<details><summary> <pre>test_parallel.py::test_nested_exception_dispatch[loky]</pre></summary><pre>

</pre>
</details>
<h3 id="test_parallelpytest_nested_exception_dispatchthreading">test_parallel.py::test_nested_exception_dispatch[threading]</h3>
<details><summary> <pre>test_parallel.py::test_nested_exception_dispatch[threading]</pre></summary><pre>

</pre>
</details>

<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/joblib/_cloudpickle_wrapper.py b/joblib/_cloudpickle_wrapper.py</span>
<span class="gh">index 78a1b36..daf899d 100644</span>
<span class="gd">--- a/joblib/_cloudpickle_wrapper.py</span>
<span class="gi">+++ b/joblib/_cloudpickle_wrapper.py</span>
<span class="gu">@@ -2,9 +2,18 @@</span>
<span class="w"> </span>Small shim of loky&#39;s cloudpickle_wrapper to avoid failure when
<span class="w"> </span>multiprocessing is not available.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>from ._multiprocessing_helpers import mp
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _my_wrap_non_picklable_objects(obj, keep_wrapper=True):</span>
<span class="gi">+    return obj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    from .externals.loky import wrap_non_picklable_objects
<span class="w"> </span>else:
<span class="w"> </span>    wrap_non_picklable_objects = _my_wrap_non_picklable_objects
<span class="gd">-__all__ = [&#39;wrap_non_picklable_objects&#39;]</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;wrap_non_picklable_objects&quot;]</span>
<span class="gh">diff --git a/joblib/_dask.py b/joblib/_dask.py</span>
<span class="gh">index 726f453..4288ed0 100644</span>
<span class="gd">--- a/joblib/_dask.py</span>
<span class="gi">+++ b/joblib/_dask.py</span>
<span class="gu">@@ -1,30 +1,56 @@</span>
<span class="w"> </span>from __future__ import print_function, division, absolute_import
<span class="gi">+</span>
<span class="w"> </span>import asyncio
<span class="w"> </span>import concurrent.futures
<span class="w"> </span>import contextlib
<span class="gi">+</span>
<span class="w"> </span>import time
<span class="w"> </span>from uuid import uuid4
<span class="w"> </span>import weakref
<span class="gi">+</span>
<span class="w"> </span>from .parallel import parallel_config
<span class="w"> </span>from .parallel import AutoBatchingMixin, ParallelBackendBase
<span class="gd">-from ._utils import _TracebackCapturingWrapper, _retrieve_traceback_capturing_wrapped_call</span>
<span class="gi">+</span>
<span class="gi">+from ._utils import (</span>
<span class="gi">+    _TracebackCapturingWrapper,</span>
<span class="gi">+    _retrieve_traceback_capturing_wrapped_call</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import dask
<span class="w"> </span>    import distributed
<span class="w"> </span>except ImportError:
<span class="w"> </span>    dask = None
<span class="w"> </span>    distributed = None
<span class="gi">+</span>
<span class="w"> </span>if dask is not None and distributed is not None:
<span class="w"> </span>    from dask.utils import funcname
<span class="w"> </span>    from dask.sizeof import sizeof
<span class="gd">-    from dask.distributed import Client, as_completed, get_client, secede, rejoin</span>
<span class="gi">+    from dask.distributed import (</span>
<span class="gi">+        Client,</span>
<span class="gi">+        as_completed,</span>
<span class="gi">+        get_client,</span>
<span class="gi">+        secede,</span>
<span class="gi">+        rejoin,</span>
<span class="gi">+    )</span>
<span class="w"> </span>    from distributed.utils import thread_state
<span class="gi">+</span>
<span class="w"> </span>    try:
<span class="gi">+        # asyncio.TimeoutError, Python3-only error thrown by recent versions of</span>
<span class="gi">+        # distributed</span>
<span class="w"> </span>        from distributed.utils import TimeoutError as _TimeoutError
<span class="w"> </span>    except ImportError:
<span class="w"> </span>        from tornado.gen import TimeoutError as _TimeoutError


<span class="gi">+def is_weakrefable(obj):</span>
<span class="gi">+    try:</span>
<span class="gi">+        weakref.ref(obj)</span>
<span class="gi">+        return True</span>
<span class="gi">+    except TypeError:</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class _WeakKeyDictionary:
<span class="w"> </span>    &quot;&quot;&quot;A variant of weakref.WeakKeyDictionary for unhashable objects.

<span class="gu">@@ -42,6 +68,7 @@ class _WeakKeyDictionary:</span>
<span class="w"> </span>    def __getitem__(self, obj):
<span class="w"> </span>        ref, val = self._data[id(obj)]
<span class="w"> </span>        if ref() is not obj:
<span class="gi">+            # In case of a race condition with on_destroy.</span>
<span class="w"> </span>            raise KeyError(obj)
<span class="w"> </span>        return val

<span class="gu">@@ -50,9 +77,12 @@ class _WeakKeyDictionary:</span>
<span class="w"> </span>        try:
<span class="w"> </span>            ref, _ = self._data[key]
<span class="w"> </span>            if ref() is not obj:
<span class="gi">+                # In case of race condition with on_destroy.</span>
<span class="w"> </span>                raise KeyError(obj)
<span class="w"> </span>        except KeyError:
<span class="gd">-</span>
<span class="gi">+            # Insert the new entry in the mapping along with a weakref</span>
<span class="gi">+            # callback to automatically delete the entry from the mapping</span>
<span class="gi">+            # as soon as the object used as key is garbage collected.</span>
<span class="w"> </span>            def on_destroy(_):
<span class="w"> </span>                del self._data[key]
<span class="w"> </span>            ref = weakref.ref(obj, on_destroy)
<span class="gu">@@ -61,18 +91,38 @@ class _WeakKeyDictionary:</span>
<span class="w"> </span>    def __len__(self):
<span class="w"> </span>        return len(self._data)

<span class="gi">+    def clear(self):</span>
<span class="gi">+        self._data.clear()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _funcname(x):</span>
<span class="gi">+    try:</span>
<span class="gi">+        if isinstance(x, list):</span>
<span class="gi">+            x = x[0][0]</span>
<span class="gi">+    except Exception:</span>
<span class="gi">+        pass</span>
<span class="gi">+    return funcname(x)</span>
<span class="gi">+</span>

<span class="w"> </span>def _make_tasks_summary(tasks):
<span class="w"> </span>    &quot;&quot;&quot;Summarize of list of (func, args, kwargs) function calls&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    unique_funcs = {func for func, args, kwargs in tasks}</span>
<span class="gi">+</span>
<span class="gi">+    if len(unique_funcs) == 1:</span>
<span class="gi">+        mixed = False</span>
<span class="gi">+    else:</span>
<span class="gi">+        mixed = True</span>
<span class="gi">+    return len(tasks), mixed, _funcname(tasks)</span>


<span class="w"> </span>class Batch:
<span class="w"> </span>    &quot;&quot;&quot;dask-compatible wrapper that executes a batch of tasks&quot;&quot;&quot;
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, tasks):
<span class="gi">+        # collect some metadata from the tasks to ease Batch calls</span>
<span class="gi">+        # introspection when debugging</span>
<span class="w"> </span>        self._num_tasks, self._mixed, self._funcname = _make_tasks_summary(
<span class="gd">-            tasks)</span>
<span class="gi">+            tasks</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __call__(self, tasks=None):
<span class="w"> </span>        results = []
<span class="gu">@@ -82,46 +132,58 @@ class Batch:</span>
<span class="w"> </span>            return results

<span class="w"> </span>    def __repr__(self):
<span class="gd">-        descr = f&#39;batch_of_{self._funcname}_{self._num_tasks}_calls&#39;</span>
<span class="gi">+        descr = f&quot;batch_of_{self._funcname}_{self._num_tasks}_calls&quot;</span>
<span class="w"> </span>        if self._mixed:
<span class="gd">-            descr = &#39;mixed_&#39; + descr</span>
<span class="gi">+            descr = &quot;mixed_&quot; + descr</span>
<span class="w"> </span>        return descr


<span class="gi">+def _joblib_probe_task():</span>
<span class="gi">+    # Noop used by the joblib connector to probe when workers are ready.</span>
<span class="gi">+    pass</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class DaskDistributedBackend(AutoBatchingMixin, ParallelBackendBase):
<span class="w"> </span>    MIN_IDEAL_BATCH_DURATION = 0.2
<span class="w"> </span>    MAX_IDEAL_BATCH_DURATION = 1.0
<span class="w"> </span>    supports_retrieve_callback = True
<span class="w"> </span>    default_n_jobs = -1

<span class="gd">-    def __init__(self, scheduler_host=None, scatter=None, client=None, loop</span>
<span class="gd">-        =None, wait_for_workers_timeout=10, **submit_kwargs):</span>
<span class="gi">+    def __init__(self, scheduler_host=None, scatter=None,</span>
<span class="gi">+                 client=None, loop=None, wait_for_workers_timeout=10,</span>
<span class="gi">+                 **submit_kwargs):</span>
<span class="w"> </span>        super().__init__()
<span class="gi">+</span>
<span class="w"> </span>        if distributed is None:
<span class="gd">-            msg = (</span>
<span class="gd">-                &quot;You are trying to use &#39;dask&#39; as a joblib parallel backend but dask is not installed. Please install dask to fix this error.&quot;</span>
<span class="gd">-                )</span>
<span class="gi">+            msg = (&quot;You are trying to use &#39;dask&#39; as a joblib parallel backend &quot;</span>
<span class="gi">+                   &quot;but dask is not installed. Please install dask &quot;</span>
<span class="gi">+                   &quot;to fix this error.&quot;)</span>
<span class="w"> </span>            raise ValueError(msg)
<span class="gi">+</span>
<span class="w"> </span>        if client is None:
<span class="w"> </span>            if scheduler_host:
<span class="gd">-                client = Client(scheduler_host, loop=loop, set_as_default=False</span>
<span class="gd">-                    )</span>
<span class="gi">+                client = Client(scheduler_host, loop=loop,</span>
<span class="gi">+                                set_as_default=False)</span>
<span class="w"> </span>            else:
<span class="w"> </span>                try:
<span class="w"> </span>                    client = get_client()
<span class="w"> </span>                except ValueError as e:
<span class="gd">-                    msg = &quot;&quot;&quot;To use Joblib with Dask first create a Dask Client</span>
<span class="gd">-</span>
<span class="gd">-    from dask.distributed import Client</span>
<span class="gd">-    client = Client()</span>
<span class="gd">-or</span>
<span class="gd">-    client = Client(&#39;scheduler-address:8786&#39;)&quot;&quot;&quot;</span>
<span class="gi">+                    msg = (&quot;To use Joblib with Dask first create a Dask Client&quot;</span>
<span class="gi">+                           &quot;\n\n&quot;</span>
<span class="gi">+                           &quot;    from dask.distributed import Client\n&quot;</span>
<span class="gi">+                           &quot;    client = Client()\n&quot;</span>
<span class="gi">+                           &quot;or\n&quot;</span>
<span class="gi">+                           &quot;    client = Client(&#39;scheduler-address:8786&#39;)&quot;)</span>
<span class="w"> </span>                    raise ValueError(msg) from e
<span class="gi">+</span>
<span class="w"> </span>        self.client = client
<span class="gi">+</span>
<span class="w"> </span>        if scatter is not None and not isinstance(scatter, (list, tuple)):
<span class="gd">-            raise TypeError(&#39;scatter must be a list/tuple, got `%s`&#39; % type</span>
<span class="gd">-                (scatter).__name__)</span>
<span class="gi">+            raise TypeError(&quot;scatter must be a list/tuple, got &quot;</span>
<span class="gi">+                            &quot;`%s`&quot; % type(scatter).__name__)</span>
<span class="gi">+</span>
<span class="w"> </span>        if scatter is not None and len(scatter) &gt; 0:
<span class="gi">+            # Keep a reference to the scattered data to keep the ids the same</span>
<span class="w"> </span>            self._scatter = list(scatter)
<span class="w"> </span>            scattered = self.client.scatter(scatter, broadcast=True)
<span class="w"> </span>            self.data_futures = {id(x): f for x, f in zip(scatter, scattered)}
<span class="gu">@@ -130,20 +192,173 @@ or</span>
<span class="w"> </span>            self.data_futures = {}
<span class="w"> </span>        self.wait_for_workers_timeout = wait_for_workers_timeout
<span class="w"> </span>        self.submit_kwargs = submit_kwargs
<span class="gd">-        self.waiting_futures = as_completed([], loop=client.loop,</span>
<span class="gd">-            with_results=True, raise_errors=False)</span>
<span class="gi">+        self.waiting_futures = as_completed(</span>
<span class="gi">+            [],</span>
<span class="gi">+            loop=client.loop,</span>
<span class="gi">+            with_results=True,</span>
<span class="gi">+            raise_errors=False</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._results = {}
<span class="w"> </span>        self._callbacks = {}

<span class="gi">+    async def _collect(self):</span>
<span class="gi">+        while self._continue:</span>
<span class="gi">+            async for future, result in self.waiting_futures:</span>
<span class="gi">+                cf_future = self._results.pop(future)</span>
<span class="gi">+                callback = self._callbacks.pop(future)</span>
<span class="gi">+                if future.status == &quot;error&quot;:</span>
<span class="gi">+                    typ, exc, tb = result</span>
<span class="gi">+                    cf_future.set_exception(exc)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    cf_future.set_result(result)</span>
<span class="gi">+                    callback(result)</span>
<span class="gi">+            await asyncio.sleep(0.01)</span>
<span class="gi">+</span>
<span class="w"> </span>    def __reduce__(self):
<span class="gd">-        return DaskDistributedBackend, ()</span>
<span class="gi">+        return (DaskDistributedBackend, ())</span>
<span class="gi">+</span>
<span class="gi">+    def get_nested_backend(self):</span>
<span class="gi">+        return DaskDistributedBackend(client=self.client), -1</span>
<span class="gi">+</span>
<span class="gi">+    def configure(self, n_jobs=1, parallel=None, **backend_args):</span>
<span class="gi">+        self.parallel = parallel</span>
<span class="gi">+        return self.effective_n_jobs(n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    def start_call(self):</span>
<span class="gi">+        self._continue = True</span>
<span class="gi">+        self.client.loop.add_callback(self._collect)</span>
<span class="gi">+        self.call_data_futures = _WeakKeyDictionary()</span>
<span class="gi">+</span>
<span class="gi">+    def stop_call(self):</span>
<span class="gi">+        # The explicit call to clear is required to break a cycling reference</span>
<span class="gi">+        # to the futures.</span>
<span class="gi">+        self._continue = False</span>
<span class="gi">+        # wait for the future collection routine (self._backend._collect) to</span>
<span class="gi">+        # finish in order to limit asyncio warnings due to aborting _collect</span>
<span class="gi">+        # during a following backend termination call</span>
<span class="gi">+        time.sleep(0.01)</span>
<span class="gi">+        self.call_data_futures.clear()</span>
<span class="gi">+</span>
<span class="gi">+    def effective_n_jobs(self, n_jobs):</span>
<span class="gi">+        effective_n_jobs = sum(self.client.ncores().values())</span>
<span class="gi">+        if effective_n_jobs != 0 or not self.wait_for_workers_timeout:</span>
<span class="gi">+            return effective_n_jobs</span>
<span class="gi">+</span>
<span class="gi">+        # If there is no worker, schedule a probe task to wait for the workers</span>
<span class="gi">+        # to come up and be available. If the dask cluster is in adaptive mode</span>
<span class="gi">+        # task might cause the cluster to provision some workers.</span>
<span class="gi">+        try:</span>
<span class="gi">+            self.client.submit(_joblib_probe_task).result(</span>
<span class="gi">+                timeout=self.wait_for_workers_timeout</span>
<span class="gi">+            )</span>
<span class="gi">+        except _TimeoutError as e:</span>
<span class="gi">+            error_msg = (</span>
<span class="gi">+                &quot;DaskDistributedBackend has no worker after {} seconds. &quot;</span>
<span class="gi">+                &quot;Make sure that workers are started and can properly connect &quot;</span>
<span class="gi">+                &quot;to the scheduler and increase the joblib/dask connection &quot;</span>
<span class="gi">+                &quot;timeout with:\n\n&quot;</span>
<span class="gi">+                &quot;parallel_config(backend=&#39;dask&#39;, wait_for_workers_timeout={})&quot;</span>
<span class="gi">+            ).format(self.wait_for_workers_timeout,</span>
<span class="gi">+                     max(10, 2 * self.wait_for_workers_timeout))</span>
<span class="gi">+            raise TimeoutError(error_msg) from e</span>
<span class="gi">+        return sum(self.client.ncores().values())</span>
<span class="gi">+</span>
<span class="gi">+    async def _to_func_args(self, func):</span>
<span class="gi">+        itemgetters = dict()</span>
<span class="gi">+</span>
<span class="gi">+        # Futures that are dynamically generated during a single call to</span>
<span class="gi">+        # Parallel.__call__.</span>
<span class="gi">+        call_data_futures = getattr(self, &#39;call_data_futures&#39;, None)</span>
<span class="gi">+</span>
<span class="gi">+        async def maybe_to_futures(args):</span>
<span class="gi">+            out = []</span>
<span class="gi">+            for arg in args:</span>
<span class="gi">+                arg_id = id(arg)</span>
<span class="gi">+                if arg_id in itemgetters:</span>
<span class="gi">+                    out.append(itemgetters[arg_id])</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                f = self.data_futures.get(arg_id, None)</span>
<span class="gi">+                if f is None and call_data_futures is not None:</span>
<span class="gi">+                    try:</span>
<span class="gi">+                        f = await call_data_futures[arg]</span>
<span class="gi">+                    except KeyError:</span>
<span class="gi">+                        pass</span>
<span class="gi">+                    if f is None:</span>
<span class="gi">+                        if is_weakrefable(arg) and sizeof(arg) &gt; 1e3:</span>
<span class="gi">+                            # Automatically scatter large objects to some of</span>
<span class="gi">+                            # the workers to avoid duplicated data transfers.</span>
<span class="gi">+                            # Rely on automated inter-worker data stealing if</span>
<span class="gi">+                            # more workers need to reuse this data</span>
<span class="gi">+                            # concurrently.</span>
<span class="gi">+                            # set hash=False - nested scatter calls (i.e</span>
<span class="gi">+                            # calling client.scatter inside a dask worker)</span>
<span class="gi">+                            # using hash=True often raise CancelledError,</span>
<span class="gi">+                            # see dask/distributed#3703</span>
<span class="gi">+                            _coro = self.client.scatter(</span>
<span class="gi">+                                arg,</span>
<span class="gi">+                                asynchronous=True,</span>
<span class="gi">+                                hash=False</span>
<span class="gi">+                            )</span>
<span class="gi">+                            # Centralize the scattering of identical arguments</span>
<span class="gi">+                            # between concurrent apply_async callbacks by</span>
<span class="gi">+                            # exposing the running coroutine in</span>
<span class="gi">+                            # call_data_futures before it completes.</span>
<span class="gi">+                            t = asyncio.Task(_coro)</span>
<span class="gi">+                            call_data_futures[arg] = t</span>
<span class="gi">+</span>
<span class="gi">+                            f = await t</span>
<span class="gi">+</span>
<span class="gi">+                if f is not None:</span>
<span class="gi">+                    out.append(f)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    out.append(arg)</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        tasks = []</span>
<span class="gi">+        for f, args, kwargs in func.items:</span>
<span class="gi">+            args = list(await maybe_to_futures(args))</span>
<span class="gi">+            kwargs = dict(zip(kwargs.keys(),</span>
<span class="gi">+                              await maybe_to_futures(kwargs.values())))</span>
<span class="gi">+            tasks.append((f, args, kwargs))</span>
<span class="gi">+</span>
<span class="gi">+        return (Batch(tasks), tasks)</span>
<span class="gi">+</span>
<span class="gi">+    def apply_async(self, func, callback=None):</span>
<span class="gi">+</span>
<span class="gi">+        cf_future = concurrent.futures.Future()</span>
<span class="gi">+        cf_future.get = cf_future.result  # achieve AsyncResult API</span>
<span class="gi">+</span>
<span class="gi">+        async def f(func, callback):</span>
<span class="gi">+            batch, tasks = await self._to_func_args(func)</span>
<span class="gi">+            key = f&#39;{repr(batch)}-{uuid4().hex}&#39;</span>
<span class="gi">+</span>
<span class="gi">+            dask_future = self.client.submit(</span>
<span class="gi">+                _TracebackCapturingWrapper(batch),</span>
<span class="gi">+                tasks=tasks,</span>
<span class="gi">+                key=key,</span>
<span class="gi">+                **self.submit_kwargs</span>
<span class="gi">+            )</span>
<span class="gi">+            self.waiting_futures.add(dask_future)</span>
<span class="gi">+            self._callbacks[dask_future] = callback</span>
<span class="gi">+            self._results[dask_future] = cf_future</span>
<span class="gi">+</span>
<span class="gi">+        self.client.loop.add_callback(f, func, callback)</span>
<span class="gi">+</span>
<span class="gi">+        return cf_future</span>
<span class="gi">+</span>
<span class="gi">+    def retrieve_result_callback(self, out):</span>
<span class="gi">+        return _retrieve_traceback_capturing_wrapped_call(out)</span>

<span class="w"> </span>    def abort_everything(self, ensure_ready=True):
<span class="w"> </span>        &quot;&quot;&quot; Tell the client to cancel any task submitted via this instance

<span class="w"> </span>        joblib.Parallel will never access those results
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self.waiting_futures.lock:</span>
<span class="gi">+            self.waiting_futures.futures.clear()</span>
<span class="gi">+            while not self.waiting_futures.queue.empty():</span>
<span class="gi">+                self.waiting_futures.queue.get()</span>

<span class="w"> </span>    @contextlib.contextmanager
<span class="w"> </span>    def retrieval_context(self):
<span class="gu">@@ -152,4 +367,13 @@ or</span>
<span class="w"> </span>        This removes thread from the worker&#39;s thread pool (using &#39;secede&#39;).
<span class="w"> </span>        Seceding avoids deadlock in nested parallelism settings.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # See &#39;joblib.Parallel.__call__&#39; and &#39;joblib.Parallel.retrieve&#39; for how</span>
<span class="gi">+        # this is used.</span>
<span class="gi">+        if hasattr(thread_state, &#39;execution_state&#39;):</span>
<span class="gi">+            # we are in a worker. Secede to avoid deadlock.</span>
<span class="gi">+            secede()</span>
<span class="gi">+</span>
<span class="gi">+        yield</span>
<span class="gi">+</span>
<span class="gi">+        if hasattr(thread_state, &#39;execution_state&#39;):</span>
<span class="gi">+            rejoin()</span>
<span class="gh">diff --git a/joblib/_memmapping_reducer.py b/joblib/_memmapping_reducer.py</span>
<span class="gh">index 5012683..13f5c4a 100644</span>
<span class="gd">--- a/joblib/_memmapping_reducer.py</span>
<span class="gi">+++ b/joblib/_memmapping_reducer.py</span>
<span class="gu">@@ -1,6 +1,10 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Reducer using memory mapping for numpy arrays
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Author: Thomas Moreau &lt;thomas.moreau.2010@gmail.com&gt;</span>
<span class="gi">+# Copyright: 2017, Thomas Moreau</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>from mmap import mmap
<span class="w"> </span>import errno
<span class="w"> </span>import os
<span class="gu">@@ -13,27 +17,63 @@ import warnings</span>
<span class="w"> </span>import weakref
<span class="w"> </span>from uuid import uuid4
<span class="w"> </span>from multiprocessing import util
<span class="gi">+</span>
<span class="w"> </span>from pickle import whichmodule, loads, dumps, HIGHEST_PROTOCOL, PicklingError
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    WindowsError
<span class="w"> </span>except NameError:
<span class="w"> </span>    WindowsError = type(None)
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import numpy as np
<span class="w"> </span>    from numpy.lib.stride_tricks import as_strided
<span class="w"> </span>except ImportError:
<span class="w"> </span>    np = None
<span class="gi">+</span>
<span class="w"> </span>from .numpy_pickle import dump, load, load_temporary_memmap
<span class="w"> </span>from .backports import make_memmap
<span class="w"> </span>from .disk import delete_folder
<span class="w"> </span>from .externals.loky.backend import resource_tracker
<span class="gi">+</span>
<span class="gi">+# Some system have a ramdisk mounted by default, we can use it instead of /tmp</span>
<span class="gi">+# as the default folder to dump big arrays to share with subprocesses.</span>
<span class="w"> </span>SYSTEM_SHARED_MEM_FS = &#39;/dev/shm&#39;
<span class="gd">-SYSTEM_SHARED_MEM_FS_MIN_SIZE = int(2000000000.0)</span>
<span class="gi">+</span>
<span class="gi">+# Minimal number of bytes available on SYSTEM_SHARED_MEM_FS to consider using</span>
<span class="gi">+# it as the default folder to dump big arrays to share with subprocesses.</span>
<span class="gi">+SYSTEM_SHARED_MEM_FS_MIN_SIZE = int(2e9)</span>
<span class="gi">+</span>
<span class="gi">+# Folder and file permissions to chmod temporary files generated by the</span>
<span class="gi">+# memmapping pool. Only the owner of the Python process can access the</span>
<span class="gi">+# temporary files and folder.</span>
<span class="w"> </span>FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
<span class="w"> </span>FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
<span class="gi">+</span>
<span class="gi">+# Set used in joblib workers, referencing the filenames of temporary memmaps</span>
<span class="gi">+# created by joblib to speed up data communication. In child processes, we add</span>
<span class="gi">+# a finalizer to these memmaps that sends a maybe_unlink call to the</span>
<span class="gi">+# resource_tracker, in order to free main memory as fast as possible.</span>
<span class="w"> </span>JOBLIB_MMAPS = set()


<span class="gi">+def _log_and_unlink(filename):</span>
<span class="gi">+    from .externals.loky.backend.resource_tracker import _resource_tracker</span>
<span class="gi">+    util.debug(</span>
<span class="gi">+        &quot;[FINALIZER CALL] object mapping to {} about to be deleted,&quot;</span>
<span class="gi">+        &quot; decrementing the refcount of the file (pid: {})&quot;.format(</span>
<span class="gi">+            os.path.basename(filename), os.getpid()))</span>
<span class="gi">+    _resource_tracker.maybe_unlink(filename, &quot;file&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def add_maybe_unlink_finalizer(memmap):</span>
<span class="gi">+    util.debug(</span>
<span class="gi">+        &quot;[FINALIZER ADD] adding finalizer to {} (id {}, filename {}, pid  {})&quot;</span>
<span class="gi">+        &quot;&quot;.format(type(memmap), id(memmap), os.path.basename(memmap.filename),</span>
<span class="gi">+                  os.getpid()))</span>
<span class="gi">+    weakref.finalize(memmap, _log_and_unlink, memmap.filename)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def unlink_file(filename):
<span class="w"> </span>    &quot;&quot;&quot;Wrapper around os.unlink with a retry mechanism.

<span class="gu">@@ -45,7 +85,24 @@ def unlink_file(filename):</span>
<span class="w"> </span>    it takes for the last reference of the memmap to be closed, yielding (on
<span class="w"> </span>    Windows) a PermissionError in the resource_tracker loop.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    NUM_RETRIES = 10</span>
<span class="gi">+    for retry_no in range(1, NUM_RETRIES + 1):</span>
<span class="gi">+        try:</span>
<span class="gi">+            os.unlink(filename)</span>
<span class="gi">+            break</span>
<span class="gi">+        except PermissionError:</span>
<span class="gi">+            util.debug(</span>
<span class="gi">+                &#39;[ResourceTracker] tried to unlink {}, got &#39;</span>
<span class="gi">+                &#39;PermissionError&#39;.format(filename)</span>
<span class="gi">+            )</span>
<span class="gi">+            if retry_no == NUM_RETRIES:</span>
<span class="gi">+                raise</span>
<span class="gi">+            else:</span>
<span class="gi">+                time.sleep(.2)</span>
<span class="gi">+        except FileNotFoundError:</span>
<span class="gi">+            # In case of a race condition when deleting the temporary folder,</span>
<span class="gi">+            # avoid noisy FileNotFoundError exception in the resource tracker.</span>
<span class="gi">+            pass</span>


<span class="w"> </span>resource_tracker._CLEANUP_FUNCS[&#39;file&#39;] = unlink_file
<span class="gu">@@ -62,13 +119,54 @@ class _WeakArrayKeyMap:</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        self._data = {}

<span class="gi">+    def get(self, obj):</span>
<span class="gi">+        ref, val = self._data[id(obj)]</span>
<span class="gi">+        if ref() is not obj:</span>
<span class="gi">+            # In case of race condition with on_destroy: could never be</span>
<span class="gi">+            # triggered by the joblib tests with CPython.</span>
<span class="gi">+            raise KeyError(obj)</span>
<span class="gi">+        return val</span>
<span class="gi">+</span>
<span class="gi">+    def set(self, obj, value):</span>
<span class="gi">+        key = id(obj)</span>
<span class="gi">+        try:</span>
<span class="gi">+            ref, _ = self._data[key]</span>
<span class="gi">+            if ref() is not obj:</span>
<span class="gi">+                # In case of race condition with on_destroy: could never be</span>
<span class="gi">+                # triggered by the joblib tests with CPython.</span>
<span class="gi">+                raise KeyError(obj)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            # Insert the new entry in the mapping along with a weakref</span>
<span class="gi">+            # callback to automatically delete the entry from the mapping</span>
<span class="gi">+            # as soon as the object used as key is garbage collected.</span>
<span class="gi">+            def on_destroy(_):</span>
<span class="gi">+                del self._data[key]</span>
<span class="gi">+            ref = weakref.ref(obj, on_destroy)</span>
<span class="gi">+        self._data[key] = ref, value</span>
<span class="gi">+</span>
<span class="w"> </span>    def __getstate__(self):
<span class="gd">-        raise PicklingError(&#39;_WeakArrayKeyMap is not pickleable&#39;)</span>
<span class="gi">+        raise PicklingError(&quot;_WeakArrayKeyMap is not pickleable&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Support for efficient transient pickling of numpy data structures</span>


<span class="w"> </span>def _get_backing_memmap(a):
<span class="w"> </span>    &quot;&quot;&quot;Recursively look up the original np.memmap instance base if any.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    b = getattr(a, &#39;base&#39;, None)</span>
<span class="gi">+    if b is None:</span>
<span class="gi">+        # TODO: check scipy sparse datastructure if scipy is installed</span>
<span class="gi">+        # a nor its descendants do not have a memmap base</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    elif isinstance(b, mmap):</span>
<span class="gi">+        # a is already a real memmap instance.</span>
<span class="gi">+        return a</span>
<span class="gi">+</span>
<span class="gi">+    else:</span>
<span class="gi">+        # Recursive exploration of the base ancestry</span>
<span class="gi">+        return _get_backing_memmap(b)</span>


<span class="w"> </span>def _get_temp_dir(pool_folder_name, temp_folder=None):
<span class="gu">@@ -101,18 +199,61 @@ def _get_temp_dir(pool_folder_name, temp_folder=None):</span>
<span class="w"> </span>       whether the temporary folder is written to the system shared memory
<span class="w"> </span>       folder or some other temporary folder.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    use_shared_mem = False</span>
<span class="gi">+    if temp_folder is None:</span>
<span class="gi">+        temp_folder = os.environ.get(&#39;JOBLIB_TEMP_FOLDER&#39;, None)</span>
<span class="gi">+    if temp_folder is None:</span>
<span class="gi">+        if os.path.exists(SYSTEM_SHARED_MEM_FS) and hasattr(os, &#39;statvfs&#39;):</span>
<span class="gi">+            try:</span>
<span class="gi">+                shm_stats = os.statvfs(SYSTEM_SHARED_MEM_FS)</span>
<span class="gi">+                available_nbytes = shm_stats.f_bsize * shm_stats.f_bavail</span>
<span class="gi">+                if available_nbytes &gt; SYSTEM_SHARED_MEM_FS_MIN_SIZE:</span>
<span class="gi">+                    # Try to see if we have write access to the shared mem</span>
<span class="gi">+                    # folder only if it is reasonably large (that is 2GB or</span>
<span class="gi">+                    # more).</span>
<span class="gi">+                    temp_folder = SYSTEM_SHARED_MEM_FS</span>
<span class="gi">+                    pool_folder = os.path.join(temp_folder, pool_folder_name)</span>
<span class="gi">+                    if not os.path.exists(pool_folder):</span>
<span class="gi">+                        os.makedirs(pool_folder)</span>
<span class="gi">+                    use_shared_mem = True</span>
<span class="gi">+            except (IOError, OSError):</span>
<span class="gi">+                # Missing rights in the /dev/shm partition, fallback to regular</span>
<span class="gi">+                # temp folder.</span>
<span class="gi">+                temp_folder = None</span>
<span class="gi">+    if temp_folder is None:</span>
<span class="gi">+        # Fallback to the default tmp folder, typically /tmp</span>
<span class="gi">+        temp_folder = tempfile.gettempdir()</span>
<span class="gi">+    temp_folder = os.path.abspath(os.path.expanduser(temp_folder))</span>
<span class="gi">+    pool_folder = os.path.join(temp_folder, pool_folder_name)</span>
<span class="gi">+    return pool_folder, use_shared_mem</span>


<span class="w"> </span>def has_shareable_memory(a):
<span class="w"> </span>    &quot;&quot;&quot;Return True if a is backed by some mmap buffer directly or not.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return _get_backing_memmap(a) is not None</span>


<span class="gd">-def _strided_from_memmap(filename, dtype, mode, offset, order, shape,</span>
<span class="gd">-    strides, total_buffer_len, unlink_on_gc_collect):</span>
<span class="gi">+def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,</span>
<span class="gi">+                         total_buffer_len, unlink_on_gc_collect):</span>
<span class="w"> </span>    &quot;&quot;&quot;Reconstruct an array view on a memory mapped file.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if mode == &#39;w+&#39;:</span>
<span class="gi">+        # Do not zero the original data when unpickling</span>
<span class="gi">+        mode = &#39;r+&#39;</span>
<span class="gi">+</span>
<span class="gi">+    if strides is None:</span>
<span class="gi">+        # Simple, contiguous memmap</span>
<span class="gi">+        return make_memmap(</span>
<span class="gi">+            filename, dtype=dtype, shape=shape, mode=mode, offset=offset,</span>
<span class="gi">+            order=order, unlink_on_gc_collect=unlink_on_gc_collect</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        # For non-contiguous data, memmap the total enclosing buffer and then</span>
<span class="gi">+        # extract the non-contiguous view with the stride-tricks API</span>
<span class="gi">+        base = make_memmap(</span>
<span class="gi">+            filename, dtype=dtype, shape=total_buffer_len, offset=offset,</span>
<span class="gi">+            mode=mode, order=order, unlink_on_gc_collect=unlink_on_gc_collect</span>
<span class="gi">+        )</span>
<span class="gi">+        return as_strided(base, shape=shape, strides=strides)</span>


<span class="w"> </span>def _reduce_memmap_backed(a, m):
<span class="gu">@@ -122,12 +263,60 @@ def _reduce_memmap_backed(a, m):</span>
<span class="w"> </span>    m is expected to be an instance of np.memmap on the top of the ``base``
<span class="w"> </span>    attribute ancestry of a. ``m.base`` should be the real python mmap object.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # offset that comes from the striding differences between a and m</span>
<span class="gi">+    util.debug(&#39;[MEMMAP REDUCE] reducing a memmap-backed array &#39;</span>
<span class="gi">+               &#39;(shape, {}, pid: {})&#39;.format(a.shape, os.getpid()))</span>
<span class="gi">+    try:</span>
<span class="gi">+        from numpy.lib.array_utils import byte_bounds</span>
<span class="gi">+    except (ModuleNotFoundError, ImportError):</span>
<span class="gi">+        # Backward-compat for numpy &lt; 2.0</span>
<span class="gi">+        from numpy import byte_bounds</span>
<span class="gi">+    a_start, a_end = byte_bounds(a)</span>
<span class="gi">+    m_start = byte_bounds(m)[0]</span>
<span class="gi">+    offset = a_start - m_start</span>
<span class="gi">+</span>
<span class="gi">+    # offset from the backing memmap</span>
<span class="gi">+    offset += m.offset</span>
<span class="gi">+</span>
<span class="gi">+    if m.flags[&#39;F_CONTIGUOUS&#39;]:</span>
<span class="gi">+        order = &#39;F&#39;</span>
<span class="gi">+    else:</span>
<span class="gi">+        # The backing memmap buffer is necessarily contiguous hence C if not</span>
<span class="gi">+        # Fortran</span>
<span class="gi">+        order = &#39;C&#39;</span>
<span class="gi">+</span>
<span class="gi">+    if a.flags[&#39;F_CONTIGUOUS&#39;] or a.flags[&#39;C_CONTIGUOUS&#39;]:</span>
<span class="gi">+        # If the array is a contiguous view, no need to pass the strides</span>
<span class="gi">+        strides = None</span>
<span class="gi">+        total_buffer_len = None</span>
<span class="gi">+    else:</span>
<span class="gi">+        # Compute the total number of items to map from which the strided</span>
<span class="gi">+        # view will be extracted.</span>
<span class="gi">+        strides = a.strides</span>
<span class="gi">+        total_buffer_len = (a_end - a_start) // a.itemsize</span>
<span class="gi">+</span>
<span class="gi">+    return (_strided_from_memmap,</span>
<span class="gi">+            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,</span>
<span class="gi">+             total_buffer_len, False))</span>


<span class="w"> </span>def reduce_array_memmap_backward(a):
<span class="w"> </span>    &quot;&quot;&quot;reduce a np.array or a np.memmap from a child process&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    m = _get_backing_memmap(a)</span>
<span class="gi">+    if isinstance(m, np.memmap) and m.filename not in JOBLIB_MMAPS:</span>
<span class="gi">+        # if a is backed by a memmaped file, reconstruct a using the</span>
<span class="gi">+        # memmaped file.</span>
<span class="gi">+        return _reduce_memmap_backed(a, m)</span>
<span class="gi">+    else:</span>
<span class="gi">+        # a is either a regular (not memmap-backed) numpy array, or an array</span>
<span class="gi">+        # backed by a shared temporary file created by joblib. In the latter</span>
<span class="gi">+        # case, in order to limit the lifespan of these temporary files, we</span>
<span class="gi">+        # serialize the memmap as a regular numpy array, and decref the</span>
<span class="gi">+        # file backing the memmap (done implicitly in a previously registered</span>
<span class="gi">+        # finalizer, see ``unlink_on_gc_collect`` for more details)</span>
<span class="gi">+        return (</span>
<span class="gi">+            loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL), )</span>
<span class="gi">+        )</span>


<span class="w"> </span>class ArrayMemmapForwardReducer(object):
<span class="gu">@@ -156,14 +345,15 @@ class ArrayMemmapForwardReducer(object):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, max_nbytes, temp_folder_resolver, mmap_mode,
<span class="gd">-        unlink_on_gc_collect, verbose=0, prewarm=True):</span>
<span class="gi">+                 unlink_on_gc_collect, verbose=0, prewarm=True):</span>
<span class="w"> </span>        self._max_nbytes = max_nbytes
<span class="w"> </span>        self._temp_folder_resolver = temp_folder_resolver
<span class="w"> </span>        self._mmap_mode = mmap_mode
<span class="w"> </span>        self.verbose = int(verbose)
<span class="gd">-        if prewarm == &#39;auto&#39;:</span>
<span class="gi">+        if prewarm == &quot;auto&quot;:</span>
<span class="w"> </span>            self._prewarm = not self._temp_folder.startswith(
<span class="gd">-                SYSTEM_SHARED_MEM_FS)</span>
<span class="gi">+                SYSTEM_SHARED_MEM_FS</span>
<span class="gi">+            )</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self._prewarm = prewarm
<span class="w"> </span>        self._prewarm = prewarm
<span class="gu">@@ -171,68 +361,157 @@ class ArrayMemmapForwardReducer(object):</span>
<span class="w"> </span>        self._temporary_memmaped_filenames = set()
<span class="w"> </span>        self._unlink_on_gc_collect = unlink_on_gc_collect

<span class="gi">+    @property</span>
<span class="gi">+    def _temp_folder(self):</span>
<span class="gi">+        return self._temp_folder_resolver()</span>
<span class="gi">+</span>
<span class="w"> </span>    def __reduce__(self):
<span class="gd">-        args = (self._max_nbytes, None, self._mmap_mode, self.</span>
<span class="gd">-            _unlink_on_gc_collect)</span>
<span class="gd">-        kwargs = {&#39;verbose&#39;: self.verbose, &#39;prewarm&#39;: self._prewarm}</span>
<span class="gi">+        # The ArrayMemmapForwardReducer is passed to the children processes: it</span>
<span class="gi">+        # needs to be pickled but the _WeakArrayKeyMap need to be skipped as</span>
<span class="gi">+        # it&#39;s only guaranteed to be consistent with the parent process memory</span>
<span class="gi">+        # garbage collection.</span>
<span class="gi">+        # Although this reducer is pickled, it is not needed in its destination</span>
<span class="gi">+        # process (child processes), as we only use this reducer to send</span>
<span class="gi">+        # memmaps from the parent process to the children processes. For this</span>
<span class="gi">+        # reason, we can afford skipping the resolver, (which would otherwise</span>
<span class="gi">+        # be unpicklable), and pass it as None instead.</span>
<span class="gi">+        args = (self._max_nbytes, None, self._mmap_mode,</span>
<span class="gi">+                self._unlink_on_gc_collect)</span>
<span class="gi">+        kwargs = {</span>
<span class="gi">+            &#39;verbose&#39;: self.verbose,</span>
<span class="gi">+            &#39;prewarm&#39;: self._prewarm,</span>
<span class="gi">+        }</span>
<span class="w"> </span>        return ArrayMemmapForwardReducer, args, kwargs

<span class="w"> </span>    def __call__(self, a):
<span class="w"> </span>        m = _get_backing_memmap(a)
<span class="w"> </span>        if m is not None and isinstance(m, np.memmap):
<span class="gi">+            # a is already backed by a memmap file, let&#39;s reuse it directly</span>
<span class="w"> </span>            return _reduce_memmap_backed(a, m)
<span class="gd">-        if (not a.dtype.hasobject and self._max_nbytes is not None and a.</span>
<span class="gd">-            nbytes &gt; self._max_nbytes):</span>
<span class="gi">+</span>
<span class="gi">+        if (not a.dtype.hasobject and self._max_nbytes is not None and</span>
<span class="gi">+                a.nbytes &gt; self._max_nbytes):</span>
<span class="gi">+            # check that the folder exists (lazily create the pool temp folder</span>
<span class="gi">+            # if required)</span>
<span class="w"> </span>            try:
<span class="w"> </span>                os.makedirs(self._temp_folder)
<span class="w"> </span>                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
<span class="w"> </span>            except OSError as e:
<span class="w"> </span>                if e.errno != errno.EEXIST:
<span class="w"> </span>                    raise e
<span class="gi">+</span>
<span class="w"> </span>            try:
<span class="w"> </span>                basename = self._memmaped_arrays.get(a)
<span class="w"> </span>            except KeyError:
<span class="gd">-                basename = &#39;{}-{}-{}.pkl&#39;.format(os.getpid(), id(threading.</span>
<span class="gd">-                    current_thread()), uuid4().hex)</span>
<span class="gi">+                # Generate a new unique random filename. The process and thread</span>
<span class="gi">+                # ids are only useful for debugging purpose and to make it</span>
<span class="gi">+                # easier to cleanup orphaned files in case of hard process</span>
<span class="gi">+                # kill (e.g. by &quot;kill -9&quot; or segfault).</span>
<span class="gi">+                basename = &quot;{}-{}-{}.pkl&quot;.format(</span>
<span class="gi">+                    os.getpid(), id(threading.current_thread()), uuid4().hex)</span>
<span class="w"> </span>                self._memmaped_arrays.set(a, basename)
<span class="w"> </span>            filename = os.path.join(self._temp_folder, basename)
<span class="gi">+</span>
<span class="gi">+            # In case the same array with the same content is passed several</span>
<span class="gi">+            # times to the pool subprocess children, serialize it only once</span>
<span class="gi">+</span>
<span class="w"> </span>            is_new_memmap = filename not in self._temporary_memmaped_filenames
<span class="gi">+</span>
<span class="gi">+            # add the memmap to the list of temporary memmaps created by joblib</span>
<span class="w"> </span>            self._temporary_memmaped_filenames.add(filename)
<span class="gi">+</span>
<span class="w"> </span>            if self._unlink_on_gc_collect:
<span class="gd">-                resource_tracker.register(filename, &#39;file&#39;)</span>
<span class="gi">+                # Bump reference count of the memmap by 1 to account for</span>
<span class="gi">+                # shared usage of the memmap by a child process. The</span>
<span class="gi">+                # corresponding decref call will be executed upon calling</span>
<span class="gi">+                # resource_tracker.maybe_unlink, registered as a finalizer in</span>
<span class="gi">+                # the child.</span>
<span class="gi">+                # the incref/decref calls here are only possible when the child</span>
<span class="gi">+                # and the parent share the same resource_tracker. It is not the</span>
<span class="gi">+                # case for the multiprocessing backend, but it does not matter</span>
<span class="gi">+                # because unlinking a memmap from a child process is only</span>
<span class="gi">+                # useful to control the memory usage of long-lasting child</span>
<span class="gi">+                # processes, while the multiprocessing-based pools terminate</span>
<span class="gi">+                # their workers at the end of a map() call.</span>
<span class="gi">+                resource_tracker.register(filename, &quot;file&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>            if is_new_memmap:
<span class="gd">-                resource_tracker.register(filename, &#39;file&#39;)</span>
<span class="gi">+                # Incref each temporary memmap created by joblib one extra</span>
<span class="gi">+                # time.  This means that these memmaps will only be deleted</span>
<span class="gi">+                # once an extra maybe_unlink() is called, which is done once</span>
<span class="gi">+                # all the jobs have completed (or been canceled) in the</span>
<span class="gi">+                # Parallel._terminate_backend() method.</span>
<span class="gi">+                resource_tracker.register(filename, &quot;file&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>            if not os.path.exists(filename):
<span class="w"> </span>                util.debug(
<span class="gd">-                    &#39;[ARRAY DUMP] Pickling new array (shape={}, dtype={}) creating a new memmap at {}&#39;</span>
<span class="gd">-                    .format(a.shape, a.dtype, filename))</span>
<span class="gi">+                    &quot;[ARRAY DUMP] Pickling new array (shape={}, dtype={}) &quot;</span>
<span class="gi">+                    &quot;creating a new memmap at {}&quot;.format(</span>
<span class="gi">+                        a.shape, a.dtype, filename))</span>
<span class="w"> </span>                for dumped_filename in dump(a, filename):
<span class="w"> </span>                    os.chmod(dumped_filename, FILE_PERMISSIONS)
<span class="gi">+</span>
<span class="w"> </span>                if self._prewarm:
<span class="gi">+                    # Warm up the data by accessing it. This operation ensures</span>
<span class="gi">+                    # that the disk access required to create the memmapping</span>
<span class="gi">+                    # file are performed in the reducing process and avoids</span>
<span class="gi">+                    # concurrent memmap creation in multiple children</span>
<span class="gi">+                    # processes.</span>
<span class="w"> </span>                    load(filename, mmap_mode=self._mmap_mode).max()
<span class="gi">+</span>
<span class="w"> </span>            else:
<span class="w"> </span>                util.debug(
<span class="gd">-                    &#39;[ARRAY DUMP] Pickling known array (shape={}, dtype={}) reusing memmap file: {}&#39;</span>
<span class="gd">-                    .format(a.shape, a.dtype, os.path.basename(filename)))</span>
<span class="gd">-            return load_temporary_memmap, (filename, self._mmap_mode, self.</span>
<span class="gd">-                _unlink_on_gc_collect)</span>
<span class="gi">+                    &quot;[ARRAY DUMP] Pickling known array (shape={}, dtype={}) &quot;</span>
<span class="gi">+                    &quot;reusing memmap file: {}&quot;.format(</span>
<span class="gi">+                        a.shape, a.dtype, os.path.basename(filename)))</span>
<span class="gi">+</span>
<span class="gi">+            # The worker process will use joblib.load to memmap the data</span>
<span class="gi">+            return (</span>
<span class="gi">+                (load_temporary_memmap, (filename, self._mmap_mode,</span>
<span class="gi">+                                         self._unlink_on_gc_collect))</span>
<span class="gi">+            )</span>
<span class="w"> </span>        else:
<span class="gi">+            # do not convert a into memmap, let pickler do its usual copy with</span>
<span class="gi">+            # the default system pickler</span>
<span class="w"> </span>            util.debug(
<span class="gd">-                &#39;[ARRAY DUMP] Pickling array (NO MEMMAPPING) (shape={},  dtype={}).&#39;</span>
<span class="gd">-                .format(a.shape, a.dtype))</span>
<span class="gd">-            return loads, (dumps(a, protocol=HIGHEST_PROTOCOL),)</span>
<span class="gi">+                &#39;[ARRAY DUMP] Pickling array (NO MEMMAPPING) (shape={}, &#39;</span>
<span class="gi">+                &#39; dtype={}).&#39;.format(a.shape, a.dtype))</span>
<span class="gi">+            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))</span>


<span class="gd">-def get_memmapping_reducers(forward_reducers=None, backward_reducers=None,</span>
<span class="gd">-    temp_folder_resolver=None, max_nbytes=1000000.0, mmap_mode=&#39;r&#39;, verbose</span>
<span class="gd">-    =0, prewarm=False, unlink_on_gc_collect=True, **kwargs):</span>
<span class="gi">+def get_memmapping_reducers(</span>
<span class="gi">+        forward_reducers=None, backward_reducers=None,</span>
<span class="gi">+        temp_folder_resolver=None, max_nbytes=1e6, mmap_mode=&#39;r&#39;, verbose=0,</span>
<span class="gi">+        prewarm=False, unlink_on_gc_collect=True, **kwargs):</span>
<span class="w"> </span>    &quot;&quot;&quot;Construct a pair of memmapping reducer linked to a tmpdir.

<span class="w"> </span>    This function manage the creation and the clean up of the temporary folders
<span class="w"> </span>    underlying the memory maps and should be use to get the reducers necessary
<span class="w"> </span>    to construct joblib pool or executor.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if forward_reducers is None:</span>
<span class="gi">+        forward_reducers = dict()</span>
<span class="gi">+    if backward_reducers is None:</span>
<span class="gi">+        backward_reducers = dict()</span>
<span class="gi">+</span>
<span class="gi">+    if np is not None:</span>
<span class="gi">+        # Register smart numpy.ndarray reducers that detects memmap backed</span>
<span class="gi">+        # arrays and that is also able to dump to memmap large in-memory</span>
<span class="gi">+        # arrays over the max_nbytes threshold</span>
<span class="gi">+        forward_reduce_ndarray = ArrayMemmapForwardReducer(</span>
<span class="gi">+            max_nbytes, temp_folder_resolver, mmap_mode, unlink_on_gc_collect,</span>
<span class="gi">+            verbose, prewarm=prewarm)</span>
<span class="gi">+        forward_reducers[np.ndarray] = forward_reduce_ndarray</span>
<span class="gi">+        forward_reducers[np.memmap] = forward_reduce_ndarray</span>
<span class="gi">+</span>
<span class="gi">+        # Communication from child process to the parent process always</span>
<span class="gi">+        # pickles in-memory numpy.ndarray without dumping them as memmap</span>
<span class="gi">+        # to avoid confusing the caller and make it tricky to collect the</span>
<span class="gi">+        # temporary folder</span>
<span class="gi">+        backward_reducers[np.ndarray] = reduce_array_memmap_backward</span>
<span class="gi">+        backward_reducers[np.memmap] = reduce_array_memmap_backward</span>
<span class="gi">+</span>
<span class="gi">+    return forward_reducers, backward_reducers</span>


<span class="w"> </span>class TemporaryResourcesManager(object):
<span class="gu">@@ -253,14 +532,126 @@ class TemporaryResourcesManager(object):</span>
<span class="w"> </span>        self._id = uuid4().hex
<span class="w"> </span>        self._finalizers = {}
<span class="w"> </span>        if context_id is None:
<span class="gi">+            # It would be safer to not assign a default context id (less silent</span>
<span class="gi">+            # bugs), but doing this while maintaining backward compatibility</span>
<span class="gi">+            # with the previous, context-unaware version get_memmaping_executor</span>
<span class="gi">+            # exposes too many low-level details.</span>
<span class="w"> </span>            context_id = uuid4().hex
<span class="w"> </span>        self.set_current_context(context_id)

<span class="gi">+    def set_current_context(self, context_id):</span>
<span class="gi">+        self._current_context_id = context_id</span>
<span class="gi">+        self.register_new_context(context_id)</span>
<span class="gi">+</span>
<span class="gi">+    def register_new_context(self, context_id):</span>
<span class="gi">+        # Prepare a sub-folder name specific to a context (usually a unique id</span>
<span class="gi">+        # generated by each instance of the Parallel class). Do not create in</span>
<span class="gi">+        # advance to spare FS write access if no array is to be dumped).</span>
<span class="gi">+        if context_id in self._cached_temp_folders:</span>
<span class="gi">+            return</span>
<span class="gi">+        else:</span>
<span class="gi">+            # During its lifecycle, one Parallel object can have several</span>
<span class="gi">+            # executors associated to it (for instance, if a loky worker raises</span>
<span class="gi">+            # an exception, joblib shutdowns the executor and instantly</span>
<span class="gi">+            # recreates a new one before raising the error - see</span>
<span class="gi">+            # ``ensure_ready``.  Because we don&#39;t want two executors tied to</span>
<span class="gi">+            # the same Parallel object (and thus the same context id) to</span>
<span class="gi">+            # register/use/delete the same folder, we also add an id specific</span>
<span class="gi">+            # to the current Manager (and thus specific to its associated</span>
<span class="gi">+            # executor) to the folder name.</span>
<span class="gi">+            new_folder_name = (</span>
<span class="gi">+                &quot;joblib_memmapping_folder_{}_{}_{}&quot;.format(</span>
<span class="gi">+                    os.getpid(), self._id, context_id)</span>
<span class="gi">+            )</span>
<span class="gi">+            new_folder_path, _ = _get_temp_dir(</span>
<span class="gi">+                new_folder_name, self._temp_folder_root</span>
<span class="gi">+            )</span>
<span class="gi">+            self.register_folder_finalizer(new_folder_path, context_id)</span>
<span class="gi">+            self._cached_temp_folders[context_id] = new_folder_path</span>
<span class="gi">+</span>
<span class="w"> </span>    def resolve_temp_folder_name(self):
<span class="w"> </span>        &quot;&quot;&quot;Return a folder name specific to the currently activated context&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._cached_temp_folders[self._current_context_id]</span>
<span class="gi">+</span>
<span class="gi">+    # resource management API</span>
<span class="gi">+</span>
<span class="gi">+    def register_folder_finalizer(self, pool_subfolder, context_id):</span>
<span class="gi">+        # Register the garbage collector at program exit in case caller forgets</span>
<span class="gi">+        # to call terminate explicitly: note we do not pass any reference to</span>
<span class="gi">+        # ensure that this callback won&#39;t prevent garbage collection of</span>
<span class="gi">+        # parallel instance and related file handler resources such as POSIX</span>
<span class="gi">+        # semaphores and pipes</span>
<span class="gi">+        pool_module_name = whichmodule(delete_folder, &#39;delete_folder&#39;)</span>
<span class="gi">+        resource_tracker.register(pool_subfolder, &quot;folder&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        def _cleanup():</span>
<span class="gi">+            # In some cases the Python runtime seems to set delete_folder to</span>
<span class="gi">+            # None just before exiting when accessing the delete_folder</span>
<span class="gi">+            # function from the closure namespace. So instead we reimport</span>
<span class="gi">+            # the delete_folder function explicitly.</span>
<span class="gi">+            # https://github.com/joblib/joblib/issues/328</span>
<span class="gi">+            # We cannot just use from &#39;joblib.pool import delete_folder&#39;</span>
<span class="gi">+            # because joblib should only use relative imports to allow</span>
<span class="gi">+            # easy vendoring.</span>
<span class="gi">+            delete_folder = __import__(</span>
<span class="gi">+                pool_module_name, fromlist=[&#39;delete_folder&#39;]</span>
<span class="gi">+            ).delete_folder</span>
<span class="gi">+            try:</span>
<span class="gi">+                delete_folder(pool_subfolder, allow_non_empty=True)</span>
<span class="gi">+                resource_tracker.unregister(pool_subfolder, &quot;folder&quot;)</span>
<span class="gi">+            except OSError:</span>
<span class="gi">+                warnings.warn(&quot;Failed to delete temporary folder: {}&quot;</span>
<span class="gi">+                              .format(pool_subfolder))</span>
<span class="gi">+</span>
<span class="gi">+        self._finalizers[context_id] = atexit.register(_cleanup)</span>

<span class="w"> </span>    def _clean_temporary_resources(self, context_id=None, force=False,
<span class="gd">-        allow_non_empty=False):</span>
<span class="gi">+                                   allow_non_empty=False):</span>
<span class="w"> </span>        &quot;&quot;&quot;Clean temporary resources created by a process-based pool&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if context_id is None:</span>
<span class="gi">+            # Iterates over a copy of the cache keys to avoid Error due to</span>
<span class="gi">+            # iterating over a changing size dictionary.</span>
<span class="gi">+            for context_id in list(self._cached_temp_folders):</span>
<span class="gi">+                self._clean_temporary_resources(</span>
<span class="gi">+                    context_id, force=force, allow_non_empty=allow_non_empty</span>
<span class="gi">+                )</span>
<span class="gi">+        else:</span>
<span class="gi">+            temp_folder = self._cached_temp_folders.get(context_id)</span>
<span class="gi">+            if temp_folder and os.path.exists(temp_folder):</span>
<span class="gi">+                for filename in os.listdir(temp_folder):</span>
<span class="gi">+                    if force:</span>
<span class="gi">+                        # Some workers have failed and the ref counted might</span>
<span class="gi">+                        # be off. The workers should have shut down by this</span>
<span class="gi">+                        # time so forcefully clean up the files.</span>
<span class="gi">+                        resource_tracker.unregister(</span>
<span class="gi">+                            os.path.join(temp_folder, filename), &quot;file&quot;</span>
<span class="gi">+                        )</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        resource_tracker.maybe_unlink(</span>
<span class="gi">+                            os.path.join(temp_folder, filename), &quot;file&quot;</span>
<span class="gi">+                        )</span>
<span class="gi">+</span>
<span class="gi">+                # When forcing clean-up, try to delete the folder even if some</span>
<span class="gi">+                # files are still in it. Otherwise, try to delete the folder</span>
<span class="gi">+                allow_non_empty |= force</span>
<span class="gi">+</span>
<span class="gi">+                # Clean up the folder if possible, either if it is empty or</span>
<span class="gi">+                # if none of the files in it are in used and allow_non_empty.</span>
<span class="gi">+                try:</span>
<span class="gi">+                    delete_folder(</span>
<span class="gi">+                        temp_folder, allow_non_empty=allow_non_empty</span>
<span class="gi">+                    )</span>
<span class="gi">+                    # Forget the folder once it has been deleted</span>
<span class="gi">+                    self._cached_temp_folders.pop(context_id, None)</span>
<span class="gi">+                    resource_tracker.unregister(temp_folder, &quot;folder&quot;)</span>
<span class="gi">+</span>
<span class="gi">+                    # Also cancel the finalizers  that gets triggered at gc.</span>
<span class="gi">+                    finalizer = self._finalizers.pop(context_id, None)</span>
<span class="gi">+                    if finalizer is not None:</span>
<span class="gi">+                        atexit.unregister(finalizer)</span>
<span class="gi">+</span>
<span class="gi">+                except OSError:</span>
<span class="gi">+                    # Temporary folder cannot be deleted right now.</span>
<span class="gi">+                    # This folder will be cleaned up by an atexit</span>
<span class="gi">+                    # finalizer registered by the memmapping_reducer.</span>
<span class="gi">+                    pass</span>
<span class="gh">diff --git a/joblib/_multiprocessing_helpers.py b/joblib/_multiprocessing_helpers.py</span>
<span class="gh">index 6441d34..bde4bc1 100644</span>
<span class="gd">--- a/joblib/_multiprocessing_helpers.py</span>
<span class="gi">+++ b/joblib/_multiprocessing_helpers.py</span>
<span class="gu">@@ -5,31 +5,48 @@ circular dependencies (for instance for the assert_spawning name).</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import os
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Obtain possible configuration from the environment, assuming 1 (on)</span>
<span class="gi">+# by default, upon 0 set to None. Should instructively fail if some non</span>
<span class="gi">+# 0/1 value is set.</span>
<span class="w"> </span>mp = int(os.environ.get(&#39;JOBLIB_MULTIPROCESSING&#39;, 1)) or None
<span class="w"> </span>if mp:
<span class="w"> </span>    try:
<span class="w"> </span>        import multiprocessing as mp
<span class="gd">-        import _multiprocessing</span>
<span class="gi">+        import _multiprocessing  # noqa</span>
<span class="w"> </span>    except ImportError:
<span class="w"> </span>        mp = None
<span class="gi">+</span>
<span class="gi">+# 2nd stage: validate that locking is available on the system and</span>
<span class="gi">+#            issue a warning if not</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    try:
<span class="gi">+        # try to create a named semaphore using SemLock to make sure they are</span>
<span class="gi">+        # available on this platform. We use the low level object</span>
<span class="gi">+        # _multiprocessing.SemLock to avoid spawning a resource tracker on</span>
<span class="gi">+        # Unix system or changing the default backend.</span>
<span class="w"> </span>        import tempfile
<span class="w"> </span>        from _multiprocessing import SemLock
<span class="gi">+</span>
<span class="w"> </span>        _rand = tempfile._RandomNameSequence()
<span class="w"> </span>        for i in range(100):
<span class="w"> </span>            try:
<span class="gd">-                name = &#39;/joblib-{}-{}&#39;.format(os.getpid(), next(_rand))</span>
<span class="gi">+                name = &#39;/joblib-{}-{}&#39; .format(</span>
<span class="gi">+                    os.getpid(), next(_rand))</span>
<span class="w"> </span>                _sem = SemLock(0, 0, 1, name=name, unlink=True)
<span class="gd">-                del _sem</span>
<span class="gi">+                del _sem  # cleanup</span>
<span class="w"> </span>                break
<span class="gd">-            except FileExistsError as e:</span>
<span class="gi">+            except FileExistsError as e:  # pragma: no cover</span>
<span class="w"> </span>                if i &gt;= 99:
<span class="gd">-                    raise FileExistsError(&#39;cannot find name for semaphore&#39;</span>
<span class="gd">-                        ) from e</span>
<span class="gi">+                    raise FileExistsError(</span>
<span class="gi">+                        &#39;cannot find name for semaphore&#39;) from e</span>
<span class="w"> </span>    except (FileExistsError, AttributeError, ImportError, OSError) as e:
<span class="w"> </span>        mp = None
<span class="w"> </span>        warnings.warn(&#39;%s.  joblib will operate in serial mode&#39; % (e,))
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# 3rd stage: backward compat for the assert_spawning helper</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    from multiprocessing.context import assert_spawning
<span class="w"> </span>else:
<span class="gh">diff --git a/joblib/_parallel_backends.py b/joblib/_parallel_backends.py</span>
<span class="gh">index 87fe642..8201c96 100644</span>
<span class="gd">--- a/joblib/_parallel_backends.py</span>
<span class="gi">+++ b/joblib/_parallel_backends.py</span>
<span class="gu">@@ -1,38 +1,61 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Backends for embarrassingly parallel code.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import gc
<span class="w"> </span>import os
<span class="w"> </span>import warnings
<span class="w"> </span>import threading
<span class="w"> </span>import contextlib
<span class="w"> </span>from abc import ABCMeta, abstractmethod
<span class="gd">-from ._utils import _TracebackCapturingWrapper, _retrieve_traceback_capturing_wrapped_call</span>
<span class="gi">+</span>
<span class="gi">+from ._utils import (</span>
<span class="gi">+    _TracebackCapturingWrapper,</span>
<span class="gi">+    _retrieve_traceback_capturing_wrapped_call</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from ._multiprocessing_helpers import mp
<span class="gi">+</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    from .pool import MemmappingPool
<span class="w"> </span>    from multiprocessing.pool import ThreadPool
<span class="w"> </span>    from .executor import get_memmapping_executor
<span class="gi">+</span>
<span class="gi">+    # Import loky only if multiprocessing is present</span>
<span class="w"> </span>    from .externals.loky import process_executor, cpu_count
<span class="w"> </span>    from .externals.loky.process_executor import ShutdownExecutorError


<span class="w"> </span>class ParallelBackendBase(metaclass=ABCMeta):
<span class="w"> </span>    &quot;&quot;&quot;Helper abc which defines all methods a ParallelBackend must implement&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    supports_inner_max_num_threads = False
<span class="w"> </span>    supports_retrieve_callback = False
<span class="w"> </span>    default_n_jobs = 1
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def supports_return_generator(self):</span>
<span class="gi">+        return self.supports_retrieve_callback</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def supports_timeout(self):</span>
<span class="gi">+        return self.supports_retrieve_callback</span>
<span class="gi">+</span>
<span class="w"> </span>    nesting_level = None

<span class="gd">-    def __init__(self, nesting_level=None, inner_max_num_threads=None, **kwargs</span>
<span class="gd">-        ):</span>
<span class="gi">+    def __init__(self, nesting_level=None, inner_max_num_threads=None,</span>
<span class="gi">+                 **kwargs):</span>
<span class="w"> </span>        super().__init__(**kwargs)
<span class="w"> </span>        self.nesting_level = nesting_level
<span class="w"> </span>        self.inner_max_num_threads = inner_max_num_threads
<span class="gd">-    MAX_NUM_THREADS_VARS = [&#39;OMP_NUM_THREADS&#39;, &#39;OPENBLAS_NUM_THREADS&#39;,</span>
<span class="gd">-        &#39;MKL_NUM_THREADS&#39;, &#39;BLIS_NUM_THREADS&#39;, &#39;VECLIB_MAXIMUM_THREADS&#39;,</span>
<span class="gd">-        &#39;NUMBA_NUM_THREADS&#39;, &#39;NUMEXPR_NUM_THREADS&#39;]</span>
<span class="gd">-    TBB_ENABLE_IPC_VAR = &#39;ENABLE_IPC&#39;</span>
<span class="gi">+</span>
<span class="gi">+    MAX_NUM_THREADS_VARS = [</span>
<span class="gi">+        &#39;OMP_NUM_THREADS&#39;, &#39;OPENBLAS_NUM_THREADS&#39;, &#39;MKL_NUM_THREADS&#39;,</span>
<span class="gi">+        &#39;BLIS_NUM_THREADS&#39;, &#39;VECLIB_MAXIMUM_THREADS&#39;, &#39;NUMBA_NUM_THREADS&#39;,</span>
<span class="gi">+        &#39;NUMEXPR_NUM_THREADS&#39;,</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    TBB_ENABLE_IPC_VAR = &quot;ENABLE_IPC&quot;</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def effective_n_jobs(self, n_jobs):
<span class="gu">@@ -51,12 +74,10 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        scheduling overhead and better use of CPU cache prefetching heuristics)
<span class="w"> </span>        as long as all the workers have enough work to do.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def apply_async(self, func, callback=None):
<span class="w"> </span>        &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def retrieve_result_callback(self, out):
<span class="w"> </span>        &quot;&quot;&quot;Called within the callback function passed in apply_async.
<span class="gu">@@ -65,40 +86,36 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        the considered backend. It is supposed to return the outcome of a task
<span class="w"> </span>        if it succeeded or raise the exception if it failed.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
<span class="gd">-        **backend_args):</span>
<span class="gi">+                  **backend_args):</span>
<span class="w"> </span>        &quot;&quot;&quot;Reconfigure the backend and return the number of workers.

<span class="w"> </span>        This makes it possible to reuse an existing backend instance for
<span class="w"> </span>        successive independent calls to Parallel with different parameters.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.parallel = parallel</span>
<span class="gi">+        return self.effective_n_jobs(n_jobs)</span>

<span class="w"> </span>    def start_call(self):
<span class="w"> </span>        &quot;&quot;&quot;Call-back method called at the beginning of a Parallel call&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def stop_call(self):
<span class="w"> </span>        &quot;&quot;&quot;Call-back method called at the end of a Parallel call&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def terminate(self):
<span class="w"> </span>        &quot;&quot;&quot;Shutdown the workers and free the shared memory.&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def compute_batch_size(self):
<span class="w"> </span>        &quot;&quot;&quot;Determine the optimal batch size&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return 1</span>

<span class="w"> </span>    def batch_completed(self, batch_size, duration):
<span class="w"> </span>        &quot;&quot;&quot;Callback indicate how long it took to run a batch&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    def get_exceptions(self):
<span class="w"> </span>        &quot;&quot;&quot;List of exception types to be captured.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return []</span>

<span class="w"> </span>    def abort_everything(self, ensure_ready=True):
<span class="w"> </span>        &quot;&quot;&quot;Abort any running tasks
<span class="gu">@@ -120,6 +137,8 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        managed by the backend it-self: if we expect no new tasks, there is no
<span class="w"> </span>        point in re-creating new workers.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gi">+        # Does nothing by default: to be overridden in subclasses when</span>
<span class="gi">+        # canceling tasks is possible.</span>
<span class="w"> </span>        pass

<span class="w"> </span>    def get_nested_backend(self):
<span class="gu">@@ -129,7 +148,11 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        nesting. Beyond, switch to sequential backend to avoid spawning too
<span class="w"> </span>        many threads on the host.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        nesting_level = getattr(self, &#39;nesting_level&#39;, 0) + 1</span>
<span class="gi">+        if nesting_level &gt; 1:</span>
<span class="gi">+            return SequentialBackend(nesting_level=nesting_level), None</span>
<span class="gi">+        else:</span>
<span class="gi">+            return ThreadingBackend(nesting_level=nesting_level), None</span>

<span class="w"> </span>    @contextlib.contextmanager
<span class="w"> </span>    def retrieval_context(self):
<span class="gu">@@ -146,7 +169,7 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        calls to finish, but the backend has no free workers to execute those
<span class="w"> </span>        tasks.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        yield</span>

<span class="w"> </span>    def _prepare_worker_env(self, n_jobs):
<span class="w"> </span>        &quot;&quot;&quot;Return environment variables limiting threadpools in external libs.
<span class="gu">@@ -156,7 +179,30 @@ class ParallelBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        number of threads to `n_threads` for OpenMP, MKL, Accelerated and
<span class="w"> </span>        OpenBLAS libraries in the child processes.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        explicit_n_threads = self.inner_max_num_threads</span>
<span class="gi">+        default_n_threads = max(cpu_count() // n_jobs, 1)</span>
<span class="gi">+</span>
<span class="gi">+        # Set the inner environment variables to self.inner_max_num_threads if</span>
<span class="gi">+        # it is given. Else, default to cpu_count // n_jobs unless the variable</span>
<span class="gi">+        # is already present in the parent process environment.</span>
<span class="gi">+        env = {}</span>
<span class="gi">+        for var in self.MAX_NUM_THREADS_VARS:</span>
<span class="gi">+            if explicit_n_threads is None:</span>
<span class="gi">+                var_value = os.environ.get(var, default_n_threads)</span>
<span class="gi">+            else:</span>
<span class="gi">+                var_value = explicit_n_threads</span>
<span class="gi">+</span>
<span class="gi">+            env[var] = str(var_value)</span>
<span class="gi">+</span>
<span class="gi">+        if self.TBB_ENABLE_IPC_VAR not in os.environ:</span>
<span class="gi">+            # To avoid over-subscription when using TBB, let the TBB schedulers</span>
<span class="gi">+            # use Inter Process Communication to coordinate:</span>
<span class="gi">+            env[self.TBB_ENABLE_IPC_VAR] = &quot;1&quot;</span>
<span class="gi">+        return env</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def in_main_thread():</span>
<span class="gi">+        return isinstance(threading.current_thread(), threading._MainThread)</span>


<span class="w"> </span>class SequentialBackend(ParallelBackendBase):
<span class="gu">@@ -165,6 +211,7 @@ class SequentialBackend(ParallelBackendBase):</span>
<span class="w"> </span>    Does not use/create any threading objects, and hence has minimal
<span class="w"> </span>    overhead. Used when n_jobs == 1.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    uses_threads = True
<span class="w"> </span>    supports_timeout = False
<span class="w"> </span>    supports_retrieve_callback = False
<span class="gu">@@ -172,46 +219,90 @@ class SequentialBackend(ParallelBackendBase):</span>

<span class="w"> </span>    def effective_n_jobs(self, n_jobs):
<span class="w"> </span>        &quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if n_jobs == 0:</span>
<span class="gi">+            raise ValueError(&#39;n_jobs == 0 in Parallel has no meaning&#39;)</span>
<span class="gi">+        return 1</span>

<span class="w"> </span>    def apply_async(self, func, callback=None):
<span class="w"> </span>        &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        raise RuntimeError(&quot;Should never be called for SequentialBackend.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def retrieve_result_callback(self, out):</span>
<span class="gi">+        raise RuntimeError(&quot;Should never be called for SequentialBackend.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def get_nested_backend(self):</span>
<span class="gi">+        # import is not top level to avoid cyclic import errors.</span>
<span class="gi">+        from .parallel import get_active_backend</span>
<span class="gi">+</span>
<span class="gi">+        # SequentialBackend should neither change the nesting level, the</span>
<span class="gi">+        # default backend or the number of jobs. Just return the current one.</span>
<span class="gi">+        return get_active_backend()</span>


<span class="w"> </span>class PoolManagerMixin(object):
<span class="w"> </span>    &quot;&quot;&quot;A helper class for managing pool of workers.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    _pool = None

<span class="w"> </span>    def effective_n_jobs(self, n_jobs):
<span class="w"> </span>        &quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if n_jobs == 0:</span>
<span class="gi">+            raise ValueError(&#39;n_jobs == 0 in Parallel has no meaning&#39;)</span>
<span class="gi">+        elif mp is None or n_jobs is None:</span>
<span class="gi">+            # multiprocessing is not available or disabled, fallback</span>
<span class="gi">+            # to sequential mode</span>
<span class="gi">+            return 1</span>
<span class="gi">+        elif n_jobs &lt; 0:</span>
<span class="gi">+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)</span>
<span class="gi">+        return n_jobs</span>

<span class="w"> </span>    def terminate(self):
<span class="w"> </span>        &quot;&quot;&quot;Shutdown the process or thread pool&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self._pool is not None:</span>
<span class="gi">+            self._pool.close()</span>
<span class="gi">+            self._pool.terminate()  # terminate does a join()</span>
<span class="gi">+            self._pool = None</span>

<span class="w"> </span>    def _get_pool(self):
<span class="w"> </span>        &quot;&quot;&quot;Used by apply_async to make it possible to implement lazy init&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._pool</span>

<span class="w"> </span>    def apply_async(self, func, callback=None):
<span class="w"> </span>        &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Here, we need a wrapper to avoid crashes on KeyboardInterruptErrors.</span>
<span class="gi">+        # We also call the callback on error, to make sure the pool does not</span>
<span class="gi">+        # wait on crashed jobs.</span>
<span class="gi">+        return self._get_pool().apply_async(</span>
<span class="gi">+            _TracebackCapturingWrapper(func), (),</span>
<span class="gi">+            callback=callback, error_callback=callback</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def retrieve_result_callback(self, out):
<span class="w"> </span>        &quot;&quot;&quot;Mimic concurrent.futures results, raising an error if needed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return _retrieve_traceback_capturing_wrapped_call(out)</span>

<span class="w"> </span>    def abort_everything(self, ensure_ready=True):
<span class="w"> </span>        &quot;&quot;&quot;Shutdown the pool and restart a new one with the same parameters&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.terminate()</span>
<span class="gi">+        if ensure_ready:</span>
<span class="gi">+            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,</span>
<span class="gi">+                           **self.parallel._backend_args)</span>


<span class="w"> </span>class AutoBatchingMixin(object):
<span class="w"> </span>    &quot;&quot;&quot;A helper class for automagically batching jobs.&quot;&quot;&quot;
<span class="gd">-    MIN_IDEAL_BATCH_DURATION = 0.2</span>
<span class="gi">+</span>
<span class="gi">+    # In seconds, should be big enough to hide multiprocessing dispatching</span>
<span class="gi">+    # overhead.</span>
<span class="gi">+    # This settings was found by running benchmarks/bench_auto_batching.py</span>
<span class="gi">+    # with various parameters on various platforms.</span>
<span class="gi">+    MIN_IDEAL_BATCH_DURATION = .2</span>
<span class="gi">+</span>
<span class="gi">+    # Should not be too high to avoid stragglers: long jobs running alone</span>
<span class="gi">+    # on a single worker while other workers have no work to process any more.</span>
<span class="w"> </span>    MAX_IDEAL_BATCH_DURATION = 2
<span class="gi">+</span>
<span class="gi">+    # Batching counters default values</span>
<span class="w"> </span>    _DEFAULT_EFFECTIVE_BATCH_SIZE = 1
<span class="w"> </span>    _DEFAULT_SMOOTHED_BATCH_DURATION = 0.0

<span class="gu">@@ -222,18 +313,89 @@ class AutoBatchingMixin(object):</span>

<span class="w"> </span>    def compute_batch_size(self):
<span class="w"> </span>        &quot;&quot;&quot;Determine the optimal batch size&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        old_batch_size = self._effective_batch_size</span>
<span class="gi">+        batch_duration = self._smoothed_batch_duration</span>
<span class="gi">+        if (batch_duration &gt; 0 and</span>
<span class="gi">+                batch_duration &lt; self.MIN_IDEAL_BATCH_DURATION):</span>
<span class="gi">+            # The current batch size is too small: the duration of the</span>
<span class="gi">+            # processing of a batch of task is not large enough to hide</span>
<span class="gi">+            # the scheduling overhead.</span>
<span class="gi">+            ideal_batch_size = int(old_batch_size *</span>
<span class="gi">+                                   self.MIN_IDEAL_BATCH_DURATION /</span>
<span class="gi">+                                   batch_duration)</span>
<span class="gi">+            # Multiply by two to limit oscilations between min and max.</span>
<span class="gi">+            ideal_batch_size *= 2</span>
<span class="gi">+</span>
<span class="gi">+            # dont increase the batch size too fast to limit huge batch sizes</span>
<span class="gi">+            # potentially leading to starving worker</span>
<span class="gi">+            batch_size = min(2 * old_batch_size, ideal_batch_size)</span>
<span class="gi">+</span>
<span class="gi">+            batch_size = max(batch_size, 1)</span>
<span class="gi">+</span>
<span class="gi">+            self._effective_batch_size = batch_size</span>
<span class="gi">+            if self.parallel.verbose &gt;= 10:</span>
<span class="gi">+                self.parallel._print(</span>
<span class="gi">+                    f&quot;Batch computation too fast ({batch_duration}s.) &quot;</span>
<span class="gi">+                    f&quot;Setting batch_size={batch_size}.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+        elif (batch_duration &gt; self.MAX_IDEAL_BATCH_DURATION and</span>
<span class="gi">+              old_batch_size &gt;= 2):</span>
<span class="gi">+            # The current batch size is too big. If we schedule overly long</span>
<span class="gi">+            # running batches some CPUs might wait with nothing left to do</span>
<span class="gi">+            # while a couple of CPUs a left processing a few long running</span>
<span class="gi">+            # batches. Better reduce the batch size a bit to limit the</span>
<span class="gi">+            # likelihood of scheduling such stragglers.</span>
<span class="gi">+</span>
<span class="gi">+            # decrease the batch size quickly to limit potential starving</span>
<span class="gi">+            ideal_batch_size = int(</span>
<span class="gi">+                old_batch_size * self.MIN_IDEAL_BATCH_DURATION / batch_duration</span>
<span class="gi">+            )</span>
<span class="gi">+            # Multiply by two to limit oscilations between min and max.</span>
<span class="gi">+            batch_size = max(2 * ideal_batch_size, 1)</span>
<span class="gi">+            self._effective_batch_size = batch_size</span>
<span class="gi">+            if self.parallel.verbose &gt;= 10:</span>
<span class="gi">+                self.parallel._print(</span>
<span class="gi">+                    f&quot;Batch computation too slow ({batch_duration}s.) &quot;</span>
<span class="gi">+                    f&quot;Setting batch_size={batch_size}.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+        else:</span>
<span class="gi">+            # No batch size adjustment</span>
<span class="gi">+            batch_size = old_batch_size</span>
<span class="gi">+</span>
<span class="gi">+        if batch_size != old_batch_size:</span>
<span class="gi">+            # Reset estimation of the smoothed mean batch duration: this</span>
<span class="gi">+            # estimate is updated in the multiprocessing apply_async</span>
<span class="gi">+            # CallBack as long as the batch_size is constant. Therefore</span>
<span class="gi">+            # we need to reset the estimate whenever we re-tune the batch</span>
<span class="gi">+            # size.</span>
<span class="gi">+            self._smoothed_batch_duration = \</span>
<span class="gi">+                self._DEFAULT_SMOOTHED_BATCH_DURATION</span>
<span class="gi">+</span>
<span class="gi">+        return batch_size</span>

<span class="w"> </span>    def batch_completed(self, batch_size, duration):
<span class="w"> </span>        &quot;&quot;&quot;Callback indicate how long it took to run a batch&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if batch_size == self._effective_batch_size:</span>
<span class="gi">+            # Update the smoothed streaming estimate of the duration of a batch</span>
<span class="gi">+            # from dispatch to completion</span>
<span class="gi">+            old_duration = self._smoothed_batch_duration</span>
<span class="gi">+            if old_duration == self._DEFAULT_SMOOTHED_BATCH_DURATION:</span>
<span class="gi">+                # First record of duration for this batch size after the last</span>
<span class="gi">+                # reset.</span>
<span class="gi">+                new_duration = duration</span>
<span class="gi">+            else:</span>
<span class="gi">+                # Update the exponentially weighted average of the duration of</span>
<span class="gi">+                # batch for the current effective size.</span>
<span class="gi">+                new_duration = 0.8 * old_duration + 0.2 * duration</span>
<span class="gi">+            self._smoothed_batch_duration = new_duration</span>

<span class="w"> </span>    def reset_batch_stats(self):
<span class="w"> </span>        &quot;&quot;&quot;Reset batch statistics to default values.

<span class="w"> </span>        This avoids interferences with future jobs.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE</span>
<span class="gi">+        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION</span>


<span class="w"> </span>class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
<span class="gu">@@ -250,13 +412,21 @@ class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):</span>

<span class="w"> </span>    ThreadingBackend is used as the default backend for nested calls.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    supports_retrieve_callback = True
<span class="w"> </span>    uses_threads = True
<span class="w"> </span>    supports_sharedmem = True

<span class="w"> </span>    def configure(self, n_jobs=1, parallel=None, **backend_args):
<span class="w"> </span>        &quot;&quot;&quot;Build a process or thread pool and return the number of workers&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        n_jobs = self.effective_n_jobs(n_jobs)</span>
<span class="gi">+        if n_jobs == 1:</span>
<span class="gi">+            # Avoid unnecessary overhead and use sequential backend instead.</span>
<span class="gi">+            raise FallbackToBackend(</span>
<span class="gi">+                SequentialBackend(nesting_level=self.nesting_level))</span>
<span class="gi">+        self.parallel = parallel</span>
<span class="gi">+        self._n_jobs = n_jobs</span>
<span class="gi">+        return n_jobs</span>

<span class="w"> </span>    def _get_pool(self):
<span class="w"> </span>        &quot;&quot;&quot;Lazily initialize the thread pool
<span class="gu">@@ -264,17 +434,20 @@ class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):</span>
<span class="w"> </span>        The actual pool of worker threads is only initialized at the first
<span class="w"> </span>        call to apply_async.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self._pool is None:</span>
<span class="gi">+            self._pool = ThreadPool(self._n_jobs)</span>
<span class="gi">+        return self._pool</span>


<span class="w"> </span>class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,
<span class="gd">-    ParallelBackendBase):</span>
<span class="gi">+                             ParallelBackendBase):</span>
<span class="w"> </span>    &quot;&quot;&quot;A ParallelBackend which will use a multiprocessing.Pool.

<span class="w"> </span>    Will introduce some communication and memory overhead when exchanging
<span class="w"> </span>    input and output data with the with the worker Python processes.
<span class="w"> </span>    However, does not suffer from the Python Global Interpreter Lock.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    supports_retrieve_callback = True
<span class="w"> </span>    supports_return_generator = False

<span class="gu">@@ -284,40 +457,171 @@ class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,</span>
<span class="w"> </span>        This also checks if we are attempting to create a nested parallel
<span class="w"> </span>        loop.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if mp is None:</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>
<span class="gi">+        if mp.current_process().daemon:</span>
<span class="gi">+            # Daemonic processes cannot have children</span>
<span class="gi">+            if n_jobs != 1:</span>
<span class="gi">+                if inside_dask_worker():</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        &quot;Inside a Dask worker with daemon=True, &quot;</span>
<span class="gi">+                        &quot;setting n_jobs=1.\nPossible work-arounds:\n&quot;</span>
<span class="gi">+                        &quot;- dask.config.set(&quot;</span>
<span class="gi">+                        &quot;{&#39;distributed.worker.daemon&#39;: False})&quot;</span>
<span class="gi">+                        &quot;- set the environment variable &quot;</span>
<span class="gi">+                        &quot;DASK_DISTRIBUTED__WORKER__DAEMON=False\n&quot;</span>
<span class="gi">+                        &quot;before creating your Dask cluster.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        &#39;Multiprocessing-backed parallel loops &#39;</span>
<span class="gi">+                        &#39;cannot be nested, setting n_jobs=1&#39;</span>
<span class="gi">+                    )</span>
<span class="gi">+                warnings.warn(msg, stacklevel=3)</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>
<span class="gi">+        if process_executor._CURRENT_DEPTH &gt; 0:</span>
<span class="gi">+            # Mixing loky and multiprocessing in nested loop is not supported</span>
<span class="gi">+            if n_jobs != 1:</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &#39;Multiprocessing-backed parallel loops cannot be nested,&#39;</span>
<span class="gi">+                    &#39; below loky, setting n_jobs=1&#39;,</span>
<span class="gi">+                    stacklevel=3)</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>
<span class="gi">+        elif not (self.in_main_thread() or self.nesting_level == 0):</span>
<span class="gi">+            # Prevent posix fork inside in non-main posix threads</span>
<span class="gi">+            if n_jobs != 1:</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &#39;Multiprocessing-backed parallel loops cannot be nested&#39;</span>
<span class="gi">+                    &#39; below threads, setting n_jobs=1&#39;,</span>
<span class="gi">+                    stacklevel=3)</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>
<span class="gi">+        return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)</span>

<span class="w"> </span>    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
<span class="gd">-        **memmappingpool_args):</span>
<span class="gi">+                  **memmappingpool_args):</span>
<span class="w"> </span>        &quot;&quot;&quot;Build a process or thread pool and return the number of workers&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        n_jobs = self.effective_n_jobs(n_jobs)</span>
<span class="gi">+        if n_jobs == 1:</span>
<span class="gi">+            raise FallbackToBackend(</span>
<span class="gi">+                SequentialBackend(nesting_level=self.nesting_level))</span>
<span class="gi">+</span>
<span class="gi">+        # Make sure to free as much memory as possible before forking</span>
<span class="gi">+        gc.collect()</span>
<span class="gi">+        self._pool = MemmappingPool(n_jobs, **memmappingpool_args)</span>
<span class="gi">+        self.parallel = parallel</span>
<span class="gi">+        return n_jobs</span>

<span class="w"> </span>    def terminate(self):
<span class="w"> </span>        &quot;&quot;&quot;Shutdown the process or thread pool&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        super(MultiprocessingBackend, self).terminate()</span>
<span class="gi">+        self.reset_batch_stats()</span>


<span class="w"> </span>class LokyBackend(AutoBatchingMixin, ParallelBackendBase):
<span class="w"> </span>    &quot;&quot;&quot;Managing pool of workers with loky instead of multiprocessing.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    supports_retrieve_callback = True
<span class="w"> </span>    supports_inner_max_num_threads = True

<span class="w"> </span>    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
<span class="gd">-        idle_worker_timeout=300, **memmappingexecutor_args):</span>
<span class="gi">+                  idle_worker_timeout=300, **memmappingexecutor_args):</span>
<span class="w"> </span>        &quot;&quot;&quot;Build a process executor and return the number of workers&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        n_jobs = self.effective_n_jobs(n_jobs)</span>
<span class="gi">+        if n_jobs == 1:</span>
<span class="gi">+            raise FallbackToBackend(</span>
<span class="gi">+                SequentialBackend(nesting_level=self.nesting_level))</span>
<span class="gi">+</span>
<span class="gi">+        self._workers = get_memmapping_executor(</span>
<span class="gi">+            n_jobs, timeout=idle_worker_timeout,</span>
<span class="gi">+            env=self._prepare_worker_env(n_jobs=n_jobs),</span>
<span class="gi">+            context_id=parallel._id, **memmappingexecutor_args)</span>
<span class="gi">+        self.parallel = parallel</span>
<span class="gi">+        return n_jobs</span>

<span class="w"> </span>    def effective_n_jobs(self, n_jobs):
<span class="w"> </span>        &quot;&quot;&quot;Determine the number of jobs which are going to run in parallel&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if n_jobs == 0:</span>
<span class="gi">+            raise ValueError(&#39;n_jobs == 0 in Parallel has no meaning&#39;)</span>
<span class="gi">+        elif mp is None or n_jobs is None:</span>
<span class="gi">+            # multiprocessing is not available or disabled, fallback</span>
<span class="gi">+            # to sequential mode</span>
<span class="gi">+            return 1</span>
<span class="gi">+        elif mp.current_process().daemon:</span>
<span class="gi">+            # Daemonic processes cannot have children</span>
<span class="gi">+            if n_jobs != 1:</span>
<span class="gi">+                if inside_dask_worker():</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        &quot;Inside a Dask worker with daemon=True, &quot;</span>
<span class="gi">+                        &quot;setting n_jobs=1.\nPossible work-arounds:\n&quot;</span>
<span class="gi">+                        &quot;- dask.config.set(&quot;</span>
<span class="gi">+                        &quot;{&#39;distributed.worker.daemon&#39;: False})\n&quot;</span>
<span class="gi">+                        &quot;- set the environment variable &quot;</span>
<span class="gi">+                        &quot;DASK_DISTRIBUTED__WORKER__DAEMON=False\n&quot;</span>
<span class="gi">+                        &quot;before creating your Dask cluster.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    msg = (</span>
<span class="gi">+                        &#39;Loky-backed parallel loops cannot be called in a&#39;</span>
<span class="gi">+                        &#39; multiprocessing, setting n_jobs=1&#39;</span>
<span class="gi">+                    )</span>
<span class="gi">+                warnings.warn(msg, stacklevel=3)</span>
<span class="gi">+</span>
<span class="gi">+            return 1</span>
<span class="gi">+        elif not (self.in_main_thread() or self.nesting_level == 0):</span>
<span class="gi">+            # Prevent posix fork inside in non-main posix threads</span>
<span class="gi">+            if n_jobs != 1:</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &#39;Loky-backed parallel loops cannot be nested below &#39;</span>
<span class="gi">+                    &#39;threads, setting n_jobs=1&#39;,</span>
<span class="gi">+                    stacklevel=3)</span>
<span class="gi">+            return 1</span>
<span class="gi">+        elif n_jobs &lt; 0:</span>
<span class="gi">+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)</span>
<span class="gi">+        return n_jobs</span>

<span class="w"> </span>    def apply_async(self, func, callback=None):
<span class="w"> </span>        &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        future = self._workers.submit(func)</span>
<span class="gi">+        if callback is not None:</span>
<span class="gi">+            future.add_done_callback(callback)</span>
<span class="gi">+        return future</span>
<span class="gi">+</span>
<span class="gi">+    def retrieve_result_callback(self, out):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return out.result()</span>
<span class="gi">+        except ShutdownExecutorError:</span>
<span class="gi">+            raise RuntimeError(</span>
<span class="gi">+                &quot;The executor underlying Parallel has been shutdown. &quot;</span>
<span class="gi">+                &quot;This is likely due to the garbage collection of a previous &quot;</span>
<span class="gi">+                &quot;generator from a call to Parallel with return_as=&#39;generator&#39;.&quot;</span>
<span class="gi">+                &quot; Make sure the generator is not garbage collected when &quot;</span>
<span class="gi">+                &quot;submitting a new job or that it is first properly exhausted.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def terminate(self):</span>
<span class="gi">+        if self._workers is not None:</span>
<span class="gi">+            # Don&#39;t terminate the workers as we want to reuse them in later</span>
<span class="gi">+            # calls, but cleanup the temporary resources that the Parallel call</span>
<span class="gi">+            # created. This &#39;hack&#39; requires a private, low-level operation.</span>
<span class="gi">+            self._workers._temp_folder_manager._clean_temporary_resources(</span>
<span class="gi">+                context_id=self.parallel._id, force=False</span>
<span class="gi">+            )</span>
<span class="gi">+            self._workers = None</span>
<span class="gi">+</span>
<span class="gi">+        self.reset_batch_stats()</span>

<span class="w"> </span>    def abort_everything(self, ensure_ready=True):
<span class="w"> </span>        &quot;&quot;&quot;Shutdown the workers and restart a new one with the same parameters
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._workers.terminate(kill_workers=True)</span>
<span class="gi">+        self._workers = None</span>
<span class="gi">+</span>
<span class="gi">+        if ensure_ready:</span>
<span class="gi">+            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel)</span>


<span class="w"> </span>class FallbackToBackend(Exception):
<span class="gu">@@ -330,4 +634,16 @@ class FallbackToBackend(Exception):</span>
<span class="w"> </span>def inside_dask_worker():
<span class="w"> </span>    &quot;&quot;&quot;Check whether the current function is executed inside a Dask worker.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # This function can not be in joblib._dask because there would be a</span>
<span class="gi">+    # circular import:</span>
<span class="gi">+    # _dask imports _parallel_backend that imports _dask ...</span>
<span class="gi">+    try:</span>
<span class="gi">+        from distributed import get_worker</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        get_worker()</span>
<span class="gi">+        return True</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        return False</span>
<span class="gh">diff --git a/joblib/_store_backends.py b/joblib/_store_backends.py</span>
<span class="gh">index 0ce3682..68e207c 100644</span>
<span class="gd">--- a/joblib/_store_backends.py</span>
<span class="gi">+++ b/joblib/_store_backends.py</span>
<span class="gu">@@ -1,4 +1,5 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Storage providers backends for Memory caching.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from pickle import PicklingError
<span class="w"> </span>import re
<span class="w"> </span>import os
<span class="gu">@@ -12,12 +13,14 @@ import collections</span>
<span class="w"> </span>import operator
<span class="w"> </span>import threading
<span class="w"> </span>from abc import ABCMeta, abstractmethod
<span class="gi">+</span>
<span class="w"> </span>from .backports import concurrency_safe_rename
<span class="w"> </span>from .disk import mkdirp, memstr_to_bytes, rm_subdirs
<span class="w"> </span>from .logger import format_time
<span class="w"> </span>from . import numpy_pickle
<span class="gd">-CacheItemInfo = collections.namedtuple(&#39;CacheItemInfo&#39;, &#39;path size last_access&#39;</span>
<span class="gd">-    )</span>
<span class="gi">+</span>
<span class="gi">+CacheItemInfo = collections.namedtuple(&#39;CacheItemInfo&#39;,</span>
<span class="gi">+                                       &#39;path size last_access&#39;)</span>


<span class="w"> </span>class CacheWarning(Warning):
<span class="gu">@@ -27,12 +30,18 @@ class CacheWarning(Warning):</span>

<span class="w"> </span>def concurrency_safe_write(object_to_write, filename, write_func):
<span class="w"> </span>    &quot;&quot;&quot;Writes an object into a unique file in a concurrency-safe way.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    thread_id = id(threading.current_thread())</span>
<span class="gi">+    temporary_filename = &#39;{}.thread-{}-pid-{}&#39;.format(</span>
<span class="gi">+        filename, thread_id, os.getpid())</span>
<span class="gi">+    write_func(object_to_write, temporary_filename)</span>
<span class="gi">+</span>
<span class="gi">+    return temporary_filename</span>


<span class="w"> </span>class StoreBackendBase(metaclass=ABCMeta):
<span class="w"> </span>    &quot;&quot;&quot;Helper Abstract Base Class which defines all methods that
<span class="w"> </span>       a StorageBackend must implement.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    location = None

<span class="w"> </span>    @abstractmethod
<span class="gu">@@ -53,7 +62,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        -------
<span class="w"> </span>        a file-like object
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def _item_exists(self, location):
<span class="gu">@@ -71,7 +79,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        -------
<span class="w"> </span>        True if the item exists, False otherwise
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def _move_item(self, src, dst):
<span class="gu">@@ -86,7 +93,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        dst: string
<span class="w"> </span>            The destination location of an item
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def create_location(self, location):
<span class="gu">@@ -98,7 +104,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>            The location in the store. On a filesystem, this corresponds to a
<span class="w"> </span>            directory.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def clear_location(self, location):
<span class="gu">@@ -110,7 +115,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>            The location in the store. On a filesystem, this corresponds to a
<span class="w"> </span>            directory or a filename absolute path
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def get_items(self):
<span class="gu">@@ -121,7 +125,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>        The list of items identified by their ids (e.g filename in a
<span class="w"> </span>        filesystem).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def configure(self, location, verbose=0, backend_options=dict()):
<span class="gu">@@ -138,7 +141,6 @@ class StoreBackendBase(metaclass=ABCMeta):</span>
<span class="w"> </span>            Contains a dictionary of named parameters used to configure the
<span class="w"> </span>            store backend.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>


<span class="w"> </span>class StoreBackendMixin(object):
<span class="gu">@@ -153,101 +155,320 @@ class StoreBackendMixin(object):</span>

<span class="w"> </span>    def load_item(self, call_id, verbose=1, timestamp=None, metadata=None):
<span class="w"> </span>        &quot;&quot;&quot;Load an item from the store given its id as a list of str.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        full_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+</span>
<span class="gi">+        if verbose &gt; 1:</span>
<span class="gi">+            ts_string = (&#39;{: &lt;16}&#39;.format(format_time(time.time() - timestamp))</span>
<span class="gi">+                         if timestamp is not None else &#39;&#39;)</span>
<span class="gi">+            signature = os.path.basename(call_id[0])</span>
<span class="gi">+            if metadata is not None and &#39;input_args&#39; in metadata:</span>
<span class="gi">+                kwargs = &#39;, &#39;.join(&#39;{}={}&#39;.format(*item)</span>
<span class="gi">+                                   for item in metadata[&#39;input_args&#39;].items())</span>
<span class="gi">+                signature += &#39;({})&#39;.format(kwargs)</span>
<span class="gi">+            msg = &#39;[Memory]{}: Loading {}&#39;.format(ts_string, signature)</span>
<span class="gi">+            if verbose &lt; 10:</span>
<span class="gi">+                print(&#39;{0}...&#39;.format(msg))</span>
<span class="gi">+            else:</span>
<span class="gi">+                print(&#39;{0} from {1}&#39;.format(msg, full_path))</span>
<span class="gi">+</span>
<span class="gi">+        mmap_mode = (None if not hasattr(self, &#39;mmap_mode&#39;)</span>
<span class="gi">+                     else self.mmap_mode)</span>
<span class="gi">+</span>
<span class="gi">+        filename = os.path.join(full_path, &#39;output.pkl&#39;)</span>
<span class="gi">+        if not self._item_exists(filename):</span>
<span class="gi">+            raise KeyError(&quot;Non-existing item (may have been &quot;</span>
<span class="gi">+                           &quot;cleared).\nFile %s does not exist&quot; % filename)</span>
<span class="gi">+</span>
<span class="gi">+        # file-like object cannot be used when mmap_mode is set</span>
<span class="gi">+        if mmap_mode is None:</span>
<span class="gi">+            with self._open_item(filename, &quot;rb&quot;) as f:</span>
<span class="gi">+                item = numpy_pickle.load(f)</span>
<span class="gi">+        else:</span>
<span class="gi">+            item = numpy_pickle.load(filename, mmap_mode=mmap_mode)</span>
<span class="gi">+        return item</span>

<span class="w"> </span>    def dump_item(self, call_id, item, verbose=1):
<span class="w"> </span>        &quot;&quot;&quot;Dump an item in the store at the id given as a list of str.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            item_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+            if not self._item_exists(item_path):</span>
<span class="gi">+                self.create_location(item_path)</span>
<span class="gi">+            filename = os.path.join(item_path, &#39;output.pkl&#39;)</span>
<span class="gi">+            if verbose &gt; 10:</span>
<span class="gi">+                print(&#39;Persisting in %s&#39; % item_path)</span>
<span class="gi">+</span>
<span class="gi">+            def write_func(to_write, dest_filename):</span>
<span class="gi">+                with self._open_item(dest_filename, &quot;wb&quot;) as f:</span>
<span class="gi">+                    try:</span>
<span class="gi">+                        numpy_pickle.dump(to_write, f, compress=self.compress)</span>
<span class="gi">+                    except PicklingError as e:</span>
<span class="gi">+                        # TODO(1.5) turn into error</span>
<span class="gi">+                        warnings.warn(</span>
<span class="gi">+                            &quot;Unable to cache to disk: failed to pickle &quot;</span>
<span class="gi">+                            &quot;output. In version 1.5 this will raise an &quot;</span>
<span class="gi">+                            f&quot;exception. Exception: {e}.&quot;,</span>
<span class="gi">+                            FutureWarning</span>
<span class="gi">+                        )</span>
<span class="gi">+</span>
<span class="gi">+            self._concurrency_safe_write(item, filename, write_func)</span>
<span class="gi">+        except Exception as e:  # noqa: E722</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;Unable to cache to disk. Possibly a race condition in the &quot;</span>
<span class="gi">+                f&quot;creation of the directory. Exception: {e}.&quot;,</span>
<span class="gi">+                CacheWarning</span>
<span class="gi">+            )</span>

<span class="w"> </span>    def clear_item(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Clear the item at the id, given as a list of str.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        item_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+        if self._item_exists(item_path):</span>
<span class="gi">+            self.clear_location(item_path)</span>

<span class="w"> </span>    def contains_item(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Check if there is an item at the id, given as a list of str.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        item_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+        filename = os.path.join(item_path, &#39;output.pkl&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        return self._item_exists(filename)</span>

<span class="w"> </span>    def get_item_info(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Return information about item.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return {&#39;location&#39;: os.path.join(self.location, *call_id)}</span>

<span class="w"> </span>    def get_metadata(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Return actual metadata of an item.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            item_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+            filename = os.path.join(item_path, &#39;metadata.json&#39;)</span>
<span class="gi">+            with self._open_item(filename, &#39;rb&#39;) as f:</span>
<span class="gi">+                return json.loads(f.read().decode(&#39;utf-8&#39;))</span>
<span class="gi">+        except:  # noqa: E722</span>
<span class="gi">+            return {}</span>

<span class="w"> </span>    def store_metadata(self, call_id, metadata):
<span class="w"> </span>        &quot;&quot;&quot;Store metadata of a computation.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            item_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+            self.create_location(item_path)</span>
<span class="gi">+            filename = os.path.join(item_path, &#39;metadata.json&#39;)</span>
<span class="gi">+</span>
<span class="gi">+            def write_func(to_write, dest_filename):</span>
<span class="gi">+                with self._open_item(dest_filename, &quot;wb&quot;) as f:</span>
<span class="gi">+                    f.write(json.dumps(to_write).encode(&#39;utf-8&#39;))</span>
<span class="gi">+</span>
<span class="gi">+            self._concurrency_safe_write(metadata, filename, write_func)</span>
<span class="gi">+        except:  # noqa: E722</span>
<span class="gi">+            pass</span>

<span class="w"> </span>    def contains_path(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Check cached function is available in store.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        func_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+        return self.object_exists(func_path)</span>

<span class="w"> </span>    def clear_path(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Clear all items with a common path in the store.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        func_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+        if self._item_exists(func_path):</span>
<span class="gi">+            self.clear_location(func_path)</span>

<span class="w"> </span>    def store_cached_func_code(self, call_id, func_code=None):
<span class="w"> </span>        &quot;&quot;&quot;Store the code of the cached function.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        func_path = os.path.join(self.location, *call_id)</span>
<span class="gi">+        if not self._item_exists(func_path):</span>
<span class="gi">+            self.create_location(func_path)</span>
<span class="gi">+</span>
<span class="gi">+        if func_code is not None:</span>
<span class="gi">+            filename = os.path.join(func_path, &quot;func_code.py&quot;)</span>
<span class="gi">+            with self._open_item(filename, &#39;wb&#39;) as f:</span>
<span class="gi">+                f.write(func_code.encode(&#39;utf-8&#39;))</span>

<span class="w"> </span>    def get_cached_func_code(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Store the code of the cached function.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        filename = os.path.join(self.location, *call_id, &#39;func_code.py&#39;)</span>
<span class="gi">+        try:</span>
<span class="gi">+            with self._open_item(filename, &#39;rb&#39;) as f:</span>
<span class="gi">+                return f.read().decode(&#39;utf-8&#39;)</span>
<span class="gi">+        except:  # noqa: E722</span>
<span class="gi">+            raise</span>

<span class="w"> </span>    def get_cached_func_info(self, call_id):
<span class="w"> </span>        &quot;&quot;&quot;Return information related to the cached function if it exists.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return {&#39;location&#39;: os.path.join(self.location, *call_id)}</span>

<span class="w"> </span>    def clear(self):
<span class="w"> </span>        &quot;&quot;&quot;Clear the whole store content.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.clear_location(self.location)</span>

<span class="gd">-    def enforce_store_limits(self, bytes_limit, items_limit=None, age_limit</span>
<span class="gd">-        =None):</span>
<span class="gi">+    def enforce_store_limits(</span>
<span class="gi">+            self, bytes_limit, items_limit=None, age_limit=None</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Remove the store&#39;s oldest files to enforce item, byte, and age limits.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def _get_items_to_delete(self, bytes_limit, items_limit=None, age_limit</span>
<span class="gd">-        =None):</span>
<span class="gi">+        items_to_delete = self._get_items_to_delete(</span>
<span class="gi">+            bytes_limit, items_limit, age_limit</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        for item in items_to_delete:</span>
<span class="gi">+            if self.verbose &gt; 10:</span>
<span class="gi">+                print(&#39;Deleting item {0}&#39;.format(item))</span>
<span class="gi">+            try:</span>
<span class="gi">+                self.clear_location(item.path)</span>
<span class="gi">+            except OSError:</span>
<span class="gi">+                # Even with ignore_errors=True shutil.rmtree can raise OSError</span>
<span class="gi">+                # with:</span>
<span class="gi">+                # [Errno 116] Stale file handle if another process has deleted</span>
<span class="gi">+                # the folder already.</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+    def _get_items_to_delete(</span>
<span class="gi">+            self, bytes_limit, items_limit=None, age_limit=None</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Get items to delete to keep the store under size, file, &amp; age limits.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(bytes_limit, str):</span>
<span class="gi">+            bytes_limit = memstr_to_bytes(bytes_limit)</span>
<span class="gi">+</span>
<span class="gi">+        items = self.get_items()</span>
<span class="gi">+        if not items:</span>
<span class="gi">+            return []</span>
<span class="gi">+</span>
<span class="gi">+        size = sum(item.size for item in items)</span>
<span class="gi">+</span>
<span class="gi">+        if bytes_limit is not None:</span>
<span class="gi">+            to_delete_size = size - bytes_limit</span>
<span class="gi">+        else:</span>
<span class="gi">+            to_delete_size = 0</span>
<span class="gi">+</span>
<span class="gi">+        if items_limit is not None:</span>
<span class="gi">+            to_delete_items = len(items) - items_limit</span>
<span class="gi">+        else:</span>
<span class="gi">+            to_delete_items = 0</span>
<span class="gi">+</span>
<span class="gi">+        if age_limit is not None:</span>
<span class="gi">+            older_item = min(item.last_access for item in items)</span>
<span class="gi">+            deadline = datetime.datetime.now() - age_limit</span>
<span class="gi">+        else:</span>
<span class="gi">+            deadline = None</span>
<span class="gi">+</span>
<span class="gi">+        if (</span>
<span class="gi">+            to_delete_size &lt;= 0 and to_delete_items &lt;= 0</span>
<span class="gi">+            and (deadline is None or older_item &gt; deadline)</span>
<span class="gi">+        ):</span>
<span class="gi">+            return []</span>
<span class="gi">+</span>
<span class="gi">+        # We want to delete first the cache items that were accessed a</span>
<span class="gi">+        # long time ago</span>
<span class="gi">+        items.sort(key=operator.attrgetter(&#39;last_access&#39;))</span>
<span class="gi">+</span>
<span class="gi">+        items_to_delete = []</span>
<span class="gi">+        size_so_far = 0</span>
<span class="gi">+        items_so_far = 0</span>
<span class="gi">+</span>
<span class="gi">+        for item in items:</span>
<span class="gi">+            if (</span>
<span class="gi">+                (size_so_far &gt;= to_delete_size)</span>
<span class="gi">+                and items_so_far &gt;= to_delete_items</span>
<span class="gi">+                and (deadline is None or deadline &lt; item.last_access)</span>
<span class="gi">+            ):</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+            items_to_delete.append(item)</span>
<span class="gi">+            size_so_far += item.size</span>
<span class="gi">+            items_so_far += 1</span>
<span class="gi">+</span>
<span class="gi">+        return items_to_delete</span>

<span class="w"> </span>    def _concurrency_safe_write(self, to_write, filename, write_func):
<span class="w"> </span>        &quot;&quot;&quot;Writes an object into a file in a concurrency-safe way.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        temporary_filename = concurrency_safe_write(to_write,</span>
<span class="gi">+                                                    filename, write_func)</span>
<span class="gi">+        self._move_item(temporary_filename, filename)</span>

<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        &quot;&quot;&quot;Printable representation of the store location.&quot;&quot;&quot;
<span class="gd">-        return &#39;{class_name}(location=&quot;{location}&quot;)&#39;.format(class_name=self</span>
<span class="gd">-            .__class__.__name__, location=self.location)</span>
<span class="gi">+        return &#39;{class_name}(location=&quot;{location}&quot;)&#39;.format(</span>
<span class="gi">+            class_name=self.__class__.__name__, location=self.location)</span>


<span class="w"> </span>class FileSystemStoreBackend(StoreBackendBase, StoreBackendMixin):
<span class="w"> </span>    &quot;&quot;&quot;A StoreBackend used with local or network file systems.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    _open_item = staticmethod(open)
<span class="w"> </span>    _item_exists = staticmethod(os.path.exists)
<span class="w"> </span>    _move_item = staticmethod(concurrency_safe_rename)

<span class="w"> </span>    def clear_location(self, location):
<span class="w"> </span>        &quot;&quot;&quot;Delete location on store.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if (location == self.location):</span>
<span class="gi">+            rm_subdirs(location)</span>
<span class="gi">+        else:</span>
<span class="gi">+            shutil.rmtree(location, ignore_errors=True)</span>

<span class="w"> </span>    def create_location(self, location):
<span class="w"> </span>        &quot;&quot;&quot;Create object location on store&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        mkdirp(location)</span>

<span class="w"> </span>    def get_items(self):
<span class="w"> </span>        &quot;&quot;&quot;Returns the whole list of items available in the store.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        items = []</span>
<span class="gi">+</span>
<span class="gi">+        for dirpath, _, filenames in os.walk(self.location):</span>
<span class="gi">+            is_cache_hash_dir = re.match(&#39;[a-f0-9]{32}&#39;,</span>
<span class="gi">+                                         os.path.basename(dirpath))</span>
<span class="gi">+</span>
<span class="gi">+            if is_cache_hash_dir:</span>
<span class="gi">+                output_filename = os.path.join(dirpath, &#39;output.pkl&#39;)</span>
<span class="gi">+                try:</span>
<span class="gi">+                    last_access = os.path.getatime(output_filename)</span>
<span class="gi">+                except OSError:</span>
<span class="gi">+                    try:</span>
<span class="gi">+                        last_access = os.path.getatime(dirpath)</span>
<span class="gi">+                    except OSError:</span>
<span class="gi">+                        # The directory has already been deleted</span>
<span class="gi">+                        continue</span>
<span class="gi">+</span>
<span class="gi">+                last_access = datetime.datetime.fromtimestamp(last_access)</span>
<span class="gi">+                try:</span>
<span class="gi">+                    full_filenames = [os.path.join(dirpath, fn)</span>
<span class="gi">+                                      for fn in filenames]</span>
<span class="gi">+                    dirsize = sum(os.path.getsize(fn)</span>
<span class="gi">+                                  for fn in full_filenames)</span>
<span class="gi">+                except OSError:</span>
<span class="gi">+                    # Either output_filename or one of the files in</span>
<span class="gi">+                    # dirpath does not exist any more. We assume this</span>
<span class="gi">+                    # directory is being cleaned by another process already</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                items.append(CacheItemInfo(dirpath, dirsize,</span>
<span class="gi">+                                           last_access))</span>
<span class="gi">+</span>
<span class="gi">+        return items</span>

<span class="w"> </span>    def configure(self, location, verbose=1, backend_options=None):
<span class="w"> </span>        &quot;&quot;&quot;Configure the store backend.

<span class="w"> </span>        For this backend, valid store options are &#39;compress&#39; and &#39;mmap_mode&#39;
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if backend_options is None:</span>
<span class="gi">+            backend_options = {}</span>
<span class="gi">+</span>
<span class="gi">+        # setup location directory</span>
<span class="gi">+        self.location = location</span>
<span class="gi">+        if not os.path.exists(self.location):</span>
<span class="gi">+            mkdirp(self.location)</span>
<span class="gi">+</span>
<span class="gi">+        # item can be stored compressed for faster I/O</span>
<span class="gi">+        self.compress = backend_options.get(&#39;compress&#39;, False)</span>
<span class="gi">+</span>
<span class="gi">+        # FileSystemStoreBackend can be used with mmap_mode options under</span>
<span class="gi">+        # certain conditions.</span>
<span class="gi">+        mmap_mode = backend_options.get(&#39;mmap_mode&#39;)</span>
<span class="gi">+        if self.compress and mmap_mode is not None:</span>
<span class="gi">+            warnings.warn(&#39;Compressed items cannot be memmapped in a &#39;</span>
<span class="gi">+                          &#39;filesystem store. Option will be ignored.&#39;,</span>
<span class="gi">+                          stacklevel=2)</span>
<span class="gi">+</span>
<span class="gi">+        self.mmap_mode = mmap_mode</span>
<span class="gi">+        self.verbose = verbose</span>
<span class="gh">diff --git a/joblib/_utils.py b/joblib/_utils.py</span>
<span class="gh">index d2feff7..0b7cc64 100644</span>
<span class="gd">--- a/joblib/_utils.py</span>
<span class="gi">+++ b/joblib/_utils.py</span>
<span class="gu">@@ -1,12 +1,27 @@</span>
<span class="gi">+# Adapted from https://stackoverflow.com/a/9558001/2536294</span>
<span class="gi">+</span>
<span class="w"> </span>import ast
<span class="w"> </span>from dataclasses import dataclass
<span class="w"> </span>import operator as op
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>from ._multiprocessing_helpers import mp
<span class="gi">+</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    from .externals.loky.process_executor import _ExceptionWithTraceback
<span class="gd">-operators = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div:</span>
<span class="gd">-    op.truediv, ast.FloorDiv: op.floordiv, ast.Mod: op.mod, ast.Pow: op.pow,</span>
<span class="gd">-    ast.USub: op.neg}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# supported operators</span>
<span class="gi">+operators = {</span>
<span class="gi">+    ast.Add: op.add,</span>
<span class="gi">+    ast.Sub: op.sub,</span>
<span class="gi">+    ast.Mult: op.mul,</span>
<span class="gi">+    ast.Div: op.truediv,</span>
<span class="gi">+    ast.FloorDiv: op.floordiv,</span>
<span class="gi">+    ast.Mod: op.mod,</span>
<span class="gi">+    ast.Pow: op.pow,</span>
<span class="gi">+    ast.USub: op.neg,</span>
<span class="gi">+}</span>


<span class="w"> </span>def eval_expr(expr):
<span class="gu">@@ -18,7 +33,23 @@ def eval_expr(expr):</span>
<span class="w"> </span>    &gt;&gt;&gt; eval_expr(&#39;1 + 2*3**(4) / (6 + -7)&#39;)
<span class="w"> </span>    -161.0
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        return eval_(ast.parse(expr, mode=&quot;eval&quot;).body)</span>
<span class="gi">+    except (TypeError, SyntaxError, KeyError) as e:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;{expr!r} is not a valid or supported arithmetic expression.&quot;</span>
<span class="gi">+        ) from e</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def eval_(node):</span>
<span class="gi">+    if isinstance(node, ast.Constant):  # &lt;constant&gt;</span>
<span class="gi">+        return node.value</span>
<span class="gi">+    elif isinstance(node, ast.BinOp):  # &lt;left&gt; &lt;operator&gt; &lt;right&gt;</span>
<span class="gi">+        return operators[type(node.op)](eval_(node.left), eval_(node.right))</span>
<span class="gi">+    elif isinstance(node, ast.UnaryOp):  # &lt;operator&gt; &lt;operand&gt; e.g., -1</span>
<span class="gi">+        return operators[type(node.op)](eval_(node.operand))</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise TypeError(node)</span>


<span class="w"> </span>@dataclass(frozen=True)
<span class="gu">@@ -27,7 +58,7 @@ class _Sentinel:</span>
<span class="w"> </span>    default_value: object

<span class="w"> </span>    def __repr__(self):
<span class="gd">-        return f&#39;default({self.default_value!r})&#39;</span>
<span class="gi">+        return f&quot;default({self.default_value!r})&quot;</span>


<span class="w"> </span>class _TracebackCapturingWrapper:
<span class="gu">@@ -41,3 +72,12 @@ class _TracebackCapturingWrapper:</span>
<span class="w"> </span>            return self.func(**kwargs)
<span class="w"> </span>        except BaseException as e:
<span class="w"> </span>            return _ExceptionWithTraceback(e)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _retrieve_traceback_capturing_wrapped_call(out):</span>
<span class="gi">+    if isinstance(out, _ExceptionWithTraceback):</span>
<span class="gi">+        rebuild, args = out.__reduce__()</span>
<span class="gi">+        out = rebuild(*args)</span>
<span class="gi">+    if isinstance(out, BaseException):</span>
<span class="gi">+        raise out</span>
<span class="gi">+    return out</span>
<span class="gh">diff --git a/joblib/backports.py b/joblib/backports.py</span>
<span class="gh">index b7178c8..3a14f10 100644</span>
<span class="gd">--- a/joblib/backports.py</span>
<span class="gi">+++ b/joblib/backports.py</span>
<span class="gu">@@ -4,6 +4,7 @@ Backports of fixes for joblib dependencies</span>
<span class="w"> </span>import os
<span class="w"> </span>import re
<span class="w"> </span>import time
<span class="gi">+</span>
<span class="w"> </span>from os.path import basename
<span class="w"> </span>from multiprocessing import util

<span class="gu">@@ -65,24 +66,53 @@ class LooseVersion(Version):</span>
<span class="w"> </span>    We might rexplore this choice in the future if all major Python projects
<span class="w"> </span>    introduce a dependency on packaging anyway.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    component_re = re.compile(&#39;(\\d+ | [a-z]+ | \\.)&#39;, re.VERBOSE)</span>
<span class="gi">+</span>
<span class="gi">+    component_re = re.compile(r&#39;(\d+ | [a-z]+ | \.)&#39;, re.VERBOSE)</span>

<span class="w"> </span>    def __init__(self, vstring=None):
<span class="w"> </span>        if vstring:
<span class="w"> </span>            self.parse(vstring)

<span class="gi">+    def parse(self, vstring):</span>
<span class="gi">+        # I&#39;ve given up on thinking I can reconstruct the version string</span>
<span class="gi">+        # from the parsed tuple -- so I just store the string here for</span>
<span class="gi">+        # use by __str__</span>
<span class="gi">+        self.vstring = vstring</span>
<span class="gi">+        components = [x for x in self.component_re.split(vstring)</span>
<span class="gi">+                      if x and x != &#39;.&#39;]</span>
<span class="gi">+        for i, obj in enumerate(components):</span>
<span class="gi">+            try:</span>
<span class="gi">+                components[i] = int(obj)</span>
<span class="gi">+            except ValueError:</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+        self.version = components</span>
<span class="gi">+</span>
<span class="w"> </span>    def __str__(self):
<span class="w"> </span>        return self.vstring

<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        return &quot;LooseVersion (&#39;%s&#39;)&quot; % str(self)

<span class="gi">+    def _cmp(self, other):</span>
<span class="gi">+        if isinstance(other, str):</span>
<span class="gi">+            other = LooseVersion(other)</span>
<span class="gi">+        elif not isinstance(other, LooseVersion):</span>
<span class="gi">+            return NotImplemented</span>
<span class="gi">+</span>
<span class="gi">+        if self.version == other.version:</span>
<span class="gi">+            return 0</span>
<span class="gi">+        if self.version &lt; other.version:</span>
<span class="gi">+            return -1</span>
<span class="gi">+        if self.version &gt; other.version:</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>

<span class="w"> </span>try:
<span class="w"> </span>    import numpy as np

<span class="gd">-    def make_memmap(filename, dtype=&#39;uint8&#39;, mode=&#39;r+&#39;, offset=0, shape=</span>
<span class="gd">-        None, order=&#39;C&#39;, unlink_on_gc_collect=False):</span>
<span class="gi">+    def make_memmap(filename, dtype=&#39;uint8&#39;, mode=&#39;r+&#39;, offset=0,</span>
<span class="gi">+                    shape=None, order=&#39;C&#39;, unlink_on_gc_collect=False):</span>
<span class="w"> </span>        &quot;&quot;&quot;Custom memmap constructor compatible with numpy.memmap.

<span class="w"> </span>        This function:
<span class="gu">@@ -95,10 +125,30 @@ try:</span>
<span class="w"> </span>          newly-created memmap that sends a maybe_unlink request for the
<span class="w"> </span>          memmaped file to resource_tracker.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        util.debug(</span>
<span class="gi">+            &quot;[MEMMAP READ] creating a memmap (shape {}, filename {}, &quot;</span>
<span class="gi">+            &quot;pid {})&quot;.format(shape, basename(filename), os.getpid())</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        mm = np.memmap(filename, dtype=dtype, mode=mode, offset=offset,</span>
<span class="gi">+                       shape=shape, order=order)</span>
<span class="gi">+        if LooseVersion(np.__version__) &lt; &#39;1.13&#39;:</span>
<span class="gi">+            mm.offset = offset</span>
<span class="gi">+        if unlink_on_gc_collect:</span>
<span class="gi">+            from ._memmapping_reducer import add_maybe_unlink_finalizer</span>
<span class="gi">+            add_maybe_unlink_finalizer(mm)</span>
<span class="gi">+        return mm</span>
<span class="w"> </span>except ImportError:
<span class="gi">+    def make_memmap(filename, dtype=&#39;uint8&#39;, mode=&#39;r+&#39;, offset=0,</span>
<span class="gi">+                    shape=None, order=&#39;C&#39;, unlink_on_gc_collect=False):</span>
<span class="gi">+        raise NotImplementedError(</span>
<span class="gi">+            &quot;&#39;joblib.backports.make_memmap&#39; should not be used &quot;</span>
<span class="gi">+            &#39;if numpy is not installed.&#39;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>if os.name == &#39;nt&#39;:
<span class="gd">-    access_denied_errors = 5, 13</span>
<span class="gi">+    # https://github.com/joblib/joblib/issues/540</span>
<span class="gi">+    access_denied_errors = (5, 13)</span>
<span class="w"> </span>    from os import replace

<span class="w"> </span>    def concurrency_safe_rename(src, dst):
<span class="gu">@@ -107,6 +157,21 @@ if os.name == &#39;nt&#39;:</span>
<span class="w"> </span>        On Windows os.replace can yield permission errors if executed by two
<span class="w"> </span>        different processes.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        max_sleep_time = 1</span>
<span class="gi">+        total_sleep_time = 0</span>
<span class="gi">+        sleep_time = 0.001</span>
<span class="gi">+        while total_sleep_time &lt; max_sleep_time:</span>
<span class="gi">+            try:</span>
<span class="gi">+                replace(src, dst)</span>
<span class="gi">+                break</span>
<span class="gi">+            except Exception as exc:</span>
<span class="gi">+                if getattr(exc, &#39;winerror&#39;, None) in access_denied_errors:</span>
<span class="gi">+                    time.sleep(sleep_time)</span>
<span class="gi">+                    total_sleep_time += sleep_time</span>
<span class="gi">+                    sleep_time *= 2</span>
<span class="gi">+                else:</span>
<span class="gi">+                    raise</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise</span>
<span class="w"> </span>else:
<span class="gd">-    from os import replace as concurrency_safe_rename</span>
<span class="gi">+    from os import replace as concurrency_safe_rename  # noqa</span>
<span class="gh">diff --git a/joblib/compressor.py b/joblib/compressor.py</span>
<span class="gh">index 7a72b91..0d9e261 100644</span>
<span class="gd">--- a/joblib/compressor.py</span>
<span class="gi">+++ b/joblib/compressor.py</span>
<span class="gu">@@ -1,38 +1,49 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Classes and functions for managing compressors.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import io
<span class="w"> </span>import zlib
<span class="w"> </span>from joblib.backports import LooseVersion
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    from threading import RLock
<span class="w"> </span>except ImportError:
<span class="w"> </span>    from dummy_threading import RLock
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import bz2
<span class="w"> </span>except ImportError:
<span class="w"> </span>    bz2 = None
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import lz4
<span class="w"> </span>    from lz4.frame import LZ4FrameFile
<span class="w"> </span>except ImportError:
<span class="w"> </span>    lz4 = None
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import lzma
<span class="w"> </span>except ImportError:
<span class="w"> </span>    lzma = None
<span class="gd">-LZ4_NOT_INSTALLED_ERROR = (</span>
<span class="gd">-    &#39;LZ4 is not installed. Install it with pip: https://python-lz4.readthedocs.io/&#39;</span>
<span class="gd">-    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+LZ4_NOT_INSTALLED_ERROR = (&#39;LZ4 is not installed. Install it with pip: &#39;</span>
<span class="gi">+                           &#39;https://python-lz4.readthedocs.io/&#39;)</span>
<span class="gi">+</span>
<span class="gi">+# Registered compressors</span>
<span class="w"> </span>_COMPRESSORS = {}
<span class="gd">-_ZFILE_PREFIX = b&#39;ZF&#39;</span>
<span class="gd">-_ZLIB_PREFIX = b&#39;x&#39;</span>
<span class="gi">+</span>
<span class="gi">+# Magic numbers of supported compression file formats.</span>
<span class="gi">+_ZFILE_PREFIX = b&#39;ZF&#39;  # used with pickle files created before 0.9.3.</span>
<span class="gi">+_ZLIB_PREFIX = b&#39;\x78&#39;</span>
<span class="w"> </span>_GZIP_PREFIX = b&#39;\x1f\x8b&#39;
<span class="w"> </span>_BZ2_PREFIX = b&#39;BZ&#39;
<span class="gd">-_XZ_PREFIX = b&#39;\xfd7zXZ&#39;</span>
<span class="gd">-_LZMA_PREFIX = b&#39;]\x00&#39;</span>
<span class="gd">-_LZ4_PREFIX = b&#39;\x04&quot;M\x18&#39;</span>
<span class="gi">+_XZ_PREFIX = b&#39;\xfd\x37\x7a\x58\x5a&#39;</span>
<span class="gi">+_LZMA_PREFIX = b&#39;\x5d\x00&#39;</span>
<span class="gi">+_LZ4_PREFIX = b&#39;\x04\x22\x4D\x18&#39;</span>


<span class="gd">-def register_compressor(compressor_name, compressor, force=False):</span>
<span class="gi">+def register_compressor(compressor_name, compressor,</span>
<span class="gi">+                        force=False):</span>
<span class="w"> </span>    &quot;&quot;&quot;Register a new compressor.

<span class="w"> </span>    Parameters
<span class="gu">@@ -42,10 +53,32 @@ def register_compressor(compressor_name, compressor, force=False):</span>
<span class="w"> </span>    compressor: CompressorWrapper
<span class="w"> </span>        An instance of a &#39;CompressorWrapper&#39;.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    global _COMPRESSORS</span>
<span class="gi">+    if not isinstance(compressor_name, str):</span>
<span class="gi">+        raise ValueError(&quot;Compressor name should be a string, &quot;</span>
<span class="gi">+                         &quot;&#39;{}&#39; given.&quot;.format(compressor_name))</span>
<span class="gi">+</span>
<span class="gi">+    if not isinstance(compressor, CompressorWrapper):</span>
<span class="gi">+        raise ValueError(&quot;Compressor should implement the CompressorWrapper &quot;</span>
<span class="gi">+                         &quot;interface, &#39;{}&#39; given.&quot;.format(compressor))</span>
<span class="gi">+</span>
<span class="gi">+    if (compressor.fileobj_factory is not None and</span>
<span class="gi">+            (not hasattr(compressor.fileobj_factory, &#39;read&#39;) or</span>
<span class="gi">+             not hasattr(compressor.fileobj_factory, &#39;write&#39;) or</span>
<span class="gi">+             not hasattr(compressor.fileobj_factory, &#39;seek&#39;) or</span>
<span class="gi">+             not hasattr(compressor.fileobj_factory, &#39;tell&#39;))):</span>
<span class="gi">+        raise ValueError(&quot;Compressor &#39;fileobj_factory&#39; attribute should &quot;</span>
<span class="gi">+                         &quot;implement the file object interface, &#39;{}&#39; given.&quot;</span>
<span class="gi">+                         .format(compressor.fileobj_factory))</span>
<span class="gi">+</span>
<span class="gi">+    if compressor_name in _COMPRESSORS and not force:</span>
<span class="gi">+        raise ValueError(&quot;Compressor &#39;{}&#39; already registered.&quot;</span>
<span class="gi">+                         .format(compressor_name))</span>

<span class="gi">+    _COMPRESSORS[compressor_name] = compressor</span>

<span class="gd">-class CompressorWrapper:</span>
<span class="gi">+</span>
<span class="gi">+class CompressorWrapper():</span>
<span class="w"> </span>    &quot;&quot;&quot;A wrapper around a compressor file object.

<span class="w"> </span>    Attributes
<span class="gu">@@ -68,14 +101,19 @@ class CompressorWrapper:</span>

<span class="w"> </span>    def compressor_file(self, fileobj, compresslevel=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a compressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if compresslevel is None:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;,</span>
<span class="gi">+                                        compresslevel=compresslevel)</span>

<span class="w"> </span>    def decompressor_file(self, fileobj):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a decompressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.fileobj_factory(fileobj, &#39;rb&#39;)</span>


<span class="w"> </span>class BZ2CompressorWrapper(CompressorWrapper):
<span class="gi">+</span>
<span class="w"> </span>    prefix = _BZ2_PREFIX
<span class="w"> </span>    extension = &#39;.bz2&#39;

<span class="gu">@@ -85,16 +123,29 @@ class BZ2CompressorWrapper(CompressorWrapper):</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.fileobj_factory = None

<span class="gi">+    def _check_versions(self):</span>
<span class="gi">+        if bz2 is None:</span>
<span class="gi">+            raise ValueError(&#39;bz2 module is not compiled on your python &#39;</span>
<span class="gi">+                             &#39;standard library.&#39;)</span>
<span class="gi">+</span>
<span class="w"> </span>    def compressor_file(self, fileobj, compresslevel=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a compressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_versions()</span>
<span class="gi">+        if compresslevel is None:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;,</span>
<span class="gi">+                                        compresslevel=compresslevel)</span>

<span class="w"> </span>    def decompressor_file(self, fileobj):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a decompressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_versions()</span>
<span class="gi">+        fileobj = self.fileobj_factory(fileobj, &#39;rb&#39;)</span>
<span class="gi">+        return fileobj</span>


<span class="w"> </span>class LZMACompressorWrapper(CompressorWrapper):
<span class="gi">+</span>
<span class="w"> </span>    prefix = _LZMA_PREFIX
<span class="w"> </span>    extension = &#39;.lzma&#39;
<span class="w"> </span>    _lzma_format_name = &#39;FORMAT_ALONE&#39;
<span class="gu">@@ -106,22 +157,35 @@ class LZMACompressorWrapper(CompressorWrapper):</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.fileobj_factory = None

<span class="gi">+    def _check_versions(self):</span>
<span class="gi">+        if lzma is None:</span>
<span class="gi">+            raise ValueError(&#39;lzma module is not compiled on your python &#39;</span>
<span class="gi">+                             &#39;standard library.&#39;)</span>
<span class="gi">+</span>
<span class="w"> </span>    def compressor_file(self, fileobj, compresslevel=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a compressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if compresslevel is None:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;,</span>
<span class="gi">+                                        format=self._lzma_format)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;,</span>
<span class="gi">+                                        format=self._lzma_format,</span>
<span class="gi">+                                        preset=compresslevel)</span>

<span class="w"> </span>    def decompressor_file(self, fileobj):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a decompressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return lzma.LZMAFile(fileobj, &#39;rb&#39;)</span>


<span class="w"> </span>class XZCompressorWrapper(LZMACompressorWrapper):
<span class="gi">+</span>
<span class="w"> </span>    prefix = _XZ_PREFIX
<span class="w"> </span>    extension = &#39;.xz&#39;
<span class="w"> </span>    _lzma_format_name = &#39;FORMAT_XZ&#39;


<span class="w"> </span>class LZ4CompressorWrapper(CompressorWrapper):
<span class="gi">+</span>
<span class="w"> </span>    prefix = _LZ4_PREFIX
<span class="w"> </span>    extension = &#39;.lz4&#39;

<span class="gu">@@ -131,15 +195,32 @@ class LZ4CompressorWrapper(CompressorWrapper):</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.fileobj_factory = None

<span class="gi">+    def _check_versions(self):</span>
<span class="gi">+        if lz4 is None:</span>
<span class="gi">+            raise ValueError(LZ4_NOT_INSTALLED_ERROR)</span>
<span class="gi">+        lz4_version = lz4.__version__</span>
<span class="gi">+        if lz4_version.startswith(&quot;v&quot;):</span>
<span class="gi">+            lz4_version = lz4_version[1:]</span>
<span class="gi">+        if LooseVersion(lz4_version) &lt; LooseVersion(&#39;0.19&#39;):</span>
<span class="gi">+            raise ValueError(LZ4_NOT_INSTALLED_ERROR)</span>
<span class="gi">+</span>
<span class="w"> </span>    def compressor_file(self, fileobj, compresslevel=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a compressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_versions()</span>
<span class="gi">+        if compresslevel is None:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self.fileobj_factory(fileobj, &#39;wb&#39;,</span>
<span class="gi">+                                        compression_level=compresslevel)</span>

<span class="w"> </span>    def decompressor_file(self, fileobj):
<span class="w"> </span>        &quot;&quot;&quot;Returns an instance of a decompressor file object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_versions()</span>
<span class="gi">+        return self.fileobj_factory(fileobj, &#39;rb&#39;)</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+#  base file compression/decompression object definition</span>
<span class="w"> </span>_MODE_CLOSED = 0
<span class="w"> </span>_MODE_READ = 1
<span class="w"> </span>_MODE_READ_EOF = 2
<span class="gu">@@ -170,9 +251,12 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>    and 9 specifying the level of compression: 1 produces the least
<span class="w"> </span>    compression, and 9 produces the most compression. 3 is the default.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    wbits = zlib.MAX_WBITS

<span class="gd">-    def __init__(self, filename, mode=&#39;rb&#39;, compresslevel=3):</span>
<span class="gi">+    def __init__(self, filename, mode=&quot;rb&quot;, compresslevel=3):</span>
<span class="gi">+        # This lock must be recursive, so that BufferedIOBase&#39;s</span>
<span class="gi">+        # readline(), readlines() and writelines() don&#39;t deadlock.</span>
<span class="w"> </span>        self._lock = RLock()
<span class="w"> </span>        self._fp = None
<span class="w"> </span>        self._closefp = False
<span class="gu">@@ -180,29 +264,33 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>        self._pos = 0
<span class="w"> </span>        self._size = -1
<span class="w"> </span>        self.compresslevel = compresslevel
<span class="gd">-        if not isinstance(compresslevel, int) or not 1 &lt;= compresslevel &lt;= 9:</span>
<span class="gd">-            raise ValueError(</span>
<span class="gd">-                &quot;&#39;compresslevel&#39; must be an integer between 1 and 9. You provided &#39;compresslevel={}&#39;&quot;</span>
<span class="gd">-                .format(compresslevel))</span>
<span class="gd">-        if mode == &#39;rb&#39;:</span>
<span class="gi">+</span>
<span class="gi">+        if not isinstance(compresslevel, int) or not (1 &lt;= compresslevel &lt;= 9):</span>
<span class="gi">+            raise ValueError(&quot;&#39;compresslevel&#39; must be an integer &quot;</span>
<span class="gi">+                             &quot;between 1 and 9. You provided &#39;compresslevel={}&#39;&quot;</span>
<span class="gi">+                             .format(compresslevel))</span>
<span class="gi">+</span>
<span class="gi">+        if mode == &quot;rb&quot;:</span>
<span class="w"> </span>            self._mode = _MODE_READ
<span class="w"> </span>            self._decompressor = zlib.decompressobj(self.wbits)
<span class="gd">-            self._buffer = b&#39;&#39;</span>
<span class="gi">+            self._buffer = b&quot;&quot;</span>
<span class="w"> </span>            self._buffer_offset = 0
<span class="gd">-        elif mode == &#39;wb&#39;:</span>
<span class="gi">+        elif mode == &quot;wb&quot;:</span>
<span class="w"> </span>            self._mode = _MODE_WRITE
<span class="gd">-            self._compressor = zlib.compressobj(self.compresslevel, zlib.</span>
<span class="gd">-                DEFLATED, self.wbits, zlib.DEF_MEM_LEVEL, 0)</span>
<span class="gi">+            self._compressor = zlib.compressobj(self.compresslevel,</span>
<span class="gi">+                                                zlib.DEFLATED, self.wbits,</span>
<span class="gi">+                                                zlib.DEF_MEM_LEVEL, 0)</span>
<span class="w"> </span>        else:
<span class="gd">-            raise ValueError(&#39;Invalid mode: %r&#39; % (mode,))</span>
<span class="gi">+            raise ValueError(&quot;Invalid mode: %r&quot; % (mode,))</span>
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(filename, str):
<span class="w"> </span>            self._fp = io.open(filename, mode)
<span class="w"> </span>            self._closefp = True
<span class="gd">-        elif hasattr(filename, &#39;read&#39;) or hasattr(filename, &#39;write&#39;):</span>
<span class="gi">+        elif hasattr(filename, &quot;read&quot;) or hasattr(filename, &quot;write&quot;):</span>
<span class="w"> </span>            self._fp = filename
<span class="w"> </span>        else:
<span class="gd">-            raise TypeError(&#39;filename must be a str or bytes object, or a file&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+            raise TypeError(&quot;filename must be a str or bytes object, &quot;</span>
<span class="gi">+                            &quot;or a file&quot;)</span>

<span class="w"> </span>    def close(self):
<span class="w"> </span>        &quot;&quot;&quot;Flush and close the file.
<span class="gu">@@ -210,28 +298,147 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>        May be called more than once without error. Once the file is
<span class="w"> </span>        closed, any other operation on it will raise a ValueError.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            if self._mode == _MODE_CLOSED:</span>
<span class="gi">+                return</span>
<span class="gi">+            try:</span>
<span class="gi">+                if self._mode in (_MODE_READ, _MODE_READ_EOF):</span>
<span class="gi">+                    self._decompressor = None</span>
<span class="gi">+                elif self._mode == _MODE_WRITE:</span>
<span class="gi">+                    self._fp.write(self._compressor.flush())</span>
<span class="gi">+                    self._compressor = None</span>
<span class="gi">+            finally:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    if self._closefp:</span>
<span class="gi">+                        self._fp.close()</span>
<span class="gi">+                finally:</span>
<span class="gi">+                    self._fp = None</span>
<span class="gi">+                    self._closefp = False</span>
<span class="gi">+                    self._mode = _MODE_CLOSED</span>
<span class="gi">+                    self._buffer = b&quot;&quot;</span>
<span class="gi">+                    self._buffer_offset = 0</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def closed(self):
<span class="w"> </span>        &quot;&quot;&quot;True if this file is closed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._mode == _MODE_CLOSED</span>

<span class="w"> </span>    def fileno(self):
<span class="w"> </span>        &quot;&quot;&quot;Return the file descriptor for the underlying file.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_not_closed()</span>
<span class="gi">+        return self._fp.fileno()</span>

<span class="w"> </span>    def seekable(self):
<span class="w"> </span>        &quot;&quot;&quot;Return whether the file supports seeking.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.readable() and self._fp.seekable()</span>

<span class="w"> </span>    def readable(self):
<span class="w"> </span>        &quot;&quot;&quot;Return whether the file was opened for reading.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_not_closed()</span>
<span class="gi">+        return self._mode in (_MODE_READ, _MODE_READ_EOF)</span>

<span class="w"> </span>    def writable(self):
<span class="w"> </span>        &quot;&quot;&quot;Return whether the file was opened for writing.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._check_not_closed()</span>
<span class="gi">+        return self._mode == _MODE_WRITE</span>
<span class="gi">+</span>
<span class="gi">+    # Mode-checking helper functions.</span>
<span class="gi">+</span>
<span class="gi">+    def _check_not_closed(self):</span>
<span class="gi">+        if self.closed:</span>
<span class="gi">+            fname = getattr(self._fp, &#39;name&#39;, None)</span>
<span class="gi">+            msg = &quot;I/O operation on closed file&quot;</span>
<span class="gi">+            if fname is not None:</span>
<span class="gi">+                msg += &quot; {}&quot;.format(fname)</span>
<span class="gi">+            msg += &quot;.&quot;</span>
<span class="gi">+            raise ValueError(msg)</span>
<span class="gi">+</span>
<span class="gi">+    def _check_can_read(self):</span>
<span class="gi">+        if self._mode not in (_MODE_READ, _MODE_READ_EOF):</span>
<span class="gi">+            self._check_not_closed()</span>
<span class="gi">+            raise io.UnsupportedOperation(&quot;File not open for reading&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _check_can_write(self):</span>
<span class="gi">+        if self._mode != _MODE_WRITE:</span>
<span class="gi">+            self._check_not_closed()</span>
<span class="gi">+            raise io.UnsupportedOperation(&quot;File not open for writing&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _check_can_seek(self):</span>
<span class="gi">+        if self._mode not in (_MODE_READ, _MODE_READ_EOF):</span>
<span class="gi">+            self._check_not_closed()</span>
<span class="gi">+            raise io.UnsupportedOperation(&quot;Seeking is only supported &quot;</span>
<span class="gi">+                                          &quot;on files open for reading&quot;)</span>
<span class="gi">+        if not self._fp.seekable():</span>
<span class="gi">+            raise io.UnsupportedOperation(&quot;The underlying file object &quot;</span>
<span class="gi">+                                          &quot;does not support seeking&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # Fill the readahead buffer if it is empty. Returns False on EOF.</span>
<span class="gi">+    def _fill_buffer(self):</span>
<span class="gi">+        if self._mode == _MODE_READ_EOF:</span>
<span class="gi">+            return False</span>
<span class="gi">+        # Depending on the input data, our call to the decompressor may not</span>
<span class="gi">+        # return any data. In this case, try again after reading another block.</span>
<span class="gi">+        while self._buffer_offset == len(self._buffer):</span>
<span class="gi">+            try:</span>
<span class="gi">+                rawblock = (self._decompressor.unused_data or</span>
<span class="gi">+                            self._fp.read(_BUFFER_SIZE))</span>
<span class="gi">+                if not rawblock:</span>
<span class="gi">+                    raise EOFError</span>
<span class="gi">+            except EOFError:</span>
<span class="gi">+                # End-of-stream marker and end of file. We&#39;re good.</span>
<span class="gi">+                self._mode = _MODE_READ_EOF</span>
<span class="gi">+                self._size = self._pos</span>
<span class="gi">+                return False</span>
<span class="gi">+            else:</span>
<span class="gi">+                self._buffer = self._decompressor.decompress(rawblock)</span>
<span class="gi">+            self._buffer_offset = 0</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    # Read data until EOF.</span>
<span class="gi">+    # If return_data is false, consume the data without returning it.</span>
<span class="gi">+    def _read_all(self, return_data=True):</span>
<span class="gi">+        # The loop assumes that _buffer_offset is 0. Ensure that this is true.</span>
<span class="gi">+        self._buffer = self._buffer[self._buffer_offset:]</span>
<span class="gi">+        self._buffer_offset = 0</span>
<span class="gi">+</span>
<span class="gi">+        blocks = []</span>
<span class="gi">+        while self._fill_buffer():</span>
<span class="gi">+            if return_data:</span>
<span class="gi">+                blocks.append(self._buffer)</span>
<span class="gi">+            self._pos += len(self._buffer)</span>
<span class="gi">+            self._buffer = b&quot;&quot;</span>
<span class="gi">+        if return_data:</span>
<span class="gi">+            return b&quot;&quot;.join(blocks)</span>
<span class="gi">+</span>
<span class="gi">+    # Read a block of up to n bytes.</span>
<span class="gi">+    # If return_data is false, consume the data without returning it.</span>
<span class="gi">+    def _read_block(self, n_bytes, return_data=True):</span>
<span class="gi">+        # If we have enough data buffered, return immediately.</span>
<span class="gi">+        end = self._buffer_offset + n_bytes</span>
<span class="gi">+        if end &lt;= len(self._buffer):</span>
<span class="gi">+            data = self._buffer[self._buffer_offset: end]</span>
<span class="gi">+            self._buffer_offset = end</span>
<span class="gi">+            self._pos += len(data)</span>
<span class="gi">+            return data if return_data else None</span>
<span class="gi">+</span>
<span class="gi">+        # The loop assumes that _buffer_offset is 0. Ensure that this is true.</span>
<span class="gi">+        self._buffer = self._buffer[self._buffer_offset:]</span>
<span class="gi">+        self._buffer_offset = 0</span>
<span class="gi">+</span>
<span class="gi">+        blocks = []</span>
<span class="gi">+        while n_bytes &gt; 0 and self._fill_buffer():</span>
<span class="gi">+            if n_bytes &lt; len(self._buffer):</span>
<span class="gi">+                data = self._buffer[:n_bytes]</span>
<span class="gi">+                self._buffer_offset = n_bytes</span>
<span class="gi">+            else:</span>
<span class="gi">+                data = self._buffer</span>
<span class="gi">+                self._buffer = b&quot;&quot;</span>
<span class="gi">+            if return_data:</span>
<span class="gi">+                blocks.append(data)</span>
<span class="gi">+            self._pos += len(data)</span>
<span class="gi">+            n_bytes -= len(data)</span>
<span class="gi">+        if return_data:</span>
<span class="gi">+            return b&quot;&quot;.join(blocks)</span>

<span class="w"> </span>    def read(self, size=-1):
<span class="w"> </span>        &quot;&quot;&quot;Read up to size uncompressed bytes from the file.
<span class="gu">@@ -239,14 +446,22 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>        If size is negative or omitted, read until EOF is reached.
<span class="w"> </span>        Returns b&#39;&#39; if the file is already at EOF.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            self._check_can_read()</span>
<span class="gi">+            if size == 0:</span>
<span class="gi">+                return b&quot;&quot;</span>
<span class="gi">+            elif size &lt; 0:</span>
<span class="gi">+                return self._read_all()</span>
<span class="gi">+            else:</span>
<span class="gi">+                return self._read_block(size)</span>

<span class="w"> </span>    def readinto(self, b):
<span class="w"> </span>        &quot;&quot;&quot;Read up to len(b) bytes into b.

<span class="w"> </span>        Returns the number of bytes read (0 for EOF).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            return io.BufferedIOBase.readinto(self, b)</span>

<span class="w"> </span>    def write(self, data):
<span class="w"> </span>        &quot;&quot;&quot;Write a byte string to the file.
<span class="gu">@@ -255,7 +470,25 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>        always len(data). Note that due to buffering, the file on disk
<span class="w"> </span>        may not reflect the data written until close() is called.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            self._check_can_write()</span>
<span class="gi">+            # Convert data type if called by io.BufferedWriter.</span>
<span class="gi">+            if isinstance(data, memoryview):</span>
<span class="gi">+                data = data.tobytes()</span>
<span class="gi">+</span>
<span class="gi">+            compressed = self._compressor.compress(data)</span>
<span class="gi">+            self._fp.write(compressed)</span>
<span class="gi">+            self._pos += len(data)</span>
<span class="gi">+            return len(data)</span>
<span class="gi">+</span>
<span class="gi">+    # Rewind the file to the beginning of the data stream.</span>
<span class="gi">+    def _rewind(self):</span>
<span class="gi">+        self._fp.seek(0, 0)</span>
<span class="gi">+        self._mode = _MODE_READ</span>
<span class="gi">+        self._pos = 0</span>
<span class="gi">+        self._decompressor = zlib.decompressobj(self.wbits)</span>
<span class="gi">+        self._buffer = b&quot;&quot;</span>
<span class="gi">+        self._buffer_offset = 0</span>

<span class="w"> </span>    def seek(self, offset, whence=0):
<span class="w"> </span>        &quot;&quot;&quot;Change the file position.
<span class="gu">@@ -272,18 +505,45 @@ class BinaryZlibFile(io.BufferedIOBase):</span>
<span class="w"> </span>        Note that seeking is emulated, so depending on the parameters,
<span class="w"> </span>        this operation may be extremely slow.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            self._check_can_seek()</span>
<span class="gi">+</span>
<span class="gi">+            # Recalculate offset as an absolute file position.</span>
<span class="gi">+            if whence == 0:</span>
<span class="gi">+                pass</span>
<span class="gi">+            elif whence == 1:</span>
<span class="gi">+                offset = self._pos + offset</span>
<span class="gi">+            elif whence == 2:</span>
<span class="gi">+                # Seeking relative to EOF - we need to know the file&#39;s size.</span>
<span class="gi">+                if self._size &lt; 0:</span>
<span class="gi">+                    self._read_all(return_data=False)</span>
<span class="gi">+                offset = self._size + offset</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(&quot;Invalid value for whence: %s&quot; % (whence,))</span>
<span class="gi">+</span>
<span class="gi">+            # Make it so that offset is the number of bytes to skip forward.</span>
<span class="gi">+            if offset &lt; self._pos:</span>
<span class="gi">+                self._rewind()</span>
<span class="gi">+            else:</span>
<span class="gi">+                offset -= self._pos</span>
<span class="gi">+</span>
<span class="gi">+            # Read and discard data until we reach the desired position.</span>
<span class="gi">+            self._read_block(offset, return_data=False)</span>
<span class="gi">+</span>
<span class="gi">+            return self._pos</span>

<span class="w"> </span>    def tell(self):
<span class="w"> </span>        &quot;&quot;&quot;Return the current file position.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            self._check_not_closed()</span>
<span class="gi">+            return self._pos</span>


<span class="w"> </span>class ZlibCompressorWrapper(CompressorWrapper):

<span class="w"> </span>    def __init__(self):
<span class="gd">-        CompressorWrapper.__init__(self, obj=BinaryZlibFile, prefix=</span>
<span class="gd">-            _ZLIB_PREFIX, extension=&#39;.z&#39;)</span>
<span class="gi">+        CompressorWrapper.__init__(self, obj=BinaryZlibFile,</span>
<span class="gi">+                                   prefix=_ZLIB_PREFIX, extension=&#39;.z&#39;)</span>


<span class="w"> </span>class BinaryGzipFile(BinaryZlibFile):
<span class="gu">@@ -299,11 +559,12 @@ class BinaryGzipFile(BinaryZlibFile):</span>
<span class="w"> </span>    and 9 specifying the level of compression: 1 produces the least
<span class="w"> </span>    compression, and 9 produces the most compression. 3 is the default.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    wbits = 31</span>
<span class="gi">+</span>
<span class="gi">+    wbits = 31  # zlib compressor/decompressor wbits value for gzip format.</span>


<span class="w"> </span>class GzipCompressorWrapper(CompressorWrapper):

<span class="w"> </span>    def __init__(self):
<span class="gd">-        CompressorWrapper.__init__(self, obj=BinaryGzipFile, prefix=</span>
<span class="gd">-            _GZIP_PREFIX, extension=&#39;.gz&#39;)</span>
<span class="gi">+        CompressorWrapper.__init__(self, obj=BinaryGzipFile,</span>
<span class="gi">+                                   prefix=_GZIP_PREFIX, extension=&#39;.gz&#39;)</span>
<span class="gh">diff --git a/joblib/disk.py b/joblib/disk.py</span>
<span class="gh">index b35e507..32fbb89 100644</span>
<span class="gd">--- a/joblib/disk.py</span>
<span class="gi">+++ b/joblib/disk.py</span>
<span class="gu">@@ -1,12 +1,22 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Disk management utilities.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+#          Lars Buitinck</span>
<span class="gi">+# Copyright (c) 2010 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import time
<span class="w"> </span>import errno
<span class="w"> </span>import shutil
<span class="gi">+</span>
<span class="w"> </span>from multiprocessing import util
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    WindowsError
<span class="w"> </span>except NameError:
<span class="gu">@@ -15,22 +25,49 @@ except NameError:</span>

<span class="w"> </span>def disk_used(path):
<span class="w"> </span>    &quot;&quot;&quot; Return the disk usage in a directory.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    size = 0</span>
<span class="gi">+    for file in os.listdir(path) + [&#39;.&#39;]:</span>
<span class="gi">+        stat = os.stat(os.path.join(path, file))</span>
<span class="gi">+        if hasattr(stat, &#39;st_blocks&#39;):</span>
<span class="gi">+            size += stat.st_blocks * 512</span>
<span class="gi">+        else:</span>
<span class="gi">+            # on some platform st_blocks is not available (e.g., Windows)</span>
<span class="gi">+            # approximate by rounding to next multiple of 512</span>
<span class="gi">+            size += (stat.st_size // 512 + 1) * 512</span>
<span class="gi">+    # We need to convert to int to avoid having longs on some systems (we</span>
<span class="gi">+    # don&#39;t want longs to avoid problems we SQLite)</span>
<span class="gi">+    return int(size / 1024.)</span>


<span class="w"> </span>def memstr_to_bytes(text):
<span class="w"> </span>    &quot;&quot;&quot; Convert a memory text to its value in bytes.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    kilo = 1024</span>
<span class="gi">+    units = dict(K=kilo, M=kilo ** 2, G=kilo ** 3)</span>
<span class="gi">+    try:</span>
<span class="gi">+        size = int(units[text[-1]] * float(text[:-1]))</span>
<span class="gi">+    except (KeyError, ValueError) as e:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;Invalid literal for size give: %s (type %s) should be &quot;</span>
<span class="gi">+            &quot;alike &#39;10G&#39;, &#39;500M&#39;, &#39;50K&#39;.&quot; % (text, type(text))) from e</span>
<span class="gi">+    return size</span>


<span class="w"> </span>def mkdirp(d):
<span class="w"> </span>    &quot;&quot;&quot;Ensure directory d exists (like mkdir -p on Unix)
<span class="w"> </span>    No guarantee that the directory is writable.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        os.makedirs(d)</span>
<span class="gi">+    except OSError as e:</span>
<span class="gi">+        if e.errno != errno.EEXIST:</span>
<span class="gi">+            raise</span>


<span class="gi">+# if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),</span>
<span class="gi">+# then retry up to RM_SUBDIRS_N_RETRY times. If it still fails, raise the</span>
<span class="gi">+# exception. this mechanism ensures that the sub-process gc have the time to</span>
<span class="gi">+# collect and close the memmaps before we fail.</span>
<span class="w"> </span>RM_SUBDIRS_RETRY_TIME = 0.1
<span class="w"> </span>RM_SUBDIRS_N_RETRY = 10

<span class="gu">@@ -47,9 +84,53 @@ def rm_subdirs(path, onerror=None):</span>
<span class="w"> </span>    exc_info is a tuple returned by sys.exc_info().  If onerror is None,
<span class="w"> </span>    an exception is raised.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    # NOTE this code is adapted from the one in shutil.rmtree, and is</span>
<span class="gi">+    # just as fast</span>
<span class="gi">+</span>
<span class="gi">+    names = []</span>
<span class="gi">+    try:</span>
<span class="gi">+        names = os.listdir(path)</span>
<span class="gi">+    except os.error:</span>
<span class="gi">+        if onerror is not None:</span>
<span class="gi">+            onerror(os.listdir, path, sys.exc_info())</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise</span>
<span class="gi">+</span>
<span class="gi">+    for name in names:</span>
<span class="gi">+        fullname = os.path.join(path, name)</span>
<span class="gi">+        delete_folder(fullname, onerror=onerror)</span>


<span class="w"> </span>def delete_folder(folder_path, onerror=None, allow_non_empty=True):
<span class="w"> </span>    &quot;&quot;&quot;Utility function to cleanup a temporary folder if it still exists.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if os.path.isdir(folder_path):</span>
<span class="gi">+        if onerror is not None:</span>
<span class="gi">+            shutil.rmtree(folder_path, False, onerror)</span>
<span class="gi">+        else:</span>
<span class="gi">+            # allow the rmtree to fail once, wait and re-try.</span>
<span class="gi">+            # if the error is raised again, fail</span>
<span class="gi">+            err_count = 0</span>
<span class="gi">+            while True:</span>
<span class="gi">+                files = os.listdir(folder_path)</span>
<span class="gi">+                try:</span>
<span class="gi">+                    if len(files) == 0 or allow_non_empty:</span>
<span class="gi">+                        shutil.rmtree(</span>
<span class="gi">+                            folder_path, ignore_errors=False, onerror=None</span>
<span class="gi">+                        )</span>
<span class="gi">+                        util.debug(</span>
<span class="gi">+                            &quot;Successfully deleted {}&quot;.format(folder_path))</span>
<span class="gi">+                        break</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        raise OSError(</span>
<span class="gi">+                            &quot;Expected empty folder {} but got {} &quot;</span>
<span class="gi">+                            &quot;files.&quot;.format(folder_path, len(files))</span>
<span class="gi">+                        )</span>
<span class="gi">+                except (OSError, WindowsError):</span>
<span class="gi">+                    err_count += 1</span>
<span class="gi">+                    if err_count &gt; RM_SUBDIRS_N_RETRY:</span>
<span class="gi">+                        # the folder cannot be deleted right now. It maybe</span>
<span class="gi">+                        # because some temporary files have not been deleted</span>
<span class="gi">+                        # yet.</span>
<span class="gi">+                        raise</span>
<span class="gi">+                time.sleep(RM_SUBDIRS_RETRY_TIME)</span>
<span class="gh">diff --git a/joblib/executor.py b/joblib/executor.py</span>
<span class="gh">index 6eea29c..6837a7d 100644</span>
<span class="gd">--- a/joblib/executor.py</span>
<span class="gi">+++ b/joblib/executor.py</span>
<span class="gu">@@ -4,22 +4,102 @@ This module provides efficient ways of working with data stored in</span>
<span class="w"> </span>shared memory with numpy.memmap arrays without inducing any memory
<span class="w"> </span>copy between the parent and child processes.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Author: Thomas Moreau &lt;thomas.moreau.2010@gmail.com&gt;</span>
<span class="gi">+# Copyright: 2017, Thomas Moreau</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>from ._memmapping_reducer import get_memmapping_reducers
<span class="w"> </span>from ._memmapping_reducer import TemporaryResourcesManager
<span class="w"> </span>from .externals.loky.reusable_executor import _ReusablePoolExecutor
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>_executor_args = None


<span class="gi">+def get_memmapping_executor(n_jobs, **kwargs):</span>
<span class="gi">+    return MemmappingExecutor.get_memmapping_executor(n_jobs, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class MemmappingExecutor(_ReusablePoolExecutor):

<span class="w"> </span>    @classmethod
<span class="w"> </span>    def get_memmapping_executor(cls, n_jobs, timeout=300, initializer=None,
<span class="gd">-        initargs=(), env=None, temp_folder=None, context_id=None, **</span>
<span class="gd">-        backend_args):</span>
<span class="gi">+                                initargs=(), env=None, temp_folder=None,</span>
<span class="gi">+                                context_id=None, **backend_args):</span>
<span class="w"> </span>        &quot;&quot;&quot;Factory for ReusableExecutor with automatic memmapping for large
<span class="w"> </span>        numpy arrays.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        global _executor_args</span>
<span class="gi">+        # Check if we can reuse the executor here instead of deferring the test</span>
<span class="gi">+        # to loky as the reducers are objects that changes at each call.</span>
<span class="gi">+        executor_args = backend_args.copy()</span>
<span class="gi">+        executor_args.update(env if env else {})</span>
<span class="gi">+        executor_args.update(dict(</span>
<span class="gi">+            timeout=timeout, initializer=initializer, initargs=initargs))</span>
<span class="gi">+        reuse = _executor_args is None or _executor_args == executor_args</span>
<span class="gi">+        _executor_args = executor_args</span>
<span class="gi">+</span>
<span class="gi">+        manager = TemporaryResourcesManager(temp_folder)</span>
<span class="gi">+</span>
<span class="gi">+        # reducers access the temporary folder in which to store temporary</span>
<span class="gi">+        # pickles through a call to manager.resolve_temp_folder_name. resolving</span>
<span class="gi">+        # the folder name dynamically is useful to use different folders across</span>
<span class="gi">+        # calls of a same reusable executor</span>
<span class="gi">+        job_reducers, result_reducers = get_memmapping_reducers(</span>
<span class="gi">+            unlink_on_gc_collect=True,</span>
<span class="gi">+            temp_folder_resolver=manager.resolve_temp_folder_name,</span>
<span class="gi">+            **backend_args)</span>
<span class="gi">+        _executor, executor_is_reused = super().get_reusable_executor(</span>
<span class="gi">+            n_jobs, job_reducers=job_reducers, result_reducers=result_reducers,</span>
<span class="gi">+            reuse=reuse, timeout=timeout, initializer=initializer,</span>
<span class="gi">+            initargs=initargs, env=env</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        if not executor_is_reused:</span>
<span class="gi">+            # Only set a _temp_folder_manager for new executors. Reused</span>
<span class="gi">+            # executors already have a _temporary_folder_manager that must not</span>
<span class="gi">+            # be re-assigned like that because it is referenced in various</span>
<span class="gi">+            # places in the reducing machinery of the executor.</span>
<span class="gi">+            _executor._temp_folder_manager = manager</span>
<span class="gi">+</span>
<span class="gi">+        if context_id is not None:</span>
<span class="gi">+            # Only register the specified context once we know which manager</span>
<span class="gi">+            # the current executor is using, in order to not register an atexit</span>
<span class="gi">+            # finalizer twice for the same folder.</span>
<span class="gi">+            _executor._temp_folder_manager.register_new_context(context_id)</span>
<span class="gi">+</span>
<span class="gi">+        return _executor</span>
<span class="gi">+</span>
<span class="gi">+    def terminate(self, kill_workers=False):</span>
<span class="gi">+</span>
<span class="gi">+        self.shutdown(kill_workers=kill_workers)</span>
<span class="gi">+</span>
<span class="gi">+        # When workers are killed in a brutal manner, they cannot execute the</span>
<span class="gi">+        # finalizer of their shared memmaps. The refcount of those memmaps may</span>
<span class="gi">+        # be off by an unknown number, so instead of decref&#39;ing them, we force</span>
<span class="gi">+        # delete the whole temporary folder, and unregister them. There is no</span>
<span class="gi">+        # risk of PermissionError at folder deletion because at this</span>
<span class="gi">+        # point, all child processes are dead, so all references to temporary</span>
<span class="gi">+        # memmaps are closed. Otherwise, just try to delete as much as possible</span>
<span class="gi">+        # with allow_non_empty=True but if we can&#39;t, it will be clean up later</span>
<span class="gi">+        # on by the resource_tracker.</span>
<span class="gi">+        with self._submit_resize_lock:</span>
<span class="gi">+            self._temp_folder_manager._clean_temporary_resources(</span>
<span class="gi">+                force=kill_workers, allow_non_empty=True</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def _temp_folder(self):</span>
<span class="gi">+        # Legacy property in tests. could be removed if we refactored the</span>
<span class="gi">+        # memmapping tests. SHOULD ONLY BE USED IN TESTS!</span>
<span class="gi">+        # We cache this property because it is called late in the tests - at</span>
<span class="gi">+        # this point, all context have been unregistered, and</span>
<span class="gi">+        # resolve_temp_folder_name raises an error.</span>
<span class="gi">+        if getattr(self, &#39;_cached_temp_folder&#39;, None) is not None:</span>
<span class="gi">+            return self._cached_temp_folder</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._cached_temp_folder = self._temp_folder_manager.resolve_temp_folder_name()  # noqa</span>
<span class="gi">+            return self._cached_temp_folder</span>


<span class="w"> </span>class _TestingMemmappingExecutor(MemmappingExecutor):
<span class="gu">@@ -27,7 +107,11 @@ class _TestingMemmappingExecutor(MemmappingExecutor):</span>
<span class="w"> </span>    and Executor. This is only for testing purposes.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-</span>
<span class="w"> </span>    def apply_async(self, func, args):
<span class="w"> </span>        &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        future = self.submit(func, *args)</span>
<span class="gi">+        future.get = future.result</span>
<span class="gi">+        return future</span>
<span class="gi">+</span>
<span class="gi">+    def map(self, f, *args):</span>
<span class="gi">+        return list(super().map(f, *args))</span>
<span class="gh">diff --git a/joblib/externals/cloudpickle/cloudpickle.py b/joblib/externals/cloudpickle/cloudpickle.py</span>
<span class="gh">index 92fb769..eb43a96 100644</span>
<span class="gd">--- a/joblib/externals/cloudpickle/cloudpickle.py</span>
<span class="gi">+++ b/joblib/externals/cloudpickle/cloudpickle.py</span>
<span class="gu">@@ -49,6 +49,7 @@ LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING</span>
<span class="w"> </span>NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
<span class="w"> </span>SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import _collections_abc
<span class="w"> </span>from collections import ChainMap, OrderedDict
<span class="w"> </span>import abc
<span class="gu">@@ -72,19 +73,58 @@ import typing</span>
<span class="w"> </span>import uuid
<span class="w"> </span>import warnings
<span class="w"> </span>import weakref
<span class="gd">-from types import CellType</span>
<span class="gi">+</span>
<span class="gi">+# The following import is required to be imported in the cloudpickle</span>
<span class="gi">+# namespace to be able to load pickle files generated with older versions of</span>
<span class="gi">+# cloudpickle. See: tests/test_backward_compat.py</span>
<span class="gi">+from types import CellType  # noqa: F401</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# cloudpickle is meant for inter process communication: we expect all</span>
<span class="gi">+# communicating processes to run the same Python version hence we favor</span>
<span class="gi">+# communication speed over compatibility:</span>
<span class="w"> </span>DEFAULT_PROTOCOL = pickle.HIGHEST_PROTOCOL
<span class="gi">+</span>
<span class="gi">+# Names of modules whose resources should be treated as dynamic.</span>
<span class="w"> </span>_PICKLE_BY_VALUE_MODULES = set()
<span class="gi">+</span>
<span class="gi">+# Track the provenance of reconstructed dynamic classes to make it possible to</span>
<span class="gi">+# reconstruct instances from the matching singleton class definition when</span>
<span class="gi">+# appropriate and preserve the usual &quot;isinstance&quot; semantics of Python objects.</span>
<span class="w"> </span>_DYNAMIC_CLASS_TRACKER_BY_CLASS = weakref.WeakKeyDictionary()
<span class="w"> </span>_DYNAMIC_CLASS_TRACKER_BY_ID = weakref.WeakValueDictionary()
<span class="w"> </span>_DYNAMIC_CLASS_TRACKER_LOCK = threading.Lock()
<span class="gd">-PYPY = platform.python_implementation() == &#39;PyPy&#39;</span>
<span class="gi">+</span>
<span class="gi">+PYPY = platform.python_implementation() == &quot;PyPy&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>builtin_code_type = None
<span class="w"> </span>if PYPY:
<span class="gi">+    # builtin-code objects only exist in pypy</span>
<span class="w"> </span>    builtin_code_type = type(float.__new__.__code__)
<span class="gi">+</span>
<span class="w"> </span>_extract_code_globals_cache = weakref.WeakKeyDictionary()


<span class="gi">+def _get_or_create_tracker_id(class_def):</span>
<span class="gi">+    with _DYNAMIC_CLASS_TRACKER_LOCK:</span>
<span class="gi">+        class_tracker_id = _DYNAMIC_CLASS_TRACKER_BY_CLASS.get(class_def)</span>
<span class="gi">+        if class_tracker_id is None:</span>
<span class="gi">+            class_tracker_id = uuid.uuid4().hex</span>
<span class="gi">+            _DYNAMIC_CLASS_TRACKER_BY_CLASS[class_def] = class_tracker_id</span>
<span class="gi">+            _DYNAMIC_CLASS_TRACKER_BY_ID[class_tracker_id] = class_def</span>
<span class="gi">+    return class_tracker_id</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _lookup_class_or_track(class_tracker_id, class_def):</span>
<span class="gi">+    if class_tracker_id is not None:</span>
<span class="gi">+        with _DYNAMIC_CLASS_TRACKER_LOCK:</span>
<span class="gi">+            class_def = _DYNAMIC_CLASS_TRACKER_BY_ID.setdefault(</span>
<span class="gi">+                class_tracker_id, class_def</span>
<span class="gi">+            )</span>
<span class="gi">+            _DYNAMIC_CLASS_TRACKER_BY_CLASS[class_def] = class_tracker_id</span>
<span class="gi">+    return class_def</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def register_pickle_by_value(module):
<span class="w"> </span>    &quot;&quot;&quot;Register a module to make it functions and classes picklable by value.

<span class="gu">@@ -104,12 +144,52 @@ def register_pickle_by_value(module):</span>
<span class="w"> </span>    Note: this feature is considered experimental. See the cloudpickle
<span class="w"> </span>    README.md file for more details and limitations.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not isinstance(module, types.ModuleType):</span>
<span class="gi">+        raise ValueError(f&quot;Input should be a module object, got {str(module)} instead&quot;)</span>
<span class="gi">+    # In the future, cloudpickle may need a way to access any module registered</span>
<span class="gi">+    # for pickling by value in order to introspect relative imports inside</span>
<span class="gi">+    # functions pickled by value. (see</span>
<span class="gi">+    # https://github.com/cloudpipe/cloudpickle/pull/417#issuecomment-873684633).</span>
<span class="gi">+    # This access can be ensured by checking that module is present in</span>
<span class="gi">+    # sys.modules at registering time and assuming that it will still be in</span>
<span class="gi">+    # there when accessed during pickling. Another alternative would be to</span>
<span class="gi">+    # store a weakref to the module. Even though cloudpickle does not implement</span>
<span class="gi">+    # this introspection yet, in order to avoid a possible breaking change</span>
<span class="gi">+    # later, we still enforce the presence of module inside sys.modules.</span>
<span class="gi">+    if module.__name__ not in sys.modules:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;{module} was not imported correctly, have you used an &quot;</span>
<span class="gi">+            &quot;`import` statement to access it?&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    _PICKLE_BY_VALUE_MODULES.add(module.__name__)</span>


<span class="w"> </span>def unregister_pickle_by_value(module):
<span class="w"> </span>    &quot;&quot;&quot;Unregister that the input module should be pickled by value.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not isinstance(module, types.ModuleType):</span>
<span class="gi">+        raise ValueError(f&quot;Input should be a module object, got {str(module)} instead&quot;)</span>
<span class="gi">+    if module.__name__ not in _PICKLE_BY_VALUE_MODULES:</span>
<span class="gi">+        raise ValueError(f&quot;{module} is not registered for pickle by value&quot;)</span>
<span class="gi">+    else:</span>
<span class="gi">+        _PICKLE_BY_VALUE_MODULES.remove(module.__name__)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def list_registry_pickle_by_value():</span>
<span class="gi">+    return _PICKLE_BY_VALUE_MODULES.copy()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _is_registered_pickle_by_value(module):</span>
<span class="gi">+    module_name = module.__name__</span>
<span class="gi">+    if module_name in _PICKLE_BY_VALUE_MODULES:</span>
<span class="gi">+        return True</span>
<span class="gi">+    while True:</span>
<span class="gi">+        parent_name = module_name.rsplit(&quot;.&quot;, 1)[0]</span>
<span class="gi">+        if parent_name == module_name:</span>
<span class="gi">+            break</span>
<span class="gi">+        if parent_name in _PICKLE_BY_VALUE_MODULES:</span>
<span class="gi">+            return True</span>
<span class="gi">+        module_name = parent_name</span>
<span class="gi">+    return False</span>


<span class="w"> </span>def _whichmodule(obj, name):
<span class="gu">@@ -121,7 +201,28 @@ def _whichmodule(obj, name):</span>
<span class="w"> </span>    - Errors arising during module introspection are ignored, as those errors
<span class="w"> </span>      are considered unwanted side effects.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    module_name = getattr(obj, &quot;__module__&quot;, None)</span>
<span class="gi">+</span>
<span class="gi">+    if module_name is not None:</span>
<span class="gi">+        return module_name</span>
<span class="gi">+    # Protect the iteration by using a copy of sys.modules against dynamic</span>
<span class="gi">+    # modules that trigger imports of other modules upon calls to getattr or</span>
<span class="gi">+    # other threads importing at the same time.</span>
<span class="gi">+    for module_name, module in sys.modules.copy().items():</span>
<span class="gi">+        # Some modules such as coverage can inject non-module objects inside</span>
<span class="gi">+        # sys.modules</span>
<span class="gi">+        if (</span>
<span class="gi">+            module_name == &quot;__main__&quot;</span>
<span class="gi">+            or module is None</span>
<span class="gi">+            or not isinstance(module, types.ModuleType)</span>
<span class="gi">+        ):</span>
<span class="gi">+            continue</span>
<span class="gi">+        try:</span>
<span class="gi">+            if _getattribute(module, name)[0] is obj:</span>
<span class="gi">+                return module_name</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            pass</span>
<span class="gi">+    return None</span>


<span class="w"> </span>def _should_pickle_by_reference(obj, name=None):
<span class="gu">@@ -138,12 +239,91 @@ def _should_pickle_by_reference(obj, name=None):</span>
<span class="w"> </span>    functions and classes or for attributes of modules that have been
<span class="w"> </span>    explicitly registered to be pickled by value.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(obj, types.FunctionType) or issubclass(type(obj), type):</span>
<span class="gi">+        module_and_name = _lookup_module_and_qualname(obj, name=name)</span>
<span class="gi">+        if module_and_name is None:</span>
<span class="gi">+            return False</span>
<span class="gi">+        module, name = module_and_name</span>
<span class="gi">+        return not _is_registered_pickle_by_value(module)</span>
<span class="gi">+</span>
<span class="gi">+    elif isinstance(obj, types.ModuleType):</span>
<span class="gi">+        # We assume that sys.modules is primarily used as a cache mechanism for</span>
<span class="gi">+        # the Python import machinery. Checking if a module has been added in</span>
<span class="gi">+        # is sys.modules therefore a cheap and simple heuristic to tell us</span>
<span class="gi">+        # whether we can assume that a given module could be imported by name</span>
<span class="gi">+        # in another Python process.</span>
<span class="gi">+        if _is_registered_pickle_by_value(obj):</span>
<span class="gi">+            return False</span>
<span class="gi">+        return obj.__name__ in sys.modules</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            &quot;cannot check importability of {} instances&quot;.format(type(obj).__name__)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _lookup_module_and_qualname(obj, name=None):</span>
<span class="gi">+    if name is None:</span>
<span class="gi">+        name = getattr(obj, &quot;__qualname__&quot;, None)</span>
<span class="gi">+    if name is None:  # pragma: no cover</span>
<span class="gi">+        # This used to be needed for Python 2.7 support but is probably not</span>
<span class="gi">+        # needed anymore. However we keep the __name__ introspection in case</span>
<span class="gi">+        # users of cloudpickle rely on this old behavior for unknown reasons.</span>
<span class="gi">+        name = getattr(obj, &quot;__name__&quot;, None)</span>
<span class="gi">+</span>
<span class="gi">+    module_name = _whichmodule(obj, name)</span>
<span class="gi">+</span>
<span class="gi">+    if module_name is None:</span>
<span class="gi">+        # In this case, obj.__module__ is None AND obj was not found in any</span>
<span class="gi">+        # imported module. obj is thus treated as dynamic.</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    if module_name == &quot;__main__&quot;:</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    # Note: if module_name is in sys.modules, the corresponding module is</span>
<span class="gi">+    # assumed importable at unpickling time. See #357</span>
<span class="gi">+    module = sys.modules.get(module_name, None)</span>
<span class="gi">+    if module is None:</span>
<span class="gi">+        # The main reason why obj&#39;s module would not be imported is that this</span>
<span class="gi">+        # module has been dynamically created, using for example</span>
<span class="gi">+        # types.ModuleType. The other possibility is that module was removed</span>
<span class="gi">+        # from sys.modules after obj was created/imported. But this case is not</span>
<span class="gi">+        # supported, as the standard pickle does not support it either.</span>
<span class="gi">+        return None</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        obj2, parent = _getattribute(module, name)</span>
<span class="gi">+    except AttributeError:</span>
<span class="gi">+        # obj was not found inside the module it points to</span>
<span class="gi">+        return None</span>
<span class="gi">+    if obj2 is not obj:</span>
<span class="gi">+        return None</span>
<span class="gi">+    return module, name</span>


<span class="w"> </span>def _extract_code_globals(co):
<span class="w"> </span>    &quot;&quot;&quot;Find all globals names read or written to by codeblock co.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    out_names = _extract_code_globals_cache.get(co)</span>
<span class="gi">+    if out_names is None:</span>
<span class="gi">+        # We use a dict with None values instead of a set to get a</span>
<span class="gi">+        # deterministic order and avoid introducing non-deterministic pickle</span>
<span class="gi">+        # bytes as a results.</span>
<span class="gi">+        out_names = {name: None for name in _walk_global_ops(co)}</span>
<span class="gi">+</span>
<span class="gi">+        # Declaring a function inside another one using the &quot;def ...&quot; syntax</span>
<span class="gi">+        # generates a constant code object corresponding to the one of the</span>
<span class="gi">+        # nested function&#39;s As the nested function may itself need global</span>
<span class="gi">+        # variables, we need to introspect its code, extract its globals, (look</span>
<span class="gi">+        # for code object in it&#39;s co_consts attribute..) and add the result to</span>
<span class="gi">+        # code_globals</span>
<span class="gi">+        if co.co_consts:</span>
<span class="gi">+            for const in co.co_consts:</span>
<span class="gi">+                if isinstance(const, types.CodeType):</span>
<span class="gi">+                    out_names.update(_extract_code_globals(const))</span>
<span class="gi">+</span>
<span class="gi">+        _extract_code_globals_cache[co] = out_names</span>
<span class="gi">+</span>
<span class="gi">+    return out_names</span>


<span class="w"> </span>def _find_imported_submodules(code, top_level_dependencies):
<span class="gu">@@ -171,29 +351,82 @@ def _find_imported_submodules(code, top_level_dependencies):</span>
<span class="w"> </span>    that calling func once depickled does not fail due to concurrent.futures
<span class="w"> </span>    not being imported
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-STORE_GLOBAL = opcode.opmap[&#39;STORE_GLOBAL&#39;]</span>
<span class="gd">-DELETE_GLOBAL = opcode.opmap[&#39;DELETE_GLOBAL&#39;]</span>
<span class="gd">-LOAD_GLOBAL = opcode.opmap[&#39;LOAD_GLOBAL&#39;]</span>
<span class="gd">-GLOBAL_OPS = STORE_GLOBAL, DELETE_GLOBAL, LOAD_GLOBAL</span>
<span class="gi">+    subimports = []</span>
<span class="gi">+    # check if any known dependency is an imported package</span>
<span class="gi">+    for x in top_level_dependencies:</span>
<span class="gi">+        if (</span>
<span class="gi">+            isinstance(x, types.ModuleType)</span>
<span class="gi">+            and hasattr(x, &quot;__package__&quot;)</span>
<span class="gi">+            and x.__package__</span>
<span class="gi">+        ):</span>
<span class="gi">+            # check if the package has any currently loaded sub-imports</span>
<span class="gi">+            prefix = x.__name__ + &quot;.&quot;</span>
<span class="gi">+            # A concurrent thread could mutate sys.modules,</span>
<span class="gi">+            # make sure we iterate over a copy to avoid exceptions</span>
<span class="gi">+            for name in list(sys.modules):</span>
<span class="gi">+                # Older versions of pytest will add a &quot;None&quot; module to</span>
<span class="gi">+                # sys.modules.</span>
<span class="gi">+                if name is not None and name.startswith(prefix):</span>
<span class="gi">+                    # check whether the function can address the sub-module</span>
<span class="gi">+                    tokens = set(name[len(prefix) :].split(&quot;.&quot;))</span>
<span class="gi">+                    if not tokens - set(code.co_names):</span>
<span class="gi">+                        subimports.append(sys.modules[name])</span>
<span class="gi">+    return subimports</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# relevant opcodes</span>
<span class="gi">+STORE_GLOBAL = opcode.opmap[&quot;STORE_GLOBAL&quot;]</span>
<span class="gi">+DELETE_GLOBAL = opcode.opmap[&quot;DELETE_GLOBAL&quot;]</span>
<span class="gi">+LOAD_GLOBAL = opcode.opmap[&quot;LOAD_GLOBAL&quot;]</span>
<span class="gi">+GLOBAL_OPS = (STORE_GLOBAL, DELETE_GLOBAL, LOAD_GLOBAL)</span>
<span class="w"> </span>HAVE_ARGUMENT = dis.HAVE_ARGUMENT
<span class="w"> </span>EXTENDED_ARG = dis.EXTENDED_ARG
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>_BUILTIN_TYPE_NAMES = {}
<span class="w"> </span>for k, v in types.__dict__.items():
<span class="w"> </span>    if type(v) is type:
<span class="w"> </span>        _BUILTIN_TYPE_NAMES[v] = k


<span class="gi">+def _builtin_type(name):</span>
<span class="gi">+    if name == &quot;ClassType&quot;:  # pragma: no cover</span>
<span class="gi">+        # Backward compat to load pickle files generated with cloudpickle</span>
<span class="gi">+        # &lt; 1.3 even if loading pickle files from older versions is not</span>
<span class="gi">+        # officially supported.</span>
<span class="gi">+        return type</span>
<span class="gi">+    return getattr(types, name)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def _walk_global_ops(code):
<span class="w"> </span>    &quot;&quot;&quot;Yield referenced name for global-referencing instructions in code.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for instr in dis.get_instructions(code):</span>
<span class="gi">+        op = instr.opcode</span>
<span class="gi">+        if op in GLOBAL_OPS:</span>
<span class="gi">+            yield instr.argval</span>


<span class="w"> </span>def _extract_class_dict(cls):
<span class="w"> </span>    &quot;&quot;&quot;Retrieve a copy of the dict of a class without the inherited method.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    clsdict = dict(cls.__dict__)  # copy dict proxy to a dict</span>
<span class="gi">+    if len(cls.__bases__) == 1:</span>
<span class="gi">+        inherited_dict = cls.__bases__[0].__dict__</span>
<span class="gi">+    else:</span>
<span class="gi">+        inherited_dict = {}</span>
<span class="gi">+        for base in reversed(cls.__bases__):</span>
<span class="gi">+            inherited_dict.update(base.__dict__)</span>
<span class="gi">+    to_remove = []</span>
<span class="gi">+    for name, value in clsdict.items():</span>
<span class="gi">+        try:</span>
<span class="gi">+            base_value = inherited_dict[name]</span>
<span class="gi">+            if value is base_value:</span>
<span class="gi">+                to_remove.append(name)</span>
<span class="gi">+        except KeyError:</span>
<span class="gi">+            pass</span>
<span class="gi">+    for name in to_remove:</span>
<span class="gi">+        clsdict.pop(name)</span>
<span class="gi">+    return clsdict</span>


<span class="w"> </span>def is_tornado_coroutine(func):
<span class="gu">@@ -201,7 +434,43 @@ def is_tornado_coroutine(func):</span>

<span class="w"> </span>    Running coroutines are not supported.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    warnings.warn(</span>
<span class="gi">+        &quot;is_tornado_coroutine is deprecated in cloudpickle 3.0 and will be &quot;</span>
<span class="gi">+        &quot;removed in cloudpickle 4.0. Use tornado.gen.is_coroutine_function &quot;</span>
<span class="gi">+        &quot;directly instead.&quot;,</span>
<span class="gi">+        category=DeprecationWarning,</span>
<span class="gi">+    )</span>
<span class="gi">+    if &quot;tornado.gen&quot; not in sys.modules:</span>
<span class="gi">+        return False</span>
<span class="gi">+    gen = sys.modules[&quot;tornado.gen&quot;]</span>
<span class="gi">+    if not hasattr(gen, &quot;is_coroutine_function&quot;):</span>
<span class="gi">+        # Tornado version is too old</span>
<span class="gi">+        return False</span>
<span class="gi">+    return gen.is_coroutine_function(func)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def subimport(name):</span>
<span class="gi">+    # We cannot do simply: `return __import__(name)`: Indeed, if ``name`` is</span>
<span class="gi">+    # the name of a submodule, __import__ will return the top-level root module</span>
<span class="gi">+    # of this submodule. For instance, __import__(&#39;os.path&#39;) returns the `os`</span>
<span class="gi">+    # module.</span>
<span class="gi">+    __import__(name)</span>
<span class="gi">+    return sys.modules[name]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def dynamic_subimport(name, vars):</span>
<span class="gi">+    mod = types.ModuleType(name)</span>
<span class="gi">+    mod.__dict__.update(vars)</span>
<span class="gi">+    mod.__dict__[&quot;__builtins__&quot;] = builtins.__dict__</span>
<span class="gi">+    return mod</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_cell_contents(cell):</span>
<span class="gi">+    try:</span>
<span class="gi">+        return cell.cell_contents</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        # Handle empty cells explicitly with a sentinel value.</span>
<span class="gi">+        return _empty_cell_value</span>


<span class="w"> </span>def instance(cls):
<span class="gu">@@ -217,7 +486,7 @@ def instance(cls):</span>
<span class="w"> </span>    instance : cls
<span class="w"> </span>        A new instance of ``cls``.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return cls()</span>


<span class="w"> </span>@instance
<span class="gu">@@ -229,8 +498,31 @@ class _empty_cell_value:</span>
<span class="w"> </span>        return cls.__name__


<span class="gd">-def _make_skeleton_class(type_constructor, name, bases, type_kwargs,</span>
<span class="gd">-    class_tracker_id, extra):</span>
<span class="gi">+def _make_function(code, globals, name, argdefs, closure):</span>
<span class="gi">+    # Setting __builtins__ in globals is needed for nogil CPython.</span>
<span class="gi">+    globals[&quot;__builtins__&quot;] = __builtins__</span>
<span class="gi">+    return types.FunctionType(code, globals, name, argdefs, closure)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_empty_cell():</span>
<span class="gi">+    if False:</span>
<span class="gi">+        # trick the compiler into creating an empty cell in our lambda</span>
<span class="gi">+        cell = None</span>
<span class="gi">+        raise AssertionError(&quot;this route should not be executed&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return (lambda: cell).__closure__[0]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_cell(value=_empty_cell_value):</span>
<span class="gi">+    cell = _make_empty_cell()</span>
<span class="gi">+    if value is not _empty_cell_value:</span>
<span class="gi">+        cell.cell_contents = value</span>
<span class="gi">+    return cell</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_skeleton_class(</span>
<span class="gi">+    type_constructor, name, bases, type_kwargs, class_tracker_id, extra</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Build dynamic class with an empty __dict__ to be filled once memoized

<span class="w"> </span>    If class_tracker_id is not None, try to lookup an existing class definition
<span class="gu">@@ -241,11 +533,15 @@ def _make_skeleton_class(type_constructor, name, bases, type_kwargs,</span>
<span class="w"> </span>    The &quot;extra&quot; variable is meant to be a dict (or None) that can be used for
<span class="w"> </span>    forward compatibility shall the need arise.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    skeleton_class = types.new_class(</span>
<span class="gi">+        name, bases, {&quot;metaclass&quot;: type_constructor}, lambda ns: ns.update(type_kwargs)</span>
<span class="gi">+    )</span>
<span class="gi">+    return _lookup_class_or_track(class_tracker_id, skeleton_class)</span>


<span class="gd">-def _make_skeleton_enum(bases, name, qualname, members, module,</span>
<span class="gd">-    class_tracker_id, extra):</span>
<span class="gi">+def _make_skeleton_enum(</span>
<span class="gi">+    bases, name, qualname, members, module, class_tracker_id, extra</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Build dynamic enum with an empty __dict__ to be filled once memoized

<span class="w"> </span>    The creation of the enum class is inspired by the code of
<span class="gu">@@ -259,22 +555,447 @@ def _make_skeleton_enum(bases, name, qualname, members, module,</span>
<span class="w"> </span>    The &quot;extra&quot; variable is meant to be a dict (or None) that can be used for
<span class="w"> </span>    forward compatibility shall the need arise.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # enums always inherit from their base Enum class at the last position in</span>
<span class="gi">+    # the list of base classes:</span>
<span class="gi">+    enum_base = bases[-1]</span>
<span class="gi">+    metacls = enum_base.__class__</span>
<span class="gi">+    classdict = metacls.__prepare__(name, bases)</span>
<span class="gi">+</span>
<span class="gi">+    for member_name, member_value in members.items():</span>
<span class="gi">+        classdict[member_name] = member_value</span>
<span class="gi">+    enum_class = metacls.__new__(metacls, name, bases, classdict)</span>
<span class="gi">+    enum_class.__module__ = module</span>
<span class="gi">+    enum_class.__qualname__ = qualname</span>
<span class="gi">+</span>
<span class="gi">+    return _lookup_class_or_track(class_tracker_id, enum_class)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_typevar(name, bound, constraints, covariant, contravariant, class_tracker_id):</span>
<span class="gi">+    tv = typing.TypeVar(</span>
<span class="gi">+        name,</span>
<span class="gi">+        *constraints,</span>
<span class="gi">+        bound=bound,</span>
<span class="gi">+        covariant=covariant,</span>
<span class="gi">+        contravariant=contravariant,</span>
<span class="gi">+    )</span>
<span class="gi">+    return _lookup_class_or_track(class_tracker_id, tv)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _decompose_typevar(obj):</span>
<span class="gi">+    return (</span>
<span class="gi">+        obj.__name__,</span>
<span class="gi">+        obj.__bound__,</span>
<span class="gi">+        obj.__constraints__,</span>
<span class="gi">+        obj.__covariant__,</span>
<span class="gi">+        obj.__contravariant__,</span>
<span class="gi">+        _get_or_create_tracker_id(obj),</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _typevar_reduce(obj):</span>
<span class="gi">+    # TypeVar instances require the module information hence why we</span>
<span class="gi">+    # are not using the _should_pickle_by_reference directly</span>
<span class="gi">+    module_and_name = _lookup_module_and_qualname(obj, name=obj.__name__)</span>
<span class="gi">+</span>
<span class="gi">+    if module_and_name is None:</span>
<span class="gi">+        return (_make_typevar, _decompose_typevar(obj))</span>
<span class="gi">+    elif _is_registered_pickle_by_value(module_and_name[0]):</span>
<span class="gi">+        return (_make_typevar, _decompose_typevar(obj))</span>
<span class="gi">+</span>
<span class="gi">+    return (getattr, module_and_name)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_bases(typ):</span>
<span class="gi">+    if &quot;__orig_bases__&quot; in getattr(typ, &quot;__dict__&quot;, {}):</span>
<span class="gi">+        # For generic types (see PEP 560)</span>
<span class="gi">+        # Note that simply checking `hasattr(typ, &#39;__orig_bases__&#39;)` is not</span>
<span class="gi">+        # correct.  Subclasses of a fully-parameterized generic class does not</span>
<span class="gi">+        # have `__orig_bases__` defined, but `hasattr(typ, &#39;__orig_bases__&#39;)`</span>
<span class="gi">+        # will return True because it&#39;s defined in the base class.</span>
<span class="gi">+        bases_attr = &quot;__orig_bases__&quot;</span>
<span class="gi">+    else:</span>
<span class="gi">+        # For regular class objects</span>
<span class="gi">+        bases_attr = &quot;__bases__&quot;</span>
<span class="gi">+    return getattr(typ, bases_attr)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_dict_keys(obj, is_ordered=False):</span>
<span class="gi">+    if is_ordered:</span>
<span class="gi">+        return OrderedDict.fromkeys(obj).keys()</span>
<span class="gi">+    else:</span>
<span class="gi">+        return dict.fromkeys(obj).keys()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_dict_values(obj, is_ordered=False):</span>
<span class="gi">+    if is_ordered:</span>
<span class="gi">+        return OrderedDict((i, _) for i, _ in enumerate(obj)).values()</span>
<span class="gi">+    else:</span>
<span class="gi">+        return {i: _ for i, _ in enumerate(obj)}.values()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_dict_items(obj, is_ordered=False):</span>
<span class="gi">+    if is_ordered:</span>
<span class="gi">+        return OrderedDict(obj).items()</span>
<span class="gi">+    else:</span>
<span class="gi">+        return obj.items()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTION OF OBJECTS __getnewargs__-LIKE METHODS</span>
<span class="gi">+# -------------------------------------------------</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _class_getnewargs(obj):</span>
<span class="gi">+    type_kwargs = {}</span>
<span class="gi">+    if &quot;__module__&quot; in obj.__dict__:</span>
<span class="gi">+        type_kwargs[&quot;__module__&quot;] = obj.__module__</span>
<span class="gi">+</span>
<span class="gi">+    __dict__ = obj.__dict__.get(&quot;__dict__&quot;, None)</span>
<span class="gi">+    if isinstance(__dict__, property):</span>
<span class="gi">+        type_kwargs[&quot;__dict__&quot;] = __dict__</span>
<span class="gi">+</span>
<span class="gi">+    return (</span>
<span class="gi">+        type(obj),</span>
<span class="gi">+        obj.__name__,</span>
<span class="gi">+        _get_bases(obj),</span>
<span class="gi">+        type_kwargs,</span>
<span class="gi">+        _get_or_create_tracker_id(obj),</span>
<span class="gi">+        None,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _enum_getnewargs(obj):</span>
<span class="gi">+    members = {e.name: e.value for e in obj}</span>
<span class="gi">+    return (</span>
<span class="gi">+        obj.__bases__,</span>
<span class="gi">+        obj.__name__,</span>
<span class="gi">+        obj.__qualname__,</span>
<span class="gi">+        members,</span>
<span class="gi">+        obj.__module__,</span>
<span class="gi">+        _get_or_create_tracker_id(obj),</span>
<span class="gi">+        None,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTION OF OBJECTS RECONSTRUCTORS</span>
<span class="gi">+# ------------------------------------</span>
<span class="gi">+def _file_reconstructor(retval):</span>
<span class="gi">+    return retval</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTION OF OBJECTS STATE GETTERS</span>
<span class="gi">+# -----------------------------------</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _function_getstate(func):</span>
<span class="gi">+    # - Put func&#39;s dynamic attributes (stored in func.__dict__) in state. These</span>
<span class="gi">+    #   attributes will be restored at unpickling time using</span>
<span class="gi">+    #   f.__dict__.update(state)</span>
<span class="gi">+    # - Put func&#39;s members into slotstate. Such attributes will be restored at</span>
<span class="gi">+    #   unpickling time by iterating over slotstate and calling setattr(func,</span>
<span class="gi">+    #   slotname, slotvalue)</span>
<span class="gi">+    slotstate = {</span>
<span class="gi">+        &quot;__name__&quot;: func.__name__,</span>
<span class="gi">+        &quot;__qualname__&quot;: func.__qualname__,</span>
<span class="gi">+        &quot;__annotations__&quot;: func.__annotations__,</span>
<span class="gi">+        &quot;__kwdefaults__&quot;: func.__kwdefaults__,</span>
<span class="gi">+        &quot;__defaults__&quot;: func.__defaults__,</span>
<span class="gi">+        &quot;__module__&quot;: func.__module__,</span>
<span class="gi">+        &quot;__doc__&quot;: func.__doc__,</span>
<span class="gi">+        &quot;__closure__&quot;: func.__closure__,</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    f_globals_ref = _extract_code_globals(func.__code__)</span>
<span class="gi">+    f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in func.__globals__}</span>
<span class="gi">+</span>
<span class="gi">+    if func.__closure__ is not None:</span>
<span class="gi">+        closure_values = list(map(_get_cell_contents, func.__closure__))</span>
<span class="gi">+    else:</span>
<span class="gi">+        closure_values = ()</span>
<span class="gi">+</span>
<span class="gi">+    # Extract currently-imported submodules used by func. Storing these modules</span>
<span class="gi">+    # in a smoke _cloudpickle_subimports attribute of the object&#39;s state will</span>
<span class="gi">+    # trigger the side effect of importing these modules at unpickling time</span>
<span class="gi">+    # (which is necessary for func to work correctly once depickled)</span>
<span class="gi">+    slotstate[&quot;_cloudpickle_submodules&quot;] = _find_imported_submodules(</span>
<span class="gi">+        func.__code__, itertools.chain(f_globals.values(), closure_values)</span>
<span class="gi">+    )</span>
<span class="gi">+    slotstate[&quot;__globals__&quot;] = f_globals</span>
<span class="gi">+</span>
<span class="gi">+    state = func.__dict__</span>
<span class="gi">+    return state, slotstate</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _class_getstate(obj):</span>
<span class="gi">+    clsdict = _extract_class_dict(obj)</span>
<span class="gi">+    clsdict.pop(&quot;__weakref__&quot;, None)</span>
<span class="gi">+</span>
<span class="gi">+    if issubclass(type(obj), abc.ABCMeta):</span>
<span class="gi">+        # If obj is an instance of an ABCMeta subclass, don&#39;t pickle the</span>
<span class="gi">+        # cache/negative caches populated during isinstance/issubclass</span>
<span class="gi">+        # checks, but pickle the list of registered subclasses of obj.</span>
<span class="gi">+        clsdict.pop(&quot;_abc_cache&quot;, None)</span>
<span class="gi">+        clsdict.pop(&quot;_abc_negative_cache&quot;, None)</span>
<span class="gi">+        clsdict.pop(&quot;_abc_negative_cache_version&quot;, None)</span>
<span class="gi">+        registry = clsdict.pop(&quot;_abc_registry&quot;, None)</span>
<span class="gi">+        if registry is None:</span>
<span class="gi">+            # The abc caches and registered subclasses of a</span>
<span class="gi">+            # class are bundled into the single _abc_impl attribute</span>
<span class="gi">+            clsdict.pop(&quot;_abc_impl&quot;, None)</span>
<span class="gi">+            (registry, _, _, _) = abc._get_dump(obj)</span>
<span class="gi">+</span>
<span class="gi">+            clsdict[&quot;_abc_impl&quot;] = [subclass_weakref() for subclass_weakref in registry]</span>
<span class="gi">+        else:</span>
<span class="gi">+            # In the above if clause, registry is a set of weakrefs -- in</span>
<span class="gi">+            # this case, registry is a WeakSet</span>
<span class="gi">+            clsdict[&quot;_abc_impl&quot;] = [type_ for type_ in registry]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;__slots__&quot; in clsdict:</span>
<span class="gi">+        # pickle string length optimization: member descriptors of obj are</span>
<span class="gi">+        # created automatically from obj&#39;s __slots__ attribute, no need to</span>
<span class="gi">+        # save them in obj&#39;s state</span>
<span class="gi">+        if isinstance(obj.__slots__, str):</span>
<span class="gi">+            clsdict.pop(obj.__slots__)</span>
<span class="gi">+        else:</span>
<span class="gi">+            for k in obj.__slots__:</span>
<span class="gi">+                clsdict.pop(k, None)</span>
<span class="gi">+</span>
<span class="gi">+    clsdict.pop(&quot;__dict__&quot;, None)  # unpicklable property object</span>
<span class="gi">+</span>
<span class="gi">+    return (clsdict, {})</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _enum_getstate(obj):</span>
<span class="gi">+    clsdict, slotstate = _class_getstate(obj)</span>
<span class="gi">+</span>
<span class="gi">+    members = {e.name: e.value for e in obj}</span>
<span class="gi">+    # Cleanup the clsdict that will be passed to _make_skeleton_enum:</span>
<span class="gi">+    # Those attributes are already handled by the metaclass.</span>
<span class="gi">+    for attrname in [</span>
<span class="gi">+        &quot;_generate_next_value_&quot;,</span>
<span class="gi">+        &quot;_member_names_&quot;,</span>
<span class="gi">+        &quot;_member_map_&quot;,</span>
<span class="gi">+        &quot;_member_type_&quot;,</span>
<span class="gi">+        &quot;_value2member_map_&quot;,</span>
<span class="gi">+    ]:</span>
<span class="gi">+        clsdict.pop(attrname, None)</span>
<span class="gi">+    for member in members:</span>
<span class="gi">+        clsdict.pop(member)</span>
<span class="gi">+        # Special handling of Enum subclasses</span>
<span class="gi">+    return clsdict, slotstate</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTIONS OF OBJECTS REDUCERS</span>
<span class="gi">+# -------------------------------</span>
<span class="gi">+# A reducer is a function taking a single argument (obj), and that returns a</span>
<span class="gi">+# tuple with all the necessary data to re-construct obj. Apart from a few</span>
<span class="gi">+# exceptions (list, dict, bytes, int, etc.), a reducer is necessary to</span>
<span class="gi">+# correctly pickle an object.</span>
<span class="gi">+# While many built-in objects (Exceptions objects, instances of the &quot;object&quot;</span>
<span class="gi">+# class, etc), are shipped with their own built-in reducer (invoked using</span>
<span class="gi">+# obj.__reduce__), some do not. The following methods were created to &quot;fill</span>
<span class="gi">+# these holes&quot;.</span>


<span class="w"> </span>def _code_reduce(obj):
<span class="w"> </span>    &quot;&quot;&quot;code object reducer.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # If you are not sure about the order of arguments, take a look at help</span>
<span class="gi">+    # of the specific type from types, for example:</span>
<span class="gi">+    # &gt;&gt;&gt; from types import CodeType</span>
<span class="gi">+    # &gt;&gt;&gt; help(CodeType)</span>
<span class="gi">+    if hasattr(obj, &quot;co_exceptiontable&quot;):</span>
<span class="gi">+        # Python 3.11 and later: there are some new attributes</span>
<span class="gi">+        # related to the enhanced exceptions.</span>
<span class="gi">+        args = (</span>
<span class="gi">+            obj.co_argcount,</span>
<span class="gi">+            obj.co_posonlyargcount,</span>
<span class="gi">+            obj.co_kwonlyargcount,</span>
<span class="gi">+            obj.co_nlocals,</span>
<span class="gi">+            obj.co_stacksize,</span>
<span class="gi">+            obj.co_flags,</span>
<span class="gi">+            obj.co_code,</span>
<span class="gi">+            obj.co_consts,</span>
<span class="gi">+            obj.co_names,</span>
<span class="gi">+            obj.co_varnames,</span>
<span class="gi">+            obj.co_filename,</span>
<span class="gi">+            obj.co_name,</span>
<span class="gi">+            obj.co_qualname,</span>
<span class="gi">+            obj.co_firstlineno,</span>
<span class="gi">+            obj.co_linetable,</span>
<span class="gi">+            obj.co_exceptiontable,</span>
<span class="gi">+            obj.co_freevars,</span>
<span class="gi">+            obj.co_cellvars,</span>
<span class="gi">+        )</span>
<span class="gi">+    elif hasattr(obj, &quot;co_linetable&quot;):</span>
<span class="gi">+        # Python 3.10 and later: obj.co_lnotab is deprecated and constructor</span>
<span class="gi">+        # expects obj.co_linetable instead.</span>
<span class="gi">+        args = (</span>
<span class="gi">+            obj.co_argcount,</span>
<span class="gi">+            obj.co_posonlyargcount,</span>
<span class="gi">+            obj.co_kwonlyargcount,</span>
<span class="gi">+            obj.co_nlocals,</span>
<span class="gi">+            obj.co_stacksize,</span>
<span class="gi">+            obj.co_flags,</span>
<span class="gi">+            obj.co_code,</span>
<span class="gi">+            obj.co_consts,</span>
<span class="gi">+            obj.co_names,</span>
<span class="gi">+            obj.co_varnames,</span>
<span class="gi">+            obj.co_filename,</span>
<span class="gi">+            obj.co_name,</span>
<span class="gi">+            obj.co_firstlineno,</span>
<span class="gi">+            obj.co_linetable,</span>
<span class="gi">+            obj.co_freevars,</span>
<span class="gi">+            obj.co_cellvars,</span>
<span class="gi">+        )</span>
<span class="gi">+    elif hasattr(obj, &quot;co_nmeta&quot;):  # pragma: no cover</span>
<span class="gi">+        # &quot;nogil&quot; Python: modified attributes from 3.9</span>
<span class="gi">+        args = (</span>
<span class="gi">+            obj.co_argcount,</span>
<span class="gi">+            obj.co_posonlyargcount,</span>
<span class="gi">+            obj.co_kwonlyargcount,</span>
<span class="gi">+            obj.co_nlocals,</span>
<span class="gi">+            obj.co_framesize,</span>
<span class="gi">+            obj.co_ndefaultargs,</span>
<span class="gi">+            obj.co_nmeta,</span>
<span class="gi">+            obj.co_flags,</span>
<span class="gi">+            obj.co_code,</span>
<span class="gi">+            obj.co_consts,</span>
<span class="gi">+            obj.co_varnames,</span>
<span class="gi">+            obj.co_filename,</span>
<span class="gi">+            obj.co_name,</span>
<span class="gi">+            obj.co_firstlineno,</span>
<span class="gi">+            obj.co_lnotab,</span>
<span class="gi">+            obj.co_exc_handlers,</span>
<span class="gi">+            obj.co_jump_table,</span>
<span class="gi">+            obj.co_freevars,</span>
<span class="gi">+            obj.co_cellvars,</span>
<span class="gi">+            obj.co_free2reg,</span>
<span class="gi">+            obj.co_cell2reg,</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        # Backward compat for 3.8 and 3.9</span>
<span class="gi">+        args = (</span>
<span class="gi">+            obj.co_argcount,</span>
<span class="gi">+            obj.co_posonlyargcount,</span>
<span class="gi">+            obj.co_kwonlyargcount,</span>
<span class="gi">+            obj.co_nlocals,</span>
<span class="gi">+            obj.co_stacksize,</span>
<span class="gi">+            obj.co_flags,</span>
<span class="gi">+            obj.co_code,</span>
<span class="gi">+            obj.co_consts,</span>
<span class="gi">+            obj.co_names,</span>
<span class="gi">+            obj.co_varnames,</span>
<span class="gi">+            obj.co_filename,</span>
<span class="gi">+            obj.co_name,</span>
<span class="gi">+            obj.co_firstlineno,</span>
<span class="gi">+            obj.co_lnotab,</span>
<span class="gi">+            obj.co_freevars,</span>
<span class="gi">+            obj.co_cellvars,</span>
<span class="gi">+        )</span>
<span class="gi">+    return types.CodeType, args</span>


<span class="w"> </span>def _cell_reduce(obj):
<span class="w"> </span>    &quot;&quot;&quot;Cell (containing values of a function&#39;s free variables) reducer.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        obj.cell_contents</span>
<span class="gi">+    except ValueError:  # cell is empty</span>
<span class="gi">+        return _make_empty_cell, ()</span>
<span class="gi">+    else:</span>
<span class="gi">+        return _make_cell, (obj.cell_contents,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _classmethod_reduce(obj):</span>
<span class="gi">+    orig_func = obj.__func__</span>
<span class="gi">+    return type(obj), (orig_func,)</span>


<span class="w"> </span>def _file_reduce(obj):
<span class="w"> </span>    &quot;&quot;&quot;Save a file.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import io</span>
<span class="gi">+</span>
<span class="gi">+    if not hasattr(obj, &quot;name&quot;) or not hasattr(obj, &quot;mode&quot;):</span>
<span class="gi">+        raise pickle.PicklingError(</span>
<span class="gi">+            &quot;Cannot pickle files that do not map to an actual file&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if obj is sys.stdout:</span>
<span class="gi">+        return getattr, (sys, &quot;stdout&quot;)</span>
<span class="gi">+    if obj is sys.stderr:</span>
<span class="gi">+        return getattr, (sys, &quot;stderr&quot;)</span>
<span class="gi">+    if obj is sys.stdin:</span>
<span class="gi">+        raise pickle.PicklingError(&quot;Cannot pickle standard input&quot;)</span>
<span class="gi">+    if obj.closed:</span>
<span class="gi">+        raise pickle.PicklingError(&quot;Cannot pickle closed files&quot;)</span>
<span class="gi">+    if hasattr(obj, &quot;isatty&quot;) and obj.isatty():</span>
<span class="gi">+        raise pickle.PicklingError(&quot;Cannot pickle files that map to tty objects&quot;)</span>
<span class="gi">+    if &quot;r&quot; not in obj.mode and &quot;+&quot; not in obj.mode:</span>
<span class="gi">+        raise pickle.PicklingError(</span>
<span class="gi">+            &quot;Cannot pickle files that are not opened for reading: %s&quot; % obj.mode</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    name = obj.name</span>
<span class="gi">+</span>
<span class="gi">+    retval = io.StringIO()</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        # Read the whole file</span>
<span class="gi">+        curloc = obj.tell()</span>
<span class="gi">+        obj.seek(0)</span>
<span class="gi">+        contents = obj.read()</span>
<span class="gi">+        obj.seek(curloc)</span>
<span class="gi">+    except OSError as e:</span>
<span class="gi">+        raise pickle.PicklingError(</span>
<span class="gi">+            &quot;Cannot pickle file %s as it cannot be read&quot; % name</span>
<span class="gi">+        ) from e</span>
<span class="gi">+    retval.write(contents)</span>
<span class="gi">+    retval.seek(curloc)</span>
<span class="gi">+</span>
<span class="gi">+    retval.name = name</span>
<span class="gi">+    return _file_reconstructor, (retval,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _getset_descriptor_reduce(obj):</span>
<span class="gi">+    return getattr, (obj.__objclass__, obj.__name__)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _mappingproxy_reduce(obj):</span>
<span class="gi">+    return types.MappingProxyType, (dict(obj),)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _memoryview_reduce(obj):</span>
<span class="gi">+    return bytes, (obj.tobytes(),)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _module_reduce(obj):</span>
<span class="gi">+    if _should_pickle_by_reference(obj):</span>
<span class="gi">+        return subimport, (obj.__name__,)</span>
<span class="gi">+    else:</span>
<span class="gi">+        # Some external libraries can populate the &quot;__builtins__&quot; entry of a</span>
<span class="gi">+        # module&#39;s `__dict__` with unpicklable objects (see #316). For that</span>
<span class="gi">+        # reason, we do not attempt to pickle the &quot;__builtins__&quot; entry, and</span>
<span class="gi">+        # restore a default value for it at unpickling time.</span>
<span class="gi">+        state = obj.__dict__.copy()</span>
<span class="gi">+        state.pop(&quot;__builtins__&quot;, None)</span>
<span class="gi">+        return dynamic_subimport, (obj.__name__, state)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _method_reduce(obj):</span>
<span class="gi">+    return (types.MethodType, (obj.__func__, obj.__self__))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _logger_reduce(obj):</span>
<span class="gi">+    return logging.getLogger, (obj.name,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _root_logger_reduce(obj):</span>
<span class="gi">+    return logging.getLogger, ()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _property_reduce(obj):</span>
<span class="gi">+    return property, (obj.fget, obj.fset, obj.fdel, obj.__doc__)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _weakset_reduce(obj):</span>
<span class="gi">+    return weakref.WeakSet, (list(obj),)</span>


<span class="w"> </span>def _dynamic_class_reduce(obj):
<span class="gu">@@ -284,12 +1005,85 @@ def _dynamic_class_reduce(obj):</span>
<span class="w"> </span>    functions, or that otherwise can&#39;t be serialized as attribute lookups
<span class="w"> </span>    from importable modules.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if Enum is not None and issubclass(obj, Enum):</span>
<span class="gi">+        return (</span>
<span class="gi">+            _make_skeleton_enum,</span>
<span class="gi">+            _enum_getnewargs(obj),</span>
<span class="gi">+            _enum_getstate(obj),</span>
<span class="gi">+            None,</span>
<span class="gi">+            None,</span>
<span class="gi">+            _class_setstate,</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        return (</span>
<span class="gi">+            _make_skeleton_class,</span>
<span class="gi">+            _class_getnewargs(obj),</span>
<span class="gi">+            _class_getstate(obj),</span>
<span class="gi">+            None,</span>
<span class="gi">+            None,</span>
<span class="gi">+            _class_setstate,</span>
<span class="gi">+        )</span>


<span class="w"> </span>def _class_reduce(obj):
<span class="w"> </span>    &quot;&quot;&quot;Select the reducer depending on the dynamic nature of the class obj.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if obj is type(None):  # noqa</span>
<span class="gi">+        return type, (None,)</span>
<span class="gi">+    elif obj is type(Ellipsis):</span>
<span class="gi">+        return type, (Ellipsis,)</span>
<span class="gi">+    elif obj is type(NotImplemented):</span>
<span class="gi">+        return type, (NotImplemented,)</span>
<span class="gi">+    elif obj in _BUILTIN_TYPE_NAMES:</span>
<span class="gi">+        return _builtin_type, (_BUILTIN_TYPE_NAMES[obj],)</span>
<span class="gi">+    elif not _should_pickle_by_reference(obj):</span>
<span class="gi">+        return _dynamic_class_reduce(obj)</span>
<span class="gi">+    return NotImplemented</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _dict_keys_reduce(obj):</span>
<span class="gi">+    # Safer not to ship the full dict as sending the rest might</span>
<span class="gi">+    # be unintended and could potentially cause leaking of</span>
<span class="gi">+    # sensitive information</span>
<span class="gi">+    return _make_dict_keys, (list(obj),)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _dict_values_reduce(obj):</span>
<span class="gi">+    # Safer not to ship the full dict as sending the rest might</span>
<span class="gi">+    # be unintended and could potentially cause leaking of</span>
<span class="gi">+    # sensitive information</span>
<span class="gi">+    return _make_dict_values, (list(obj),)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _dict_items_reduce(obj):</span>
<span class="gi">+    return _make_dict_items, (dict(obj),)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _odict_keys_reduce(obj):</span>
<span class="gi">+    # Safer not to ship the full dict as sending the rest might</span>
<span class="gi">+    # be unintended and could potentially cause leaking of</span>
<span class="gi">+    # sensitive information</span>
<span class="gi">+    return _make_dict_keys, (list(obj), True)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _odict_values_reduce(obj):</span>
<span class="gi">+    # Safer not to ship the full dict as sending the rest might</span>
<span class="gi">+    # be unintended and could potentially cause leaking of</span>
<span class="gi">+    # sensitive information</span>
<span class="gi">+    return _make_dict_values, (list(obj), True)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _odict_items_reduce(obj):</span>
<span class="gi">+    return _make_dict_items, (dict(obj), True)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _dataclass_field_base_reduce(obj):</span>
<span class="gi">+    return _get_dataclass_field_type_sentinel, (obj.name,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTIONS OF OBJECTS STATE SETTERS</span>
<span class="gi">+# ------------------------------------</span>
<span class="gi">+# state setters are called at unpickling time, once the object is created and</span>
<span class="gi">+# it has to be updated to how it was at unpickling time.</span>


<span class="w"> </span>def _function_setstate(obj, state):
<span class="gu">@@ -299,15 +1093,68 @@ def _function_setstate(obj, state):</span>
<span class="w"> </span>    cannot rely on the native setstate routine of pickle.load_build, that calls
<span class="w"> </span>    setattr on items of the slotstate. Instead, we have to modify them inplace.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    state, slotstate = state</span>
<span class="gi">+    obj.__dict__.update(state)</span>
<span class="gi">+</span>
<span class="gi">+    obj_globals = slotstate.pop(&quot;__globals__&quot;)</span>
<span class="gi">+    obj_closure = slotstate.pop(&quot;__closure__&quot;)</span>
<span class="gi">+    # _cloudpickle_subimports is a set of submodules that must be loaded for</span>
<span class="gi">+    # the pickled function to work correctly at unpickling time. Now that these</span>
<span class="gi">+    # submodules are depickled (hence imported), they can be removed from the</span>
<span class="gi">+    # object&#39;s state (the object state only served as a reference holder to</span>
<span class="gi">+    # these submodules)</span>
<span class="gi">+    slotstate.pop(&quot;_cloudpickle_submodules&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    obj.__globals__.update(obj_globals)</span>
<span class="gi">+    obj.__globals__[&quot;__builtins__&quot;] = __builtins__</span>
<span class="gi">+</span>
<span class="gi">+    if obj_closure is not None:</span>
<span class="gi">+        for i, cell in enumerate(obj_closure):</span>
<span class="gi">+            try:</span>
<span class="gi">+                value = cell.cell_contents</span>
<span class="gi">+            except ValueError:  # cell is empty</span>
<span class="gi">+                continue</span>
<span class="gi">+            obj.__closure__[i].cell_contents = value</span>
<span class="gi">+</span>
<span class="gi">+    for k, v in slotstate.items():</span>
<span class="gi">+        setattr(obj, k, v)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _class_setstate(obj, state):</span>
<span class="gi">+    state, slotstate = state</span>
<span class="gi">+    registry = None</span>
<span class="gi">+    for attrname, attr in state.items():</span>
<span class="gi">+        if attrname == &quot;_abc_impl&quot;:</span>
<span class="gi">+            registry = attr</span>
<span class="gi">+        else:</span>
<span class="gi">+            setattr(obj, attrname, attr)</span>
<span class="gi">+    if registry is not None:</span>
<span class="gi">+        for subclass in registry:</span>
<span class="gi">+            obj.register(subclass)</span>

<span class="gi">+    return obj</span>

<span class="gd">-_DATACLASSE_FIELD_TYPE_SENTINELS = {dataclasses._FIELD.name: dataclasses.</span>
<span class="gd">-    _FIELD, dataclasses._FIELD_CLASSVAR.name: dataclasses._FIELD_CLASSVAR,</span>
<span class="gd">-    dataclasses._FIELD_INITVAR.name: dataclasses._FIELD_INITVAR}</span>
<span class="gi">+</span>
<span class="gi">+# COLLECTION OF DATACLASS UTILITIES</span>
<span class="gi">+# ---------------------------------</span>
<span class="gi">+# There are some internal sentinel values whose identity must be preserved when</span>
<span class="gi">+# unpickling dataclass fields. Each sentinel value has a unique name that we can</span>
<span class="gi">+# use to retrieve its identity at unpickling time.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_DATACLASSE_FIELD_TYPE_SENTINELS = {</span>
<span class="gi">+    dataclasses._FIELD.name: dataclasses._FIELD,</span>
<span class="gi">+    dataclasses._FIELD_CLASSVAR.name: dataclasses._FIELD_CLASSVAR,</span>
<span class="gi">+    dataclasses._FIELD_INITVAR.name: dataclasses._FIELD_INITVAR,</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_dataclass_field_type_sentinel(name):</span>
<span class="gi">+    return _DATACLASSE_FIELD_TYPE_SENTINELS[name]</span>


<span class="w"> </span>class Pickler(pickle.Pickler):
<span class="gi">+    # set of reducers defined and used by cloudpickle (private)</span>
<span class="w"> </span>    _dispatch_table = {}
<span class="w"> </span>    _dispatch_table[classmethod] = _classmethod_reduce
<span class="w"> </span>    _dispatch_table[io.TextIOWrapper] = _file_reduce
<span class="gu">@@ -335,11 +1182,16 @@ class Pickler(pickle.Pickler):</span>
<span class="w"> </span>    _dispatch_table[abc.abstractstaticmethod] = _classmethod_reduce
<span class="w"> </span>    _dispatch_table[abc.abstractproperty] = _property_reduce
<span class="w"> </span>    _dispatch_table[dataclasses._FIELD_BASE] = _dataclass_field_base_reduce
<span class="gi">+</span>
<span class="w"> </span>    dispatch_table = ChainMap(_dispatch_table, copyreg.dispatch_table)

<span class="gi">+    # function reducers are defined as instance methods of cloudpickle.Pickler</span>
<span class="gi">+    # objects, as they rely on a cloudpickle.Pickler attribute (globals_ref)</span>
<span class="w"> </span>    def _dynamic_function_reduce(self, func):
<span class="w"> </span>        &quot;&quot;&quot;Reduce a function that is not pickleable via attribute lookup.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        newargs = self._function_getnewargs(func)</span>
<span class="gi">+        state = _function_getstate(func)</span>
<span class="gi">+        return (_make_function, newargs, state, None, None, _function_setstate)</span>

<span class="w"> </span>    def _function_reduce(self, obj):
<span class="w"> </span>        &quot;&quot;&quot;Reducer for function objects.
<span class="gu">@@ -350,18 +1202,91 @@ class Pickler(pickle.Pickler):</span>
<span class="w"> </span>        obj using a custom cloudpickle reducer designed specifically to handle
<span class="w"> </span>        dynamic functions.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if _should_pickle_by_reference(obj):</span>
<span class="gi">+            return NotImplemented</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self._dynamic_function_reduce(obj)</span>
<span class="gi">+</span>
<span class="gi">+    def _function_getnewargs(self, func):</span>
<span class="gi">+        code = func.__code__</span>
<span class="gi">+</span>
<span class="gi">+        # base_globals represents the future global namespace of func at</span>
<span class="gi">+        # unpickling time. Looking it up and storing it in</span>
<span class="gi">+        # cloudpickle.Pickler.globals_ref allow functions sharing the same</span>
<span class="gi">+        # globals at pickling time to also share them once unpickled, at one</span>
<span class="gi">+        # condition: since globals_ref is an attribute of a cloudpickle.Pickler</span>
<span class="gi">+        # instance, and that a new cloudpickle.Pickler is created each time</span>
<span class="gi">+        # cloudpickle.dump or cloudpickle.dumps is called, functions also need</span>
<span class="gi">+        # to be saved within the same invocation of</span>
<span class="gi">+        # cloudpickle.dump/cloudpickle.dumps (for example:</span>
<span class="gi">+        # cloudpickle.dumps([f1, f2])). There is no such limitation when using</span>
<span class="gi">+        # cloudpickle.Pickler.dump, as long as the multiple invocations are</span>
<span class="gi">+        # bound to the same cloudpickle.Pickler instance.</span>
<span class="gi">+        base_globals = self.globals_ref.setdefault(id(func.__globals__), {})</span>
<span class="gi">+</span>
<span class="gi">+        if base_globals == {}:</span>
<span class="gi">+            # Add module attributes used to resolve relative imports</span>
<span class="gi">+            # instructions inside func.</span>
<span class="gi">+            for k in [&quot;__package__&quot;, &quot;__name__&quot;, &quot;__path__&quot;, &quot;__file__&quot;]:</span>
<span class="gi">+                if k in func.__globals__:</span>
<span class="gi">+                    base_globals[k] = func.__globals__[k]</span>
<span class="gi">+</span>
<span class="gi">+        # Do not bind the free variables before the function is created to</span>
<span class="gi">+        # avoid infinite recursion.</span>
<span class="gi">+        if func.__closure__ is None:</span>
<span class="gi">+            closure = None</span>
<span class="gi">+        else:</span>
<span class="gi">+            closure = tuple(_make_empty_cell() for _ in range(len(code.co_freevars)))</span>
<span class="gi">+</span>
<span class="gi">+        return code, base_globals, None, None, closure</span>
<span class="gi">+</span>
<span class="gi">+    def dump(self, obj):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return super().dump(obj)</span>
<span class="gi">+        except RuntimeError as e:</span>
<span class="gi">+            if len(e.args) &gt; 0 and &quot;recursion&quot; in e.args[0]:</span>
<span class="gi">+                msg = &quot;Could not pickle object as excessively deep recursion required.&quot;</span>
<span class="gi">+                raise pickle.PicklingError(msg) from e</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise</span>

<span class="w"> </span>    def __init__(self, file, protocol=None, buffer_callback=None):
<span class="w"> </span>        if protocol is None:
<span class="w"> </span>            protocol = DEFAULT_PROTOCOL
<span class="gd">-        super().__init__(file, protocol=protocol, buffer_callback=</span>
<span class="gd">-            buffer_callback)</span>
<span class="gi">+        super().__init__(file, protocol=protocol, buffer_callback=buffer_callback)</span>
<span class="gi">+        # map functions __globals__ attribute ids, to ensure that functions</span>
<span class="gi">+        # sharing the same global namespace at pickling time also share</span>
<span class="gi">+        # their global namespace at unpickling time.</span>
<span class="w"> </span>        self.globals_ref = {}
<span class="w"> </span>        self.proto = int(protocol)
<span class="gi">+</span>
<span class="w"> </span>    if not PYPY:
<span class="gi">+        # pickle.Pickler is the C implementation of the CPython pickler and</span>
<span class="gi">+        # therefore we rely on reduce_override method to customize the pickler</span>
<span class="gi">+        # behavior.</span>
<span class="gi">+</span>
<span class="gi">+        # `cloudpickle.Pickler.dispatch` is only left for backward</span>
<span class="gi">+        # compatibility - note that when using protocol 5,</span>
<span class="gi">+        # `cloudpickle.Pickler.dispatch` is not an extension of</span>
<span class="gi">+        # `pickle._Pickler.dispatch` dictionary, because `cloudpickle.Pickler`</span>
<span class="gi">+        # subclasses the C-implemented `pickle.Pickler`, which does not expose</span>
<span class="gi">+        # a `dispatch` attribute.  Earlier versions of `cloudpickle.Pickler`</span>
<span class="gi">+        # used `cloudpickle.Pickler.dispatch` as a class-level attribute</span>
<span class="gi">+        # storing all reducers implemented by cloudpickle, but the attribute</span>
<span class="gi">+        # name was not a great choice given because it would collide with a</span>
<span class="gi">+        # similarly named attribute in the pure-Python `pickle._Pickler`</span>
<span class="gi">+        # implementation in the standard library.</span>
<span class="w"> </span>        dispatch = dispatch_table

<span class="gi">+        # Implementation of the reducer_override callback, in order to</span>
<span class="gi">+        # efficiently serialize dynamic functions and classes by subclassing</span>
<span class="gi">+        # the C-implemented `pickle.Pickler`.</span>
<span class="gi">+        # TODO: decorrelate reducer_override (which is tied to CPython&#39;s</span>
<span class="gi">+        # implementation - would it make sense to backport it to pypy? - and</span>
<span class="gi">+        # pickle&#39;s protocol 5 which is implementation agnostic. Currently, the</span>
<span class="gi">+        # availability of both notions coincide on CPython&#39;s pickle, but it may</span>
<span class="gi">+        # not be the case anymore when pypy implements protocol 5.</span>
<span class="gi">+</span>
<span class="w"> </span>        def reducer_override(self, obj):
<span class="w"> </span>            &quot;&quot;&quot;Type-agnostic reducing callback for function and classes.

<span class="gu">@@ -393,17 +1318,85 @@ class Pickler(pickle.Pickler):</span>
<span class="w"> </span>              reducers, such as Exceptions. See
<span class="w"> </span>              https://github.com/cloudpipe/cloudpickle/issues/248
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            t = type(obj)</span>
<span class="gi">+            try:</span>
<span class="gi">+                is_anyclass = issubclass(t, type)</span>
<span class="gi">+            except TypeError:  # t is not a class (old Boost; see SF #502085)</span>
<span class="gi">+                is_anyclass = False</span>
<span class="gi">+</span>
<span class="gi">+            if is_anyclass:</span>
<span class="gi">+                return _class_reduce(obj)</span>
<span class="gi">+            elif isinstance(obj, types.FunctionType):</span>
<span class="gi">+                return self._function_reduce(obj)</span>
<span class="gi">+            else:</span>
<span class="gi">+                # fallback to save_global, including the Pickler&#39;s</span>
<span class="gi">+                # dispatch_table</span>
<span class="gi">+                return NotImplemented</span>
<span class="gi">+</span>
<span class="w"> </span>    else:
<span class="gi">+        # When reducer_override is not available, hack the pure-Python</span>
<span class="gi">+        # Pickler&#39;s types.FunctionType and type savers. Note: the type saver</span>
<span class="gi">+        # must override Pickler.save_global, because pickle.py contains a</span>
<span class="gi">+        # hard-coded call to save_global when pickling meta-classes.</span>
<span class="w"> </span>        dispatch = pickle.Pickler.dispatch.copy()

<span class="gi">+        def _save_reduce_pickle5(</span>
<span class="gi">+            self,</span>
<span class="gi">+            func,</span>
<span class="gi">+            args,</span>
<span class="gi">+            state=None,</span>
<span class="gi">+            listitems=None,</span>
<span class="gi">+            dictitems=None,</span>
<span class="gi">+            state_setter=None,</span>
<span class="gi">+            obj=None,</span>
<span class="gi">+        ):</span>
<span class="gi">+            save = self.save</span>
<span class="gi">+            write = self.write</span>
<span class="gi">+            self.save_reduce(</span>
<span class="gi">+                func,</span>
<span class="gi">+                args,</span>
<span class="gi">+                state=None,</span>
<span class="gi">+                listitems=listitems,</span>
<span class="gi">+                dictitems=dictitems,</span>
<span class="gi">+                obj=obj,</span>
<span class="gi">+            )</span>
<span class="gi">+            # backport of the Python 3.8 state_setter pickle operations</span>
<span class="gi">+            save(state_setter)</span>
<span class="gi">+            save(obj)  # simple BINGET opcode as obj is already memoized.</span>
<span class="gi">+            save(state)</span>
<span class="gi">+            write(pickle.TUPLE2)</span>
<span class="gi">+            # Trigger a state_setter(obj, state) function call.</span>
<span class="gi">+            write(pickle.REDUCE)</span>
<span class="gi">+            # The purpose of state_setter is to carry-out an</span>
<span class="gi">+            # inplace modification of obj. We do not care about what the</span>
<span class="gi">+            # method might return, so its output is eventually removed from</span>
<span class="gi">+            # the stack.</span>
<span class="gi">+            write(pickle.POP)</span>
<span class="gi">+</span>
<span class="w"> </span>        def save_global(self, obj, name=None, pack=struct.pack):
<span class="w"> </span>            &quot;&quot;&quot;Main dispatch method.

<span class="w"> </span>            The name of this method is somewhat misleading: all types get
<span class="w"> </span>            dispatched here.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if obj is type(None):  # noqa</span>
<span class="gi">+                return self.save_reduce(type, (None,), obj=obj)</span>
<span class="gi">+            elif obj is type(Ellipsis):</span>
<span class="gi">+                return self.save_reduce(type, (Ellipsis,), obj=obj)</span>
<span class="gi">+            elif obj is type(NotImplemented):</span>
<span class="gi">+                return self.save_reduce(type, (NotImplemented,), obj=obj)</span>
<span class="gi">+            elif obj in _BUILTIN_TYPE_NAMES:</span>
<span class="gi">+                return self.save_reduce(</span>
<span class="gi">+                    _builtin_type, (_BUILTIN_TYPE_NAMES[obj],), obj=obj</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if name is not None:</span>
<span class="gi">+                super().save_global(obj, name=name)</span>
<span class="gi">+            elif not _should_pickle_by_reference(obj, name=name):</span>
<span class="gi">+                self._save_reduce_pickle5(*_dynamic_class_reduce(obj), obj=obj)</span>
<span class="gi">+            else:</span>
<span class="gi">+                super().save_global(obj, name=name)</span>
<span class="gi">+</span>
<span class="w"> </span>        dispatch[type] = save_global

<span class="w"> </span>        def save_function(self, obj, name=None):
<span class="gu">@@ -412,7 +1405,14 @@ class Pickler(pickle.Pickler):</span>
<span class="w"> </span>            Determines what kind of function obj is (e.g. lambda, defined at
<span class="w"> </span>            interactive prompt, etc) and handles the pickling appropriately.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if _should_pickle_by_reference(obj, name=name):</span>
<span class="gi">+                return super().save_global(obj, name=name)</span>
<span class="gi">+            elif PYPY and isinstance(obj.__code__, builtin_code_type):</span>
<span class="gi">+                return self.save_pypy_builtin_func(obj)</span>
<span class="gi">+            else:</span>
<span class="gi">+                return self._save_reduce_pickle5(</span>
<span class="gi">+                    *self._dynamic_function_reduce(obj), obj=obj</span>
<span class="gi">+                )</span>

<span class="w"> </span>        def save_pypy_builtin_func(self, obj):
<span class="w"> </span>            &quot;&quot;&quot;Save pypy equivalent of builtin functions.
<span class="gu">@@ -432,10 +1432,19 @@ class Pickler(pickle.Pickler):</span>
<span class="w"> </span>            this routing should be removed when cloudpickle supports only PyPy
<span class="w"> </span>            3.6 and later.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            rv = (</span>
<span class="gi">+                types.FunctionType,</span>
<span class="gi">+                (obj.__code__, {}, obj.__name__, obj.__defaults__, obj.__closure__),</span>
<span class="gi">+                obj.__dict__,</span>
<span class="gi">+            )</span>
<span class="gi">+            self.save_reduce(*rv, obj=obj)</span>
<span class="gi">+</span>
<span class="w"> </span>        dispatch[types.FunctionType] = save_function


<span class="gi">+# Shorthands similar to pickle.dump/pickle.dumps</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def dump(obj, file, protocol=None, buffer_callback=None):
<span class="w"> </span>    &quot;&quot;&quot;Serialize obj as bytes streamed into file

<span class="gu">@@ -449,7 +1458,7 @@ def dump(obj, file, protocol=None, buffer_callback=None):</span>
<span class="w"> </span>    implementation details that can change from one Python version to the
<span class="w"> </span>    next).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    Pickler(file, protocol=protocol, buffer_callback=buffer_callback).dump(obj)</span>


<span class="w"> </span>def dumps(obj, protocol=None, buffer_callback=None):
<span class="gu">@@ -465,8 +1474,14 @@ def dumps(obj, protocol=None, buffer_callback=None):</span>
<span class="w"> </span>    implementation details that can change from one Python version to the
<span class="w"> </span>    next).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with io.BytesIO() as file:</span>
<span class="gi">+        cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)</span>
<span class="gi">+        cp.dump(obj)</span>
<span class="gi">+        return file.getvalue()</span>


<span class="gi">+# Include pickles unloading functions in this namespace for convenience.</span>
<span class="w"> </span>load, loads = pickle.load, pickle.loads
<span class="gi">+</span>
<span class="gi">+# Backward compat alias.</span>
<span class="w"> </span>CloudPickler = Pickler
<span class="gh">diff --git a/joblib/externals/loky/_base.py b/joblib/externals/loky/_base.py</span>
<span class="gh">index 6d789c8..da0abc1 100644</span>
<span class="gd">--- a/joblib/externals/loky/_base.py</span>
<span class="gi">+++ b/joblib/externals/loky/_base.py</span>
<span class="gu">@@ -1,6 +1,28 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Modification of concurrent.futures.Future</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from concurrent/futures/_base.py (17/02/2017)</span>
<span class="gi">+#  * Do not use yield from</span>
<span class="gi">+#  * Use old super syntax</span>
<span class="gi">+#</span>
<span class="gi">+# Copyright 2009 Brian Quinlan. All Rights Reserved.</span>
<span class="gi">+# Licensed to PSF under a Contributor Agreement.</span>
<span class="gi">+</span>
<span class="w"> </span>from concurrent.futures import Future as _BaseFuture
<span class="w"> </span>from concurrent.futures._base import LOGGER


<span class="gi">+# To make loky._base.Future instances awaitable  by concurrent.futures.wait,</span>
<span class="gi">+# derive our custom Future class from _BaseFuture. _invoke_callback is the only</span>
<span class="gi">+# modification made to this class in loky.</span>
<span class="gi">+# TODO investigate why using `concurrent.futures.Future` directly does not</span>
<span class="gi">+# always work in our test suite.</span>
<span class="w"> </span>class Future(_BaseFuture):
<span class="gd">-    pass</span>
<span class="gi">+    def _invoke_callbacks(self):</span>
<span class="gi">+        for callback in self._done_callbacks:</span>
<span class="gi">+            try:</span>
<span class="gi">+                callback(self)</span>
<span class="gi">+            except BaseException:</span>
<span class="gi">+                LOGGER.exception(f&quot;exception calling callback for {self!r}&quot;)</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/_posix_reduction.py b/joblib/externals/loky/backend/_posix_reduction.py</span>
<span class="gh">index c819d41..4b800ec 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/_posix_reduction.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/_posix_reduction.py</span>
<span class="gu">@@ -1,16 +1,65 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Extra reducers for Unix based system and connections objects</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/reduction.py (17/02/2017)</span>
<span class="gi">+#  * Add adapted reduction for LokyProcesses and socket/Connection</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import socket
<span class="w"> </span>import _socket
<span class="w"> </span>from multiprocessing.connection import Connection
<span class="w"> </span>from multiprocessing.context import get_spawning_popen
<span class="gi">+</span>
<span class="w"> </span>from .reduction import register
<span class="gd">-HAVE_SEND_HANDLE = hasattr(socket, &#39;CMSG_LEN&#39;) and hasattr(socket, &#39;SCM_RIGHTS&#39;</span>
<span class="gd">-    ) and hasattr(socket.socket, &#39;sendmsg&#39;)</span>
<span class="gi">+</span>
<span class="gi">+HAVE_SEND_HANDLE = (</span>
<span class="gi">+    hasattr(socket, &quot;CMSG_LEN&quot;)</span>
<span class="gi">+    and hasattr(socket, &quot;SCM_RIGHTS&quot;)</span>
<span class="gi">+    and hasattr(socket.socket, &quot;sendmsg&quot;)</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _mk_inheritable(fd):</span>
<span class="gi">+    os.set_inheritable(fd, True)</span>
<span class="gi">+    return fd</span>


<span class="w"> </span>def DupFd(fd):
<span class="w"> </span>    &quot;&quot;&quot;Return a wrapper for an fd.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    popen_obj = get_spawning_popen()</span>
<span class="gi">+    if popen_obj is not None:</span>
<span class="gi">+        return popen_obj.DupFd(popen_obj.duplicate_for_child(fd))</span>
<span class="gi">+    elif HAVE_SEND_HANDLE:</span>
<span class="gi">+        from multiprocessing import resource_sharer</span>
<span class="gi">+</span>
<span class="gi">+        return resource_sharer.DupFd(fd)</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            &quot;Cannot pickle connection object. This object can only be &quot;</span>
<span class="gi">+            &quot;passed when spawning a new process&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _reduce_socket(s):</span>
<span class="gi">+    df = DupFd(s.fileno())</span>
<span class="gi">+    return _rebuild_socket, (df, s.family, s.type, s.proto)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _rebuild_socket(df, family, type, proto):</span>
<span class="gi">+    fd = df.detach()</span>
<span class="gi">+    return socket.fromfd(fd, family, type, proto)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def rebuild_connection(df, readable, writable):</span>
<span class="gi">+    fd = df.detach()</span>
<span class="gi">+    return Connection(fd, readable, writable)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def reduce_connection(conn):</span>
<span class="gi">+    df = DupFd(conn.fileno())</span>
<span class="gi">+    return rebuild_connection, (df, conn.readable, conn.writable)</span>


<span class="w"> </span>register(socket.socket, _reduce_socket)
<span class="gh">diff --git a/joblib/externals/loky/backend/_win_reduction.py b/joblib/externals/loky/backend/_win_reduction.py</span>
<span class="gh">index 0a4276c..506d0ec 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/_win_reduction.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/_win_reduction.py</span>
<span class="gu">@@ -1,7 +1,18 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Extra reducers for Windows system and connections objects</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/reduction.py (17/02/2017)</span>
<span class="gi">+#  * Add adapted reduction for LokyProcesses and socket/PipeConnection</span>
<span class="gi">+#</span>
<span class="w"> </span>import socket
<span class="w"> </span>from multiprocessing import connection
<span class="w"> </span>from multiprocessing.reduction import _reduce_socket
<span class="gi">+</span>
<span class="w"> </span>from .reduction import register
<span class="gi">+</span>
<span class="gi">+# register reduction for win32 communication objects</span>
<span class="w"> </span>register(socket.socket, _reduce_socket)
<span class="w"> </span>register(connection.Connection, connection.reduce_connection)
<span class="w"> </span>register(connection.PipeConnection, connection.reduce_pipe_connection)
<span class="gh">diff --git a/joblib/externals/loky/backend/context.py b/joblib/externals/loky/backend/context.py</span>
<span class="gh">index 1e0d413..d0f5903 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/context.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/context.py</span>
<span class="gu">@@ -1,3 +1,14 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Basic context management with LokyContext</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/context.py</span>
<span class="gi">+#  * Create a context ensuring loky uses only objects that are compatible</span>
<span class="gi">+#  * Add LokyContext to the list of context of multiprocessing so loky can be</span>
<span class="gi">+#    used with multiprocessing.set_start_method</span>
<span class="gi">+#  * Implement a CFS-aware amd physical-core aware cpu_count function.</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import math
<span class="gu">@@ -7,20 +18,68 @@ import warnings</span>
<span class="w"> </span>import multiprocessing as mp
<span class="w"> </span>from multiprocessing import get_context as mp_get_context
<span class="w"> </span>from multiprocessing.context import BaseContext
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>from .process import LokyProcess, LokyInitMainProcess
<span class="gi">+</span>
<span class="gi">+# Apparently, on older Python versions, loky cannot work 61 workers on Windows</span>
<span class="gi">+# but instead 60: \_()_/</span>
<span class="w"> </span>if sys.version_info &gt;= (3, 8):
<span class="w"> </span>    from concurrent.futures.process import _MAX_WINDOWS_WORKERS
<span class="gi">+</span>
<span class="w"> </span>    if sys.version_info &lt; (3, 10):
<span class="w"> </span>        _MAX_WINDOWS_WORKERS = _MAX_WINDOWS_WORKERS - 1
<span class="w"> </span>else:
<span class="gi">+    # compat for versions before 3.8 which do not define this.</span>
<span class="w"> </span>    _MAX_WINDOWS_WORKERS = 60
<span class="gd">-START_METHODS = [&#39;loky&#39;, &#39;loky_init_main&#39;, &#39;spawn&#39;]</span>
<span class="gd">-if sys.platform != &#39;win32&#39;:</span>
<span class="gd">-    START_METHODS += [&#39;fork&#39;, &#39;forkserver&#39;]</span>
<span class="gi">+</span>
<span class="gi">+START_METHODS = [&quot;loky&quot;, &quot;loky_init_main&quot;, &quot;spawn&quot;]</span>
<span class="gi">+if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+    START_METHODS += [&quot;fork&quot;, &quot;forkserver&quot;]</span>
<span class="gi">+</span>
<span class="w"> </span>_DEFAULT_START_METHOD = None
<span class="gi">+</span>
<span class="gi">+# Cache for the number of physical cores to avoid repeating subprocess calls.</span>
<span class="gi">+# It should not change during the lifetime of the program.</span>
<span class="w"> </span>physical_cores_cache = None


<span class="gi">+def get_context(method=None):</span>
<span class="gi">+    # Try to overload the default context</span>
<span class="gi">+    method = method or _DEFAULT_START_METHOD or &quot;loky&quot;</span>
<span class="gi">+    if method == &quot;fork&quot;:</span>
<span class="gi">+        # If &#39;fork&#39; is explicitly requested, warn user about potential issues.</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`fork` start method should not be used with &quot;</span>
<span class="gi">+            &quot;`loky` as it does not respect POSIX. Try using &quot;</span>
<span class="gi">+            &quot;`spawn` or `loky` instead.&quot;,</span>
<span class="gi">+            UserWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+    try:</span>
<span class="gi">+        return mp_get_context(method)</span>
<span class="gi">+    except ValueError:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;Unknown context &#39;{method}&#39;. Value should be in &quot;</span>
<span class="gi">+            f&quot;{START_METHODS}.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def set_start_method(method, force=False):</span>
<span class="gi">+    global _DEFAULT_START_METHOD</span>
<span class="gi">+    if _DEFAULT_START_METHOD is not None and not force:</span>
<span class="gi">+        raise RuntimeError(&quot;context has already been set&quot;)</span>
<span class="gi">+    assert method is None or method in START_METHODS, (</span>
<span class="gi">+        f&quot;&#39;{method}&#39; is not a valid start_method. It should be in &quot;</span>
<span class="gi">+        f&quot;{START_METHODS}&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    _DEFAULT_START_METHOD = method</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_start_method():</span>
<span class="gi">+    return _DEFAULT_START_METHOD</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def cpu_count(only_physical_cores=False):
<span class="w"> </span>    &quot;&quot;&quot;Return the number of CPUs the current process can use.

<span class="gu">@@ -47,12 +106,127 @@ def cpu_count(only_physical_cores=False):</span>

<span class="w"> </span>    It is also always larger or equal to 1.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Note: os.cpu_count() is allowed to return None in its docstring</span>
<span class="gi">+    os_cpu_count = os.cpu_count() or 1</span>
<span class="gi">+    if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+        # On Windows, attempting to use more than 61 CPUs would result in a</span>
<span class="gi">+        # OS-level error. See https://bugs.python.org/issue26903. According to</span>
<span class="gi">+        # https://learn.microsoft.com/en-us/windows/win32/procthread/processor-groups</span>
<span class="gi">+        # it might be possible to go beyond with a lot of extra work but this</span>
<span class="gi">+        # does not look easy.</span>
<span class="gi">+        os_cpu_count = min(os_cpu_count, _MAX_WINDOWS_WORKERS)</span>
<span class="gi">+</span>
<span class="gi">+    cpu_count_user = _cpu_count_user(os_cpu_count)</span>
<span class="gi">+    aggregate_cpu_count = max(min(os_cpu_count, cpu_count_user), 1)</span>
<span class="gi">+</span>
<span class="gi">+    if not only_physical_cores:</span>
<span class="gi">+        return aggregate_cpu_count</span>
<span class="gi">+</span>
<span class="gi">+    if cpu_count_user &lt; os_cpu_count:</span>
<span class="gi">+        # Respect user setting</span>
<span class="gi">+        return max(cpu_count_user, 1)</span>
<span class="gi">+</span>
<span class="gi">+    cpu_count_physical, exception = _count_physical_cores()</span>
<span class="gi">+    if cpu_count_physical != &quot;not found&quot;:</span>
<span class="gi">+        return cpu_count_physical</span>
<span class="gi">+</span>
<span class="gi">+    # Fallback to default behavior</span>
<span class="gi">+    if exception is not None:</span>
<span class="gi">+        # warns only the first time</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;Could not find the number of physical cores for the &quot;</span>
<span class="gi">+            f&quot;following reason:\n{exception}\n&quot;</span>
<span class="gi">+            &quot;Returning the number of logical cores instead. You can &quot;</span>
<span class="gi">+            &quot;silence this warning by setting LOKY_MAX_CPU_COUNT to &quot;</span>
<span class="gi">+            &quot;the number of cores you want to use.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        traceback.print_tb(exception.__traceback__)</span>
<span class="gi">+</span>
<span class="gi">+    return aggregate_cpu_count</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _cpu_count_cgroup(os_cpu_count):</span>
<span class="gi">+    # Cgroup CPU bandwidth limit available in Linux since 2.6 kernel</span>
<span class="gi">+    cpu_max_fname = &quot;/sys/fs/cgroup/cpu.max&quot;</span>
<span class="gi">+    cfs_quota_fname = &quot;/sys/fs/cgroup/cpu/cpu.cfs_quota_us&quot;</span>
<span class="gi">+    cfs_period_fname = &quot;/sys/fs/cgroup/cpu/cpu.cfs_period_us&quot;</span>
<span class="gi">+    if os.path.exists(cpu_max_fname):</span>
<span class="gi">+        # cgroup v2</span>
<span class="gi">+        # https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html</span>
<span class="gi">+        with open(cpu_max_fname) as fh:</span>
<span class="gi">+            cpu_quota_us, cpu_period_us = fh.read().strip().split()</span>
<span class="gi">+    elif os.path.exists(cfs_quota_fname) and os.path.exists(cfs_period_fname):</span>
<span class="gi">+        # cgroup v1</span>
<span class="gi">+        # https://www.kernel.org/doc/html/latest/scheduler/sched-bwc.html#management</span>
<span class="gi">+        with open(cfs_quota_fname) as fh:</span>
<span class="gi">+            cpu_quota_us = fh.read().strip()</span>
<span class="gi">+        with open(cfs_period_fname) as fh:</span>
<span class="gi">+            cpu_period_us = fh.read().strip()</span>
<span class="gi">+    else:</span>
<span class="gi">+        # No Cgroup CPU bandwidth limit (e.g. non-Linux platform)</span>
<span class="gi">+        cpu_quota_us = &quot;max&quot;</span>
<span class="gi">+        cpu_period_us = 100_000  # unused, for consistency with default values</span>
<span class="gi">+</span>
<span class="gi">+    if cpu_quota_us == &quot;max&quot;:</span>
<span class="gi">+        # No active Cgroup quota on a Cgroup-capable platform</span>
<span class="gi">+        return os_cpu_count</span>
<span class="gi">+    else:</span>
<span class="gi">+        cpu_quota_us = int(cpu_quota_us)</span>
<span class="gi">+        cpu_period_us = int(cpu_period_us)</span>
<span class="gi">+        if cpu_quota_us &gt; 0 and cpu_period_us &gt; 0:</span>
<span class="gi">+            return math.ceil(cpu_quota_us / cpu_period_us)</span>
<span class="gi">+        else:  # pragma: no cover</span>
<span class="gi">+            # Setting a negative cpu_quota_us value is a valid way to disable</span>
<span class="gi">+            # cgroup CPU bandwith limits</span>
<span class="gi">+            return os_cpu_count</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _cpu_count_affinity(os_cpu_count):</span>
<span class="gi">+    # Number of available CPUs given affinity settings</span>
<span class="gi">+    if hasattr(os, &quot;sched_getaffinity&quot;):</span>
<span class="gi">+        try:</span>
<span class="gi">+            return len(os.sched_getaffinity(0))</span>
<span class="gi">+        except NotImplementedError:</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+    # On PyPy and possibly other platforms, os.sched_getaffinity does not exist</span>
<span class="gi">+    # or raises NotImplementedError, let&#39;s try with the psutil if installed.</span>
<span class="gi">+    try:</span>
<span class="gi">+        import psutil</span>
<span class="gi">+</span>
<span class="gi">+        p = psutil.Process()</span>
<span class="gi">+        if hasattr(p, &quot;cpu_affinity&quot;):</span>
<span class="gi">+            return len(p.cpu_affinity())</span>
<span class="gi">+</span>
<span class="gi">+    except ImportError:  # pragma: no cover</span>
<span class="gi">+        if (</span>
<span class="gi">+            sys.platform == &quot;linux&quot;</span>
<span class="gi">+            and os.environ.get(&quot;LOKY_MAX_CPU_COUNT&quot;) is None</span>
<span class="gi">+        ):</span>
<span class="gi">+            # PyPy does not implement os.sched_getaffinity on Linux which</span>
<span class="gi">+            # can cause severe oversubscription problems. Better warn the</span>
<span class="gi">+            # user in this particularly pathological case which can wreck</span>
<span class="gi">+            # havoc, typically on CI workers.</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;Failed to inspect CPU affinity constraints on this system. &quot;</span>
<span class="gi">+                &quot;Please install psutil or explictly set LOKY_MAX_CPU_COUNT.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    # This can happen for platforms that do not implement any kind of CPU</span>
<span class="gi">+    # infinity such as macOS-based platforms.</span>
<span class="gi">+    return os_cpu_count</span>


<span class="w"> </span>def _cpu_count_user(os_cpu_count):
<span class="w"> </span>    &quot;&quot;&quot;Number of user defined available CPUs&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cpu_count_affinity = _cpu_count_affinity(os_cpu_count)</span>
<span class="gi">+</span>
<span class="gi">+    cpu_count_cgroup = _cpu_count_cgroup(os_cpu_count)</span>
<span class="gi">+</span>
<span class="gi">+    # User defined soft-limit passed as a loky specific environment variable.</span>
<span class="gi">+    cpu_count_loky = int(os.environ.get(&quot;LOKY_MAX_CPU_COUNT&quot;, os_cpu_count))</span>
<span class="gi">+</span>
<span class="gi">+    return min(cpu_count_affinity, cpu_count_cgroup, cpu_count_loky)</span>


<span class="w"> </span>def _count_physical_cores():
<span class="gu">@@ -63,23 +237,80 @@ def _count_physical_cores():</span>

<span class="w"> </span>    The number of physical cores is cached to avoid repeating subprocess calls.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    exception = None</span>
<span class="gi">+</span>
<span class="gi">+    # First check if the value is cached</span>
<span class="gi">+    global physical_cores_cache</span>
<span class="gi">+    if physical_cores_cache is not None:</span>
<span class="gi">+        return physical_cores_cache, exception</span>
<span class="gi">+</span>
<span class="gi">+    # Not cached yet, find it</span>
<span class="gi">+    try:</span>
<span class="gi">+        if sys.platform == &quot;linux&quot;:</span>
<span class="gi">+            cpu_info = subprocess.run(</span>
<span class="gi">+                &quot;lscpu --parse=core&quot;.split(), capture_output=True, text=True</span>
<span class="gi">+            )</span>
<span class="gi">+            cpu_info = cpu_info.stdout.splitlines()</span>
<span class="gi">+            cpu_info = {line for line in cpu_info if not line.startswith(&quot;#&quot;)}</span>
<span class="gi">+            cpu_count_physical = len(cpu_info)</span>
<span class="gi">+        elif sys.platform == &quot;win32&quot;:</span>
<span class="gi">+            cpu_info = subprocess.run(</span>
<span class="gi">+                &quot;wmic CPU Get NumberOfCores /Format:csv&quot;.split(),</span>
<span class="gi">+                capture_output=True,</span>
<span class="gi">+                text=True,</span>
<span class="gi">+            )</span>
<span class="gi">+            cpu_info = cpu_info.stdout.splitlines()</span>
<span class="gi">+            cpu_info = [</span>
<span class="gi">+                l.split(&quot;,&quot;)[1]</span>
<span class="gi">+                for l in cpu_info</span>
<span class="gi">+                if (l and l != &quot;Node,NumberOfCores&quot;)</span>
<span class="gi">+            ]</span>
<span class="gi">+            cpu_count_physical = sum(map(int, cpu_info))</span>
<span class="gi">+        elif sys.platform == &quot;darwin&quot;:</span>
<span class="gi">+            cpu_info = subprocess.run(</span>
<span class="gi">+                &quot;sysctl -n hw.physicalcpu&quot;.split(),</span>
<span class="gi">+                capture_output=True,</span>
<span class="gi">+                text=True,</span>
<span class="gi">+            )</span>
<span class="gi">+            cpu_info = cpu_info.stdout</span>
<span class="gi">+            cpu_count_physical = int(cpu_info)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise NotImplementedError(f&quot;unsupported platform: {sys.platform}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # if cpu_count_physical &lt; 1, we did not find a valid value</span>
<span class="gi">+        if cpu_count_physical &lt; 1:</span>
<span class="gi">+            raise ValueError(f&quot;found {cpu_count_physical} physical cores &lt; 1&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    except Exception as e:</span>
<span class="gi">+        exception = e</span>
<span class="gi">+        cpu_count_physical = &quot;not found&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # Put the result in cache</span>
<span class="gi">+    physical_cores_cache = cpu_count_physical</span>
<span class="gi">+</span>
<span class="gi">+    return cpu_count_physical, exception</span>


<span class="w"> </span>class LokyContext(BaseContext):
<span class="w"> </span>    &quot;&quot;&quot;Context relying on the LokyProcess.&quot;&quot;&quot;
<span class="gd">-    _name = &#39;loky&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _name = &quot;loky&quot;</span>
<span class="w"> </span>    Process = LokyProcess
<span class="w"> </span>    cpu_count = staticmethod(cpu_count)

<span class="w"> </span>    def Queue(self, maxsize=0, reducers=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns a queue object&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        from .queues import Queue</span>
<span class="gi">+</span>
<span class="gi">+        return Queue(maxsize, reducers=reducers, ctx=self.get_context())</span>

<span class="w"> </span>    def SimpleQueue(self, reducers=None):
<span class="w"> </span>        &quot;&quot;&quot;Returns a queue object&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-    if sys.platform != &#39;win32&#39;:</span>
<span class="gi">+        from .queues import SimpleQueue</span>
<span class="gi">+</span>
<span class="gi">+        return SimpleQueue(reducers=reducers, ctx=self.get_context())</span>
<span class="gi">+</span>
<span class="gi">+    if sys.platform != &quot;win32&quot;:</span>
<span class="w"> </span>        &quot;&quot;&quot;For Unix platform, use our custom implementation of synchronize
<span class="w"> </span>        ensuring that we use the loky.backend.resource_tracker to clean-up
<span class="w"> </span>        the semaphores in case of a worker crash.
<span class="gu">@@ -87,27 +318,39 @@ class LokyContext(BaseContext):</span>

<span class="w"> </span>        def Semaphore(self, value=1):
<span class="w"> </span>            &quot;&quot;&quot;Returns a semaphore object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import Semaphore</span>
<span class="gi">+</span>
<span class="gi">+            return Semaphore(value=value)</span>

<span class="w"> </span>        def BoundedSemaphore(self, value):
<span class="w"> </span>            &quot;&quot;&quot;Returns a bounded semaphore object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import BoundedSemaphore</span>
<span class="gi">+</span>
<span class="gi">+            return BoundedSemaphore(value)</span>

<span class="w"> </span>        def Lock(self):
<span class="w"> </span>            &quot;&quot;&quot;Returns a lock object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import Lock</span>
<span class="gi">+</span>
<span class="gi">+            return Lock()</span>

<span class="w"> </span>        def RLock(self):
<span class="w"> </span>            &quot;&quot;&quot;Returns a recurrent lock object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import RLock</span>
<span class="gi">+</span>
<span class="gi">+            return RLock()</span>

<span class="w"> </span>        def Condition(self, lock=None):
<span class="w"> </span>            &quot;&quot;&quot;Returns a condition object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import Condition</span>
<span class="gi">+</span>
<span class="gi">+            return Condition(lock)</span>

<span class="w"> </span>        def Event(self):
<span class="w"> </span>            &quot;&quot;&quot;Returns an event object&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            from .synchronize import Event</span>
<span class="gi">+</span>
<span class="gi">+            return Event()</span>


<span class="w"> </span>class LokyInitMainContext(LokyContext):
<span class="gu">@@ -124,10 +367,12 @@ class LokyInitMainContext(LokyContext):</span>
<span class="w"> </span>    For more details, see the end of the following section of python doc
<span class="w"> </span>    https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _name = &#39;loky_init_main&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _name = &quot;loky_init_main&quot;</span>
<span class="w"> </span>    Process = LokyInitMainProcess


<span class="gi">+# Register loky context so it works with multiprocessing.get_context</span>
<span class="w"> </span>ctx_loky = LokyContext()
<span class="gd">-mp.context._concrete_contexts[&#39;loky&#39;] = ctx_loky</span>
<span class="gd">-mp.context._concrete_contexts[&#39;loky_init_main&#39;] = LokyInitMainContext()</span>
<span class="gi">+mp.context._concrete_contexts[&quot;loky&quot;] = ctx_loky</span>
<span class="gi">+mp.context._concrete_contexts[&quot;loky_init_main&quot;] = LokyInitMainContext()</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/fork_exec.py b/joblib/externals/loky/backend/fork_exec.py</span>
<span class="gh">index a8af34a..2353c42 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/fork_exec.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/fork_exec.py</span>
<span class="gu">@@ -1,7 +1,43 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Launch a subprocess using forkexec and make sure only the needed fd are</span>
<span class="gi">+# shared in the two process.</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys


<span class="gd">-def close_fds(keep_fds):</span>
<span class="gi">+def close_fds(keep_fds):  # pragma: no cover</span>
<span class="w"> </span>    &quot;&quot;&quot;Close all the file descriptors except those in keep_fds.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    # Make sure to keep stdout and stderr open for logging purpose</span>
<span class="gi">+    keep_fds = {*keep_fds, 1, 2}</span>
<span class="gi">+</span>
<span class="gi">+    # We try to retrieve all the open fds</span>
<span class="gi">+    try:</span>
<span class="gi">+        open_fds = {int(fd) for fd in os.listdir(&quot;/proc/self/fd&quot;)}</span>
<span class="gi">+    except FileNotFoundError:</span>
<span class="gi">+        import resource</span>
<span class="gi">+</span>
<span class="gi">+        max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]</span>
<span class="gi">+        open_fds = {*range(max_nfds)}</span>
<span class="gi">+</span>
<span class="gi">+    for i in open_fds - keep_fds:</span>
<span class="gi">+        try:</span>
<span class="gi">+            os.close(i)</span>
<span class="gi">+        except OSError:</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def fork_exec(cmd, keep_fds, env=None):</span>
<span class="gi">+    # copy the environment variables to set in the child process</span>
<span class="gi">+    env = env or {}</span>
<span class="gi">+    child_env = {**os.environ, **env}</span>
<span class="gi">+</span>
<span class="gi">+    pid = os.fork()</span>
<span class="gi">+    if pid == 0:  # pragma: no cover</span>
<span class="gi">+        close_fds(keep_fds)</span>
<span class="gi">+        os.execve(sys.executable, cmd, child_env)</span>
<span class="gi">+    else:</span>
<span class="gi">+        return pid</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/popen_loky_posix.py b/joblib/externals/loky/backend/popen_loky_posix.py</span>
<span class="gh">index ef2f2b6..74395be 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/popen_loky_posix.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/popen_loky_posix.py</span>
<span class="gu">@@ -1,3 +1,8 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Popen for LokyProcess.</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import signal
<span class="gu">@@ -6,18 +11,33 @@ from io import BytesIO</span>
<span class="w"> </span>from multiprocessing import util, process
<span class="w"> </span>from multiprocessing.connection import wait
<span class="w"> </span>from multiprocessing.context import set_spawning_popen
<span class="gi">+</span>
<span class="w"> </span>from . import reduction, resource_tracker, spawn
<span class="gd">-__all__ = [&#39;Popen&#39;]</span>


<span class="gd">-class _DupFd:</span>
<span class="gi">+__all__ = [&quot;Popen&quot;]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Wrapper for an fd used while launching a process</span>
<span class="gi">+#</span>

<span class="gi">+</span>
<span class="gi">+class _DupFd:</span>
<span class="w"> </span>    def __init__(self, fd):
<span class="w"> </span>        self.fd = reduction._mk_inheritable(fd)

<span class="gi">+    def detach(self):</span>
<span class="gi">+        return self.fd</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Start child process using subprocess.Popen</span>
<span class="gi">+#</span>
<span class="gi">+</span>

<span class="w"> </span>class Popen:
<span class="gd">-    method = &#39;loky&#39;</span>
<span class="gi">+    method = &quot;loky&quot;</span>
<span class="w"> </span>    DupFd = _DupFd

<span class="w"> </span>    def __init__(self, process_obj):
<span class="gu">@@ -27,19 +47,128 @@ class Popen:</span>
<span class="w"> </span>        self._fds = []
<span class="w"> </span>        self._launch(process_obj)

<span class="gi">+    def duplicate_for_child(self, fd):</span>
<span class="gi">+        self._fds.append(fd)</span>
<span class="gi">+        return reduction._mk_inheritable(fd)</span>
<span class="gi">+</span>
<span class="gi">+    def poll(self, flag=os.WNOHANG):</span>
<span class="gi">+        if self.returncode is None:</span>
<span class="gi">+            while True:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    pid, sts = os.waitpid(self.pid, flag)</span>
<span class="gi">+                except OSError:</span>
<span class="gi">+                    # Child process not yet created. See #1731717</span>
<span class="gi">+                    # e.errno == errno.ECHILD == 10</span>
<span class="gi">+                    return None</span>
<span class="gi">+                else:</span>
<span class="gi">+                    break</span>
<span class="gi">+            if pid == self.pid:</span>
<span class="gi">+                if os.WIFSIGNALED(sts):</span>
<span class="gi">+                    self.returncode = -os.WTERMSIG(sts)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    assert os.WIFEXITED(sts)</span>
<span class="gi">+                    self.returncode = os.WEXITSTATUS(sts)</span>
<span class="gi">+        return self.returncode</span>
<span class="gi">+</span>
<span class="gi">+    def wait(self, timeout=None):</span>
<span class="gi">+        if self.returncode is None:</span>
<span class="gi">+            if timeout is not None:</span>
<span class="gi">+                if not wait([self.sentinel], timeout):</span>
<span class="gi">+                    return None</span>
<span class="gi">+            # This shouldn&#39;t block if wait() returned successfully.</span>
<span class="gi">+            return self.poll(os.WNOHANG if timeout == 0.0 else 0)</span>
<span class="gi">+        return self.returncode</span>
<span class="gi">+</span>
<span class="gi">+    def terminate(self):</span>
<span class="gi">+        if self.returncode is None:</span>
<span class="gi">+            try:</span>
<span class="gi">+                os.kill(self.pid, signal.SIGTERM)</span>
<span class="gi">+            except ProcessLookupError:</span>
<span class="gi">+                pass</span>
<span class="gi">+            except OSError:</span>
<span class="gi">+                if self.wait(timeout=0.1) is None:</span>
<span class="gi">+                    raise</span>
<span class="gi">+</span>
<span class="gi">+    def _launch(self, process_obj):</span>
<span class="gi">+</span>
<span class="gi">+        tracker_fd = resource_tracker._resource_tracker.getfd()</span>

<span class="gd">-if __name__ == &#39;__main__&#39;:</span>
<span class="gi">+        fp = BytesIO()</span>
<span class="gi">+        set_spawning_popen(self)</span>
<span class="gi">+        try:</span>
<span class="gi">+            prep_data = spawn.get_preparation_data(</span>
<span class="gi">+                process_obj._name,</span>
<span class="gi">+                getattr(process_obj, &quot;init_main_module&quot;, True),</span>
<span class="gi">+            )</span>
<span class="gi">+            reduction.dump(prep_data, fp)</span>
<span class="gi">+            reduction.dump(process_obj, fp)</span>
<span class="gi">+</span>
<span class="gi">+        finally:</span>
<span class="gi">+            set_spawning_popen(None)</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            parent_r, child_w = os.pipe()</span>
<span class="gi">+            child_r, parent_w = os.pipe()</span>
<span class="gi">+            # for fd in self._fds:</span>
<span class="gi">+            #     _mk_inheritable(fd)</span>
<span class="gi">+</span>
<span class="gi">+            cmd_python = [sys.executable]</span>
<span class="gi">+            cmd_python += [&quot;-m&quot;, self.__module__]</span>
<span class="gi">+            cmd_python += [&quot;--process-name&quot;, str(process_obj.name)]</span>
<span class="gi">+            cmd_python += [&quot;--pipe&quot;, str(reduction._mk_inheritable(child_r))]</span>
<span class="gi">+            reduction._mk_inheritable(child_w)</span>
<span class="gi">+            reduction._mk_inheritable(tracker_fd)</span>
<span class="gi">+            self._fds += [child_r, child_w, tracker_fd]</span>
<span class="gi">+            if sys.version_info &gt;= (3, 8) and os.name == &quot;posix&quot;:</span>
<span class="gi">+                mp_tracker_fd = prep_data[&quot;mp_tracker_args&quot;][&quot;fd&quot;]</span>
<span class="gi">+                self.duplicate_for_child(mp_tracker_fd)</span>
<span class="gi">+</span>
<span class="gi">+            from .fork_exec import fork_exec</span>
<span class="gi">+</span>
<span class="gi">+            pid = fork_exec(cmd_python, self._fds, env=process_obj.env)</span>
<span class="gi">+            util.debug(</span>
<span class="gi">+                f&quot;launched python with pid {pid} and cmd:\n{cmd_python}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            self.sentinel = parent_r</span>
<span class="gi">+</span>
<span class="gi">+            method = &quot;getbuffer&quot;</span>
<span class="gi">+            if not hasattr(fp, method):</span>
<span class="gi">+                method = &quot;getvalue&quot;</span>
<span class="gi">+            with os.fdopen(parent_w, &quot;wb&quot;) as f:</span>
<span class="gi">+                f.write(getattr(fp, method)())</span>
<span class="gi">+            self.pid = pid</span>
<span class="gi">+        finally:</span>
<span class="gi">+            if parent_r is not None:</span>
<span class="gi">+                util.Finalize(self, os.close, (parent_r,))</span>
<span class="gi">+            for fd in (child_r, child_w):</span>
<span class="gi">+                if fd is not None:</span>
<span class="gi">+                    os.close(fd)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def thread_is_spawning():</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+if __name__ == &quot;__main__&quot;:</span>
<span class="w"> </span>    import argparse
<span class="gd">-    parser = argparse.ArgumentParser(&#39;Command line parser&#39;)</span>
<span class="gd">-    parser.add_argument(&#39;--pipe&#39;, type=int, required=True, help=</span>
<span class="gd">-        &#39;File handle for the pipe&#39;)</span>
<span class="gd">-    parser.add_argument(&#39;--process-name&#39;, type=str, default=None, help=</span>
<span class="gd">-        &#39;Identifier for debugging purpose&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    parser = argparse.ArgumentParser(&quot;Command line parser&quot;)</span>
<span class="gi">+    parser.add_argument(</span>
<span class="gi">+        &quot;--pipe&quot;, type=int, required=True, help=&quot;File handle for the pipe&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    parser.add_argument(</span>
<span class="gi">+        &quot;--process-name&quot;,</span>
<span class="gi">+        type=str,</span>
<span class="gi">+        default=None,</span>
<span class="gi">+        help=&quot;Identifier for debugging purpose&quot;,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="w"> </span>    args = parser.parse_args()
<span class="gi">+</span>
<span class="w"> </span>    info = {}
<span class="w"> </span>    exitcode = 1
<span class="w"> </span>    try:
<span class="gd">-        with os.fdopen(args.pipe, &#39;rb&#39;) as from_parent:</span>
<span class="gi">+        with os.fdopen(args.pipe, &quot;rb&quot;) as from_parent:</span>
<span class="w"> </span>            process.current_process()._inheriting = True
<span class="w"> </span>            try:
<span class="w"> </span>                prep_data = pickle.load(from_parent)
<span class="gu">@@ -47,15 +176,18 @@ if __name__ == &#39;__main__&#39;:</span>
<span class="w"> </span>                process_obj = pickle.load(from_parent)
<span class="w"> </span>            finally:
<span class="w"> </span>                del process.current_process()._inheriting
<span class="gi">+</span>
<span class="w"> </span>        exitcode = process_obj._bootstrap()
<span class="w"> </span>    except Exception:
<span class="gd">-        print(&#39;\n\n&#39; + &#39;-&#39; * 80)</span>
<span class="gd">-        print(f&#39;{args.process_name} failed with traceback: &#39;)</span>
<span class="gd">-        print(&#39;-&#39; * 80)</span>
<span class="gi">+        print(&quot;\n\n&quot; + &quot;-&quot; * 80)</span>
<span class="gi">+        print(f&quot;{args.process_name} failed with traceback: &quot;)</span>
<span class="gi">+        print(&quot;-&quot; * 80)</span>
<span class="w"> </span>        import traceback
<span class="gi">+</span>
<span class="w"> </span>        print(traceback.format_exc())
<span class="gd">-        print(&#39;\n&#39; + &#39;-&#39; * 80)</span>
<span class="gi">+        print(&quot;\n&quot; + &quot;-&quot; * 80)</span>
<span class="w"> </span>    finally:
<span class="w"> </span>        if from_parent is not None:
<span class="w"> </span>            from_parent.close()
<span class="gi">+</span>
<span class="w"> </span>        sys.exit(exitcode)
<span class="gh">diff --git a/joblib/externals/loky/backend/popen_loky_win32.py b/joblib/externals/loky/backend/popen_loky_win32.py</span>
<span class="gh">index b174751..4f85f65 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/popen_loky_win32.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/popen_loky_win32.py</span>
<span class="gu">@@ -6,10 +6,35 @@ from pickle import load</span>
<span class="w"> </span>from multiprocessing import process, util
<span class="w"> </span>from multiprocessing.context import set_spawning_popen
<span class="w"> </span>from multiprocessing.popen_spawn_win32 import Popen as _Popen
<span class="gi">+</span>
<span class="w"> </span>from . import reduction, spawn
<span class="gd">-__all__ = [&#39;Popen&#39;]</span>
<span class="gd">-WINENV = hasattr(sys, &#39;_base_executable&#39;) and not _path_eq(sys.executable,</span>
<span class="gd">-    sys._base_executable)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;Popen&quot;]</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+#</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _path_eq(p1, p2):</span>
<span class="gi">+    return p1 == p2 or os.path.normcase(p1) == os.path.normcase(p2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+WINENV = hasattr(sys, &quot;_base_executable&quot;) and not _path_eq(</span>
<span class="gi">+    sys.executable, sys._base_executable</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _close_handles(*handles):</span>
<span class="gi">+    for handle in handles:</span>
<span class="gi">+        _winapi.CloseHandle(handle)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# We define a Popen class similar to the one from subprocess, but</span>
<span class="gi">+# whose constructor takes a process object as its argument.</span>
<span class="gi">+#</span>


<span class="w"> </span>class Popen(_Popen):
<span class="gu">@@ -24,34 +49,66 @@ class Popen(_Popen):</span>
<span class="w"> </span>    We also use the loky preparation data, in particular to handle main_module
<span class="w"> </span>    inits and the loky resource tracker.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    method = &#39;loky&#39;</span>
<span class="gi">+</span>
<span class="gi">+    method = &quot;loky&quot;</span>

<span class="w"> </span>    def __init__(self, process_obj):
<span class="gd">-        prep_data = spawn.get_preparation_data(process_obj._name, getattr(</span>
<span class="gd">-            process_obj, &#39;init_main_module&#39;, True))</span>
<span class="gi">+        prep_data = spawn.get_preparation_data(</span>
<span class="gi">+            process_obj._name, getattr(process_obj, &quot;init_main_module&quot;, True)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # read end of pipe will be duplicated by the child process</span>
<span class="gi">+        # -- see spawn_main() in spawn.py.</span>
<span class="gi">+        #</span>
<span class="gi">+        # bpo-33929: Previously, the read end of pipe was &quot;stolen&quot; by the child</span>
<span class="gi">+        # process, but it leaked a handle if the child process had been</span>
<span class="gi">+        # terminated before it could steal the handle from the parent process.</span>
<span class="w"> </span>        rhandle, whandle = _winapi.CreatePipe(None, 0)
<span class="w"> </span>        wfd = msvcrt.open_osfhandle(whandle, 0)
<span class="w"> </span>        cmd = get_command_line(parent_pid=os.getpid(), pipe_handle=rhandle)
<span class="gi">+</span>
<span class="w"> </span>        python_exe = spawn.get_executable()
<span class="gi">+</span>
<span class="gi">+        # copy the environment variables to set in the child process</span>
<span class="w"> </span>        child_env = {**os.environ, **process_obj.env}
<span class="gi">+</span>
<span class="gi">+        # bpo-35797: When running in a venv, we bypass the redirect</span>
<span class="gi">+        # executor and launch our base Python.</span>
<span class="w"> </span>        if WINENV and _path_eq(python_exe, sys.executable):
<span class="w"> </span>            cmd[0] = python_exe = sys._base_executable
<span class="gd">-            child_env[&#39;__PYVENV_LAUNCHER__&#39;] = sys.executable</span>
<span class="gd">-        cmd = &#39; &#39;.join(f&#39;&quot;{x}&quot;&#39; for x in cmd)</span>
<span class="gd">-        with open(wfd, &#39;wb&#39;) as to_child:</span>
<span class="gi">+            child_env[&quot;__PYVENV_LAUNCHER__&quot;] = sys.executable</span>
<span class="gi">+</span>
<span class="gi">+        cmd = &quot; &quot;.join(f&#39;&quot;{x}&quot;&#39; for x in cmd)</span>
<span class="gi">+</span>
<span class="gi">+        with open(wfd, &quot;wb&quot;) as to_child:</span>
<span class="gi">+            # start process</span>
<span class="w"> </span>            try:
<span class="gd">-                hp, ht, pid, _ = _winapi.CreateProcess(python_exe, cmd,</span>
<span class="gd">-                    None, None, False, 0, child_env, None, None)</span>
<span class="gi">+                hp, ht, pid, _ = _winapi.CreateProcess(</span>
<span class="gi">+                    python_exe,</span>
<span class="gi">+                    cmd,</span>
<span class="gi">+                    None,</span>
<span class="gi">+                    None,</span>
<span class="gi">+                    False,</span>
<span class="gi">+                    0,</span>
<span class="gi">+                    child_env,</span>
<span class="gi">+                    None,</span>
<span class="gi">+                    None,</span>
<span class="gi">+                )</span>
<span class="w"> </span>                _winapi.CloseHandle(ht)
<span class="w"> </span>            except BaseException:
<span class="w"> </span>                _winapi.CloseHandle(rhandle)
<span class="w"> </span>                raise
<span class="gi">+</span>
<span class="gi">+            # set attributes of self</span>
<span class="w"> </span>            self.pid = pid
<span class="w"> </span>            self.returncode = None
<span class="w"> </span>            self._handle = hp
<span class="w"> </span>            self.sentinel = int(hp)
<span class="gd">-            self.finalizer = util.Finalize(self, _close_handles, (self.</span>
<span class="gd">-                sentinel, int(rhandle)))</span>
<span class="gi">+            self.finalizer = util.Finalize(</span>
<span class="gi">+                self, _close_handles, (self.sentinel, int(rhandle))</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            # send information to child</span>
<span class="w"> </span>            set_spawning_popen(self)
<span class="w"> </span>            try:
<span class="w"> </span>                reduction.dump(prep_data, to_child)
<span class="gu">@@ -62,14 +119,55 @@ class Popen(_Popen):</span>

<span class="w"> </span>def get_command_line(pipe_handle, parent_pid, **kwds):
<span class="w"> </span>    &quot;&quot;&quot;Returns prefix of command line used for spawning a child process.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if getattr(sys, &quot;frozen&quot;, False):</span>
<span class="gi">+        return [sys.executable, &quot;--multiprocessing-fork&quot;, pipe_handle]</span>
<span class="gi">+    else:</span>
<span class="gi">+        prog = (</span>
<span class="gi">+            &quot;from joblib.externals.loky.backend.popen_loky_win32 import main; &quot;</span>
<span class="gi">+            f&quot;main(pipe_handle={pipe_handle}, parent_pid={parent_pid})&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        opts = util._args_from_interpreter_flags()</span>
<span class="gi">+        return [</span>
<span class="gi">+            spawn.get_executable(),</span>
<span class="gi">+            *opts,</span>
<span class="gi">+            &quot;-c&quot;,</span>
<span class="gi">+            prog,</span>
<span class="gi">+            &quot;--multiprocessing-fork&quot;,</span>
<span class="gi">+        ]</span>


<span class="w"> </span>def is_forking(argv):
<span class="w"> </span>    &quot;&quot;&quot;Return whether commandline indicates we are forking.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if len(argv) &gt;= 2 and argv[1] == &quot;--multiprocessing-fork&quot;:</span>
<span class="gi">+        return True</span>
<span class="gi">+    else:</span>
<span class="gi">+        return False</span>


<span class="w"> </span>def main(pipe_handle, parent_pid=None):
<span class="w"> </span>    &quot;&quot;&quot;Run code specified by data received over pipe.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    assert is_forking(sys.argv), &quot;Not forking&quot;</span>
<span class="gi">+</span>
<span class="gi">+    if parent_pid is not None:</span>
<span class="gi">+        source_process = _winapi.OpenProcess(</span>
<span class="gi">+            _winapi.SYNCHRONIZE | _winapi.PROCESS_DUP_HANDLE, False, parent_pid</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        source_process = None</span>
<span class="gi">+    new_handle = reduction.duplicate(</span>
<span class="gi">+        pipe_handle, source_process=source_process</span>
<span class="gi">+    )</span>
<span class="gi">+    fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)</span>
<span class="gi">+    parent_sentinel = source_process</span>
<span class="gi">+</span>
<span class="gi">+    with os.fdopen(fd, &quot;rb&quot;, closefd=True) as from_parent:</span>
<span class="gi">+        process.current_process()._inheriting = True</span>
<span class="gi">+        try:</span>
<span class="gi">+            preparation_data = load(from_parent)</span>
<span class="gi">+            spawn.prepare(preparation_data, parent_sentinel)</span>
<span class="gi">+            self = load(from_parent)</span>
<span class="gi">+        finally:</span>
<span class="gi">+            del process.current_process()._inheriting</span>
<span class="gi">+</span>
<span class="gi">+    exitcode = self._bootstrap(parent_sentinel)</span>
<span class="gi">+    sys.exit(exitcode)</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/process.py b/joblib/externals/loky/backend/process.py</span>
<span class="gh">index 26a97f9..3562550 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/process.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/process.py</span>
<span class="gu">@@ -1,36 +1,85 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# LokyProcess implementation</span>
<span class="gi">+#</span>
<span class="gi">+# authors: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# based on multiprocessing/process.py  (17/02/2017)</span>
<span class="gi">+#</span>
<span class="w"> </span>import sys
<span class="w"> </span>from multiprocessing.context import assert_spawning
<span class="w"> </span>from multiprocessing.process import BaseProcess


<span class="w"> </span>class LokyProcess(BaseProcess):
<span class="gd">-    _start_method = &#39;loky&#39;</span>
<span class="gi">+    _start_method = &quot;loky&quot;</span>

<span class="gd">-    def __init__(self, group=None, target=None, name=None, args=(), kwargs=</span>
<span class="gd">-        {}, daemon=None, init_main_module=False, env=None):</span>
<span class="gd">-        super().__init__(group=group, target=target, name=name, args=args,</span>
<span class="gd">-            kwargs=kwargs, daemon=daemon)</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        group=None,</span>
<span class="gi">+        target=None,</span>
<span class="gi">+        name=None,</span>
<span class="gi">+        args=(),</span>
<span class="gi">+        kwargs={},</span>
<span class="gi">+        daemon=None,</span>
<span class="gi">+        init_main_module=False,</span>
<span class="gi">+        env=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            group=group,</span>
<span class="gi">+            target=target,</span>
<span class="gi">+            name=name,</span>
<span class="gi">+            args=args,</span>
<span class="gi">+            kwargs=kwargs,</span>
<span class="gi">+            daemon=daemon,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.env = {} if env is None else env
<span class="w"> </span>        self.authkey = self.authkey
<span class="w"> </span>        self.init_main_module = init_main_module

<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _Popen(process_obj):</span>
<span class="gi">+        if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+            from .popen_loky_win32 import Popen</span>
<span class="gi">+        else:</span>
<span class="gi">+            from .popen_loky_posix import Popen</span>
<span class="gi">+        return Popen(process_obj)</span>
<span class="gi">+</span>

<span class="w"> </span>class LokyInitMainProcess(LokyProcess):
<span class="gd">-    _start_method = &#39;loky_init_main&#39;</span>
<span class="gi">+    _start_method = &quot;loky_init_main&quot;</span>

<span class="gd">-    def __init__(self, group=None, target=None, name=None, args=(), kwargs=</span>
<span class="gd">-        {}, daemon=None):</span>
<span class="gd">-        super().__init__(group=group, target=target, name=name, args=args,</span>
<span class="gd">-            kwargs=kwargs, daemon=daemon, init_main_module=True)</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        group=None,</span>
<span class="gi">+        target=None,</span>
<span class="gi">+        name=None,</span>
<span class="gi">+        args=(),</span>
<span class="gi">+        kwargs={},</span>
<span class="gi">+        daemon=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            group=group,</span>
<span class="gi">+            target=target,</span>
<span class="gi">+            name=name,</span>
<span class="gi">+            args=args,</span>
<span class="gi">+            kwargs=kwargs,</span>
<span class="gi">+            daemon=daemon,</span>
<span class="gi">+            init_main_module=True,</span>
<span class="gi">+        )</span>


<span class="gd">-class AuthenticationKey(bytes):</span>
<span class="gi">+#</span>
<span class="gi">+# We subclass bytes to avoid accidental transmission of auth keys over network</span>
<span class="gi">+#</span>

<span class="gi">+</span>
<span class="gi">+class AuthenticationKey(bytes):</span>
<span class="w"> </span>    def __reduce__(self):
<span class="w"> </span>        try:
<span class="w"> </span>            assert_spawning(self)
<span class="w"> </span>        except RuntimeError:
<span class="w"> </span>            raise TypeError(
<span class="gd">-                &#39;Pickling an AuthenticationKey object is disallowed for security reasons&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                &quot;Pickling an AuthenticationKey object is &quot;</span>
<span class="gi">+                &quot;disallowed for security reasons&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>        return AuthenticationKey, (bytes(self),)
<span class="gh">diff --git a/joblib/externals/loky/backend/queues.py b/joblib/externals/loky/backend/queues.py</span>
<span class="gh">index 704e2a3..5afd99b 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/queues.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/queues.py</span>
<span class="gu">@@ -1,55 +1,236 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Queue and SimpleQueue implementation for loky</span>
<span class="gi">+#</span>
<span class="gi">+# authors: Thomas Moreau, Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# based on multiprocessing/queues.py (16/02/2017)</span>
<span class="gi">+# * Add some custom reducers for the Queues/SimpleQueue to tweak the</span>
<span class="gi">+#   pickling process. (overload Queue._feed/SimpleQueue.put)</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import errno
<span class="w"> </span>import weakref
<span class="w"> </span>import threading
<span class="w"> </span>from multiprocessing import util
<span class="gd">-from multiprocessing.queues import Full, Queue as mp_Queue, SimpleQueue as mp_SimpleQueue, _sentinel</span>
<span class="gi">+from multiprocessing.queues import (</span>
<span class="gi">+    Full,</span>
<span class="gi">+    Queue as mp_Queue,</span>
<span class="gi">+    SimpleQueue as mp_SimpleQueue,</span>
<span class="gi">+    _sentinel,</span>
<span class="gi">+)</span>
<span class="w"> </span>from multiprocessing.context import assert_spawning
<span class="gi">+</span>
<span class="w"> </span>from .reduction import dumps
<span class="gd">-__all__ = [&#39;Queue&#39;, &#39;SimpleQueue&#39;, &#39;Full&#39;]</span>


<span class="gd">-class Queue(mp_Queue):</span>
<span class="gi">+__all__ = [&quot;Queue&quot;, &quot;SimpleQueue&quot;, &quot;Full&quot;]</span>
<span class="gi">+</span>

<span class="gi">+class Queue(mp_Queue):</span>
<span class="w"> </span>    def __init__(self, maxsize=0, reducers=None, ctx=None):
<span class="w"> </span>        super().__init__(maxsize=maxsize, ctx=ctx)
<span class="w"> </span>        self._reducers = reducers

<span class="gi">+    # Use custom queue set/get state to be able to reduce the custom reducers</span>
<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        assert_spawning(self)
<span class="gd">-        return (self._ignore_epipe, self._maxsize, self._reader, self.</span>
<span class="gd">-            _writer, self._reducers, self._rlock, self._wlock, self._sem,</span>
<span class="gd">-            self._opid)</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._ignore_epipe,</span>
<span class="gi">+            self._maxsize,</span>
<span class="gi">+            self._reader,</span>
<span class="gi">+            self._writer,</span>
<span class="gi">+            self._reducers,</span>
<span class="gi">+            self._rlock,</span>
<span class="gi">+            self._wlock,</span>
<span class="gi">+            self._sem,</span>
<span class="gi">+            self._opid,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="gd">-        (self._ignore_epipe, self._maxsize, self._reader, self._writer,</span>
<span class="gd">-            self._reducers, self._rlock, self._wlock, self._sem, self._opid</span>
<span class="gd">-            ) = state</span>
<span class="gi">+        (</span>
<span class="gi">+            self._ignore_epipe,</span>
<span class="gi">+            self._maxsize,</span>
<span class="gi">+            self._reader,</span>
<span class="gi">+            self._writer,</span>
<span class="gi">+            self._reducers,</span>
<span class="gi">+            self._rlock,</span>
<span class="gi">+            self._wlock,</span>
<span class="gi">+            self._sem,</span>
<span class="gi">+            self._opid,</span>
<span class="gi">+        ) = state</span>
<span class="w"> </span>        if sys.version_info &gt;= (3, 9):
<span class="w"> </span>            self._reset()
<span class="w"> </span>        else:
<span class="w"> </span>            self._after_fork()

<span class="gi">+    # Overload _start_thread to correctly call our custom _feed</span>
<span class="gi">+    def _start_thread(self):</span>
<span class="gi">+        util.debug(&quot;Queue._start_thread()&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # Start thread which transfers data from buffer to pipe</span>
<span class="gi">+        self._buffer.clear()</span>
<span class="gi">+        self._thread = threading.Thread(</span>
<span class="gi">+            target=Queue._feed,</span>
<span class="gi">+            args=(</span>
<span class="gi">+                self._buffer,</span>
<span class="gi">+                self._notempty,</span>
<span class="gi">+                self._send_bytes,</span>
<span class="gi">+                self._wlock,</span>
<span class="gi">+                self._writer.close,</span>
<span class="gi">+                self._reducers,</span>
<span class="gi">+                self._ignore_epipe,</span>
<span class="gi">+                self._on_queue_feeder_error,</span>
<span class="gi">+                self._sem,</span>
<span class="gi">+            ),</span>
<span class="gi">+            name=&quot;QueueFeederThread&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        self._thread.daemon = True</span>
<span class="gi">+</span>
<span class="gi">+        util.debug(&quot;doing self._thread.start()&quot;)</span>
<span class="gi">+        self._thread.start()</span>
<span class="gi">+        util.debug(&quot;... done self._thread.start()&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # On process exit we will wait for data to be flushed to pipe.</span>
<span class="gi">+        #</span>
<span class="gi">+        # However, if this process created the queue then all</span>
<span class="gi">+        # processes which use the queue will be descendants of this</span>
<span class="gi">+        # process.  Therefore waiting for the queue to be flushed</span>
<span class="gi">+        # is pointless once all the child processes have been joined.</span>
<span class="gi">+        created_by_this_process = self._opid == os.getpid()</span>
<span class="gi">+        if not self._joincancelled and not created_by_this_process:</span>
<span class="gi">+            self._jointhread = util.Finalize(</span>
<span class="gi">+                self._thread,</span>
<span class="gi">+                Queue._finalize_join,</span>
<span class="gi">+                [weakref.ref(self._thread)],</span>
<span class="gi">+                exitpriority=-5,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Send sentinel to the thread queue object when garbage collected</span>
<span class="gi">+        self._close = util.Finalize(</span>
<span class="gi">+            self,</span>
<span class="gi">+            Queue._finalize_close,</span>
<span class="gi">+            [self._buffer, self._notempty],</span>
<span class="gi">+            exitpriority=10,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # Overload the _feed methods to use our custom pickling strategy.</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _feed(</span>
<span class="gi">+        buffer,</span>
<span class="gi">+        notempty,</span>
<span class="gi">+        send_bytes,</span>
<span class="gi">+        writelock,</span>
<span class="gi">+        close,</span>
<span class="gi">+        reducers,</span>
<span class="gi">+        ignore_epipe,</span>
<span class="gi">+        onerror,</span>
<span class="gi">+        queue_sem,</span>
<span class="gi">+    ):</span>
<span class="gi">+        util.debug(&quot;starting thread to feed data to pipe&quot;)</span>
<span class="gi">+        nacquire = notempty.acquire</span>
<span class="gi">+        nrelease = notempty.release</span>
<span class="gi">+        nwait = notempty.wait</span>
<span class="gi">+        bpopleft = buffer.popleft</span>
<span class="gi">+        sentinel = _sentinel</span>
<span class="gi">+        if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+            wacquire = writelock.acquire</span>
<span class="gi">+            wrelease = writelock.release</span>
<span class="gi">+        else:</span>
<span class="gi">+            wacquire = None</span>
<span class="gi">+</span>
<span class="gi">+        while True:</span>
<span class="gi">+            try:</span>
<span class="gi">+                nacquire()</span>
<span class="gi">+                try:</span>
<span class="gi">+                    if not buffer:</span>
<span class="gi">+                        nwait()</span>
<span class="gi">+                finally:</span>
<span class="gi">+                    nrelease()</span>
<span class="gi">+                try:</span>
<span class="gi">+                    while True:</span>
<span class="gi">+                        obj = bpopleft()</span>
<span class="gi">+                        if obj is sentinel:</span>
<span class="gi">+                            util.debug(&quot;feeder thread got sentinel -- exiting&quot;)</span>
<span class="gi">+                            close()</span>
<span class="gi">+                            return</span>
<span class="gi">+</span>
<span class="gi">+                        # serialize the data before acquiring the lock</span>
<span class="gi">+                        obj_ = dumps(obj, reducers=reducers)</span>
<span class="gi">+                        if wacquire is None:</span>
<span class="gi">+                            send_bytes(obj_)</span>
<span class="gi">+                        else:</span>
<span class="gi">+                            wacquire()</span>
<span class="gi">+                            try:</span>
<span class="gi">+                                send_bytes(obj_)</span>
<span class="gi">+                            finally:</span>
<span class="gi">+                                wrelease()</span>
<span class="gi">+                        # Remove references early to avoid leaking memory</span>
<span class="gi">+                        del obj, obj_</span>
<span class="gi">+                except IndexError:</span>
<span class="gi">+                    pass</span>
<span class="gi">+            except BaseException as e:</span>
<span class="gi">+                if ignore_epipe and getattr(e, &quot;errno&quot;, 0) == errno.EPIPE:</span>
<span class="gi">+                    return</span>
<span class="gi">+                # Since this runs in a daemon thread the resources it uses</span>
<span class="gi">+                # may be become unusable while the process is cleaning up.</span>
<span class="gi">+                # We ignore errors which happen after the process has</span>
<span class="gi">+                # started to cleanup.</span>
<span class="gi">+                if util.is_exiting():</span>
<span class="gi">+                    util.info(f&quot;error in queue thread: {e}&quot;)</span>
<span class="gi">+                    return</span>
<span class="gi">+                else:</span>
<span class="gi">+                    queue_sem.release()</span>
<span class="gi">+                    onerror(e, obj)</span>
<span class="gi">+</span>
<span class="w"> </span>    def _on_queue_feeder_error(self, e, obj):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Private API hook called when feeding data in the background thread
<span class="w"> </span>        raises an exception.  For overriding by concurrent.futures.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        import traceback</span>

<span class="gi">+        traceback.print_exc()</span>

<span class="gd">-class SimpleQueue(mp_SimpleQueue):</span>

<span class="gi">+class SimpleQueue(mp_SimpleQueue):</span>
<span class="w"> </span>    def __init__(self, reducers=None, ctx=None):
<span class="w"> </span>        super().__init__(ctx=ctx)
<span class="gi">+</span>
<span class="gi">+        # Add possiblity to use custom reducers</span>
<span class="w"> </span>        self._reducers = reducers

<span class="gi">+    def close(self):</span>
<span class="gi">+        self._reader.close()</span>
<span class="gi">+        self._writer.close()</span>
<span class="gi">+</span>
<span class="gi">+    # Use custom queue set/get state to be able to reduce the custom reducers</span>
<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        assert_spawning(self)
<span class="gd">-        return (self._reader, self._writer, self._reducers, self._rlock,</span>
<span class="gd">-            self._wlock)</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._reader,</span>
<span class="gi">+            self._writer,</span>
<span class="gi">+            self._reducers,</span>
<span class="gi">+            self._rlock,</span>
<span class="gi">+            self._wlock,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="gd">-        (self._reader, self._writer, self._reducers, self._rlock, self._wlock</span>
<span class="gd">-            ) = state</span>
<span class="gi">+        (</span>
<span class="gi">+            self._reader,</span>
<span class="gi">+            self._writer,</span>
<span class="gi">+            self._reducers,</span>
<span class="gi">+            self._rlock,</span>
<span class="gi">+            self._wlock,</span>
<span class="gi">+        ) = state</span>
<span class="gi">+</span>
<span class="gi">+    # Overload put to use our customizable reducer</span>
<span class="gi">+    def put(self, obj):</span>
<span class="gi">+        # serialize the data before acquiring the lock</span>
<span class="gi">+        obj = dumps(obj, reducers=self._reducers)</span>
<span class="gi">+        if self._wlock is None:</span>
<span class="gi">+            # writes to a message oriented win32 pipe are atomic</span>
<span class="gi">+            self._writer.send_bytes(obj)</span>
<span class="gi">+        else:</span>
<span class="gi">+            with self._wlock:</span>
<span class="gi">+                self._writer.send_bytes(obj)</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/reduction.py b/joblib/externals/loky/backend/reduction.py</span>
<span class="gh">index 6770d67..bed32ba 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/reduction.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/reduction.py</span>
<span class="gu">@@ -1,45 +1,224 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Customizable Pickler with some basic reducers</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/reduction.py (17/02/2017)</span>
<span class="gi">+#  * Replace the ForkingPickler with a similar _LokyPickler,</span>
<span class="gi">+#  * Add CustomizableLokyPickler to allow customizing pickling process</span>
<span class="gi">+#    on the fly.</span>
<span class="gi">+#</span>
<span class="w"> </span>import copyreg
<span class="w"> </span>import io
<span class="w"> </span>import functools
<span class="w"> </span>import types
<span class="w"> </span>import sys
<span class="w"> </span>import os
<span class="gi">+</span>
<span class="w"> </span>from multiprocessing import util
<span class="w"> </span>from pickle import loads, HIGHEST_PROTOCOL
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Enable custom pickling in Loky.</span>
<span class="gi">+</span>
<span class="w"> </span>_dispatch_table = {}


<span class="gi">+def register(type_, reduce_function):</span>
<span class="gi">+    _dispatch_table[type_] = reduce_function</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Registers extra pickling routines to improve picklization  for loky</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# make methods picklable</span>
<span class="gi">+def _reduce_method(m):</span>
<span class="gi">+    if m.__self__ is None:</span>
<span class="gi">+        return getattr, (m.__class__, m.__func__.__name__)</span>
<span class="gi">+    else:</span>
<span class="gi">+        return getattr, (m.__self__, m.__func__.__name__)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class _C:
<span class="gd">-    pass</span>
<span class="gi">+    def f(self):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def h(cls):</span>
<span class="gi">+        pass</span>


<span class="w"> </span>register(type(_C().f), _reduce_method)
<span class="w"> </span>register(type(_C.h), _reduce_method)
<span class="gd">-if not hasattr(sys, &#39;pypy_version_info&#39;):</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+if not hasattr(sys, &quot;pypy_version_info&quot;):</span>
<span class="gi">+    # PyPy uses functions instead of method_descriptors and wrapper_descriptors</span>
<span class="gi">+    def _reduce_method_descriptor(m):</span>
<span class="gi">+        return getattr, (m.__objclass__, m.__name__)</span>
<span class="gi">+</span>
<span class="w"> </span>    register(type(list.append), _reduce_method_descriptor)
<span class="w"> </span>    register(type(int.__add__), _reduce_method_descriptor)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Make partial func pickable</span>
<span class="gi">+def _reduce_partial(p):</span>
<span class="gi">+    return _rebuild_partial, (p.func, p.args, p.keywords or {})</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _rebuild_partial(func, args, keywords):</span>
<span class="gi">+    return functools.partial(func, *args, **keywords)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>register(functools.partial, _reduce_partial)
<span class="gd">-if sys.platform != &#39;win32&#39;:</span>
<span class="gd">-    from ._posix_reduction import _mk_inheritable</span>
<span class="gi">+</span>
<span class="gi">+if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+    from ._posix_reduction import _mk_inheritable  # noqa: F401</span>
<span class="w"> </span>else:
<span class="gd">-    from . import _win_reduction</span>
<span class="gi">+    from . import _win_reduction  # noqa: F401</span>
<span class="gi">+</span>
<span class="gi">+# global variable to change the pickler behavior</span>
<span class="w"> </span>try:
<span class="gd">-    from joblib.externals import cloudpickle</span>
<span class="gd">-    DEFAULT_ENV = &#39;cloudpickle&#39;</span>
<span class="gi">+    from joblib.externals import cloudpickle  # noqa: F401</span>
<span class="gi">+</span>
<span class="gi">+    DEFAULT_ENV = &quot;cloudpickle&quot;</span>
<span class="w"> </span>except ImportError:
<span class="gd">-    DEFAULT_ENV = &#39;pickle&#39;</span>
<span class="gd">-ENV_LOKY_PICKLER = os.environ.get(&#39;LOKY_PICKLER&#39;, DEFAULT_ENV)</span>
<span class="gi">+    # If cloudpickle is not present, fallback to pickle</span>
<span class="gi">+    DEFAULT_ENV = &quot;pickle&quot;</span>
<span class="gi">+</span>
<span class="gi">+ENV_LOKY_PICKLER = os.environ.get(&quot;LOKY_PICKLER&quot;, DEFAULT_ENV)</span>
<span class="w"> </span>_LokyPickler = None
<span class="w"> </span>_loky_pickler_name = None
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def set_loky_pickler(loky_pickler=None):</span>
<span class="gi">+    global _LokyPickler, _loky_pickler_name</span>
<span class="gi">+</span>
<span class="gi">+    if loky_pickler is None:</span>
<span class="gi">+        loky_pickler = ENV_LOKY_PICKLER</span>
<span class="gi">+</span>
<span class="gi">+    loky_pickler_cls = None</span>
<span class="gi">+</span>
<span class="gi">+    # The default loky_pickler is cloudpickle</span>
<span class="gi">+    if loky_pickler in [&quot;&quot;, None]:</span>
<span class="gi">+        loky_pickler = &quot;cloudpickle&quot;</span>
<span class="gi">+</span>
<span class="gi">+    if loky_pickler == _loky_pickler_name:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    if loky_pickler == &quot;cloudpickle&quot;:</span>
<span class="gi">+        from joblib.externals.cloudpickle import CloudPickler as loky_pickler_cls</span>
<span class="gi">+    else:</span>
<span class="gi">+        try:</span>
<span class="gi">+            from importlib import import_module</span>
<span class="gi">+</span>
<span class="gi">+            module_pickle = import_module(loky_pickler)</span>
<span class="gi">+            loky_pickler_cls = module_pickle.Pickler</span>
<span class="gi">+        except (ImportError, AttributeError) as e:</span>
<span class="gi">+            extra_info = (</span>
<span class="gi">+                &quot;\nThis error occurred while setting loky_pickler to&quot;</span>
<span class="gi">+                f&quot; &#39;{loky_pickler}&#39;, as required by the env variable &quot;</span>
<span class="gi">+                &quot;LOKY_PICKLER or the function set_loky_pickler.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            e.args = (e.args[0] + extra_info,) + e.args[1:]</span>
<span class="gi">+            e.msg = e.args[0]</span>
<span class="gi">+            raise e</span>
<span class="gi">+</span>
<span class="gi">+    util.debug(</span>
<span class="gi">+        f&quot;Using &#39;{loky_pickler if loky_pickler else &#39;cloudpickle&#39;}&#39; for &quot;</span>
<span class="gi">+        &quot;serialization.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    class CustomizablePickler(loky_pickler_cls):</span>
<span class="gi">+        _loky_pickler_cls = loky_pickler_cls</span>
<span class="gi">+</span>
<span class="gi">+        def _set_dispatch_table(self, dispatch_table):</span>
<span class="gi">+            for ancestor_class in self._loky_pickler_cls.mro():</span>
<span class="gi">+                dt_attribute = getattr(ancestor_class, &quot;dispatch_table&quot;, None)</span>
<span class="gi">+                if isinstance(dt_attribute, types.MemberDescriptorType):</span>
<span class="gi">+                    # Ancestor class (typically _pickle.Pickler) has a</span>
<span class="gi">+                    # member_descriptor for its &quot;dispatch_table&quot; attribute. Use</span>
<span class="gi">+                    # it to set the dispatch_table as a member instead of a</span>
<span class="gi">+                    # dynamic attribute in the __dict__ of the instance,</span>
<span class="gi">+                    # otherwise it will not be taken into account by the C</span>
<span class="gi">+                    # implementation of the dump method if a subclass defines a</span>
<span class="gi">+                    # class-level dispatch_table attribute as was done in</span>
<span class="gi">+                    # cloudpickle 1.6.0:</span>
<span class="gi">+                    # https://github.com/joblib/loky/pull/260</span>
<span class="gi">+                    dt_attribute.__set__(self, dispatch_table)</span>
<span class="gi">+                    break</span>
<span class="gi">+</span>
<span class="gi">+            # On top of member descriptor set, also use setattr such that code</span>
<span class="gi">+            # that directly access self.dispatch_table gets a consistent view</span>
<span class="gi">+            # of the same table.</span>
<span class="gi">+            self.dispatch_table = dispatch_table</span>
<span class="gi">+</span>
<span class="gi">+        def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):</span>
<span class="gi">+            loky_pickler_cls.__init__(self, writer, protocol=protocol)</span>
<span class="gi">+            if reducers is None:</span>
<span class="gi">+                reducers = {}</span>
<span class="gi">+</span>
<span class="gi">+            if hasattr(self, &quot;dispatch_table&quot;):</span>
<span class="gi">+                # Force a copy that we will update without mutating the</span>
<span class="gi">+                # any class level defined dispatch_table.</span>
<span class="gi">+                loky_dt = dict(self.dispatch_table)</span>
<span class="gi">+            else:</span>
<span class="gi">+                # Use standard reducers as bases</span>
<span class="gi">+                loky_dt = copyreg.dispatch_table.copy()</span>
<span class="gi">+</span>
<span class="gi">+            # Register loky specific reducers</span>
<span class="gi">+            loky_dt.update(_dispatch_table)</span>
<span class="gi">+</span>
<span class="gi">+            # Set the new dispatch table, taking care of the fact that we</span>
<span class="gi">+            # need to use the member_descriptor when we inherit from a</span>
<span class="gi">+            # subclass of the C implementation of the Pickler base class</span>
<span class="gi">+            # with an class level dispatch_table attribute.</span>
<span class="gi">+            self._set_dispatch_table(loky_dt)</span>
<span class="gi">+</span>
<span class="gi">+            # Register the reducers</span>
<span class="gi">+            for type, reduce_func in reducers.items():</span>
<span class="gi">+                self.register(type, reduce_func)</span>
<span class="gi">+</span>
<span class="gi">+        def register(self, type, reduce_func):</span>
<span class="gi">+            &quot;&quot;&quot;Attach a reducer function to a given type in the dispatch table.&quot;&quot;&quot;</span>
<span class="gi">+            self.dispatch_table[type] = reduce_func</span>
<span class="gi">+</span>
<span class="gi">+    _LokyPickler = CustomizablePickler</span>
<span class="gi">+    _loky_pickler_name = loky_pickler</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_loky_pickler_name():</span>
<span class="gi">+    global _loky_pickler_name</span>
<span class="gi">+    return _loky_pickler_name</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def get_loky_pickler():</span>
<span class="gi">+    global _LokyPickler</span>
<span class="gi">+    return _LokyPickler</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Set it to its default value</span>
<span class="w"> </span>set_loky_pickler()


<span class="w"> </span>def dump(obj, file, reducers=None, protocol=None):
<span class="w"> </span>    &quot;&quot;&quot;Replacement for pickle.dump() using _LokyPickler.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    global _LokyPickler</span>
<span class="gi">+    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def dumps(obj, reducers=None, protocol=None):</span>
<span class="gi">+    global _LokyPickler</span>

<span class="gi">+    buf = io.BytesIO()</span>
<span class="gi">+    dump(obj, buf, reducers=reducers, protocol=protocol)</span>
<span class="gi">+    return buf.getbuffer()</span>

<span class="gd">-__all__ = [&#39;dump&#39;, &#39;dumps&#39;, &#39;loads&#39;, &#39;register&#39;, &#39;set_loky_pickler&#39;]</span>
<span class="gd">-if sys.platform == &#39;win32&#39;:</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;dump&quot;, &quot;dumps&quot;, &quot;loads&quot;, &quot;register&quot;, &quot;set_loky_pickler&quot;]</span>
<span class="gi">+</span>
<span class="gi">+if sys.platform == &quot;win32&quot;:</span>
<span class="w"> </span>    from multiprocessing.reduction import duplicate
<span class="gd">-    __all__ += [&#39;duplicate&#39;]</span>
<span class="gi">+</span>
<span class="gi">+    __all__ += [&quot;duplicate&quot;]</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/resource_tracker.py b/joblib/externals/loky/backend/resource_tracker.py</span>
<span class="gh">index 3550ef5..25204a7 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/resource_tracker.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/resource_tracker.py</span>
<span class="gu">@@ -1,3 +1,48 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Server process to keep track of unlinked resources, like folders and</span>
<span class="gi">+# semaphores and clean them.</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/semaphore_tracker.py  (17/02/2017)</span>
<span class="gi">+#  * include custom spawnv_passfds to start the process</span>
<span class="gi">+#  * add some VERBOSE logging</span>
<span class="gi">+#</span>
<span class="gi">+# TODO: multiprocessing.resource_tracker was contributed to Python 3.8 so</span>
<span class="gi">+# once loky drops support for Python 3.7 it might be possible to stop</span>
<span class="gi">+# maintaining this loky-specific fork. As a consequence, it might also be</span>
<span class="gi">+# possible to stop maintaining the loky.backend.synchronize fork of</span>
<span class="gi">+# multiprocessing.synchronize.</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# On Unix we run a server process which keeps track of unlinked</span>
<span class="gi">+# resources. The server ignores SIGINT and SIGTERM and reads from a</span>
<span class="gi">+# pipe. The resource_tracker implements a reference counting scheme: each time</span>
<span class="gi">+# a Python process anticipates the shared usage of a resource by another</span>
<span class="gi">+# process, it signals the resource_tracker of this shared usage, and in return,</span>
<span class="gi">+# the resource_tracker increments the resource&#39;s reference count by 1.</span>
<span class="gi">+# Similarly, when access to a resource is closed by a Python process, the</span>
<span class="gi">+# process notifies the resource_tracker by asking it to decrement the</span>
<span class="gi">+# resource&#39;s reference count by 1.  When the reference count drops to 0, the</span>
<span class="gi">+# resource_tracker attempts to clean up the underlying resource.</span>
<span class="gi">+</span>
<span class="gi">+# Finally, every other process connected to the resource tracker has a copy of</span>
<span class="gi">+# the writable end of the pipe used to communicate with it, so the resource</span>
<span class="gi">+# tracker gets EOF when all other processes have exited. Then the</span>
<span class="gi">+# resource_tracker process unlinks any remaining leaked resources (with</span>
<span class="gi">+# reference count above 0)</span>
<span class="gi">+</span>
<span class="gi">+# For semaphores, this is important because the system only supports a limited</span>
<span class="gi">+# number of named semaphores, and they will not be automatically removed till</span>
<span class="gi">+# the next reboot.  Without this resource tracker process, &quot;killall python&quot;</span>
<span class="gi">+# would probably leave unlinked semaphores.</span>
<span class="gi">+</span>
<span class="gi">+# Note that this behavior differs from CPython&#39;s resource_tracker, which only</span>
<span class="gi">+# implements list of shared resources, and not a proper refcounting scheme.</span>
<span class="gi">+# Also, CPython&#39;s resource tracker will only attempt to cleanup those shared</span>
<span class="gi">+# resources once all procsses connected to the resouce tracker have exited.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import shutil
<span class="w"> </span>import sys
<span class="gu">@@ -6,49 +51,151 @@ import warnings</span>
<span class="w"> </span>import threading
<span class="w"> </span>from _multiprocessing import sem_unlink
<span class="w"> </span>from multiprocessing import util
<span class="gi">+</span>
<span class="w"> </span>from . import spawn
<span class="gd">-if sys.platform == &#39;win32&#39;:</span>
<span class="gi">+</span>
<span class="gi">+if sys.platform == &quot;win32&quot;:</span>
<span class="w"> </span>    import _winapi
<span class="w"> </span>    import msvcrt
<span class="w"> </span>    from multiprocessing.reduction import duplicate
<span class="gd">-__all__ = [&#39;ensure_running&#39;, &#39;register&#39;, &#39;unregister&#39;]</span>
<span class="gd">-_HAVE_SIGMASK = hasattr(signal, &#39;pthread_sigmask&#39;)</span>
<span class="gd">-_IGNORED_SIGNALS = signal.SIGINT, signal.SIGTERM</span>
<span class="gd">-_CLEANUP_FUNCS = {&#39;folder&#39;: shutil.rmtree, &#39;file&#39;: os.unlink}</span>
<span class="gd">-if os.name == &#39;posix&#39;:</span>
<span class="gd">-    _CLEANUP_FUNCS[&#39;semlock&#39;] = sem_unlink</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;ensure_running&quot;, &quot;register&quot;, &quot;unregister&quot;]</span>
<span class="gi">+</span>
<span class="gi">+_HAVE_SIGMASK = hasattr(signal, &quot;pthread_sigmask&quot;)</span>
<span class="gi">+_IGNORED_SIGNALS = (signal.SIGINT, signal.SIGTERM)</span>
<span class="gi">+</span>
<span class="gi">+_CLEANUP_FUNCS = {&quot;folder&quot;: shutil.rmtree, &quot;file&quot;: os.unlink}</span>
<span class="gi">+</span>
<span class="gi">+if os.name == &quot;posix&quot;:</span>
<span class="gi">+    _CLEANUP_FUNCS[&quot;semlock&quot;] = sem_unlink</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>VERBOSE = False


<span class="w"> </span>class ResourceTracker:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        self._lock = threading.Lock()
<span class="w"> </span>        self._fd = None
<span class="w"> </span>        self._pid = None

<span class="gi">+    def getfd(self):</span>
<span class="gi">+        self.ensure_running()</span>
<span class="gi">+        return self._fd</span>
<span class="gi">+</span>
<span class="w"> </span>    def ensure_running(self):
<span class="w"> </span>        &quot;&quot;&quot;Make sure that resource tracker process is running.

<span class="w"> </span>        This can be run from any process.  Usually a child process will use
<span class="w"> </span>        the resource created by its parent.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            if self._fd is not None:</span>
<span class="gi">+                # resource tracker was launched before, is it still running?</span>
<span class="gi">+                if self._check_alive():</span>
<span class="gi">+                    # =&gt; still alive</span>
<span class="gi">+                    return</span>
<span class="gi">+                # =&gt; dead, launch it again</span>
<span class="gi">+                os.close(self._fd)</span>
<span class="gi">+                if os.name == &quot;posix&quot;:</span>
<span class="gi">+                    try:</span>
<span class="gi">+                        # At this point, the resource_tracker process has been</span>
<span class="gi">+                        # killed or crashed. Let&#39;s remove the process entry</span>
<span class="gi">+                        # from the process table to avoid zombie processes.</span>
<span class="gi">+                        os.waitpid(self._pid, 0)</span>
<span class="gi">+                    except OSError:</span>
<span class="gi">+                        # The process was terminated or is a child from an</span>
<span class="gi">+                        # ancestor of the current process.</span>
<span class="gi">+                        pass</span>
<span class="gi">+                self._fd = None</span>
<span class="gi">+                self._pid = None</span>
<span class="gi">+</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &quot;resource_tracker: process died unexpectedly, &quot;</span>
<span class="gi">+                    &quot;relaunching.  Some folders/sempahores might &quot;</span>
<span class="gi">+                    &quot;leak.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            fds_to_pass = []</span>
<span class="gi">+            try:</span>
<span class="gi">+                fds_to_pass.append(sys.stderr.fileno())</span>
<span class="gi">+            except Exception:</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+            r, w = os.pipe()</span>
<span class="gi">+            if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+                _r = duplicate(msvcrt.get_osfhandle(r), inheritable=True)</span>
<span class="gi">+                os.close(r)</span>
<span class="gi">+                r = _r</span>
<span class="gi">+</span>
<span class="gi">+            cmd = f&quot;from {main.__module__} import main; main({r}, {VERBOSE})&quot;</span>
<span class="gi">+            try:</span>
<span class="gi">+                fds_to_pass.append(r)</span>
<span class="gi">+                # process will out live us, so no need to wait on pid</span>
<span class="gi">+                exe = spawn.get_executable()</span>
<span class="gi">+                args = [exe, *util._args_from_interpreter_flags(), &quot;-c&quot;, cmd]</span>
<span class="gi">+                util.debug(f&quot;launching resource tracker: {args}&quot;)</span>
<span class="gi">+                # bpo-33613: Register a signal mask that will block the</span>
<span class="gi">+                # signals.  This signal mask will be inherited by the child</span>
<span class="gi">+                # that is going to be spawned and will protect the child from a</span>
<span class="gi">+                # race condition that can make the child die before it</span>
<span class="gi">+                # registers signal handlers for SIGINT and SIGTERM. The mask is</span>
<span class="gi">+                # unregistered after spawning the child.</span>
<span class="gi">+                try:</span>
<span class="gi">+                    if _HAVE_SIGMASK:</span>
<span class="gi">+                        signal.pthread_sigmask(</span>
<span class="gi">+                            signal.SIG_BLOCK, _IGNORED_SIGNALS</span>
<span class="gi">+                        )</span>
<span class="gi">+                    pid = spawnv_passfds(exe, args, fds_to_pass)</span>
<span class="gi">+                finally:</span>
<span class="gi">+                    if _HAVE_SIGMASK:</span>
<span class="gi">+                        signal.pthread_sigmask(</span>
<span class="gi">+                            signal.SIG_UNBLOCK, _IGNORED_SIGNALS</span>
<span class="gi">+                        )</span>
<span class="gi">+            except BaseException:</span>
<span class="gi">+                os.close(w)</span>
<span class="gi">+                raise</span>
<span class="gi">+            else:</span>
<span class="gi">+                self._fd = w</span>
<span class="gi">+                self._pid = pid</span>
<span class="gi">+            finally:</span>
<span class="gi">+                if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+                    _winapi.CloseHandle(r)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    os.close(r)</span>

<span class="w"> </span>    def _check_alive(self):
<span class="w"> </span>        &quot;&quot;&quot;Check for the existence of the resource tracker process.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            self._send(&quot;PROBE&quot;, &quot;&quot;, &quot;&quot;)</span>
<span class="gi">+        except BrokenPipeError:</span>
<span class="gi">+            return False</span>
<span class="gi">+        else:</span>
<span class="gi">+            return True</span>

<span class="w"> </span>    def register(self, name, rtype):
<span class="w"> </span>        &quot;&quot;&quot;Register a named resource, and increment its refcount.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.ensure_running()</span>
<span class="gi">+        self._send(&quot;REGISTER&quot;, name, rtype)</span>

<span class="w"> </span>    def unregister(self, name, rtype):
<span class="w"> </span>        &quot;&quot;&quot;Unregister a named resource with resource tracker.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.ensure_running()</span>
<span class="gi">+        self._send(&quot;UNREGISTER&quot;, name, rtype)</span>

<span class="w"> </span>    def maybe_unlink(self, name, rtype):
<span class="w"> </span>        &quot;&quot;&quot;Decrement the refcount of a resource, and delete it if it hits 0&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.ensure_running()</span>
<span class="gi">+        self._send(&quot;MAYBE_UNLINK&quot;, name, rtype)</span>
<span class="gi">+</span>
<span class="gi">+    def _send(self, cmd, name, rtype):</span>
<span class="gi">+        if len(name) &gt; 512:</span>
<span class="gi">+            # posix guarantees that writes to a pipe of less than PIPE_BUF</span>
<span class="gi">+            # bytes are atomic, and that PIPE_BUF &gt;= 512</span>
<span class="gi">+            raise ValueError(&quot;name too long&quot;)</span>
<span class="gi">+        msg = f&quot;{cmd}:{name}:{rtype}\n&quot;.encode(&quot;ascii&quot;)</span>
<span class="gi">+        nbytes = os.write(self._fd, msg)</span>
<span class="gi">+        assert nbytes == len(msg)</span>


<span class="w"> </span>_resource_tracker = ResourceTracker()
<span class="gu">@@ -61,4 +208,171 @@ getfd = _resource_tracker.getfd</span>

<span class="w"> </span>def main(fd, verbose=0):
<span class="w"> </span>    &quot;&quot;&quot;Run resource tracker.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # protect the process from ^C and &quot;killall python&quot; etc</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        util.log_to_stderr(level=util.DEBUG)</span>
<span class="gi">+</span>
<span class="gi">+    signal.signal(signal.SIGINT, signal.SIG_IGN)</span>
<span class="gi">+    signal.signal(signal.SIGTERM, signal.SIG_IGN)</span>
<span class="gi">+</span>
<span class="gi">+    if _HAVE_SIGMASK:</span>
<span class="gi">+        signal.pthread_sigmask(signal.SIG_UNBLOCK, _IGNORED_SIGNALS)</span>
<span class="gi">+</span>
<span class="gi">+    for f in (sys.stdin, sys.stdout):</span>
<span class="gi">+        try:</span>
<span class="gi">+            f.close()</span>
<span class="gi">+        except Exception:</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        util.debug(&quot;Main resource tracker is running&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    registry = {rtype: {} for rtype in _CLEANUP_FUNCS.keys()}</span>
<span class="gi">+    try:</span>
<span class="gi">+        # keep track of registered/unregistered resources</span>
<span class="gi">+        if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+            fd = msvcrt.open_osfhandle(fd, os.O_RDONLY)</span>
<span class="gi">+        with open(fd, &quot;rb&quot;) as f:</span>
<span class="gi">+            while True:</span>
<span class="gi">+                line = f.readline()</span>
<span class="gi">+                if line == b&quot;&quot;:  # EOF</span>
<span class="gi">+                    break</span>
<span class="gi">+                try:</span>
<span class="gi">+                    splitted = line.strip().decode(&quot;ascii&quot;).split(&quot;:&quot;)</span>
<span class="gi">+                    # name can potentially contain separator symbols (for</span>
<span class="gi">+                    # instance folders on Windows)</span>
<span class="gi">+                    cmd, name, rtype = (</span>
<span class="gi">+                        splitted[0],</span>
<span class="gi">+                        &quot;:&quot;.join(splitted[1:-1]),</span>
<span class="gi">+                        splitted[-1],</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+                    if cmd == &quot;PROBE&quot;:</span>
<span class="gi">+                        continue</span>
<span class="gi">+</span>
<span class="gi">+                    if rtype not in _CLEANUP_FUNCS:</span>
<span class="gi">+                        raise ValueError(</span>
<span class="gi">+                            f&quot;Cannot register {name} for automatic cleanup: &quot;</span>
<span class="gi">+                            f&quot;unknown resource type ({rtype}). Resource type &quot;</span>
<span class="gi">+                            &quot;should be one of the following: &quot;</span>
<span class="gi">+                            f&quot;{list(_CLEANUP_FUNCS.keys())}&quot;</span>
<span class="gi">+                        )</span>
<span class="gi">+</span>
<span class="gi">+                    if cmd == &quot;REGISTER&quot;:</span>
<span class="gi">+                        if name not in registry[rtype]:</span>
<span class="gi">+                            registry[rtype][name] = 1</span>
<span class="gi">+                        else:</span>
<span class="gi">+                            registry[rtype][name] += 1</span>
<span class="gi">+</span>
<span class="gi">+                        if verbose:</span>
<span class="gi">+                            util.debug(</span>
<span class="gi">+                                &quot;[ResourceTracker] incremented refcount of &quot;</span>
<span class="gi">+                                f&quot;{rtype} {name} &quot;</span>
<span class="gi">+                                f&quot;(current {registry[rtype][name]})&quot;</span>
<span class="gi">+                            )</span>
<span class="gi">+                    elif cmd == &quot;UNREGISTER&quot;:</span>
<span class="gi">+                        del registry[rtype][name]</span>
<span class="gi">+                        if verbose:</span>
<span class="gi">+                            util.debug(</span>
<span class="gi">+                                f&quot;[ResourceTracker] unregister {name} {rtype}: &quot;</span>
<span class="gi">+                                f&quot;registry({len(registry)})&quot;</span>
<span class="gi">+                            )</span>
<span class="gi">+                    elif cmd == &quot;MAYBE_UNLINK&quot;:</span>
<span class="gi">+                        registry[rtype][name] -= 1</span>
<span class="gi">+                        if verbose:</span>
<span class="gi">+                            util.debug(</span>
<span class="gi">+                                &quot;[ResourceTracker] decremented refcount of &quot;</span>
<span class="gi">+                                f&quot;{rtype} {name} &quot;</span>
<span class="gi">+                                f&quot;(current {registry[rtype][name]})&quot;</span>
<span class="gi">+                            )</span>
<span class="gi">+</span>
<span class="gi">+                        if registry[rtype][name] == 0:</span>
<span class="gi">+                            del registry[rtype][name]</span>
<span class="gi">+                            try:</span>
<span class="gi">+                                if verbose:</span>
<span class="gi">+                                    util.debug(</span>
<span class="gi">+                                        f&quot;[ResourceTracker] unlink {name}&quot;</span>
<span class="gi">+                                    )</span>
<span class="gi">+                                _CLEANUP_FUNCS[rtype](name)</span>
<span class="gi">+                            except Exception as e:</span>
<span class="gi">+                                warnings.warn(</span>
<span class="gi">+                                    f&quot;resource_tracker: {name}: {e!r}&quot;</span>
<span class="gi">+                                )</span>
<span class="gi">+</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        raise RuntimeError(f&quot;unrecognized command {cmd!r}&quot;)</span>
<span class="gi">+                except BaseException:</span>
<span class="gi">+                    try:</span>
<span class="gi">+                        sys.excepthook(*sys.exc_info())</span>
<span class="gi">+                    except BaseException:</span>
<span class="gi">+                        pass</span>
<span class="gi">+    finally:</span>
<span class="gi">+        # all processes have terminated; cleanup any remaining resources</span>
<span class="gi">+        def _unlink_resources(rtype_registry, rtype):</span>
<span class="gi">+            if rtype_registry:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    warnings.warn(</span>
<span class="gi">+                        &quot;resource_tracker: There appear to be &quot;</span>
<span class="gi">+                        f&quot;{len(rtype_registry)} leaked {rtype} objects to &quot;</span>
<span class="gi">+                        &quot;clean up at shutdown&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                except Exception:</span>
<span class="gi">+                    pass</span>
<span class="gi">+            for name in rtype_registry:</span>
<span class="gi">+                # For some reason the process which created and registered this</span>
<span class="gi">+                # resource has failed to unregister it. Presumably it has</span>
<span class="gi">+                # died.  We therefore clean it up.</span>
<span class="gi">+                try:</span>
<span class="gi">+                    _CLEANUP_FUNCS[rtype](name)</span>
<span class="gi">+                    if verbose:</span>
<span class="gi">+                        util.debug(f&quot;[ResourceTracker] unlink {name}&quot;)</span>
<span class="gi">+                except Exception as e:</span>
<span class="gi">+                    warnings.warn(f&quot;resource_tracker: {name}: {e!r}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        for rtype, rtype_registry in registry.items():</span>
<span class="gi">+            if rtype == &quot;folder&quot;:</span>
<span class="gi">+                continue</span>
<span class="gi">+            else:</span>
<span class="gi">+                _unlink_resources(rtype_registry, rtype)</span>
<span class="gi">+</span>
<span class="gi">+        # The default cleanup routine for folders deletes everything inside</span>
<span class="gi">+        # those folders recursively, which can include other resources tracked</span>
<span class="gi">+        # by the resource tracker). To limit the risk of the resource tracker</span>
<span class="gi">+        # attempting to delete twice a resource (once as part of a tracked</span>
<span class="gi">+        # folder, and once as a resource), we delete the folders after all</span>
<span class="gi">+        # other resource types.</span>
<span class="gi">+        if &quot;folder&quot; in registry:</span>
<span class="gi">+            _unlink_resources(registry[&quot;folder&quot;], &quot;folder&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        util.debug(&quot;resource tracker shut down&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Start a program with only specified fds kept open</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def spawnv_passfds(path, args, passfds):</span>
<span class="gi">+    passfds = sorted(passfds)</span>
<span class="gi">+    if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+        errpipe_read, errpipe_write = os.pipe()</span>
<span class="gi">+        try:</span>
<span class="gi">+            from .reduction import _mk_inheritable</span>
<span class="gi">+            from .fork_exec import fork_exec</span>
<span class="gi">+</span>
<span class="gi">+            _pass = [_mk_inheritable(fd) for fd in passfds]</span>
<span class="gi">+            return fork_exec(args, _pass)</span>
<span class="gi">+        finally:</span>
<span class="gi">+            os.close(errpipe_read)</span>
<span class="gi">+            os.close(errpipe_write)</span>
<span class="gi">+    else:</span>
<span class="gi">+        cmd = &quot; &quot;.join(f&#39;&quot;{x}&quot;&#39; for x in args)</span>
<span class="gi">+        try:</span>
<span class="gi">+            _, ht, pid, _ = _winapi.CreateProcess(</span>
<span class="gi">+                path, cmd, None, None, True, 0, None, None, None</span>
<span class="gi">+            )</span>
<span class="gi">+            _winapi.CloseHandle(ht)</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            pass</span>
<span class="gi">+        return pid</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/spawn.py b/joblib/externals/loky/backend/spawn.py</span>
<span class="gh">index aadb9e2..d011c39 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/spawn.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/spawn.py</span>
<span class="gu">@@ -1,31 +1,250 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Prepares and processes the data to setup the new process environment</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/spawn.py (17/02/2017)</span>
<span class="gi">+#  * Improve logging data</span>
<span class="gi">+#</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import runpy
<span class="w"> </span>import textwrap
<span class="w"> </span>import types
<span class="w"> </span>from multiprocessing import process, util
<span class="gd">-if sys.platform != &#39;win32&#39;:</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+if sys.platform != &quot;win32&quot;:</span>
<span class="w"> </span>    WINEXE = False
<span class="w"> </span>    WINSERVICE = False
<span class="w"> </span>else:
<span class="w"> </span>    import msvcrt
<span class="w"> </span>    from multiprocessing.reduction import duplicate
<span class="gd">-    WINEXE = sys.platform == &#39;win32&#39; and getattr(sys, &#39;frozen&#39;, False)</span>
<span class="gd">-    WINSERVICE = sys.executable.lower().endswith(&#39;pythonservice.exe&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    WINEXE = sys.platform == &quot;win32&quot; and getattr(sys, &quot;frozen&quot;, False)</span>
<span class="gi">+    WINSERVICE = sys.executable.lower().endswith(&quot;pythonservice.exe&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>if WINSERVICE:
<span class="gd">-    _python_exe = os.path.join(sys.exec_prefix, &#39;python.exe&#39;)</span>
<span class="gi">+    _python_exe = os.path.join(sys.exec_prefix, &quot;python.exe&quot;)</span>
<span class="w"> </span>else:
<span class="w"> </span>    _python_exe = sys.executable


<span class="gi">+def get_executable():</span>
<span class="gi">+    return _python_exe</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _check_not_importing_main():</span>
<span class="gi">+    if getattr(process.current_process(), &quot;_inheriting&quot;, False):</span>
<span class="gi">+        raise RuntimeError(</span>
<span class="gi">+            textwrap.dedent(</span>
<span class="gi">+                &quot;&quot;&quot;\</span>
<span class="gi">+            An attempt has been made to start a new process before the</span>
<span class="gi">+            current process has finished its bootstrapping phase.</span>
<span class="gi">+</span>
<span class="gi">+            This probably means that you are not using fork to start your</span>
<span class="gi">+            child processes and you have forgotten to use the proper idiom</span>
<span class="gi">+            in the main module:</span>
<span class="gi">+</span>
<span class="gi">+                if __name__ == &#39;__main__&#39;:</span>
<span class="gi">+                    freeze_support()</span>
<span class="gi">+                    ...</span>
<span class="gi">+</span>
<span class="gi">+            The &quot;freeze_support()&quot; line can be omitted if the program</span>
<span class="gi">+            is not going to be frozen to produce an executable.&quot;&quot;&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def get_preparation_data(name, init_main_module=True):
<span class="w"> </span>    &quot;&quot;&quot;Return info about parent needed by child to unpickle process object.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    _check_not_importing_main()</span>
<span class="gi">+    d = dict(</span>
<span class="gi">+        log_to_stderr=util._log_to_stderr,</span>
<span class="gi">+        authkey=bytes(process.current_process().authkey),</span>
<span class="gi">+        name=name,</span>
<span class="gi">+        sys_argv=sys.argv,</span>
<span class="gi">+        orig_dir=process.ORIGINAL_DIR,</span>
<span class="gi">+        dir=os.getcwd(),</span>
<span class="gi">+    )</span>

<span class="gi">+    # Send sys_path and make sure the current directory will not be changed</span>
<span class="gi">+    d[&quot;sys_path&quot;] = [p if p != &quot;&quot; else process.ORIGINAL_DIR for p in sys.path]</span>

<span class="gi">+    # Make sure to pass the information if the multiprocessing logger is active</span>
<span class="gi">+    if util._logger is not None:</span>
<span class="gi">+        d[&quot;log_level&quot;] = util._logger.getEffectiveLevel()</span>
<span class="gi">+        if util._logger.handlers:</span>
<span class="gi">+            h = util._logger.handlers[0]</span>
<span class="gi">+            d[&quot;log_fmt&quot;] = h.formatter._fmt</span>
<span class="gi">+</span>
<span class="gi">+    # Tell the child how to communicate with the resource_tracker</span>
<span class="gi">+    from .resource_tracker import _resource_tracker</span>
<span class="gi">+</span>
<span class="gi">+    _resource_tracker.ensure_running()</span>
<span class="gi">+    d[&quot;tracker_args&quot;] = {&quot;pid&quot;: _resource_tracker._pid}</span>
<span class="gi">+    if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+        d[&quot;tracker_args&quot;][&quot;fh&quot;] = msvcrt.get_osfhandle(_resource_tracker._fd)</span>
<span class="gi">+    else:</span>
<span class="gi">+        d[&quot;tracker_args&quot;][&quot;fd&quot;] = _resource_tracker._fd</span>
<span class="gi">+</span>
<span class="gi">+    if sys.version_info &gt;= (3, 8) and os.name == &quot;posix&quot;:</span>
<span class="gi">+        # joblib/loky#242: allow loky processes to retrieve the resource</span>
<span class="gi">+        # tracker of their parent in case the child processes depickles</span>
<span class="gi">+        # shared_memory objects, that are still tracked by multiprocessing&#39;s</span>
<span class="gi">+        # resource_tracker by default.</span>
<span class="gi">+        # XXX: this is a workaround that may be error prone: in the future, it</span>
<span class="gi">+        # would be better to have loky subclass multiprocessing&#39;s shared_memory</span>
<span class="gi">+        # to force registration of shared_memory segments via loky&#39;s</span>
<span class="gi">+        # resource_tracker.</span>
<span class="gi">+        from multiprocessing.resource_tracker import (</span>
<span class="gi">+            _resource_tracker as mp_resource_tracker,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # multiprocessing&#39;s resource_tracker must be running before loky</span>
<span class="gi">+        # process is created (othewise the child won&#39;t be able to use it if it</span>
<span class="gi">+        # is created later on)</span>
<span class="gi">+        mp_resource_tracker.ensure_running()</span>
<span class="gi">+        d[&quot;mp_tracker_args&quot;] = {</span>
<span class="gi">+            &quot;fd&quot;: mp_resource_tracker._fd,</span>
<span class="gi">+            &quot;pid&quot;: mp_resource_tracker._pid,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    # Figure out whether to initialise main in the subprocess as a module</span>
<span class="gi">+    # or through direct execution (or to leave it alone entirely)</span>
<span class="gi">+    if init_main_module:</span>
<span class="gi">+        main_module = sys.modules[&quot;__main__&quot;]</span>
<span class="gi">+        try:</span>
<span class="gi">+            main_mod_name = getattr(main_module.__spec__, &quot;name&quot;, None)</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            main_mod_name = None</span>
<span class="gi">+        if main_mod_name is not None:</span>
<span class="gi">+            d[&quot;init_main_from_name&quot;] = main_mod_name</span>
<span class="gi">+        elif sys.platform != &quot;win32&quot; or (not WINEXE and not WINSERVICE):</span>
<span class="gi">+            main_path = getattr(main_module, &quot;__file__&quot;, None)</span>
<span class="gi">+            if main_path is not None:</span>
<span class="gi">+                if (</span>
<span class="gi">+                    not os.path.isabs(main_path)</span>
<span class="gi">+                    and process.ORIGINAL_DIR is not None</span>
<span class="gi">+                ):</span>
<span class="gi">+                    main_path = os.path.join(process.ORIGINAL_DIR, main_path)</span>
<span class="gi">+                d[&quot;init_main_from_path&quot;] = os.path.normpath(main_path)</span>
<span class="gi">+</span>
<span class="gi">+    return d</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Prepare current process</span>
<span class="gi">+#</span>
<span class="w"> </span>old_main_modules = []


<span class="w"> </span>def prepare(data, parent_sentinel=None):
<span class="w"> </span>    &quot;&quot;&quot;Try to get current process ready to unpickle process object.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if &quot;name&quot; in data:</span>
<span class="gi">+        process.current_process().name = data[&quot;name&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;authkey&quot; in data:</span>
<span class="gi">+        process.current_process().authkey = data[&quot;authkey&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;log_to_stderr&quot; in data and data[&quot;log_to_stderr&quot;]:</span>
<span class="gi">+        util.log_to_stderr()</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;log_level&quot; in data:</span>
<span class="gi">+        util.get_logger().setLevel(data[&quot;log_level&quot;])</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;log_fmt&quot; in data:</span>
<span class="gi">+        import logging</span>
<span class="gi">+</span>
<span class="gi">+        util.get_logger().handlers[0].setFormatter(</span>
<span class="gi">+            logging.Formatter(data[&quot;log_fmt&quot;])</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;sys_path&quot; in data:</span>
<span class="gi">+        sys.path = data[&quot;sys_path&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;sys_argv&quot; in data:</span>
<span class="gi">+        sys.argv = data[&quot;sys_argv&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;dir&quot; in data:</span>
<span class="gi">+        os.chdir(data[&quot;dir&quot;])</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;orig_dir&quot; in data:</span>
<span class="gi">+        process.ORIGINAL_DIR = data[&quot;orig_dir&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;mp_tracker_args&quot; in data:</span>
<span class="gi">+        from multiprocessing.resource_tracker import (</span>
<span class="gi">+            _resource_tracker as mp_resource_tracker,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        mp_resource_tracker._fd = data[&quot;mp_tracker_args&quot;][&quot;fd&quot;]</span>
<span class="gi">+        mp_resource_tracker._pid = data[&quot;mp_tracker_args&quot;][&quot;pid&quot;]</span>
<span class="gi">+    if &quot;tracker_args&quot; in data:</span>
<span class="gi">+        from .resource_tracker import _resource_tracker</span>
<span class="gi">+</span>
<span class="gi">+        _resource_tracker._pid = data[&quot;tracker_args&quot;][&quot;pid&quot;]</span>
<span class="gi">+        if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+            handle = data[&quot;tracker_args&quot;][&quot;fh&quot;]</span>
<span class="gi">+            handle = duplicate(handle, source_process=parent_sentinel)</span>
<span class="gi">+            _resource_tracker._fd = msvcrt.open_osfhandle(handle, os.O_RDONLY)</span>
<span class="gi">+        else:</span>
<span class="gi">+            _resource_tracker._fd = data[&quot;tracker_args&quot;][&quot;fd&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    if &quot;init_main_from_name&quot; in data:</span>
<span class="gi">+        _fixup_main_from_name(data[&quot;init_main_from_name&quot;])</span>
<span class="gi">+    elif &quot;init_main_from_path&quot; in data:</span>
<span class="gi">+        _fixup_main_from_path(data[&quot;init_main_from_path&quot;])</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Multiprocessing module helpers to fix up the main module in</span>
<span class="gi">+# spawned subprocesses</span>
<span class="gi">+def _fixup_main_from_name(mod_name):</span>
<span class="gi">+    # __main__.py files for packages, directories, zip archives, etc, run</span>
<span class="gi">+    # their &quot;main only&quot; code unconditionally, so we don&#39;t even try to</span>
<span class="gi">+    # populate anything in __main__, nor do we make any changes to</span>
<span class="gi">+    # __main__ attributes</span>
<span class="gi">+    current_main = sys.modules[&quot;__main__&quot;]</span>
<span class="gi">+    if mod_name == &quot;__main__&quot; or mod_name.endswith(&quot;.__main__&quot;):</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # If this process was forked, __main__ may already be populated</span>
<span class="gi">+    if getattr(current_main.__spec__, &quot;name&quot;, None) == mod_name:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # Otherwise, __main__ may contain some non-main code where we need to</span>
<span class="gi">+    # support unpickling it properly. We rerun it as __mp_main__ and make</span>
<span class="gi">+    # the normal __main__ an alias to that</span>
<span class="gi">+    old_main_modules.append(current_main)</span>
<span class="gi">+    main_module = types.ModuleType(&quot;__mp_main__&quot;)</span>
<span class="gi">+    main_content = runpy.run_module(</span>
<span class="gi">+        mod_name, run_name=&quot;__mp_main__&quot;, alter_sys=True</span>
<span class="gi">+    )</span>
<span class="gi">+    main_module.__dict__.update(main_content)</span>
<span class="gi">+    sys.modules[&quot;__main__&quot;] = sys.modules[&quot;__mp_main__&quot;] = main_module</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _fixup_main_from_path(main_path):</span>
<span class="gi">+    # If this process was forked, __main__ may already be populated</span>
<span class="gi">+    current_main = sys.modules[&quot;__main__&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    # Unfortunately, the main ipython launch script historically had no</span>
<span class="gi">+    # &quot;if __name__ == &#39;__main__&#39;&quot; guard, so we work around that</span>
<span class="gi">+    # by treating it like a __main__.py file</span>
<span class="gi">+    # See https://github.com/ipython/ipython/issues/4698</span>
<span class="gi">+    main_name = os.path.splitext(os.path.basename(main_path))[0]</span>
<span class="gi">+    if main_name == &quot;ipython&quot;:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # Otherwise, if __file__ already has the setting we expect,</span>
<span class="gi">+    # there&#39;s nothing more to do</span>
<span class="gi">+    if getattr(current_main, &quot;__file__&quot;, None) == main_path:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # If the parent process has sent a path through rather than a module</span>
<span class="gi">+    # name we assume it is an executable script that may contain</span>
<span class="gi">+    # non-main code that needs to be executed</span>
<span class="gi">+    old_main_modules.append(current_main)</span>
<span class="gi">+    main_module = types.ModuleType(&quot;__mp_main__&quot;)</span>
<span class="gi">+    main_content = runpy.run_path(main_path, run_name=&quot;__mp_main__&quot;)</span>
<span class="gi">+    main_module.__dict__.update(main_content)</span>
<span class="gi">+    sys.modules[&quot;__main__&quot;] = sys.modules[&quot;__mp_main__&quot;] = main_module</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/synchronize.py b/joblib/externals/loky/backend/synchronize.py</span>
<span class="gh">index 3136b2c..18db3e3 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/synchronize.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/synchronize.py</span>
<span class="gu">@@ -1,3 +1,17 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Synchronization primitives based on our SemLock implementation</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from multiprocessing/synchronize.py (17/02/2017)</span>
<span class="gi">+#  * Remove ctx argument for compatibility reason</span>
<span class="gi">+#  * Registers a cleanup function with the loky resource_tracker to remove the</span>
<span class="gi">+#    semaphore when the process dies instead.</span>
<span class="gi">+#</span>
<span class="gi">+# TODO: investigate which Python version is required to be able to use</span>
<span class="gi">+# multiprocessing.resource_tracker and therefore multiprocessing.synchronize</span>
<span class="gi">+# instead of a loky-specific fork.</span>
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>import tempfile
<span class="gu">@@ -6,50 +20,100 @@ import _multiprocessing</span>
<span class="w"> </span>from time import time as _time
<span class="w"> </span>from multiprocessing import process, util
<span class="w"> </span>from multiprocessing.context import assert_spawning
<span class="gi">+</span>
<span class="w"> </span>from . import resource_tracker
<span class="gd">-__all__ = [&#39;Lock&#39;, &#39;RLock&#39;, &#39;Semaphore&#39;, &#39;BoundedSemaphore&#39;, &#39;Condition&#39;,</span>
<span class="gd">-    &#39;Event&#39;]</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [</span>
<span class="gi">+    &quot;Lock&quot;,</span>
<span class="gi">+    &quot;RLock&quot;,</span>
<span class="gi">+    &quot;Semaphore&quot;,</span>
<span class="gi">+    &quot;BoundedSemaphore&quot;,</span>
<span class="gi">+    &quot;Condition&quot;,</span>
<span class="gi">+    &quot;Event&quot;,</span>
<span class="gi">+]</span>
<span class="gi">+# Try to import the mp.synchronize module cleanly, if it fails</span>
<span class="gi">+# raise ImportError for platforms lacking a working sem_open implementation.</span>
<span class="gi">+# See issue 3770</span>
<span class="w"> </span>try:
<span class="w"> </span>    from _multiprocessing import SemLock as _SemLock
<span class="w"> </span>    from _multiprocessing import sem_unlink
<span class="w"> </span>except ImportError:
<span class="w"> </span>    raise ImportError(
<span class="gd">-        &#39;This platform lacks a functioning sem_open implementation, therefore, the required synchronization primitives needed will not function, see issue 3770.&#39;</span>
<span class="gd">-        )</span>
<span class="gi">+        &quot;This platform lacks a functioning sem_open&quot;</span>
<span class="gi">+        &quot; implementation, therefore, the required&quot;</span>
<span class="gi">+        &quot; synchronization primitives needed will not&quot;</span>
<span class="gi">+        &quot; function, see issue 3770.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Constants</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="w"> </span>RECURSIVE_MUTEX, SEMAPHORE = range(2)
<span class="w"> </span>SEM_VALUE_MAX = _multiprocessing.SemLock.SEM_VALUE_MAX


<span class="gi">+#</span>
<span class="gi">+# Base class for semaphores and mutexes; wraps `_multiprocessing.SemLock`</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class SemLock:
<span class="gi">+</span>
<span class="w"> </span>    _rand = tempfile._RandomNameSequence()

<span class="w"> </span>    def __init__(self, kind, value, maxvalue, name=None):
<span class="gi">+        # unlink_now is only used on win32 or when we are using fork.</span>
<span class="w"> </span>        unlink_now = False
<span class="w"> </span>        if name is None:
<span class="gi">+            # Try to find an unused name for the SemLock instance.</span>
<span class="w"> </span>            for _ in range(100):
<span class="w"> </span>                try:
<span class="gd">-                    self._semlock = _SemLock(kind, value, maxvalue, SemLock</span>
<span class="gd">-                        ._make_name(), unlink_now)</span>
<span class="gd">-                except FileExistsError:</span>
<span class="gi">+                    self._semlock = _SemLock(</span>
<span class="gi">+                        kind, value, maxvalue, SemLock._make_name(), unlink_now</span>
<span class="gi">+                    )</span>
<span class="gi">+                except FileExistsError:  # pragma: no cover</span>
<span class="w"> </span>                    pass
<span class="w"> </span>                else:
<span class="w"> </span>                    break
<span class="gd">-            else:</span>
<span class="gd">-                raise FileExistsError(&#39;cannot find name for semaphore&#39;)</span>
<span class="gi">+            else:  # pragma: no cover</span>
<span class="gi">+                raise FileExistsError(&quot;cannot find name for semaphore&quot;)</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self._semlock = _SemLock(kind, value, maxvalue, name, unlink_now)
<span class="w"> </span>        self.name = name
<span class="w"> </span>        util.debug(
<span class="gd">-            f&#39;created semlock with handle {self._semlock.handle} and name &quot;{self.name}&quot;&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+            f&quot;created semlock with handle {self._semlock.handle} and name &quot;</span>
<span class="gi">+            f&#39;&quot;{self.name}&quot;&#39;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>        self._make_methods()

<span class="w"> </span>        def _after_fork(obj):
<span class="w"> </span>            obj._semlock._after_fork()
<span class="gi">+</span>
<span class="w"> </span>        util.register_after_fork(self, _after_fork)
<span class="gd">-        resource_tracker.register(self._semlock.name, &#39;semlock&#39;)</span>
<span class="gd">-        util.Finalize(self, SemLock._cleanup, (self._semlock.name,),</span>
<span class="gd">-            exitpriority=0)</span>
<span class="gi">+</span>
<span class="gi">+        # When the object is garbage collected or the</span>
<span class="gi">+        # process shuts down we unlink the semaphore name</span>
<span class="gi">+        resource_tracker.register(self._semlock.name, &quot;semlock&quot;)</span>
<span class="gi">+        util.Finalize(</span>
<span class="gi">+            self, SemLock._cleanup, (self._semlock.name,), exitpriority=0</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _cleanup(name):</span>
<span class="gi">+        try:</span>
<span class="gi">+            sem_unlink(name)</span>
<span class="gi">+        except FileNotFoundError:</span>
<span class="gi">+            # Already unlinked, possibly by user code: ignore and make sure to</span>
<span class="gi">+            # unregister the semaphore from the resource tracker.</span>
<span class="gi">+            pass</span>
<span class="gi">+        finally:</span>
<span class="gi">+            resource_tracker.unregister(name, &quot;semlock&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _make_methods(self):</span>
<span class="gi">+        self.acquire = self._semlock.acquire</span>
<span class="gi">+        self.release = self._semlock.release</span>

<span class="w"> </span>    def __enter__(self):
<span class="w"> </span>        return self._semlock.acquire()
<span class="gu">@@ -61,31 +125,49 @@ class SemLock:</span>
<span class="w"> </span>        assert_spawning(self)
<span class="w"> </span>        sl = self._semlock
<span class="w"> </span>        h = sl.handle
<span class="gd">-        return h, sl.kind, sl.maxvalue, sl.name</span>
<span class="gi">+        return (h, sl.kind, sl.maxvalue, sl.name)</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="w"> </span>        self._semlock = _SemLock._rebuild(*state)
<span class="w"> </span>        util.debug(
<span class="w"> </span>            f&#39;recreated blocker with handle {state[0]!r} and name &quot;{state[3]}&quot;&#39;
<span class="gd">-            )</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self._make_methods()

<span class="gi">+    @staticmethod</span>
<span class="gi">+    def _make_name():</span>
<span class="gi">+        # OSX does not support long names for semaphores</span>
<span class="gi">+        return f&quot;/loky-{os.getpid()}-{next(SemLock._rand)}&quot;</span>

<span class="gd">-class Semaphore(SemLock):</span>

<span class="gi">+#</span>
<span class="gi">+# Semaphore</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class Semaphore(SemLock):</span>
<span class="w"> </span>    def __init__(self, value=1):
<span class="w"> </span>        SemLock.__init__(self, SEMAPHORE, value, SEM_VALUE_MAX)

<span class="gi">+    def get_value(self):</span>
<span class="gi">+        if sys.platform == &quot;darwin&quot;:</span>
<span class="gi">+            raise NotImplementedError(&quot;OSX does not implement sem_getvalue&quot;)</span>
<span class="gi">+        return self._semlock._get_value()</span>
<span class="gi">+</span>
<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        try:
<span class="w"> </span>            value = self._semlock._get_value()
<span class="w"> </span>        except Exception:
<span class="gd">-            value = &#39;unknown&#39;</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}(value={value})&gt;&#39;</span>
<span class="gi">+            value = &quot;unknown&quot;</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}(value={value})&gt;&quot;</span>


<span class="gd">-class BoundedSemaphore(Semaphore):</span>
<span class="gi">+#</span>
<span class="gi">+# Bounded semaphore</span>
<span class="gi">+#</span>
<span class="gi">+</span>

<span class="gi">+class BoundedSemaphore(Semaphore):</span>
<span class="w"> </span>    def __init__(self, value=1):
<span class="w"> </span>        SemLock.__init__(self, SEMAPHORE, value, value)

<span class="gu">@@ -93,14 +175,19 @@ class BoundedSemaphore(Semaphore):</span>
<span class="w"> </span>        try:
<span class="w"> </span>            value = self._semlock._get_value()
<span class="w"> </span>        except Exception:
<span class="gd">-            value = &#39;unknown&#39;</span>
<span class="gi">+            value = &quot;unknown&quot;</span>
<span class="w"> </span>        return (
<span class="gd">-            f&#39;&lt;{self.__class__.__name__}(value={value}, maxvalue={self._semlock.maxvalue})&gt;&#39;</span>
<span class="gd">-            )</span>
<span class="gi">+            f&quot;&lt;{self.__class__.__name__}(value={value}, &quot;</span>
<span class="gi">+            f&quot;maxvalue={self._semlock.maxvalue})&gt;&quot;</span>
<span class="gi">+        )</span>


<span class="gd">-class Lock(SemLock):</span>
<span class="gi">+#</span>
<span class="gi">+# Non-recursive lock</span>
<span class="gi">+#</span>
<span class="gi">+</span>

<span class="gi">+class Lock(SemLock):</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        super().__init__(SEMAPHORE, 1, 1)

<span class="gu">@@ -108,21 +195,25 @@ class Lock(SemLock):</span>
<span class="w"> </span>        try:
<span class="w"> </span>            if self._semlock._is_mine():
<span class="w"> </span>                name = process.current_process().name
<span class="gd">-                if threading.current_thread().name != &#39;MainThread&#39;:</span>
<span class="gd">-                    name = f&#39;{name}|{threading.current_thread().name}&#39;</span>
<span class="gi">+                if threading.current_thread().name != &quot;MainThread&quot;:</span>
<span class="gi">+                    name = f&quot;{name}|{threading.current_thread().name}&quot;</span>
<span class="w"> </span>            elif self._semlock._get_value() == 1:
<span class="gd">-                name = &#39;None&#39;</span>
<span class="gi">+                name = &quot;None&quot;</span>
<span class="w"> </span>            elif self._semlock._count() &gt; 0:
<span class="gd">-                name = &#39;SomeOtherThread&#39;</span>
<span class="gi">+                name = &quot;SomeOtherThread&quot;</span>
<span class="w"> </span>            else:
<span class="gd">-                name = &#39;SomeOtherProcess&#39;</span>
<span class="gi">+                name = &quot;SomeOtherProcess&quot;</span>
<span class="w"> </span>        except Exception:
<span class="gd">-            name = &#39;unknown&#39;</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}(owner={name})&gt;&#39;</span>
<span class="gi">+            name = &quot;unknown&quot;</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}(owner={name})&gt;&quot;</span>


<span class="gd">-class RLock(SemLock):</span>
<span class="gi">+#</span>
<span class="gi">+# Recursive lock</span>
<span class="gi">+#</span>
<span class="gi">+</span>

<span class="gi">+class RLock(SemLock):</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        super().__init__(RECURSIVE_MUTEX, 1, 1)

<span class="gu">@@ -130,22 +221,26 @@ class RLock(SemLock):</span>
<span class="w"> </span>        try:
<span class="w"> </span>            if self._semlock._is_mine():
<span class="w"> </span>                name = process.current_process().name
<span class="gd">-                if threading.current_thread().name != &#39;MainThread&#39;:</span>
<span class="gd">-                    name = f&#39;{name}|{threading.current_thread().name}&#39;</span>
<span class="gi">+                if threading.current_thread().name != &quot;MainThread&quot;:</span>
<span class="gi">+                    name = f&quot;{name}|{threading.current_thread().name}&quot;</span>
<span class="w"> </span>                count = self._semlock._count()
<span class="w"> </span>            elif self._semlock._get_value() == 1:
<span class="gd">-                name, count = &#39;None&#39;, 0</span>
<span class="gi">+                name, count = &quot;None&quot;, 0</span>
<span class="w"> </span>            elif self._semlock._count() &gt; 0:
<span class="gd">-                name, count = &#39;SomeOtherThread&#39;, &#39;nonzero&#39;</span>
<span class="gi">+                name, count = &quot;SomeOtherThread&quot;, &quot;nonzero&quot;</span>
<span class="w"> </span>            else:
<span class="gd">-                name, count = &#39;SomeOtherProcess&#39;, &#39;nonzero&#39;</span>
<span class="gi">+                name, count = &quot;SomeOtherProcess&quot;, &quot;nonzero&quot;</span>
<span class="w"> </span>        except Exception:
<span class="gd">-            name, count = &#39;unknown&#39;, &#39;unknown&#39;</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}({name}, {count})&gt;&#39;</span>
<span class="gi">+            name, count = &quot;unknown&quot;, &quot;unknown&quot;</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}({name}, {count})&gt;&quot;</span>


<span class="gd">-class Condition:</span>
<span class="gi">+#</span>
<span class="gi">+# Condition variable</span>
<span class="gi">+#</span>

<span class="gi">+</span>
<span class="gi">+class Condition:</span>
<span class="w"> </span>    def __init__(self, lock=None):
<span class="w"> </span>        self._lock = lock or RLock()
<span class="w"> </span>        self._sleeping_count = Semaphore(0)
<span class="gu">@@ -155,12 +250,20 @@ class Condition:</span>

<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        assert_spawning(self)
<span class="gd">-        return (self._lock, self._sleeping_count, self._woken_count, self.</span>
<span class="gd">-            _wait_semaphore)</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._lock,</span>
<span class="gi">+            self._sleeping_count,</span>
<span class="gi">+            self._woken_count,</span>
<span class="gi">+            self._wait_semaphore,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="gd">-        (self._lock, self._sleeping_count, self._woken_count, self.</span>
<span class="gd">-            _wait_semaphore) = state</span>
<span class="gi">+        (</span>
<span class="gi">+            self._lock,</span>
<span class="gi">+            self._sleeping_count,</span>
<span class="gi">+            self._woken_count,</span>
<span class="gi">+            self._wait_semaphore,</span>
<span class="gi">+        ) = state</span>
<span class="w"> </span>        self._make_methods()

<span class="w"> </span>    def __enter__(self):
<span class="gu">@@ -169,17 +272,138 @@ class Condition:</span>
<span class="w"> </span>    def __exit__(self, *args):
<span class="w"> </span>        return self._lock.__exit__(*args)

<span class="gi">+    def _make_methods(self):</span>
<span class="gi">+        self.acquire = self._lock.acquire</span>
<span class="gi">+        self.release = self._lock.release</span>
<span class="gi">+</span>
<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        try:
<span class="gd">-            num_waiters = self._sleeping_count._semlock._get_value(</span>
<span class="gd">-                ) - self._woken_count._semlock._get_value()</span>
<span class="gi">+            num_waiters = (</span>
<span class="gi">+                self._sleeping_count._semlock._get_value()</span>
<span class="gi">+                - self._woken_count._semlock._get_value()</span>
<span class="gi">+            )</span>
<span class="w"> </span>        except Exception:
<span class="gd">-            num_waiters = &#39;unknown&#39;</span>
<span class="gd">-        return f&#39;&lt;{self.__class__.__name__}({self._lock}, {num_waiters})&gt;&#39;</span>
<span class="gi">+            num_waiters = &quot;unknown&quot;</span>
<span class="gi">+        return f&quot;&lt;{self.__class__.__name__}({self._lock}, {num_waiters})&gt;&quot;</span>

<span class="gi">+    def wait(self, timeout=None):</span>
<span class="gi">+        assert (</span>
<span class="gi">+            self._lock._semlock._is_mine()</span>
<span class="gi">+        ), &quot;must acquire() condition before using wait()&quot;</span>

<span class="gd">-class Event:</span>
<span class="gi">+        # indicate that this thread is going to sleep</span>
<span class="gi">+        self._sleeping_count.release()</span>
<span class="gi">+</span>
<span class="gi">+        # release lock</span>
<span class="gi">+        count = self._lock._semlock._count()</span>
<span class="gi">+        for _ in range(count):</span>
<span class="gi">+            self._lock.release()</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            # wait for notification or timeout</span>
<span class="gi">+            return self._wait_semaphore.acquire(True, timeout)</span>
<span class="gi">+        finally:</span>
<span class="gi">+            # indicate that this thread has woken</span>
<span class="gi">+            self._woken_count.release()</span>
<span class="gi">+</span>
<span class="gi">+            # reacquire lock</span>
<span class="gi">+            for _ in range(count):</span>
<span class="gi">+                self._lock.acquire()</span>
<span class="gi">+</span>
<span class="gi">+    def notify(self):</span>
<span class="gi">+        assert self._lock._semlock._is_mine(), &quot;lock is not owned&quot;</span>
<span class="gi">+        assert not self._wait_semaphore.acquire(False)</span>
<span class="gi">+</span>
<span class="gi">+        # to take account of timeouts since last notify() we subtract</span>
<span class="gi">+        # woken_count from sleeping_count and rezero woken_count</span>
<span class="gi">+        while self._woken_count.acquire(False):</span>
<span class="gi">+            res = self._sleeping_count.acquire(False)</span>
<span class="gi">+            assert res</span>
<span class="gi">+</span>
<span class="gi">+        if self._sleeping_count.acquire(False):  # try grabbing a sleeper</span>
<span class="gi">+            self._wait_semaphore.release()  # wake up one sleeper</span>
<span class="gi">+            self._woken_count.acquire()  # wait for the sleeper to wake</span>
<span class="gi">+</span>
<span class="gi">+            # rezero _wait_semaphore in case a timeout just happened</span>
<span class="gi">+            self._wait_semaphore.acquire(False)</span>
<span class="gi">+</span>
<span class="gi">+    def notify_all(self):</span>
<span class="gi">+        assert self._lock._semlock._is_mine(), &quot;lock is not owned&quot;</span>
<span class="gi">+        assert not self._wait_semaphore.acquire(False)</span>
<span class="gi">+</span>
<span class="gi">+        # to take account of timeouts since last notify*() we subtract</span>
<span class="gi">+        # woken_count from sleeping_count and rezero woken_count</span>
<span class="gi">+        while self._woken_count.acquire(False):</span>
<span class="gi">+            res = self._sleeping_count.acquire(False)</span>
<span class="gi">+            assert res</span>
<span class="gi">+</span>
<span class="gi">+        sleepers = 0</span>
<span class="gi">+        while self._sleeping_count.acquire(False):</span>
<span class="gi">+            self._wait_semaphore.release()  # wake up one sleeper</span>
<span class="gi">+            sleepers += 1</span>
<span class="gi">+</span>
<span class="gi">+        if sleepers:</span>
<span class="gi">+            for _ in range(sleepers):</span>
<span class="gi">+                self._woken_count.acquire()  # wait for a sleeper to wake</span>
<span class="gi">+</span>
<span class="gi">+            # rezero wait_semaphore in case some timeouts just happened</span>
<span class="gi">+            while self._wait_semaphore.acquire(False):</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+    def wait_for(self, predicate, timeout=None):</span>
<span class="gi">+        result = predicate()</span>
<span class="gi">+        if result:</span>
<span class="gi">+            return result</span>
<span class="gi">+        if timeout is not None:</span>
<span class="gi">+            endtime = _time() + timeout</span>
<span class="gi">+        else:</span>
<span class="gi">+            endtime = None</span>
<span class="gi">+            waittime = None</span>
<span class="gi">+        while not result:</span>
<span class="gi">+            if endtime is not None:</span>
<span class="gi">+                waittime = endtime - _time()</span>
<span class="gi">+                if waittime &lt;= 0:</span>
<span class="gi">+                    break</span>
<span class="gi">+            self.wait(waittime)</span>
<span class="gi">+            result = predicate()</span>
<span class="gi">+        return result</span>

<span class="gi">+</span>
<span class="gi">+#</span>
<span class="gi">+# Event</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class Event:</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        self._cond = Condition(Lock())
<span class="w"> </span>        self._flag = Semaphore(0)
<span class="gi">+</span>
<span class="gi">+    def is_set(self):</span>
<span class="gi">+        with self._cond:</span>
<span class="gi">+            if self._flag.acquire(False):</span>
<span class="gi">+                self._flag.release()</span>
<span class="gi">+                return True</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+    def set(self):</span>
<span class="gi">+        with self._cond:</span>
<span class="gi">+            self._flag.acquire(False)</span>
<span class="gi">+            self._flag.release()</span>
<span class="gi">+            self._cond.notify_all()</span>
<span class="gi">+</span>
<span class="gi">+    def clear(self):</span>
<span class="gi">+        with self._cond:</span>
<span class="gi">+            self._flag.acquire(False)</span>
<span class="gi">+</span>
<span class="gi">+    def wait(self, timeout=None):</span>
<span class="gi">+        with self._cond:</span>
<span class="gi">+            if self._flag.acquire(False):</span>
<span class="gi">+                self._flag.release()</span>
<span class="gi">+            else:</span>
<span class="gi">+                self._cond.wait(timeout)</span>
<span class="gi">+</span>
<span class="gi">+            if self._flag.acquire(False):</span>
<span class="gi">+                self._flag.release()</span>
<span class="gi">+                return True</span>
<span class="gi">+            return False</span>
<span class="gh">diff --git a/joblib/externals/loky/backend/utils.py b/joblib/externals/loky/backend/utils.py</span>
<span class="gh">index 3a17859..aa089f7 100644</span>
<span class="gd">--- a/joblib/externals/loky/backend/utils.py</span>
<span class="gi">+++ b/joblib/externals/loky/backend/utils.py</span>
<span class="gu">@@ -6,6 +6,7 @@ import signal</span>
<span class="w"> </span>import warnings
<span class="w"> </span>import subprocess
<span class="w"> </span>import traceback
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import psutil
<span class="w"> </span>except ImportError:
<span class="gu">@@ -14,17 +15,115 @@ except ImportError:</span>

<span class="w"> </span>def kill_process_tree(process, use_psutil=True):
<span class="w"> </span>    &quot;&quot;&quot;Terminate process and its descendants with SIGKILL&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if use_psutil and psutil is not None:</span>
<span class="gi">+        _kill_process_tree_with_psutil(process)</span>
<span class="gi">+    else:</span>
<span class="gi">+        _kill_process_tree_without_psutil(process)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def recursive_terminate(process, use_psutil=True):</span>
<span class="gi">+    warnings.warn(</span>
<span class="gi">+        &quot;recursive_terminate is deprecated in loky 3.2, use kill_process_tree&quot;</span>
<span class="gi">+        &quot;instead&quot;,</span>
<span class="gi">+        DeprecationWarning,</span>
<span class="gi">+    )</span>
<span class="gi">+    kill_process_tree(process, use_psutil=use_psutil)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _kill_process_tree_with_psutil(process):</span>
<span class="gi">+    try:</span>
<span class="gi">+        descendants = psutil.Process(process.pid).children(recursive=True)</span>
<span class="gi">+    except psutil.NoSuchProcess:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # Kill the descendants in reverse order to avoid killing the parents before</span>
<span class="gi">+    # the descendant in cases where there are more processes nested.</span>
<span class="gi">+    for descendant in descendants[::-1]:</span>
<span class="gi">+        try:</span>
<span class="gi">+            descendant.kill()</span>
<span class="gi">+        except psutil.NoSuchProcess:</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        psutil.Process(process.pid).kill()</span>
<span class="gi">+    except psutil.NoSuchProcess:</span>
<span class="gi">+        pass</span>
<span class="gi">+    process.join()</span>


<span class="w"> </span>def _kill_process_tree_without_psutil(process):
<span class="w"> </span>    &quot;&quot;&quot;Terminate a process and its descendants.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+            _windows_taskkill_process_tree(process.pid)</span>
<span class="gi">+        else:</span>
<span class="gi">+            _posix_recursive_kill(process.pid)</span>
<span class="gi">+    except Exception:  # pragma: no cover</span>
<span class="gi">+        details = traceback.format_exc()</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;Failed to kill subprocesses on this platform. Please install&quot;</span>
<span class="gi">+            &quot;psutil: https://github.com/giampaolo/psutil\n&quot;</span>
<span class="gi">+            f&quot;Details:\n{details}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        # In case we cannot introspect or kill the descendants, we fall back to</span>
<span class="gi">+        # only killing the main process.</span>
<span class="gi">+        #</span>
<span class="gi">+        # Note: on Windows, process.kill() is an alias for process.terminate()</span>
<span class="gi">+        # which in turns calls the Win32 API function TerminateProcess().</span>
<span class="gi">+        process.kill()</span>
<span class="gi">+    process.join()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _windows_taskkill_process_tree(pid):</span>
<span class="gi">+    # On windows, the taskkill function with option `/T` terminate a given</span>
<span class="gi">+    # process pid and its children.</span>
<span class="gi">+    try:</span>
<span class="gi">+        subprocess.check_output(</span>
<span class="gi">+            [&quot;taskkill&quot;, &quot;/F&quot;, &quot;/T&quot;, &quot;/PID&quot;, str(pid)], stderr=None</span>
<span class="gi">+        )</span>
<span class="gi">+    except subprocess.CalledProcessError as e:</span>
<span class="gi">+        # In Windows, taskkill returns 128, 255 for no process found.</span>
<span class="gi">+        if e.returncode not in [128, 255]:</span>
<span class="gi">+            # Let&#39;s raise to let the caller log the error details in a</span>
<span class="gi">+            # warning and only kill the root process.</span>
<span class="gi">+            raise  # pragma: no cover</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _kill(pid):</span>
<span class="gi">+    # Not all systems (e.g. Windows) have a SIGKILL, but the C specification</span>
<span class="gi">+    # mandates a SIGTERM signal. While Windows is handled specifically above,</span>
<span class="gi">+    # let&#39;s try to be safe for other hypothetic platforms that only have</span>
<span class="gi">+    # SIGTERM without SIGKILL.</span>
<span class="gi">+    kill_signal = getattr(signal, &quot;SIGKILL&quot;, signal.SIGTERM)</span>
<span class="gi">+    try:</span>
<span class="gi">+        os.kill(pid, kill_signal)</span>
<span class="gi">+    except OSError as e:</span>
<span class="gi">+        # if OSError is raised with [Errno 3] no such process, the process</span>
<span class="gi">+        # is already terminated, else, raise the error and let the top</span>
<span class="gi">+        # level function raise a warning and retry to kill the process.</span>
<span class="gi">+        if e.errno != errno.ESRCH:</span>
<span class="gi">+            raise  # pragma: no cover</span>


<span class="w"> </span>def _posix_recursive_kill(pid):
<span class="w"> </span>    &quot;&quot;&quot;Recursively kill the descendants of a process before killing it.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        children_pids = subprocess.check_output(</span>
<span class="gi">+            [&quot;pgrep&quot;, &quot;-P&quot;, str(pid)], stderr=None, text=True</span>
<span class="gi">+        )</span>
<span class="gi">+    except subprocess.CalledProcessError as e:</span>
<span class="gi">+        # `ps` returns 1 when no child process has been found</span>
<span class="gi">+        if e.returncode == 1:</span>
<span class="gi">+            children_pids = &quot;&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise  # pragma: no cover</span>
<span class="gi">+</span>
<span class="gi">+    # Decode the result, split the cpid and remove the trailing line</span>
<span class="gi">+    for cpid in children_pids.splitlines():</span>
<span class="gi">+        cpid = int(cpid)</span>
<span class="gi">+        _posix_recursive_kill(cpid)</span>
<span class="gi">+</span>
<span class="gi">+    _kill(pid)</span>


<span class="w"> </span>def get_exitcodes_terminated_worker(processes):
<span class="gu">@@ -33,9 +132,50 @@ def get_exitcodes_terminated_worker(processes):</span>
<span class="w"> </span>    If necessary, wait (up to .25s) for the system to correctly set the
<span class="w"> </span>    exitcode of one terminated worker.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    patience = 5</span>
<span class="gi">+</span>
<span class="gi">+    # Catch the exitcode of the terminated workers. There should at least be</span>
<span class="gi">+    # one. If not, wait a bit for the system to correctly set the exitcode of</span>
<span class="gi">+    # the terminated worker.</span>
<span class="gi">+    exitcodes = [</span>
<span class="gi">+        p.exitcode for p in list(processes.values()) if p.exitcode is not None</span>
<span class="gi">+    ]</span>
<span class="gi">+    while not exitcodes and patience &gt; 0:</span>
<span class="gi">+        patience -= 1</span>
<span class="gi">+        exitcodes = [</span>
<span class="gi">+            p.exitcode</span>
<span class="gi">+            for p in list(processes.values())</span>
<span class="gi">+            if p.exitcode is not None</span>
<span class="gi">+        ]</span>
<span class="gi">+        time.sleep(0.05)</span>
<span class="gi">+</span>
<span class="gi">+    return _format_exitcodes(exitcodes)</span>


<span class="w"> </span>def _format_exitcodes(exitcodes):
<span class="w"> </span>    &quot;&quot;&quot;Format a list of exit code with names of the signals if possible&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    str_exitcodes = [</span>
<span class="gi">+        f&quot;{_get_exitcode_name(e)}({e})&quot; for e in exitcodes if e is not None</span>
<span class="gi">+    ]</span>
<span class="gi">+    return &quot;{&quot; + &quot;, &quot;.join(str_exitcodes) + &quot;}&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_exitcode_name(exitcode):</span>
<span class="gi">+    if sys.platform == &quot;win32&quot;:</span>
<span class="gi">+        # The exitcode are unreliable  on windows (see bpo-31863).</span>
<span class="gi">+        # For this case, return UNKNOWN</span>
<span class="gi">+        return &quot;UNKNOWN&quot;</span>
<span class="gi">+</span>
<span class="gi">+    if exitcode &lt; 0:</span>
<span class="gi">+        try:</span>
<span class="gi">+            import signal</span>
<span class="gi">+</span>
<span class="gi">+            return signal.Signals(-exitcode).name</span>
<span class="gi">+        except ValueError:</span>
<span class="gi">+            return &quot;UNKNOWN&quot;</span>
<span class="gi">+    elif exitcode != 255:</span>
<span class="gi">+        # The exitcode are unreliable on forkserver were 255 is always returned</span>
<span class="gi">+        # (see bpo-30589). For this case, return UNKNOWN</span>
<span class="gi">+        return &quot;EXIT&quot;</span>
<span class="gi">+</span>
<span class="gi">+    return &quot;UNKNOWN&quot;</span>
<span class="gh">diff --git a/joblib/externals/loky/cloudpickle_wrapper.py b/joblib/externals/loky/cloudpickle_wrapper.py</span>
<span class="gh">index 808ade4..099debc 100644</span>
<span class="gd">--- a/joblib/externals/loky/cloudpickle_wrapper.py</span>
<span class="gi">+++ b/joblib/externals/loky/cloudpickle_wrapper.py</span>
<span class="gu">@@ -1,11 +1,12 @@</span>
<span class="w"> </span>import inspect
<span class="w"> </span>from functools import partial
<span class="w"> </span>from joblib.externals.cloudpickle import dumps, loads
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>WRAP_CACHE = {}


<span class="w"> </span>class CloudpickledObjectWrapper:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, obj, keep_wrapper=False):
<span class="w"> </span>        self._obj = obj
<span class="w"> </span>        self._keep_wrapper = keep_wrapper
<span class="gu">@@ -14,20 +15,67 @@ class CloudpickledObjectWrapper:</span>
<span class="w"> </span>        _pickled_object = dumps(self._obj)
<span class="w"> </span>        if not self._keep_wrapper:
<span class="w"> </span>            return loads, (_pickled_object,)
<span class="gi">+</span>
<span class="w"> </span>        return _reconstruct_wrapper, (_pickled_object, self._keep_wrapper)

<span class="w"> </span>    def __getattr__(self, attr):
<span class="gd">-        if attr not in [&#39;_obj&#39;, &#39;_keep_wrapper&#39;]:</span>
<span class="gi">+        # Ensure that the wrapped object can be used seemlessly as the</span>
<span class="gi">+        # previous object.</span>
<span class="gi">+        if attr not in [&quot;_obj&quot;, &quot;_keep_wrapper&quot;]:</span>
<span class="w"> </span>            return getattr(self._obj, attr)
<span class="w"> </span>        return getattr(self, attr)


<span class="gi">+# Make sure the wrapped object conserves the callable property</span>
<span class="w"> </span>class CallableObjectWrapper(CloudpickledObjectWrapper):
<span class="gd">-</span>
<span class="w"> </span>    def __call__(self, *args, **kwargs):
<span class="w"> </span>        return self._obj(*args, **kwargs)


<span class="gi">+def _wrap_non_picklable_objects(obj, keep_wrapper):</span>
<span class="gi">+    if callable(obj):</span>
<span class="gi">+        return CallableObjectWrapper(obj, keep_wrapper=keep_wrapper)</span>
<span class="gi">+    return CloudpickledObjectWrapper(obj, keep_wrapper=keep_wrapper)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _reconstruct_wrapper(_pickled_object, keep_wrapper):</span>
<span class="gi">+    obj = loads(_pickled_object)</span>
<span class="gi">+    return _wrap_non_picklable_objects(obj, keep_wrapper)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _wrap_objects_when_needed(obj):</span>
<span class="gi">+    # Function to introspect an object and decide if it should be wrapped or</span>
<span class="gi">+    # not.</span>
<span class="gi">+    need_wrap = &quot;__main__&quot; in getattr(obj, &quot;__module__&quot;, &quot;&quot;)</span>
<span class="gi">+    if isinstance(obj, partial):</span>
<span class="gi">+        return partial(</span>
<span class="gi">+            _wrap_objects_when_needed(obj.func),</span>
<span class="gi">+            *[_wrap_objects_when_needed(a) for a in obj.args],</span>
<span class="gi">+            **{</span>
<span class="gi">+                k: _wrap_objects_when_needed(v)</span>
<span class="gi">+                for k, v in obj.keywords.items()</span>
<span class="gi">+            }</span>
<span class="gi">+        )</span>
<span class="gi">+    if callable(obj):</span>
<span class="gi">+        # Need wrap if the object is a function defined in a local scope of</span>
<span class="gi">+        # another function.</span>
<span class="gi">+        func_code = getattr(obj, &quot;__code__&quot;, &quot;&quot;)</span>
<span class="gi">+        need_wrap |= getattr(func_code, &quot;co_flags&quot;, 0) &amp; inspect.CO_NESTED</span>
<span class="gi">+</span>
<span class="gi">+        # Need wrap if the obj is a lambda expression</span>
<span class="gi">+        func_name = getattr(obj, &quot;__name__&quot;, &quot;&quot;)</span>
<span class="gi">+        need_wrap |= &quot;&lt;lambda&gt;&quot; in func_name</span>
<span class="gi">+</span>
<span class="gi">+    if not need_wrap:</span>
<span class="gi">+        return obj</span>
<span class="gi">+</span>
<span class="gi">+    wrapped_obj = WRAP_CACHE.get(obj)</span>
<span class="gi">+    if wrapped_obj is None:</span>
<span class="gi">+        wrapped_obj = _wrap_non_picklable_objects(obj, keep_wrapper=False)</span>
<span class="gi">+        WRAP_CACHE[obj] = wrapped_obj</span>
<span class="gi">+    return wrapped_obj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def wrap_non_picklable_objects(obj, keep_wrapper=True):
<span class="w"> </span>    &quot;&quot;&quot;Wrapper for non-picklable object to use cloudpickle to serialize them.

<span class="gu">@@ -37,4 +85,18 @@ def wrap_non_picklable_objects(obj, keep_wrapper=True):</span>
<span class="w"> </span>    objects in the main scripts and to implement __reduce__ functions for
<span class="w"> </span>    complex classes.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # If obj is a  class, create a CloudpickledClassWrapper which instantiates</span>
<span class="gi">+    # the object internally and wrap it directly in a CloudpickledObjectWrapper</span>
<span class="gi">+    if inspect.isclass(obj):</span>
<span class="gi">+</span>
<span class="gi">+        class CloudpickledClassWrapper(CloudpickledObjectWrapper):</span>
<span class="gi">+            def __init__(self, *args, **kwargs):</span>
<span class="gi">+                self._obj = obj(*args, **kwargs)</span>
<span class="gi">+                self._keep_wrapper = keep_wrapper</span>
<span class="gi">+</span>
<span class="gi">+        CloudpickledClassWrapper.__name__ = obj.__name__</span>
<span class="gi">+        return CloudpickledClassWrapper</span>
<span class="gi">+</span>
<span class="gi">+    # If obj is an instance of a class, just wrap it in a regular</span>
<span class="gi">+    # CloudpickledObjectWrapper</span>
<span class="gi">+    return _wrap_non_picklable_objects(obj, keep_wrapper=keep_wrapper)</span>
<span class="gh">diff --git a/joblib/externals/loky/initializers.py b/joblib/externals/loky/initializers.py</span>
<span class="gh">index 81c0c79..aea0e56 100644</span>
<span class="gd">--- a/joblib/externals/loky/initializers.py</span>
<span class="gi">+++ b/joblib/externals/loky/initializers.py</span>
<span class="gu">@@ -3,7 +3,30 @@ import warnings</span>

<span class="w"> </span>def _viztracer_init(init_kwargs):
<span class="w"> </span>    &quot;&quot;&quot;Initialize viztracer&#39;s profiler in worker processes&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from viztracer import VizTracer</span>
<span class="gi">+</span>
<span class="gi">+    tracer = VizTracer(**init_kwargs)</span>
<span class="gi">+    tracer.register_exit()</span>
<span class="gi">+    tracer.start()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _make_viztracer_initializer_and_initargs():</span>
<span class="gi">+    try:</span>
<span class="gi">+        import viztracer</span>
<span class="gi">+</span>
<span class="gi">+        tracer = viztracer.get_tracer()</span>
<span class="gi">+        if tracer is not None and getattr(tracer, &quot;enable&quot;, False):</span>
<span class="gi">+            # Profiler is active: introspect its configuration to</span>
<span class="gi">+            # initialize the workers with the same configuration.</span>
<span class="gi">+            return _viztracer_init, (tracer.init_kwargs,)</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        # viztracer is not installed: nothing to do</span>
<span class="gi">+        pass</span>
<span class="gi">+    except Exception as e:</span>
<span class="gi">+        # In case viztracer&#39;s API evolve, we do not want to crash loky but</span>
<span class="gi">+        # we want to know about it to be able to update loky.</span>
<span class="gi">+        warnings.warn(f&quot;Unable to introspect viztracer state: {e}&quot;)</span>
<span class="gi">+    return None, ()</span>


<span class="w"> </span>class _ChainedInitializer:
<span class="gu">@@ -26,4 +49,32 @@ def _chain_initializers(initializer_and_args):</span>

<span class="w"> </span>    If some initializers are None, they are filtered out.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    filtered_initializers = []</span>
<span class="gi">+    filtered_initargs = []</span>
<span class="gi">+    for initializer, initargs in initializer_and_args:</span>
<span class="gi">+        if initializer is not None:</span>
<span class="gi">+            filtered_initializers.append(initializer)</span>
<span class="gi">+            filtered_initargs.append(initargs)</span>
<span class="gi">+</span>
<span class="gi">+    if not filtered_initializers:</span>
<span class="gi">+        return None, ()</span>
<span class="gi">+    elif len(filtered_initializers) == 1:</span>
<span class="gi">+        return filtered_initializers[0], filtered_initargs[0]</span>
<span class="gi">+    else:</span>
<span class="gi">+        return _ChainedInitializer(filtered_initializers), filtered_initargs</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _prepare_initializer(initializer, initargs):</span>
<span class="gi">+    if initializer is not None and not callable(initializer):</span>
<span class="gi">+        raise TypeError(</span>
<span class="gi">+            f&quot;initializer must be a callable, got: {initializer!r}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # Introspect runtime to determine if we need to propagate the viztracer</span>
<span class="gi">+    # profiler information to the workers:</span>
<span class="gi">+    return _chain_initializers(</span>
<span class="gi">+        [</span>
<span class="gi">+            (initializer, initargs),</span>
<span class="gi">+            _make_viztracer_initializer_and_initargs(),</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/joblib/externals/loky/process_executor.py b/joblib/externals/loky/process_executor.py</span>
<span class="gh">index c68582c..3040719 100644</span>
<span class="gd">--- a/joblib/externals/loky/process_executor.py</span>
<span class="gi">+++ b/joblib/externals/loky/process_executor.py</span>
<span class="gu">@@ -1,3 +1,17 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Re-implementation of the ProcessPoolExecutor more robust to faults</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="gi">+# adapted from concurrent/futures/process_pool_executor.py (17/02/2017)</span>
<span class="gi">+#  * Add an extra management thread to detect executor_manager_thread failures,</span>
<span class="gi">+#  * Improve the shutdown process to avoid deadlocks,</span>
<span class="gi">+#  * Add timeout for workers,</span>
<span class="gi">+#  * More robust pickling process.</span>
<span class="gi">+#</span>
<span class="gi">+# Copyright 2009 Brian Quinlan. All Rights Reserved.</span>
<span class="gi">+# Licensed to PSF under a Contributor Agreement.</span>
<span class="gi">+</span>
<span class="w"> </span>&quot;&quot;&quot;Implements ProcessPoolExecutor.

<span class="w"> </span>The follow diagram and text describe the data-flow through the system:
<span class="gu">@@ -37,9 +51,13 @@ Local worker thread:</span>

<span class="w"> </span>Process #1..n:
<span class="w"> </span>- reads _CallItems from &quot;Call Q&quot;, executes the calls, and puts the resulting
<span class="gd">-  _ResultItems in &quot;Result Q\&quot;</span>
<span class="gi">+  _ResultItems in &quot;Result Q&quot;</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="gd">-__author__ = &#39;Thomas Moreau (thomas.moreau.2010@gmail.com)&#39;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+__author__ = &quot;Thomas Moreau (thomas.moreau.2010@gmail.com)&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import gc
<span class="w"> </span>import sys
<span class="gu">@@ -58,6 +76,7 @@ from concurrent.futures import Executor</span>
<span class="w"> </span>from concurrent.futures._base import LOGGER
<span class="w"> </span>from concurrent.futures.process import BrokenProcessPool as _BPPException
<span class="w"> </span>from multiprocessing.connection import wait
<span class="gi">+</span>
<span class="w"> </span>from ._base import Future
<span class="w"> </span>from .backend import get_context
<span class="w"> </span>from .backend.context import cpu_count, _MAX_WINDOWS_WORKERS
<span class="gu">@@ -65,23 +84,58 @@ from .backend.queues import Queue, SimpleQueue</span>
<span class="w"> </span>from .backend.reduction import set_loky_pickler, get_loky_pickler_name
<span class="w"> </span>from .backend.utils import kill_process_tree, get_exitcodes_terminated_worker
<span class="w"> </span>from .initializers import _prepare_initializer
<span class="gd">-MAX_DEPTH = int(os.environ.get(&#39;LOKY_MAX_DEPTH&#39;, 10))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Mechanism to prevent infinite process spawning. When a worker of a</span>
<span class="gi">+# ProcessPoolExecutor nested in MAX_DEPTH Executor tries to create a new</span>
<span class="gi">+# Executor, a LokyRecursionError is raised</span>
<span class="gi">+MAX_DEPTH = int(os.environ.get(&quot;LOKY_MAX_DEPTH&quot;, 10))</span>
<span class="w"> </span>_CURRENT_DEPTH = 0
<span class="gi">+</span>
<span class="gi">+# Minimum time interval between two consecutive memory leak protection checks.</span>
<span class="w"> </span>_MEMORY_LEAK_CHECK_DELAY = 1.0
<span class="gd">-_MAX_MEMORY_LEAK_SIZE = int(300000000.0)</span>
<span class="gi">+</span>
<span class="gi">+# Number of bytes of memory usage allowed over the reference process size.</span>
<span class="gi">+_MAX_MEMORY_LEAK_SIZE = int(3e8)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    from psutil import Process
<span class="gi">+</span>
<span class="w"> </span>    _USE_PSUTIL = True
<span class="gi">+</span>
<span class="gi">+    def _get_memory_usage(pid, force_gc=False):</span>
<span class="gi">+        if force_gc:</span>
<span class="gi">+            gc.collect()</span>
<span class="gi">+</span>
<span class="gi">+        mem_size = Process(pid).memory_info().rss</span>
<span class="gi">+        mp.util.debug(f&quot;psutil return memory size: {mem_size}&quot;)</span>
<span class="gi">+        return mem_size</span>
<span class="gi">+</span>
<span class="w"> </span>except ImportError:
<span class="w"> </span>    _USE_PSUTIL = False


<span class="w"> </span>class _ThreadWakeup:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self):
<span class="w"> </span>        self._closed = False
<span class="w"> </span>        self._reader, self._writer = mp.Pipe(duplex=False)

<span class="gi">+    def close(self):</span>
<span class="gi">+        if not self._closed:</span>
<span class="gi">+            self._closed = True</span>
<span class="gi">+            self._writer.close()</span>
<span class="gi">+            self._reader.close()</span>
<span class="gi">+</span>
<span class="gi">+    def wakeup(self):</span>
<span class="gi">+        if not self._closed:</span>
<span class="gi">+            self._writer.send_bytes(b&quot;&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def clear(self):</span>
<span class="gi">+        if not self._closed:</span>
<span class="gi">+            while self._reader.poll():</span>
<span class="gi">+                self._reader.recv_bytes()</span>
<span class="gi">+</span>

<span class="w"> </span>class _ExecutorFlags:
<span class="w"> </span>    &quot;&quot;&quot;necessary references to maintain executor states without preventing gc
<span class="gu">@@ -92,17 +146,84 @@ class _ExecutorFlags:</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, shutdown_lock):
<span class="gi">+</span>
<span class="w"> </span>        self.shutdown = False
<span class="w"> </span>        self.broken = None
<span class="w"> </span>        self.kill_workers = False
<span class="w"> </span>        self.shutdown_lock = shutdown_lock

<span class="gi">+    def flag_as_shutting_down(self, kill_workers=None):</span>
<span class="gi">+        with self.shutdown_lock:</span>
<span class="gi">+            self.shutdown = True</span>
<span class="gi">+            if kill_workers is not None:</span>
<span class="gi">+                self.kill_workers = kill_workers</span>
<span class="gi">+</span>
<span class="gi">+    def flag_as_broken(self, broken):</span>
<span class="gi">+        with self.shutdown_lock:</span>
<span class="gi">+            self.shutdown = True</span>
<span class="gi">+            self.broken = broken</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Prior to 3.9, executor_manager_thread is created as daemon thread. This means</span>
<span class="gi">+# that it is not joined automatically when the interpreter is shutting down.</span>
<span class="gi">+# To work around this problem, an exit handler is installed to tell the</span>
<span class="gi">+# thread to exit when the interpreter is shutting down and then waits until</span>
<span class="gi">+# it finishes. The thread needs to be daemonized because the atexit hooks are</span>
<span class="gi">+# called after all non daemonized threads are joined.</span>
<span class="gi">+#</span>
<span class="gi">+# Starting 3.9, there exists a specific atexit hook to be called before joining</span>
<span class="gi">+# the threads so the executor_manager_thread does not need to be daemonized</span>
<span class="gi">+# anymore.</span>
<span class="gi">+#</span>
<span class="gi">+# The atexit hooks are registered when starting the first ProcessPoolExecutor</span>
<span class="gi">+# to avoid import having an effect on the interpreter.</span>

<span class="w"> </span>_global_shutdown = False
<span class="w"> </span>_global_shutdown_lock = threading.Lock()
<span class="w"> </span>_threads_wakeups = weakref.WeakKeyDictionary()
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _python_exit():</span>
<span class="gi">+    global _global_shutdown</span>
<span class="gi">+    _global_shutdown = True</span>
<span class="gi">+</span>
<span class="gi">+    # Materialize the list of items to avoid error due to iterating over</span>
<span class="gi">+    # changing size dictionary.</span>
<span class="gi">+    items = list(_threads_wakeups.items())</span>
<span class="gi">+    if len(items) &gt; 0:</span>
<span class="gi">+        mp.util.debug(</span>
<span class="gi">+            &quot;Interpreter shutting down. Waking up {len(items)}&quot;</span>
<span class="gi">+            f&quot;executor_manager_thread:\n{items}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # Wake up the executor_manager_thread&#39;s so they can detect the interpreter</span>
<span class="gi">+    # is shutting down and exit.</span>
<span class="gi">+    for _, (shutdown_lock, thread_wakeup) in items:</span>
<span class="gi">+        with shutdown_lock:</span>
<span class="gi">+            thread_wakeup.wakeup()</span>
<span class="gi">+</span>
<span class="gi">+    # Collect the executor_manager_thread&#39;s to make sure we exit cleanly.</span>
<span class="gi">+    for thread, _ in items:</span>
<span class="gi">+        # This locks is to prevent situations where an executor is gc&#39;ed in one</span>
<span class="gi">+        # thread while the atexit finalizer is running in another thread. This</span>
<span class="gi">+        # can happen when joblib is used in pypy for instance.</span>
<span class="gi">+        with _global_shutdown_lock:</span>
<span class="gi">+            thread.join()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# With the fork context, _thread_wakeups is propagated to children.</span>
<span class="gi">+# Clear it after fork to avoid some situation that can cause some</span>
<span class="gi">+# freeze when joining the workers.</span>
<span class="w"> </span>mp.util.register_after_fork(_threads_wakeups, lambda obj: obj.clear())
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Module variable to register the at_exit call</span>
<span class="w"> </span>process_pool_executor_at_exit = None
<span class="gi">+</span>
<span class="gi">+# Controls how many more calls than processes will be queued in the call queue.</span>
<span class="gi">+# A smaller number will mean that processes spend more time idle waiting for</span>
<span class="gi">+# work while a larger number will make Future.cancel() succeed less frequently</span>
<span class="gi">+# (Futures in the call queue cannot be cancelled).</span>
<span class="w"> </span>EXTRA_QUEUED_CALLS = 1


<span class="gu">@@ -116,14 +237,15 @@ class _RemoteTraceback(Exception):</span>
<span class="w"> </span>        return self.tb


<span class="gi">+# Do not inherit from BaseException to mirror</span>
<span class="gi">+# concurrent.futures.process._ExceptionWithTraceback</span>
<span class="w"> </span>class _ExceptionWithTraceback:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, exc):
<span class="gd">-        tb = getattr(exc, &#39;__traceback__&#39;, None)</span>
<span class="gi">+        tb = getattr(exc, &quot;__traceback__&quot;, None)</span>
<span class="w"> </span>        if tb is None:
<span class="w"> </span>            _, _, tb = sys.exc_info()
<span class="w"> </span>        tb = traceback.format_exception(type(exc), exc, tb)
<span class="gd">-        tb = &#39;&#39;.join(tb)</span>
<span class="gi">+        tb = &quot;&quot;.join(tb)</span>
<span class="w"> </span>        self.exc = exc
<span class="w"> </span>        self.tb = tb

<span class="gu">@@ -131,8 +253,14 @@ class _ExceptionWithTraceback:</span>
<span class="w"> </span>        return _rebuild_exc, (self.exc, self.tb)


<span class="gi">+def _rebuild_exc(exc, tb):</span>
<span class="gi">+    exc.__cause__ = _RemoteTraceback(tb)</span>
<span class="gi">+    return exc</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class _WorkItem:
<span class="gd">-    __slots__ = [&#39;future&#39;, &#39;fn&#39;, &#39;args&#39;, &#39;kwargs&#39;]</span>
<span class="gi">+</span>
<span class="gi">+    __slots__ = [&quot;future&quot;, &quot;fn&quot;, &quot;args&quot;, &quot;kwargs&quot;]</span>

<span class="w"> </span>    def __init__(self, future, fn, args, kwargs):
<span class="w"> </span>        self.future = future
<span class="gu">@@ -142,7 +270,6 @@ class _WorkItem:</span>


<span class="w"> </span>class _ResultItem:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, work_id, exception=None, result=None):
<span class="w"> </span>        self.work_id = work_id
<span class="w"> </span>        self.exception = exception
<span class="gu">@@ -150,12 +277,13 @@ class _ResultItem:</span>


<span class="w"> </span>class _CallItem:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, work_id, fn, args, kwargs):
<span class="w"> </span>        self.work_id = work_id
<span class="w"> </span>        self.fn = fn
<span class="w"> </span>        self.args = args
<span class="w"> </span>        self.kwargs = kwargs
<span class="gi">+</span>
<span class="gi">+        # Store the current loky_pickler so it is correctly set in the worker</span>
<span class="w"> </span>        self.loky_pickler = get_loky_pickler_name()

<span class="w"> </span>    def __call__(self):
<span class="gu">@@ -164,23 +292,64 @@ class _CallItem:</span>

<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        return (
<span class="gd">-            f&#39;CallItem({self.work_id}, {self.fn}, {self.args}, {self.kwargs})&#39;)</span>
<span class="gi">+            f&quot;CallItem({self.work_id}, {self.fn}, {self.args}, {self.kwargs})&quot;</span>
<span class="gi">+        )</span>


<span class="w"> </span>class _SafeQueue(Queue):
<span class="w"> </span>    &quot;&quot;&quot;Safe Queue set exception to the future object linked to a job&quot;&quot;&quot;

<span class="gd">-    def __init__(self, max_size=0, ctx=None, pending_work_items=None,</span>
<span class="gd">-        running_work_items=None, thread_wakeup=None, reducers=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        max_size=0,</span>
<span class="gi">+        ctx=None,</span>
<span class="gi">+        pending_work_items=None,</span>
<span class="gi">+        running_work_items=None,</span>
<span class="gi">+        thread_wakeup=None,</span>
<span class="gi">+        reducers=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        self.thread_wakeup = thread_wakeup
<span class="w"> </span>        self.pending_work_items = pending_work_items
<span class="w"> </span>        self.running_work_items = running_work_items
<span class="w"> </span>        super().__init__(max_size, reducers=reducers, ctx=ctx)

<span class="gi">+    def _on_queue_feeder_error(self, e, obj):</span>
<span class="gi">+        if isinstance(obj, _CallItem):</span>
<span class="gi">+            # format traceback only works on python3</span>
<span class="gi">+            if isinstance(e, struct.error):</span>
<span class="gi">+                raised_error = RuntimeError(</span>
<span class="gi">+                    &quot;The task could not be sent to the workers as it is too &quot;</span>
<span class="gi">+                    &quot;large for `send_bytes`.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                raised_error = PicklingError(</span>
<span class="gi">+                    &quot;Could not pickle the task to send it to the workers.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            tb = traceback.format_exception(</span>
<span class="gi">+                type(e), e, getattr(e, &quot;__traceback__&quot;, None)</span>
<span class="gi">+            )</span>
<span class="gi">+            raised_error.__cause__ = _RemoteTraceback(&quot;&quot;.join(tb))</span>
<span class="gi">+            work_item = self.pending_work_items.pop(obj.work_id, None)</span>
<span class="gi">+            self.running_work_items.remove(obj.work_id)</span>
<span class="gi">+            # work_item can be None if another process terminated. In this</span>
<span class="gi">+            # case, the executor_manager_thread fails all work_items with</span>
<span class="gi">+            # BrokenProcessPool</span>
<span class="gi">+            if work_item is not None:</span>
<span class="gi">+                work_item.future.set_exception(raised_error)</span>
<span class="gi">+                del work_item</span>
<span class="gi">+            self.thread_wakeup.wakeup()</span>
<span class="gi">+        else:</span>
<span class="gi">+            super()._on_queue_feeder_error(e, obj)</span>
<span class="gi">+</span>

<span class="w"> </span>def _get_chunks(chunksize, *iterables):
<span class="w"> </span>    &quot;&quot;&quot;Iterates over zip()ed iterables in chunks.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    it = zip(*iterables)</span>
<span class="gi">+    while True:</span>
<span class="gi">+        chunk = tuple(itertools.islice(it, chunksize))</span>
<span class="gi">+        if not chunk:</span>
<span class="gi">+            return</span>
<span class="gi">+        yield chunk</span>


<span class="w"> </span>def _process_chunk(fn, chunk):
<span class="gu">@@ -192,16 +361,30 @@ def _process_chunk(fn, chunk):</span>
<span class="w"> </span>    This function is run in a separate process.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return [fn(*args) for args in chunk]</span>


<span class="w"> </span>def _sendback_result(result_queue, work_id, result=None, exception=None):
<span class="w"> </span>    &quot;&quot;&quot;Safely send back the given result or exception&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def _process_worker(call_queue, result_queue, initializer, initargs,</span>
<span class="gd">-    processes_management_lock, timeout, worker_exit_lock, current_depth):</span>
<span class="gi">+    try:</span>
<span class="gi">+        result_queue.put(</span>
<span class="gi">+            _ResultItem(work_id, result=result, exception=exception)</span>
<span class="gi">+        )</span>
<span class="gi">+    except BaseException as e:</span>
<span class="gi">+        exc = _ExceptionWithTraceback(e)</span>
<span class="gi">+        result_queue.put(_ResultItem(work_id, exception=exc))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _process_worker(</span>
<span class="gi">+    call_queue,</span>
<span class="gi">+    result_queue,</span>
<span class="gi">+    initializer,</span>
<span class="gi">+    initargs,</span>
<span class="gi">+    processes_management_lock,</span>
<span class="gi">+    timeout,</span>
<span class="gi">+    worker_exit_lock,</span>
<span class="gi">+    current_depth,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Evaluates calls from call_queue and places the results in result_queue.

<span class="w"> </span>    This worker is run in a separate process.
<span class="gu">@@ -221,7 +404,111 @@ def _process_worker(call_queue, result_queue, initializer, initargs,</span>
<span class="w"> </span>            workers timeout.
<span class="w"> </span>        current_depth: Nested parallelism level, to avoid infinite spawning.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if initializer is not None:</span>
<span class="gi">+        try:</span>
<span class="gi">+            initializer(*initargs)</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            LOGGER.critical(&quot;Exception in initializer:&quot;, exc_info=True)</span>
<span class="gi">+            # The parent will notice that the process stopped and</span>
<span class="gi">+            # mark the pool broken</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+    # set the global _CURRENT_DEPTH mechanism to limit recursive call</span>
<span class="gi">+    global _CURRENT_DEPTH</span>
<span class="gi">+    _CURRENT_DEPTH = current_depth</span>
<span class="gi">+    _process_reference_size = None</span>
<span class="gi">+    _last_memory_leak_check = None</span>
<span class="gi">+    pid = os.getpid()</span>
<span class="gi">+</span>
<span class="gi">+    mp.util.debug(f&quot;Worker started with timeout={timeout}&quot;)</span>
<span class="gi">+    while True:</span>
<span class="gi">+        try:</span>
<span class="gi">+            call_item = call_queue.get(block=True, timeout=timeout)</span>
<span class="gi">+            if call_item is None:</span>
<span class="gi">+                mp.util.info(&quot;Shutting down worker on sentinel&quot;)</span>
<span class="gi">+        except queue.Empty:</span>
<span class="gi">+            mp.util.info(f&quot;Shutting down worker after timeout {timeout:0.3f}s&quot;)</span>
<span class="gi">+            if processes_management_lock.acquire(block=False):</span>
<span class="gi">+                processes_management_lock.release()</span>
<span class="gi">+                call_item = None</span>
<span class="gi">+            else:</span>
<span class="gi">+                mp.util.info(&quot;Could not acquire processes_management_lock&quot;)</span>
<span class="gi">+                continue</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            previous_tb = traceback.format_exc()</span>
<span class="gi">+            try:</span>
<span class="gi">+                result_queue.put(_RemoteTraceback(previous_tb))</span>
<span class="gi">+            except BaseException:</span>
<span class="gi">+                # If we cannot format correctly the exception, at least print</span>
<span class="gi">+                # the traceback.</span>
<span class="gi">+                print(previous_tb)</span>
<span class="gi">+            mp.util.debug(&quot;Exiting with code 1&quot;)</span>
<span class="gi">+            sys.exit(1)</span>
<span class="gi">+        if call_item is None:</span>
<span class="gi">+            # Notify queue management thread about worker shutdown</span>
<span class="gi">+            result_queue.put(pid)</span>
<span class="gi">+            is_clean = worker_exit_lock.acquire(True, timeout=30)</span>
<span class="gi">+</span>
<span class="gi">+            # Early notify any loky executor running in this worker process</span>
<span class="gi">+            # (nested parallelism) that this process is about to shutdown to</span>
<span class="gi">+            # avoid a deadlock waiting undifinitely for the worker to finish.</span>
<span class="gi">+            _python_exit()</span>
<span class="gi">+</span>
<span class="gi">+            if is_clean:</span>
<span class="gi">+                mp.util.debug(&quot;Exited cleanly&quot;)</span>
<span class="gi">+            else:</span>
<span class="gi">+                mp.util.info(&quot;Main process did not release worker_exit&quot;)</span>
<span class="gi">+            return</span>
<span class="gi">+        try:</span>
<span class="gi">+            r = call_item()</span>
<span class="gi">+        except BaseException as e:</span>
<span class="gi">+            exc = _ExceptionWithTraceback(e)</span>
<span class="gi">+            result_queue.put(_ResultItem(call_item.work_id, exception=exc))</span>
<span class="gi">+        else:</span>
<span class="gi">+            _sendback_result(result_queue, call_item.work_id, result=r)</span>
<span class="gi">+            del r</span>
<span class="gi">+</span>
<span class="gi">+        # Free the resource as soon as possible, to avoid holding onto</span>
<span class="gi">+        # open files or shared memory that is not needed anymore</span>
<span class="gi">+        del call_item</span>
<span class="gi">+</span>
<span class="gi">+        if _USE_PSUTIL:</span>
<span class="gi">+            if _process_reference_size is None:</span>
<span class="gi">+                # Make reference measurement after the first call</span>
<span class="gi">+                _process_reference_size = _get_memory_usage(pid, force_gc=True)</span>
<span class="gi">+                _last_memory_leak_check = time()</span>
<span class="gi">+                continue</span>
<span class="gi">+            if time() - _last_memory_leak_check &gt; _MEMORY_LEAK_CHECK_DELAY:</span>
<span class="gi">+                mem_usage = _get_memory_usage(pid)</span>
<span class="gi">+                _last_memory_leak_check = time()</span>
<span class="gi">+                if mem_usage - _process_reference_size &lt; _MAX_MEMORY_LEAK_SIZE:</span>
<span class="gi">+                    # Memory usage stays within bounds: everything is fine.</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                # Check again memory usage; this time take the measurement</span>
<span class="gi">+                # after a forced garbage collection to break any reference</span>
<span class="gi">+                # cycles.</span>
<span class="gi">+                mem_usage = _get_memory_usage(pid, force_gc=True)</span>
<span class="gi">+                _last_memory_leak_check = time()</span>
<span class="gi">+                if mem_usage - _process_reference_size &lt; _MAX_MEMORY_LEAK_SIZE:</span>
<span class="gi">+                    # The GC managed to free the memory: everything is fine.</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                # The process is leaking memory: let the main process</span>
<span class="gi">+                # know that we need to start a new worker.</span>
<span class="gi">+                mp.util.info(&quot;Memory leak detected: shutting down worker&quot;)</span>
<span class="gi">+                result_queue.put(pid)</span>
<span class="gi">+                with worker_exit_lock:</span>
<span class="gi">+                    mp.util.debug(&quot;Exit due to memory leak&quot;)</span>
<span class="gi">+                    return</span>
<span class="gi">+        else:</span>
<span class="gi">+            # if psutil is not installed, trigger gc.collect events</span>
<span class="gi">+            # regularly to limit potential memory leaks due to reference cycles</span>
<span class="gi">+            if _last_memory_leak_check is None or (</span>
<span class="gi">+                time() - _last_memory_leak_check &gt; _MEMORY_LEAK_CHECK_DELAY</span>
<span class="gi">+            ):</span>
<span class="gi">+                gc.collect()</span>
<span class="gi">+                _last_memory_leak_check = time()</span>


<span class="w"> </span>class _ExecutorManagerThread(threading.Thread):
<span class="gu">@@ -237,42 +524,468 @@ class _ExecutorManagerThread(threading.Thread):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, executor):
<span class="gi">+        # Store references to necessary internals of the executor.</span>
<span class="gi">+</span>
<span class="gi">+        # A _ThreadWakeup to allow waking up the executor_manager_thread from</span>
<span class="gi">+        # the main Thread and avoid deadlocks caused by permanently</span>
<span class="gi">+        # locked queues.</span>
<span class="w"> </span>        self.thread_wakeup = executor._executor_manager_thread_wakeup
<span class="w"> </span>        self.shutdown_lock = executor._shutdown_lock

<span class="gd">-        def weakref_cb(_, thread_wakeup=self.thread_wakeup, shutdown_lock=</span>
<span class="gd">-            self.shutdown_lock):</span>
<span class="gi">+        # A weakref.ref to the ProcessPoolExecutor that owns this thread. Used</span>
<span class="gi">+        # to determine if the ProcessPoolExecutor has been garbage collected</span>
<span class="gi">+        # and that the manager can exit.</span>
<span class="gi">+        # When the executor gets garbage collected, the weakref callback</span>
<span class="gi">+        # will wake up the queue management thread so that it can terminate</span>
<span class="gi">+        # if there is no pending work item.</span>
<span class="gi">+        def weakref_cb(</span>
<span class="gi">+            _,</span>
<span class="gi">+            thread_wakeup=self.thread_wakeup,</span>
<span class="gi">+            shutdown_lock=self.shutdown_lock,</span>
<span class="gi">+        ):</span>
<span class="w"> </span>            if mp is not None:
<span class="gi">+                # At this point, the multiprocessing module can already be</span>
<span class="gi">+                # garbage collected. We only log debug info when still</span>
<span class="gi">+                # possible.</span>
<span class="w"> </span>                mp.util.debug(
<span class="gd">-                    &#39;Executor collected: triggering callback for QueueManager wakeup&#39;</span>
<span class="gd">-                    )</span>
<span class="gi">+                    &quot;Executor collected: triggering callback for&quot;</span>
<span class="gi">+                    &quot; QueueManager wakeup&quot;</span>
<span class="gi">+                )</span>
<span class="w"> </span>            with shutdown_lock:
<span class="w"> </span>                thread_wakeup.wakeup()
<span class="gi">+</span>
<span class="w"> </span>        self.executor_reference = weakref.ref(executor, weakref_cb)
<span class="gi">+</span>
<span class="gi">+        # The flags of the executor</span>
<span class="w"> </span>        self.executor_flags = executor._flags
<span class="gi">+</span>
<span class="gi">+        # A list of the ctx.Process instances used as workers.</span>
<span class="w"> </span>        self.processes = executor._processes
<span class="gi">+</span>
<span class="gi">+        # A ctx.Queue that will be filled with _CallItems derived from</span>
<span class="gi">+        # _WorkItems for processing by the process workers.</span>
<span class="w"> </span>        self.call_queue = executor._call_queue
<span class="gi">+</span>
<span class="gi">+        # A ctx.SimpleQueue of _ResultItems generated by the process workers.</span>
<span class="w"> </span>        self.result_queue = executor._result_queue
<span class="gi">+</span>
<span class="gi">+        # A queue.Queue of work ids e.g. Queue([5, 6, ...]).</span>
<span class="w"> </span>        self.work_ids_queue = executor._work_ids
<span class="gi">+</span>
<span class="gi">+        # A dict mapping work ids to _WorkItems e.g.</span>
<span class="gi">+        #     {5: &lt;_WorkItem...&gt;, 6: &lt;_WorkItem...&gt;, ...}</span>
<span class="w"> </span>        self.pending_work_items = executor._pending_work_items
<span class="gi">+</span>
<span class="gi">+        # A list of the work_ids that are currently running</span>
<span class="w"> </span>        self.running_work_items = executor._running_work_items
<span class="gi">+</span>
<span class="gi">+        # A lock to avoid concurrent shutdown of workers on timeout and spawn</span>
<span class="gi">+        # of new processes or shut down</span>
<span class="w"> </span>        self.processes_management_lock = executor._processes_management_lock
<span class="gd">-        super().__init__(name=&#39;ExecutorManagerThread&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        super().__init__(name=&quot;ExecutorManagerThread&quot;)</span>
<span class="w"> </span>        if sys.version_info &lt; (3, 9):
<span class="w"> </span>            self.daemon = True

<span class="gi">+    def run(self):</span>
<span class="gi">+        # Main loop for the executor manager thread.</span>
<span class="gi">+</span>
<span class="gi">+        while True:</span>
<span class="gi">+            self.add_call_item_to_queue()</span>
<span class="gi">+</span>
<span class="gi">+            result_item, is_broken, bpe = self.wait_result_broken_or_wakeup()</span>
<span class="gi">+</span>
<span class="gi">+            if is_broken:</span>
<span class="gi">+                self.terminate_broken(bpe)</span>
<span class="gi">+                return</span>
<span class="gi">+            if result_item is not None:</span>
<span class="gi">+                self.process_result_item(result_item)</span>
<span class="gi">+                # Delete reference to result_item to avoid keeping references</span>
<span class="gi">+                # while waiting on new results.</span>
<span class="gi">+                del result_item</span>
<span class="gi">+</span>
<span class="gi">+            if self.is_shutting_down():</span>
<span class="gi">+                self.flag_executor_shutting_down()</span>
<span class="gi">+</span>
<span class="gi">+                # Since no new work items can be added, it is safe to shutdown</span>
<span class="gi">+                # this thread if there are no pending work items.</span>
<span class="gi">+                if not self.pending_work_items:</span>
<span class="gi">+                    self.join_executor_internals()</span>
<span class="gi">+                    return</span>
<span class="gi">+</span>
<span class="gi">+    def add_call_item_to_queue(self):</span>
<span class="gi">+        # Fills call_queue with _WorkItems from pending_work_items.</span>
<span class="gi">+        # This function never blocks.</span>
<span class="gi">+        while True:</span>
<span class="gi">+            if self.call_queue.full():</span>
<span class="gi">+                return</span>
<span class="gi">+            try:</span>
<span class="gi">+                work_id = self.work_ids_queue.get(block=False)</span>
<span class="gi">+            except queue.Empty:</span>
<span class="gi">+                return</span>
<span class="gi">+            else:</span>
<span class="gi">+                work_item = self.pending_work_items[work_id]</span>
<span class="gi">+</span>
<span class="gi">+                if work_item.future.set_running_or_notify_cancel():</span>
<span class="gi">+                    self.running_work_items += [work_id]</span>
<span class="gi">+                    self.call_queue.put(</span>
<span class="gi">+                        _CallItem(</span>
<span class="gi">+                            work_id,</span>
<span class="gi">+                            work_item.fn,</span>
<span class="gi">+                            work_item.args,</span>
<span class="gi">+                            work_item.kwargs,</span>
<span class="gi">+                        ),</span>
<span class="gi">+                        block=True,</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    del self.pending_work_items[work_id]</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+    def wait_result_broken_or_wakeup(self):</span>
<span class="gi">+        # Wait for a result to be ready in the result_queue while checking</span>
<span class="gi">+        # that all worker processes are still running, or for a wake up</span>
<span class="gi">+        # signal send. The wake up signals come either from new tasks being</span>
<span class="gi">+        # submitted, from the executor being shutdown/gc-ed, or from the</span>
<span class="gi">+        # shutdown of the python interpreter.</span>
<span class="gi">+        result_reader = self.result_queue._reader</span>
<span class="gi">+        wakeup_reader = self.thread_wakeup._reader</span>
<span class="gi">+        readers = [result_reader, wakeup_reader]</span>
<span class="gi">+        worker_sentinels = [p.sentinel for p in list(self.processes.values())]</span>
<span class="gi">+        ready = wait(readers + worker_sentinels)</span>
<span class="gi">+</span>
<span class="gi">+        bpe = None</span>
<span class="gi">+        is_broken = True</span>
<span class="gi">+        result_item = None</span>
<span class="gi">+        if result_reader in ready:</span>
<span class="gi">+            try:</span>
<span class="gi">+                result_item = result_reader.recv()</span>
<span class="gi">+                if isinstance(result_item, _RemoteTraceback):</span>
<span class="gi">+                    bpe = BrokenProcessPool(</span>
<span class="gi">+                        &quot;A task has failed to un-serialize. Please ensure that&quot;</span>
<span class="gi">+                        &quot; the arguments of the function are all picklable.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                    bpe.__cause__ = result_item</span>
<span class="gi">+                else:</span>
<span class="gi">+                    is_broken = False</span>
<span class="gi">+            except BaseException as e:</span>
<span class="gi">+                bpe = BrokenProcessPool(</span>
<span class="gi">+                    &quot;A result has failed to un-serialize. Please ensure that &quot;</span>
<span class="gi">+                    &quot;the objects returned by the function are always &quot;</span>
<span class="gi">+                    &quot;picklable.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                tb = traceback.format_exception(</span>
<span class="gi">+                    type(e), e, getattr(e, &quot;__traceback__&quot;, None)</span>
<span class="gi">+                )</span>
<span class="gi">+                bpe.__cause__ = _RemoteTraceback(&quot;&quot;.join(tb))</span>
<span class="gi">+</span>
<span class="gi">+        elif wakeup_reader in ready:</span>
<span class="gi">+            # This is simply a wake-up event that might either trigger putting</span>
<span class="gi">+            # more tasks in the queue or trigger the clean up of resources.</span>
<span class="gi">+            is_broken = False</span>
<span class="gi">+        else:</span>
<span class="gi">+            # A worker has terminated and we don&#39;t know why, set the state of</span>
<span class="gi">+            # the executor as broken</span>
<span class="gi">+            exit_codes = &quot;&quot;</span>
<span class="gi">+            if sys.platform != &quot;win32&quot;:</span>
<span class="gi">+                # In Windows, introspecting terminated workers exitcodes seems</span>
<span class="gi">+                # unstable, therefore they are not appended in the exception</span>
<span class="gi">+                # message.</span>
<span class="gi">+                exit_codes = (</span>
<span class="gi">+                    &quot;\nThe exit codes of the workers are &quot;</span>
<span class="gi">+                    f&quot;{get_exitcodes_terminated_worker(self.processes)}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            mp.util.debug(</span>
<span class="gi">+                &quot;A worker unexpectedly terminated. Workers that &quot;</span>
<span class="gi">+                &quot;might have caused the breakage: &quot;</span>
<span class="gi">+                + str(</span>
<span class="gi">+                    {</span>
<span class="gi">+                        p.name: p.exitcode</span>
<span class="gi">+                        for p in list(self.processes.values())</span>
<span class="gi">+                        if p is not None and p.sentinel in ready</span>
<span class="gi">+                    }</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+            bpe = TerminatedWorkerError(</span>
<span class="gi">+                &quot;A worker process managed by the executor was unexpectedly &quot;</span>
<span class="gi">+                &quot;terminated. This could be caused by a segmentation fault &quot;</span>
<span class="gi">+                &quot;while calling the function or by an excessive memory usage &quot;</span>
<span class="gi">+                &quot;causing the Operating System to kill the worker.\n&quot;</span>
<span class="gi">+                f&quot;{exit_codes}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.thread_wakeup.clear()</span>
<span class="gi">+</span>
<span class="gi">+        return result_item, is_broken, bpe</span>
<span class="gi">+</span>
<span class="gi">+    def process_result_item(self, result_item):</span>
<span class="gi">+        # Process the received a result_item. This can be either the PID of a</span>
<span class="gi">+        # worker that exited gracefully or a _ResultItem</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(result_item, int):</span>
<span class="gi">+            # Clean shutdown of a worker using its PID, either on request</span>
<span class="gi">+            # by the executor.shutdown method or by the timeout of the worker</span>
<span class="gi">+            # itself: we should not mark the executor as broken.</span>
<span class="gi">+            with self.processes_management_lock:</span>
<span class="gi">+                p = self.processes.pop(result_item, None)</span>
<span class="gi">+</span>
<span class="gi">+            # p can be None if the executor is concurrently shutting down.</span>
<span class="gi">+            if p is not None:</span>
<span class="gi">+                p._worker_exit_lock.release()</span>
<span class="gi">+                mp.util.debug(</span>
<span class="gi">+                    f&quot;joining {p.name} when processing {p.pid} as result_item&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                p.join()</span>
<span class="gi">+                del p</span>
<span class="gi">+</span>
<span class="gi">+            # Make sure the executor have the right number of worker, even if a</span>
<span class="gi">+            # worker timeout while some jobs were submitted. If some work is</span>
<span class="gi">+            # pending or there is less processes than running items, we need to</span>
<span class="gi">+            # start a new Process and raise a warning.</span>
<span class="gi">+            n_pending = len(self.pending_work_items)</span>
<span class="gi">+            n_running = len(self.running_work_items)</span>
<span class="gi">+            if n_pending - n_running &gt; 0 or n_running &gt; len(self.processes):</span>
<span class="gi">+                executor = self.executor_reference()</span>
<span class="gi">+                if (</span>
<span class="gi">+                    executor is not None</span>
<span class="gi">+                    and len(self.processes) &lt; executor._max_workers</span>
<span class="gi">+                ):</span>
<span class="gi">+                    warnings.warn(</span>
<span class="gi">+                        &quot;A worker stopped while some jobs were given to the &quot;</span>
<span class="gi">+                        &quot;executor. This can be caused by a too short worker &quot;</span>
<span class="gi">+                        &quot;timeout or by a memory leak.&quot;,</span>
<span class="gi">+                        UserWarning,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    with executor._processes_management_lock:</span>
<span class="gi">+                        executor._adjust_process_count()</span>
<span class="gi">+                    executor = None</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Received a _ResultItem so mark the future as completed.</span>
<span class="gi">+            work_item = self.pending_work_items.pop(result_item.work_id, None)</span>
<span class="gi">+            # work_item can be None if another process terminated (see above)</span>
<span class="gi">+            if work_item is not None:</span>
<span class="gi">+                if result_item.exception:</span>
<span class="gi">+                    work_item.future.set_exception(result_item.exception)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    work_item.future.set_result(result_item.result)</span>
<span class="gi">+                self.running_work_items.remove(result_item.work_id)</span>
<span class="gi">+</span>
<span class="gi">+    def is_shutting_down(self):</span>
<span class="gi">+        # Check whether we should start shutting down the executor.</span>
<span class="gi">+        executor = self.executor_reference()</span>
<span class="gi">+        # No more work items can be added if:</span>
<span class="gi">+        #   - The interpreter is shutting down OR</span>
<span class="gi">+        #   - The executor that owns this thread is not broken AND</span>
<span class="gi">+        #        * The executor that owns this worker has been collected OR</span>
<span class="gi">+        #        * The executor that owns this worker has been shutdown.</span>
<span class="gi">+        # If the executor is broken, it should be detected in the next loop.</span>
<span class="gi">+        return _global_shutdown or (</span>
<span class="gi">+            (executor is None or self.executor_flags.shutdown)</span>
<span class="gi">+            and not self.executor_flags.broken</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def terminate_broken(self, bpe):</span>
<span class="gi">+        # Terminate the executor because it is in a broken state. The bpe</span>
<span class="gi">+        # argument can be used to display more information on the error that</span>
<span class="gi">+        # lead the executor into becoming broken.</span>
<span class="gi">+</span>
<span class="gi">+        # Mark the process pool broken so that submits fail right now.</span>
<span class="gi">+        self.executor_flags.flag_as_broken(bpe)</span>
<span class="gi">+</span>
<span class="gi">+        # Mark pending tasks as failed.</span>
<span class="gi">+        for work_item in self.pending_work_items.values():</span>
<span class="gi">+            work_item.future.set_exception(bpe)</span>
<span class="gi">+            # Delete references to object. See issue16284</span>
<span class="gi">+            del work_item</span>
<span class="gi">+        self.pending_work_items.clear()</span>
<span class="gi">+</span>
<span class="gi">+        # Terminate remaining workers forcibly: the queues or their</span>
<span class="gi">+        # locks may be in a dirty state and block forever.</span>
<span class="gi">+        self.kill_workers(reason=&quot;broken executor&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # clean up resources</span>
<span class="gi">+        self.join_executor_internals()</span>
<span class="gi">+</span>
<span class="gi">+    def flag_executor_shutting_down(self):</span>
<span class="gi">+        # Flag the executor as shutting down and cancel remaining tasks if</span>
<span class="gi">+        # requested as early as possible if it is not gc-ed yet.</span>
<span class="gi">+        self.executor_flags.flag_as_shutting_down()</span>
<span class="gi">+</span>
<span class="gi">+        # Cancel pending work items if requested.</span>
<span class="gi">+        if self.executor_flags.kill_workers:</span>
<span class="gi">+            while self.pending_work_items:</span>
<span class="gi">+                _, work_item = self.pending_work_items.popitem()</span>
<span class="gi">+                work_item.future.set_exception(</span>
<span class="gi">+                    ShutdownExecutorError(</span>
<span class="gi">+                        &quot;The Executor was shutdown with `kill_workers=True` &quot;</span>
<span class="gi">+                        &quot;before this job could complete.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                )</span>
<span class="gi">+                del work_item</span>
<span class="gi">+</span>
<span class="gi">+            # Kill the remaining worker forcibly to no waste time joining them</span>
<span class="gi">+            self.kill_workers(reason=&quot;executor shutting down&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def kill_workers(self, reason=&quot;&quot;):</span>
<span class="gi">+        # Terminate the remaining workers using SIGKILL. This function also</span>
<span class="gi">+        # terminates descendant workers of the children in case there is some</span>
<span class="gi">+        # nested parallelism.</span>
<span class="gi">+        while self.processes:</span>
<span class="gi">+            _, p = self.processes.popitem()</span>
<span class="gi">+            mp.util.debug(f&quot;terminate process {p.name}, reason: {reason}&quot;)</span>
<span class="gi">+            try:</span>
<span class="gi">+                kill_process_tree(p)</span>
<span class="gi">+            except ProcessLookupError:  # pragma: no cover</span>
<span class="gi">+                pass</span>
<span class="gi">+</span>
<span class="gi">+    def shutdown_workers(self):</span>
<span class="gi">+        # shutdown all workers in self.processes</span>
<span class="gi">+</span>
<span class="gi">+        # Create a list to avoid RuntimeError due to concurrent modification of</span>
<span class="gi">+        # processes. nb_children_alive is thus an upper bound. Also release the</span>
<span class="gi">+        # processes&#39; _worker_exit_lock to accelerate the shutdown procedure, as</span>
<span class="gi">+        # there is no need for hand-shake here.</span>
<span class="gi">+        with self.processes_management_lock:</span>
<span class="gi">+            n_children_to_stop = 0</span>
<span class="gi">+            for p in list(self.processes.values()):</span>
<span class="gi">+                mp.util.debug(f&quot;releasing worker exit lock on {p.name}&quot;)</span>
<span class="gi">+                p._worker_exit_lock.release()</span>
<span class="gi">+                n_children_to_stop += 1</span>
<span class="gi">+</span>
<span class="gi">+        mp.util.debug(f&quot;found {n_children_to_stop} processes to stop&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # Send the right number of sentinels, to make sure all children are</span>
<span class="gi">+        # properly terminated. Do it with a mechanism that avoid hanging on</span>
<span class="gi">+        # Full queue when all workers have already been shutdown.</span>
<span class="gi">+        n_sentinels_sent = 0</span>
<span class="gi">+        cooldown_time = 0.001</span>
<span class="gi">+        while (</span>
<span class="gi">+            n_sentinels_sent &lt; n_children_to_stop</span>
<span class="gi">+            and self.get_n_children_alive() &gt; 0</span>
<span class="gi">+        ):</span>
<span class="gi">+            for _ in range(n_children_to_stop - n_sentinels_sent):</span>
<span class="gi">+                try:</span>
<span class="gi">+                    self.call_queue.put_nowait(None)</span>
<span class="gi">+                    n_sentinels_sent += 1</span>
<span class="gi">+                except queue.Full as e:</span>
<span class="gi">+                    if cooldown_time &gt; 5.0:</span>
<span class="gi">+                        mp.util.info(</span>
<span class="gi">+                            &quot;failed to send all sentinels and exit with error.&quot;</span>
<span class="gi">+                            f&quot;\ncall_queue size={self.call_queue._maxsize}; &quot;</span>
<span class="gi">+                            f&quot; full is {self.call_queue.full()}; &quot;</span>
<span class="gi">+                        )</span>
<span class="gi">+                        raise e</span>
<span class="gi">+                    mp.util.info(</span>
<span class="gi">+                        &quot;full call_queue prevented to send all sentinels at &quot;</span>
<span class="gi">+                        &quot;once, waiting...&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                    sleep(cooldown_time)</span>
<span class="gi">+                    cooldown_time *= 1.2</span>
<span class="gi">+                    break</span>
<span class="gi">+</span>
<span class="gi">+        mp.util.debug(f&quot;sent {n_sentinels_sent} sentinels to the call queue&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def join_executor_internals(self):</span>
<span class="gi">+        self.shutdown_workers()</span>
<span class="gi">+</span>
<span class="gi">+        # Release the queue&#39;s resources as soon as possible. Flag the feeder</span>
<span class="gi">+        # thread for clean exit to avoid having the crash detection thread flag</span>
<span class="gi">+        # the Executor as broken during the shutdown. This is safe as either:</span>
<span class="gi">+        #  * We don&#39;t need to communicate with the workers anymore</span>
<span class="gi">+        #  * There is nothing left in the Queue buffer except None sentinels</span>
<span class="gi">+        mp.util.debug(&quot;closing call_queue&quot;)</span>
<span class="gi">+        self.call_queue.close()</span>
<span class="gi">+        self.call_queue.join_thread()</span>
<span class="gi">+</span>
<span class="gi">+        # Closing result_queue</span>
<span class="gi">+        mp.util.debug(&quot;closing result_queue&quot;)</span>
<span class="gi">+        self.result_queue.close()</span>
<span class="gi">+</span>
<span class="gi">+        mp.util.debug(&quot;closing thread_wakeup&quot;)</span>
<span class="gi">+        with self.shutdown_lock:</span>
<span class="gi">+            self.thread_wakeup.close()</span>
<span class="gi">+</span>
<span class="gi">+        # If .join() is not called on the created processes then</span>
<span class="gi">+        # some ctx.Queue methods may deadlock on macOS.</span>
<span class="gi">+        with self.processes_management_lock:</span>
<span class="gi">+            mp.util.debug(f&quot;joining {len(self.processes)} processes&quot;)</span>
<span class="gi">+            n_joined_processes = 0</span>
<span class="gi">+            while True:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    pid, p = self.processes.popitem()</span>
<span class="gi">+                    mp.util.debug(f&quot;joining process {p.name} with pid {pid}&quot;)</span>
<span class="gi">+                    p.join()</span>
<span class="gi">+                    n_joined_processes += 1</span>
<span class="gi">+                except KeyError:</span>
<span class="gi">+                    break</span>
<span class="gi">+</span>
<span class="gi">+            mp.util.debug(</span>
<span class="gi">+                &quot;executor management thread clean shutdown of &quot;</span>
<span class="gi">+                f&quot;{n_joined_processes} workers&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def get_n_children_alive(self):</span>
<span class="gi">+        # This is an upper bound on the number of children alive.</span>
<span class="gi">+        with self.processes_management_lock:</span>
<span class="gi">+            return sum(p.is_alive() for p in list(self.processes.values()))</span>
<span class="gi">+</span>

<span class="w"> </span>_system_limits_checked = False
<span class="w"> </span>_system_limited = None


<span class="gi">+def _check_system_limits():</span>
<span class="gi">+    global _system_limits_checked, _system_limited</span>
<span class="gi">+    if _system_limits_checked and _system_limited:</span>
<span class="gi">+        raise NotImplementedError(_system_limited)</span>
<span class="gi">+    _system_limits_checked = True</span>
<span class="gi">+    try:</span>
<span class="gi">+        nsems_max = os.sysconf(&quot;SC_SEM_NSEMS_MAX&quot;)</span>
<span class="gi">+    except (AttributeError, ValueError):</span>
<span class="gi">+        # sysconf not available or setting not available</span>
<span class="gi">+        return</span>
<span class="gi">+    if nsems_max == -1:</span>
<span class="gi">+        # undetermined limit, assume that limit is determined</span>
<span class="gi">+        # by available memory only</span>
<span class="gi">+        return</span>
<span class="gi">+    if nsems_max &gt;= 256:</span>
<span class="gi">+        # minimum number of semaphores available</span>
<span class="gi">+        # according to POSIX</span>
<span class="gi">+        return</span>
<span class="gi">+    _system_limited = (</span>
<span class="gi">+        f&quot;system provides too few semaphores ({nsems_max} available, &quot;</span>
<span class="gi">+        &quot;256 necessary)&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    raise NotImplementedError(_system_limited)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def _chain_from_iterable_of_lists(iterable):
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Specialized implementation of itertools.chain.from_iterable.
<span class="w"> </span>    Each item in *iterable* should be a list.  This function is
<span class="w"> </span>    careful not to keep references to yielded objects.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    for element in iterable:</span>
<span class="gi">+        element.reverse()</span>
<span class="gi">+        while element:</span>
<span class="gi">+            yield element.pop()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _check_max_depth(context):</span>
<span class="gi">+    # Limit the maxmal recursion level</span>
<span class="gi">+    global _CURRENT_DEPTH</span>
<span class="gi">+    if context.get_start_method() == &quot;fork&quot; and _CURRENT_DEPTH &gt; 0:</span>
<span class="gi">+        raise LokyRecursionError(</span>
<span class="gi">+            &quot;Could not spawn extra nested processes at depth superior to &quot;</span>
<span class="gi">+            &quot;MAX_DEPTH=1. It is not possible to increase this limit when &quot;</span>
<span class="gi">+            &quot;using the &#39;fork&#39; start method.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if 0 &lt; MAX_DEPTH and _CURRENT_DEPTH + 1 &gt; MAX_DEPTH:</span>
<span class="gi">+        raise LokyRecursionError(</span>
<span class="gi">+            &quot;Could not spawn extra nested processes at depth superior to &quot;</span>
<span class="gi">+            f&quot;MAX_DEPTH={MAX_DEPTH}. If this is intendend, you can change &quot;</span>
<span class="gi">+            &quot;this limit with the LOKY_MAX_DEPTH environment variable.&quot;</span>
<span class="gi">+        )</span>


<span class="w"> </span>class LokyRecursionError(RuntimeError):
<span class="gu">@@ -295,10 +1008,13 @@ class TerminatedWorkerError(BrokenProcessPool):</span>
<span class="w"> </span>    &quot;&quot;&quot;


<span class="gi">+# Alias for backward compat (for code written for loky 1.1.4 and earlier). Do</span>
<span class="gi">+# not use in new code.</span>
<span class="w"> </span>BrokenExecutor = BrokenProcessPool


<span class="w"> </span>class ShutdownExecutorError(RuntimeError):
<span class="gi">+</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Raised when a ProcessPoolExecutor is shutdown while a future was in the
<span class="w"> </span>    running or pending state.
<span class="gu">@@ -306,11 +1022,20 @@ class ShutdownExecutorError(RuntimeError):</span>


<span class="w"> </span>class ProcessPoolExecutor(Executor):
<span class="gi">+</span>
<span class="w"> </span>    _at_exit = None

<span class="gd">-    def __init__(self, max_workers=None, job_reducers=None, result_reducers</span>
<span class="gd">-        =None, timeout=None, context=None, initializer=None, initargs=(),</span>
<span class="gd">-        env=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        max_workers=None,</span>
<span class="gi">+        job_reducers=None,</span>
<span class="gi">+        result_reducers=None,</span>
<span class="gi">+        timeout=None,</span>
<span class="gi">+        context=None,</span>
<span class="gi">+        initializer=None,</span>
<span class="gi">+        initargs=(),</span>
<span class="gi">+        env=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;Initializes a new ProcessPoolExecutor instance.

<span class="w"> </span>        Args:
<span class="gu">@@ -336,30 +1061,47 @@ class ProcessPoolExecutor(Executor):</span>
<span class="w"> </span>                loaded. Note that this only works with the loky context.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        _check_system_limits()
<span class="gi">+</span>
<span class="w"> </span>        if max_workers is None:
<span class="w"> </span>            self._max_workers = cpu_count()
<span class="w"> </span>        else:
<span class="w"> </span>            if max_workers &lt;= 0:
<span class="gd">-                raise ValueError(&#39;max_workers must be greater than 0&#39;)</span>
<span class="gi">+                raise ValueError(&quot;max_workers must be greater than 0&quot;)</span>
<span class="w"> </span>            self._max_workers = max_workers
<span class="gd">-        if (sys.platform == &#39;win32&#39; and self._max_workers &gt;</span>
<span class="gd">-            _MAX_WINDOWS_WORKERS):</span>
<span class="gi">+</span>
<span class="gi">+        if (</span>
<span class="gi">+            sys.platform == &quot;win32&quot;</span>
<span class="gi">+            and self._max_workers &gt; _MAX_WINDOWS_WORKERS</span>
<span class="gi">+        ):</span>
<span class="w"> </span>            warnings.warn(
<span class="gd">-                f&#39;On Windows, max_workers cannot exceed {_MAX_WINDOWS_WORKERS} due to limitations of the operating system.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                f&quot;On Windows, max_workers cannot exceed {_MAX_WINDOWS_WORKERS} &quot;</span>
<span class="gi">+                &quot;due to limitations of the operating system.&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>            self._max_workers = _MAX_WINDOWS_WORKERS
<span class="gi">+</span>
<span class="w"> </span>        if context is None:
<span class="w"> </span>            context = get_context()
<span class="w"> </span>        self._context = context
<span class="w"> </span>        self._env = env
<span class="gd">-        self._initializer, self._initargs = _prepare_initializer(initializer,</span>
<span class="gd">-            initargs)</span>
<span class="gi">+</span>
<span class="gi">+        self._initializer, self._initargs = _prepare_initializer(</span>
<span class="gi">+            initializer, initargs</span>
<span class="gi">+        )</span>
<span class="w"> </span>        _check_max_depth(self._context)
<span class="gi">+</span>
<span class="w"> </span>        if result_reducers is None:
<span class="w"> </span>            result_reducers = job_reducers
<span class="gi">+</span>
<span class="gi">+        # Timeout</span>
<span class="w"> </span>        self._timeout = timeout
<span class="gi">+</span>
<span class="gi">+        # Management thread</span>
<span class="w"> </span>        self._executor_manager_thread = None
<span class="gi">+</span>
<span class="gi">+        # Map of pids to processes</span>
<span class="w"> </span>        self._processes = {}
<span class="gi">+</span>
<span class="gi">+        # Internal variables of the ProcessPoolExecutor</span>
<span class="w"> </span>        self._processes = {}
<span class="w"> </span>        self._queue_count = 0
<span class="w"> </span>        self._pending_work_items = {}
<span class="gu">@@ -368,14 +1110,144 @@ class ProcessPoolExecutor(Executor):</span>
<span class="w"> </span>        self._processes_management_lock = self._context.Lock()
<span class="w"> </span>        self._executor_manager_thread = None
<span class="w"> </span>        self._shutdown_lock = threading.Lock()
<span class="gi">+</span>
<span class="gi">+        # _ThreadWakeup is a communication channel used to interrupt the wait</span>
<span class="gi">+        # of the main loop of executor_manager_thread from another thread (e.g.</span>
<span class="gi">+        # when calling executor.submit or executor.shutdown). We do not use the</span>
<span class="gi">+        # _result_queue to send wakeup signals to the executor_manager_thread</span>
<span class="gi">+        # as it could result in a deadlock if a worker process dies with the</span>
<span class="gi">+        # _result_queue write lock still acquired.</span>
<span class="gi">+        #</span>
<span class="gi">+        # _shutdown_lock must be locked to access _ThreadWakeup.wakeup.</span>
<span class="w"> </span>        self._executor_manager_thread_wakeup = _ThreadWakeup()
<span class="gi">+</span>
<span class="gi">+        # Flag to hold the state of the Executor. This permits to introspect</span>
<span class="gi">+        # the Executor state even once it has been garbage collected.</span>
<span class="w"> </span>        self._flags = _ExecutorFlags(self._shutdown_lock)
<span class="gi">+</span>
<span class="gi">+        # Finally setup the queues for interprocess communication</span>
<span class="w"> </span>        self._setup_queues(job_reducers, result_reducers)
<span class="gd">-        mp.util.debug(&#39;ProcessPoolExecutor is setup&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        mp.util.debug(&quot;ProcessPoolExecutor is setup&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _setup_queues(self, job_reducers, result_reducers, queue_size=None):</span>
<span class="gi">+        # Make the call queue slightly larger than the number of processes to</span>
<span class="gi">+        # prevent the worker processes from idling. But don&#39;t make it too big</span>
<span class="gi">+        # because futures in the call queue cannot be cancelled.</span>
<span class="gi">+        if queue_size is None:</span>
<span class="gi">+            queue_size = 2 * self._max_workers + EXTRA_QUEUED_CALLS</span>
<span class="gi">+        self._call_queue = _SafeQueue(</span>
<span class="gi">+            max_size=queue_size,</span>
<span class="gi">+            pending_work_items=self._pending_work_items,</span>
<span class="gi">+            running_work_items=self._running_work_items,</span>
<span class="gi">+            thread_wakeup=self._executor_manager_thread_wakeup,</span>
<span class="gi">+            reducers=job_reducers,</span>
<span class="gi">+            ctx=self._context,</span>
<span class="gi">+        )</span>
<span class="gi">+        # Killed worker processes can produce spurious &quot;broken pipe&quot;</span>
<span class="gi">+        # tracebacks in the queue&#39;s own worker thread. But we detect killed</span>
<span class="gi">+        # processes anyway, so silence the tracebacks.</span>
<span class="gi">+        self._call_queue._ignore_epipe = True</span>
<span class="gi">+</span>
<span class="gi">+        self._result_queue = SimpleQueue(</span>
<span class="gi">+            reducers=result_reducers, ctx=self._context</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _start_executor_manager_thread(self):</span>
<span class="gi">+        if self._executor_manager_thread is None:</span>
<span class="gi">+            mp.util.debug(&quot;_start_executor_manager_thread called&quot;)</span>
<span class="gi">+</span>
<span class="gi">+            # Start the processes so that their sentinels are known.</span>
<span class="gi">+            self._executor_manager_thread = _ExecutorManagerThread(self)</span>
<span class="gi">+            self._executor_manager_thread.start()</span>
<span class="gi">+</span>
<span class="gi">+            # register this executor in a mechanism that ensures it will wakeup</span>
<span class="gi">+            # when the interpreter is exiting.</span>
<span class="gi">+            _threads_wakeups[self._executor_manager_thread] = (</span>
<span class="gi">+                self._shutdown_lock,</span>
<span class="gi">+                self._executor_manager_thread_wakeup,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            global process_pool_executor_at_exit</span>
<span class="gi">+            if process_pool_executor_at_exit is None:</span>
<span class="gi">+                # Ensure that the _python_exit function will be called before</span>
<span class="gi">+                # the multiprocessing.Queue._close finalizers which have an</span>
<span class="gi">+                # exitpriority of 10.</span>
<span class="gi">+</span>
<span class="gi">+                if sys.version_info &lt; (3, 9):</span>
<span class="gi">+                    process_pool_executor_at_exit = mp.util.Finalize(</span>
<span class="gi">+                        None, _python_exit, exitpriority=20</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    process_pool_executor_at_exit = threading._register_atexit(</span>
<span class="gi">+                        _python_exit</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+    def _adjust_process_count(self):</span>
<span class="gi">+        while len(self._processes) &lt; self._max_workers:</span>
<span class="gi">+            worker_exit_lock = self._context.BoundedSemaphore(1)</span>
<span class="gi">+            args = (</span>
<span class="gi">+                self._call_queue,</span>
<span class="gi">+                self._result_queue,</span>
<span class="gi">+                self._initializer,</span>
<span class="gi">+                self._initargs,</span>
<span class="gi">+                self._processes_management_lock,</span>
<span class="gi">+                self._timeout,</span>
<span class="gi">+                worker_exit_lock,</span>
<span class="gi">+                _CURRENT_DEPTH + 1,</span>
<span class="gi">+            )</span>
<span class="gi">+            worker_exit_lock.acquire()</span>
<span class="gi">+            try:</span>
<span class="gi">+                # Try to spawn the process with some environment variable to</span>
<span class="gi">+                # overwrite but it only works with the loky context for now.</span>
<span class="gi">+                p = self._context.Process(</span>
<span class="gi">+                    target=_process_worker, args=args, env=self._env</span>
<span class="gi">+                )</span>
<span class="gi">+            except TypeError:</span>
<span class="gi">+                p = self._context.Process(target=_process_worker, args=args)</span>
<span class="gi">+            p._worker_exit_lock = worker_exit_lock</span>
<span class="gi">+            p.start()</span>
<span class="gi">+            self._processes[p.pid] = p</span>
<span class="gi">+        mp.util.debug(</span>
<span class="gi">+            f&quot;Adjusted process count to {self._max_workers}: &quot;</span>
<span class="gi">+            f&quot;{[(p.name, pid) for pid, p in self._processes.items()]}&quot;</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _ensure_executor_running(self):
<span class="w"> </span>        &quot;&quot;&quot;ensures all workers and management thread are running&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        with self._processes_management_lock:</span>
<span class="gi">+            if len(self._processes) != self._max_workers:</span>
<span class="gi">+                self._adjust_process_count()</span>
<span class="gi">+            self._start_executor_manager_thread()</span>
<span class="gi">+</span>
<span class="gi">+    def submit(self, fn, *args, **kwargs):</span>
<span class="gi">+        with self._flags.shutdown_lock:</span>
<span class="gi">+            if self._flags.broken is not None:</span>
<span class="gi">+                raise self._flags.broken</span>
<span class="gi">+            if self._flags.shutdown:</span>
<span class="gi">+                raise ShutdownExecutorError(</span>
<span class="gi">+                    &quot;cannot schedule new futures after shutdown&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # Cannot submit a new calls once the interpreter is shutting down.</span>
<span class="gi">+            # This check avoids spawning new processes at exit.</span>
<span class="gi">+            if _global_shutdown:</span>
<span class="gi">+                raise RuntimeError(</span>
<span class="gi">+                    &quot;cannot schedule new futures after &quot; &quot;interpreter shutdown&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            f = Future()</span>
<span class="gi">+            w = _WorkItem(f, fn, args, kwargs)</span>
<span class="gi">+</span>
<span class="gi">+            self._pending_work_items[self._queue_count] = w</span>
<span class="gi">+            self._work_ids.put(self._queue_count)</span>
<span class="gi">+            self._queue_count += 1</span>
<span class="gi">+            # Wake up queue management thread</span>
<span class="gi">+            self._executor_manager_thread_wakeup.wakeup()</span>
<span class="gi">+</span>
<span class="gi">+            self._ensure_executor_running()</span>
<span class="gi">+            return f</span>
<span class="gi">+</span>
<span class="w"> </span>    submit.__doc__ = Executor.submit.__doc__

<span class="w"> </span>    def map(self, fn, *iterables, **kwargs):
<span class="gu">@@ -400,5 +1272,43 @@ class ProcessPoolExecutor(Executor):</span>
<span class="w"> </span>                before the given timeout.
<span class="w"> </span>            Exception: If fn(*args) raises for any values.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        timeout = kwargs.get(&quot;timeout&quot;, None)</span>
<span class="gi">+        chunksize = kwargs.get(&quot;chunksize&quot;, 1)</span>
<span class="gi">+        if chunksize &lt; 1:</span>
<span class="gi">+            raise ValueError(&quot;chunksize must be &gt;= 1.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        results = super().map(</span>
<span class="gi">+            partial(_process_chunk, fn),</span>
<span class="gi">+            _get_chunks(chunksize, *iterables),</span>
<span class="gi">+            timeout=timeout,</span>
<span class="gi">+        )</span>
<span class="gi">+        return _chain_from_iterable_of_lists(results)</span>
<span class="gi">+</span>
<span class="gi">+    def shutdown(self, wait=True, kill_workers=False):</span>
<span class="gi">+        mp.util.debug(f&quot;shutting down executor {self}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        self._flags.flag_as_shutting_down(kill_workers)</span>
<span class="gi">+        executor_manager_thread = self._executor_manager_thread</span>
<span class="gi">+        executor_manager_thread_wakeup = self._executor_manager_thread_wakeup</span>
<span class="gi">+</span>
<span class="gi">+        if executor_manager_thread_wakeup is not None:</span>
<span class="gi">+            # Wake up queue management thread</span>
<span class="gi">+            with self._shutdown_lock:</span>
<span class="gi">+                self._executor_manager_thread_wakeup.wakeup()</span>
<span class="gi">+</span>
<span class="gi">+        if executor_manager_thread is not None and wait:</span>
<span class="gi">+            # This locks avoids concurrent join if the interpreter</span>
<span class="gi">+            # is shutting down.</span>
<span class="gi">+            with _global_shutdown_lock:</span>
<span class="gi">+                executor_manager_thread.join()</span>
<span class="gi">+                _threads_wakeups.pop(executor_manager_thread, None)</span>
<span class="gi">+</span>
<span class="gi">+        # To reduce the risk of opening too many files, remove references to</span>
<span class="gi">+        # objects that use file descriptors.</span>
<span class="gi">+        self._executor_manager_thread = None</span>
<span class="gi">+        self._executor_manager_thread_wakeup = None</span>
<span class="gi">+        self._call_queue = None</span>
<span class="gi">+        self._result_queue = None</span>
<span class="gi">+        self._processes_management_lock = None</span>
<span class="gi">+</span>
<span class="w"> </span>    shutdown.__doc__ = Executor.shutdown.__doc__
<span class="gh">diff --git a/joblib/externals/loky/reusable_executor.py b/joblib/externals/loky/reusable_executor.py</span>
<span class="gh">index 5509ecc..ad016fd 100644</span>
<span class="gd">--- a/joblib/externals/loky/reusable_executor.py</span>
<span class="gi">+++ b/joblib/externals/loky/reusable_executor.py</span>
<span class="gu">@@ -1,11 +1,20 @@</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Reusable ProcessPoolExecutor</span>
<span class="gi">+#</span>
<span class="gi">+# author: Thomas Moreau and Olivier Grisel</span>
<span class="gi">+#</span>
<span class="w"> </span>import time
<span class="w"> </span>import warnings
<span class="w"> </span>import threading
<span class="w"> </span>import multiprocessing as mp
<span class="gi">+</span>
<span class="w"> </span>from .process_executor import ProcessPoolExecutor, EXTRA_QUEUED_CALLS
<span class="w"> </span>from .backend.context import cpu_count
<span class="w"> </span>from .backend import get_context
<span class="gd">-__all__ = [&#39;get_reusable_executor&#39;]</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;get_reusable_executor&quot;]</span>
<span class="gi">+</span>
<span class="gi">+# Singleton executor and id management</span>
<span class="w"> </span>_executor_lock = threading.RLock()
<span class="w"> </span>_next_executor_id = 0
<span class="w"> </span>_executor = None
<span class="gu">@@ -18,12 +27,25 @@ def _get_next_executor_id():</span>
<span class="w"> </span>    The purpose of this monotonic id is to help debug and test automated
<span class="w"> </span>    instance creation.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    global _next_executor_id</span>
<span class="gi">+    with _executor_lock:</span>
<span class="gi">+        executor_id = _next_executor_id</span>
<span class="gi">+        _next_executor_id += 1</span>
<span class="gi">+        return executor_id</span>


<span class="gd">-def get_reusable_executor(max_workers=None, context=None, timeout=10,</span>
<span class="gd">-    kill_workers=False, reuse=&#39;auto&#39;, job_reducers=None, result_reducers=</span>
<span class="gd">-    None, initializer=None, initargs=(), env=None):</span>
<span class="gi">+def get_reusable_executor(</span>
<span class="gi">+    max_workers=None,</span>
<span class="gi">+    context=None,</span>
<span class="gi">+    timeout=10,</span>
<span class="gi">+    kill_workers=False,</span>
<span class="gi">+    reuse=&quot;auto&quot;,</span>
<span class="gi">+    job_reducers=None,</span>
<span class="gi">+    result_reducers=None,</span>
<span class="gi">+    initializer=None,</span>
<span class="gi">+    initargs=(),</span>
<span class="gi">+    env=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the current ReusableExectutor instance.

<span class="w"> </span>    Start a new instance if it has not been started already or if the previous
<span class="gu">@@ -64,21 +86,200 @@ def get_reusable_executor(max_workers=None, context=None, timeout=10,</span>
<span class="w"> </span>    in the children before any module is loaded. This only works with the
<span class="w"> </span>    ``loky`` context.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    _executor, _ = _ReusablePoolExecutor.get_reusable_executor(</span>
<span class="gi">+        max_workers=max_workers,</span>
<span class="gi">+        context=context,</span>
<span class="gi">+        timeout=timeout,</span>
<span class="gi">+        kill_workers=kill_workers,</span>
<span class="gi">+        reuse=reuse,</span>
<span class="gi">+        job_reducers=job_reducers,</span>
<span class="gi">+        result_reducers=result_reducers,</span>
<span class="gi">+        initializer=initializer,</span>
<span class="gi">+        initargs=initargs,</span>
<span class="gi">+        env=env,</span>
<span class="gi">+    )</span>
<span class="gi">+    return _executor</span>


<span class="w"> </span>class _ReusablePoolExecutor(ProcessPoolExecutor):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, submit_resize_lock, max_workers=None, context=None,</span>
<span class="gd">-        timeout=None, executor_id=0, job_reducers=None, result_reducers=</span>
<span class="gd">-        None, initializer=None, initargs=(), env=None):</span>
<span class="gd">-        super().__init__(max_workers=max_workers, context=context, timeout=</span>
<span class="gd">-            timeout, job_reducers=job_reducers, result_reducers=</span>
<span class="gd">-            result_reducers, initializer=initializer, initargs=initargs,</span>
<span class="gd">-            env=env)</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        submit_resize_lock,</span>
<span class="gi">+        max_workers=None,</span>
<span class="gi">+        context=None,</span>
<span class="gi">+        timeout=None,</span>
<span class="gi">+        executor_id=0,</span>
<span class="gi">+        job_reducers=None,</span>
<span class="gi">+        result_reducers=None,</span>
<span class="gi">+        initializer=None,</span>
<span class="gi">+        initargs=(),</span>
<span class="gi">+        env=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            max_workers=max_workers,</span>
<span class="gi">+            context=context,</span>
<span class="gi">+            timeout=timeout,</span>
<span class="gi">+            job_reducers=job_reducers,</span>
<span class="gi">+            result_reducers=result_reducers,</span>
<span class="gi">+            initializer=initializer,</span>
<span class="gi">+            initargs=initargs,</span>
<span class="gi">+            env=env,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.executor_id = executor_id
<span class="w"> </span>        self._submit_resize_lock = submit_resize_lock

<span class="gi">+    @classmethod</span>
<span class="gi">+    def get_reusable_executor(</span>
<span class="gi">+        cls,</span>
<span class="gi">+        max_workers=None,</span>
<span class="gi">+        context=None,</span>
<span class="gi">+        timeout=10,</span>
<span class="gi">+        kill_workers=False,</span>
<span class="gi">+        reuse=&quot;auto&quot;,</span>
<span class="gi">+        job_reducers=None,</span>
<span class="gi">+        result_reducers=None,</span>
<span class="gi">+        initializer=None,</span>
<span class="gi">+        initargs=(),</span>
<span class="gi">+        env=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        with _executor_lock:</span>
<span class="gi">+            global _executor, _executor_kwargs</span>
<span class="gi">+            executor = _executor</span>
<span class="gi">+</span>
<span class="gi">+            if max_workers is None:</span>
<span class="gi">+                if reuse is True and executor is not None:</span>
<span class="gi">+                    max_workers = executor._max_workers</span>
<span class="gi">+                else:</span>
<span class="gi">+                    max_workers = cpu_count()</span>
<span class="gi">+            elif max_workers &lt;= 0:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;max_workers must be greater than 0, got {max_workers}.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if isinstance(context, str):</span>
<span class="gi">+                context = get_context(context)</span>
<span class="gi">+            if context is not None and context.get_start_method() == &quot;fork&quot;:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;Cannot use reusable executor with the &#39;fork&#39; context&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            kwargs = dict(</span>
<span class="gi">+                context=context,</span>
<span class="gi">+                timeout=timeout,</span>
<span class="gi">+                job_reducers=job_reducers,</span>
<span class="gi">+                result_reducers=result_reducers,</span>
<span class="gi">+                initializer=initializer,</span>
<span class="gi">+                initargs=initargs,</span>
<span class="gi">+                env=env,</span>
<span class="gi">+            )</span>
<span class="gi">+            if executor is None:</span>
<span class="gi">+                is_reused = False</span>
<span class="gi">+                mp.util.debug(</span>
<span class="gi">+                    f&quot;Create a executor with max_workers={max_workers}.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                executor_id = _get_next_executor_id()</span>
<span class="gi">+                _executor_kwargs = kwargs</span>
<span class="gi">+                _executor = executor = cls(</span>
<span class="gi">+                    _executor_lock,</span>
<span class="gi">+                    max_workers=max_workers,</span>
<span class="gi">+                    executor_id=executor_id,</span>
<span class="gi">+                    **kwargs,</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                if reuse == &quot;auto&quot;:</span>
<span class="gi">+                    reuse = kwargs == _executor_kwargs</span>
<span class="gi">+                if (</span>
<span class="gi">+                    executor._flags.broken</span>
<span class="gi">+                    or executor._flags.shutdown</span>
<span class="gi">+                    or not reuse</span>
<span class="gi">+                ):</span>
<span class="gi">+                    if executor._flags.broken:</span>
<span class="gi">+                        reason = &quot;broken&quot;</span>
<span class="gi">+                    elif executor._flags.shutdown:</span>
<span class="gi">+                        reason = &quot;shutdown&quot;</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        reason = &quot;arguments have changed&quot;</span>
<span class="gi">+                    mp.util.debug(</span>
<span class="gi">+                        &quot;Creating a new executor with max_workers=&quot;</span>
<span class="gi">+                        f&quot;{max_workers} as the previous instance cannot be &quot;</span>
<span class="gi">+                        f&quot;reused ({reason}).&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                    executor.shutdown(wait=True, kill_workers=kill_workers)</span>
<span class="gi">+                    _executor = executor = _executor_kwargs = None</span>
<span class="gi">+                    # Recursive call to build a new instance</span>
<span class="gi">+                    return cls.get_reusable_executor(</span>
<span class="gi">+                        max_workers=max_workers, **kwargs</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    mp.util.debug(</span>
<span class="gi">+                        &quot;Reusing existing executor with &quot;</span>
<span class="gi">+                        f&quot;max_workers={executor._max_workers}.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                    is_reused = True</span>
<span class="gi">+                    executor._resize(max_workers)</span>
<span class="gi">+</span>
<span class="gi">+        return executor, is_reused</span>
<span class="gi">+</span>
<span class="gi">+    def submit(self, fn, *args, **kwargs):</span>
<span class="gi">+        with self._submit_resize_lock:</span>
<span class="gi">+            return super().submit(fn, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def _resize(self, max_workers):</span>
<span class="gi">+        with self._submit_resize_lock:</span>
<span class="gi">+            if max_workers is None:</span>
<span class="gi">+                raise ValueError(&quot;Trying to resize with max_workers=None&quot;)</span>
<span class="gi">+            elif max_workers == self._max_workers:</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            if self._executor_manager_thread is None:</span>
<span class="gi">+                # If the executor_manager_thread has not been started</span>
<span class="gi">+                # then no processes have been spawned and we can just</span>
<span class="gi">+                # update _max_workers and return</span>
<span class="gi">+                self._max_workers = max_workers</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            self._wait_job_completion()</span>
<span class="gi">+</span>
<span class="gi">+            # Some process might have returned due to timeout so check how many</span>
<span class="gi">+            # children are still alive. Use the _process_management_lock to</span>
<span class="gi">+            # ensure that no process are spawned or timeout during the resize.</span>
<span class="gi">+            with self._processes_management_lock:</span>
<span class="gi">+                processes = list(self._processes.values())</span>
<span class="gi">+                nb_children_alive = sum(p.is_alive() for p in processes)</span>
<span class="gi">+                self._max_workers = max_workers</span>
<span class="gi">+                for _ in range(max_workers, nb_children_alive):</span>
<span class="gi">+                    self._call_queue.put(None)</span>
<span class="gi">+            while (</span>
<span class="gi">+                len(self._processes) &gt; max_workers and not self._flags.broken</span>
<span class="gi">+            ):</span>
<span class="gi">+                time.sleep(1e-3)</span>
<span class="gi">+</span>
<span class="gi">+            self._adjust_process_count()</span>
<span class="gi">+            processes = list(self._processes.values())</span>
<span class="gi">+            while not all(p.is_alive() for p in processes):</span>
<span class="gi">+                time.sleep(1e-3)</span>
<span class="gi">+</span>
<span class="w"> </span>    def _wait_job_completion(self):
<span class="w"> </span>        &quot;&quot;&quot;Wait for the cache to be empty before resizing the pool.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Issue a warning to the user about the bad effect of this usage.</span>
<span class="gi">+        if self._pending_work_items:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;Trying to resize an executor with running jobs: &quot;</span>
<span class="gi">+                &quot;waiting for jobs completion before resizing.&quot;,</span>
<span class="gi">+                UserWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            mp.util.debug(</span>
<span class="gi">+                f&quot;Executor {self.executor_id} waiting for jobs completion &quot;</span>
<span class="gi">+                &quot;before resizing&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        # Wait for the completion of the jobs</span>
<span class="gi">+        while self._pending_work_items:</span>
<span class="gi">+            time.sleep(1e-3)</span>
<span class="gi">+</span>
<span class="gi">+    def _setup_queues(self, job_reducers, result_reducers):</span>
<span class="gi">+        # As this executor can be resized, use a large queue size to avoid</span>
<span class="gi">+        # underestimating capacity and introducing overhead</span>
<span class="gi">+        queue_size = 2 * cpu_count() + EXTRA_QUEUED_CALLS</span>
<span class="gi">+        super()._setup_queues(</span>
<span class="gi">+            job_reducers, result_reducers, queue_size=queue_size</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/joblib/func_inspect.py b/joblib/func_inspect.py</span>
<span class="gh">index 5a263a0..3f80946 100644</span>
<span class="gd">--- a/joblib/func_inspect.py</span>
<span class="gi">+++ b/joblib/func_inspect.py</span>
<span class="gu">@@ -1,16 +1,24 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>My own variation on function-specific inspect-like features.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2009 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="w"> </span>import inspect
<span class="w"> </span>import warnings
<span class="w"> </span>import re
<span class="w"> </span>import os
<span class="w"> </span>import collections
<span class="gi">+</span>
<span class="w"> </span>from itertools import islice
<span class="w"> </span>from tokenize import open as open_py_source
<span class="gi">+</span>
<span class="w"> </span>from .logger import pformat
<span class="gd">-full_argspec_fields = (</span>
<span class="gd">-    &#39;args varargs varkw defaults kwonlyargs kwonlydefaults annotations&#39;)</span>
<span class="gi">+</span>
<span class="gi">+full_argspec_fields = (&#39;args varargs varkw defaults kwonlyargs &#39;</span>
<span class="gi">+                       &#39;kwonlydefaults annotations&#39;)</span>
<span class="w"> </span>full_argspec_type = collections.namedtuple(&#39;FullArgSpec&#39;, full_argspec_fields)


<span class="gu">@@ -35,12 +43,53 @@ def get_func_code(func):</span>
<span class="w"> </span>        This function does a bit more magic than inspect, and is thus
<span class="w"> </span>        more robust.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    source_file = None</span>
<span class="gi">+    try:</span>
<span class="gi">+        code = func.__code__</span>
<span class="gi">+        source_file = code.co_filename</span>
<span class="gi">+        if not os.path.exists(source_file):</span>
<span class="gi">+            # Use inspect for lambda functions and functions defined in an</span>
<span class="gi">+            # interactive shell, or in doctests</span>
<span class="gi">+            source_code = &#39;&#39;.join(inspect.getsourcelines(func)[0])</span>
<span class="gi">+            line_no = 1</span>
<span class="gi">+            if source_file.startswith(&#39;&lt;doctest &#39;):</span>
<span class="gi">+                source_file, line_no = re.match(</span>
<span class="gi">+                    r&#39;\&lt;doctest (.*\.rst)\[(.*)\]\&gt;&#39;, source_file).groups()</span>
<span class="gi">+                line_no = int(line_no)</span>
<span class="gi">+                source_file = &#39;&lt;doctest %s&gt;&#39; % source_file</span>
<span class="gi">+            return source_code, source_file, line_no</span>
<span class="gi">+        # Try to retrieve the source code.</span>
<span class="gi">+        with open_py_source(source_file) as source_file_obj:</span>
<span class="gi">+            first_line = code.co_firstlineno</span>
<span class="gi">+            # All the lines after the function definition:</span>
<span class="gi">+            source_lines = list(islice(source_file_obj, first_line - 1, None))</span>
<span class="gi">+        return &#39;&#39;.join(inspect.getblock(source_lines)), source_file, first_line</span>
<span class="gi">+    except:  # noqa: E722</span>
<span class="gi">+        # If the source code fails, we use the hash. This is fragile and</span>
<span class="gi">+        # might change from one session to another.</span>
<span class="gi">+        if hasattr(func, &#39;__code__&#39;):</span>
<span class="gi">+            # Python 3.X</span>
<span class="gi">+            return str(func.__code__.__hash__()), source_file, -1</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Weird objects like numpy ufunc don&#39;t have __code__</span>
<span class="gi">+            # This is fragile, as quite often the id of the object is</span>
<span class="gi">+            # in the repr, so it might not persist across sessions,</span>
<span class="gi">+            # however it will work for ufuncs.</span>
<span class="gi">+            return repr(func), source_file, -1</span>


<span class="w"> </span>def _clean_win_chars(string):
<span class="w"> </span>    &quot;&quot;&quot;Windows cannot encode some characters in filename.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import urllib</span>
<span class="gi">+    if hasattr(urllib, &#39;quote&#39;):</span>
<span class="gi">+        quote = urllib.quote</span>
<span class="gi">+    else:</span>
<span class="gi">+        # In Python 3, quote is elsewhere</span>
<span class="gi">+        import urllib.parse</span>
<span class="gi">+        quote = urllib.parse.quote</span>
<span class="gi">+    for char in (&#39;&lt;&#39;, &#39;&gt;&#39;, &#39;!&#39;, &#39;:&#39;, &#39;\\&#39;):</span>
<span class="gi">+        string = string.replace(char, quote(char))</span>
<span class="gi">+    return string</span>


<span class="w"> </span>def get_func_name(func, resolv_alias=True, win_characters=True):
<span class="gu">@@ -57,17 +106,96 @@ def get_func_name(func, resolv_alias=True, win_characters=True):</span>
<span class="w"> </span>            If true, substitute special characters using urllib.quote
<span class="w"> </span>            This is useful in Windows, as it cannot encode some filenames
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if hasattr(func, &#39;__module__&#39;):</span>
<span class="gi">+        module = func.__module__</span>
<span class="gi">+    else:</span>
<span class="gi">+        try:</span>
<span class="gi">+            module = inspect.getmodule(func)</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            if hasattr(func, &#39;__class__&#39;):</span>
<span class="gi">+                module = func.__class__.__module__</span>
<span class="gi">+            else:</span>
<span class="gi">+                module = &#39;unknown&#39;</span>
<span class="gi">+    if module is None:</span>
<span class="gi">+        # Happens in doctests, eg</span>
<span class="gi">+        module = &#39;&#39;</span>
<span class="gi">+    if module == &#39;__main__&#39;:</span>
<span class="gi">+        try:</span>
<span class="gi">+            filename = os.path.abspath(inspect.getsourcefile(func))</span>
<span class="gi">+        except:  # noqa: E722</span>
<span class="gi">+            filename = None</span>
<span class="gi">+        if filename is not None:</span>
<span class="gi">+            # mangling of full path to filename</span>
<span class="gi">+            parts = filename.split(os.sep)</span>
<span class="gi">+            if parts[-1].startswith(&#39;&lt;ipython-input&#39;):</span>
<span class="gi">+                # We&#39;re in a IPython (or notebook) session. parts[-1] comes</span>
<span class="gi">+                # from func.__code__.co_filename and is of the form</span>
<span class="gi">+                # &lt;ipython-input-N-XYZ&gt;, where:</span>
<span class="gi">+                # - N is the cell number where the function was defined</span>
<span class="gi">+                # - XYZ is a hash representing the function&#39;s code (and name).</span>
<span class="gi">+                #   It will be consistent across sessions and kernel restarts,</span>
<span class="gi">+                #   and will change if the function&#39;s code/name changes</span>
<span class="gi">+                # We remove N so that cache is properly hit if the cell where</span>
<span class="gi">+                # the func is defined is re-exectuted.</span>
<span class="gi">+                # The XYZ hash should avoid collisions between functions with</span>
<span class="gi">+                # the same name, both within the same notebook but also across</span>
<span class="gi">+                # notebooks</span>
<span class="gi">+                splitted = parts[-1].split(&#39;-&#39;)</span>
<span class="gi">+                parts[-1] = &#39;-&#39;.join(splitted[:2] + splitted[3:])</span>
<span class="gi">+            elif len(parts) &gt; 2 and parts[-2].startswith(&#39;ipykernel_&#39;):</span>
<span class="gi">+                # In a notebook session (ipykernel). Filename seems to be &#39;xyz&#39;</span>
<span class="gi">+                # of above. parts[-2] has the structure ipykernel_XXXXXX where</span>
<span class="gi">+                # XXXXXX is a six-digit number identifying the current run (?).</span>
<span class="gi">+                # If we split it off, the function again has the same</span>
<span class="gi">+                # identifier across runs.</span>
<span class="gi">+                parts[-2] = &#39;ipykernel&#39;</span>
<span class="gi">+            filename = &#39;-&#39;.join(parts)</span>
<span class="gi">+            if filename.endswith(&#39;.py&#39;):</span>
<span class="gi">+                filename = filename[:-3]</span>
<span class="gi">+            module = module + &#39;-&#39; + filename</span>
<span class="gi">+    module = module.split(&#39;.&#39;)</span>
<span class="gi">+    if hasattr(func, &#39;func_name&#39;):</span>
<span class="gi">+        name = func.func_name</span>
<span class="gi">+    elif hasattr(func, &#39;__name__&#39;):</span>
<span class="gi">+        name = func.__name__</span>
<span class="gi">+    else:</span>
<span class="gi">+        name = &#39;unknown&#39;</span>
<span class="gi">+    # Hack to detect functions not defined at the module-level</span>
<span class="gi">+    if resolv_alias:</span>
<span class="gi">+        # TODO: Maybe add a warning here?</span>
<span class="gi">+        if hasattr(func, &#39;func_globals&#39;) and name in func.func_globals:</span>
<span class="gi">+            if not func.func_globals[name] is func:</span>
<span class="gi">+                name = &#39;%s-alias&#39; % name</span>
<span class="gi">+    if hasattr(func, &#39;__qualname__&#39;) and func.__qualname__ != name:</span>
<span class="gi">+        # Extend the module name in case of nested functions to avoid</span>
<span class="gi">+        # (module, name) collisions</span>
<span class="gi">+        module.extend(func.__qualname__.split(&quot;.&quot;)[:-1])</span>
<span class="gi">+    if inspect.ismethod(func):</span>
<span class="gi">+        # We need to add the name of the class</span>
<span class="gi">+        if hasattr(func, &#39;im_class&#39;):</span>
<span class="gi">+            klass = func.im_class</span>
<span class="gi">+            module.append(klass.__name__)</span>
<span class="gi">+    if os.name == &#39;nt&#39; and win_characters:</span>
<span class="gi">+        # Windows can&#39;t encode certain characters in filenames</span>
<span class="gi">+        name = _clean_win_chars(name)</span>
<span class="gi">+        module = [_clean_win_chars(s) for s in module]</span>
<span class="gi">+    return module, name</span>


<span class="w"> </span>def _signature_str(function_name, arg_sig):
<span class="w"> </span>    &quot;&quot;&quot;Helper function to output a function signature&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return &#39;{}{}&#39;.format(function_name, arg_sig)</span>


<span class="w"> </span>def _function_called_str(function_name, args, kwargs):
<span class="w"> </span>    &quot;&quot;&quot;Helper function to output a function call&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    template_str = &#39;{0}({1}, {2})&#39;</span>
<span class="gi">+</span>
<span class="gi">+    args_str = repr(args)[1:-1]</span>
<span class="gi">+    kwargs_str = &#39;, &#39;.join(&#39;%s=%s&#39; % (k, v)</span>
<span class="gi">+                           for k, v in kwargs.items())</span>
<span class="gi">+    return template_str.format(function_name, args_str,</span>
<span class="gi">+                               kwargs_str)</span>


<span class="w"> </span>def filter_args(func, ignore_lst, args=(), kwargs=dict()):
<span class="gu">@@ -91,11 +219,151 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):</span>
<span class="w"> </span>        filtered_args: list
<span class="w"> </span>            List of filtered positional and keyword arguments.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    args = list(args)</span>
<span class="gi">+    if isinstance(ignore_lst, str):</span>
<span class="gi">+        # Catch a common mistake</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &#39;ignore_lst must be a list of parameters to ignore &#39;</span>
<span class="gi">+            &#39;%s (type %s) was given&#39; % (ignore_lst, type(ignore_lst)))</span>
<span class="gi">+    # Special case for functools.partial objects</span>
<span class="gi">+    if (not inspect.ismethod(func) and not inspect.isfunction(func)):</span>
<span class="gi">+        if ignore_lst:</span>
<span class="gi">+            warnings.warn(&#39;Cannot inspect object %s, ignore list will &#39;</span>
<span class="gi">+                          &#39;not work.&#39; % func, stacklevel=2)</span>
<span class="gi">+        return {&#39;*&#39;: args, &#39;**&#39;: kwargs}</span>
<span class="gi">+    arg_sig = inspect.signature(func)</span>
<span class="gi">+    arg_names = []</span>
<span class="gi">+    arg_defaults = []</span>
<span class="gi">+    arg_kwonlyargs = []</span>
<span class="gi">+    arg_varargs = None</span>
<span class="gi">+    arg_varkw = None</span>
<span class="gi">+    for param in arg_sig.parameters.values():</span>
<span class="gi">+        if param.kind is param.POSITIONAL_OR_KEYWORD:</span>
<span class="gi">+            arg_names.append(param.name)</span>
<span class="gi">+        elif param.kind is param.KEYWORD_ONLY:</span>
<span class="gi">+            arg_names.append(param.name)</span>
<span class="gi">+            arg_kwonlyargs.append(param.name)</span>
<span class="gi">+        elif param.kind is param.VAR_POSITIONAL:</span>
<span class="gi">+            arg_varargs = param.name</span>
<span class="gi">+        elif param.kind is param.VAR_KEYWORD:</span>
<span class="gi">+            arg_varkw = param.name</span>
<span class="gi">+        if param.default is not param.empty:</span>
<span class="gi">+            arg_defaults.append(param.default)</span>
<span class="gi">+    if inspect.ismethod(func):</span>
<span class="gi">+        # First argument is &#39;self&#39;, it has been removed by Python</span>
<span class="gi">+        # we need to add it back:</span>
<span class="gi">+        args = [func.__self__, ] + args</span>
<span class="gi">+        # func is an instance method, inspect.signature(func) does not</span>
<span class="gi">+        # include self, we need to fetch it from the class method, i.e</span>
<span class="gi">+        # func.__func__</span>
<span class="gi">+        class_method_sig = inspect.signature(func.__func__)</span>
<span class="gi">+        self_name = next(iter(class_method_sig.parameters))</span>
<span class="gi">+        arg_names = [self_name] + arg_names</span>
<span class="gi">+    # XXX: Maybe I need an inspect.isbuiltin to detect C-level methods, such</span>
<span class="gi">+    # as on ndarrays.</span>
<span class="gi">+</span>
<span class="gi">+    _, name = get_func_name(func, resolv_alias=False)</span>
<span class="gi">+    arg_dict = dict()</span>
<span class="gi">+    arg_position = -1</span>
<span class="gi">+    for arg_position, arg_name in enumerate(arg_names):</span>
<span class="gi">+        if arg_position &lt; len(args):</span>
<span class="gi">+            # Positional argument or keyword argument given as positional</span>
<span class="gi">+            if arg_name not in arg_kwonlyargs:</span>
<span class="gi">+                arg_dict[arg_name] = args[arg_position]</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;Keyword-only parameter &#39;%s&#39; was passed as &quot;</span>
<span class="gi">+                    &#39;positional parameter for %s:\n&#39;</span>
<span class="gi">+                    &#39;     %s was called.&#39;</span>
<span class="gi">+                    % (arg_name,</span>
<span class="gi">+                       _signature_str(name, arg_sig),</span>
<span class="gi">+                       _function_called_str(name, args, kwargs))</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        else:</span>
<span class="gi">+            position = arg_position - len(arg_names)</span>
<span class="gi">+            if arg_name in kwargs:</span>
<span class="gi">+                arg_dict[arg_name] = kwargs[arg_name]</span>
<span class="gi">+            else:</span>
<span class="gi">+                try:</span>
<span class="gi">+                    arg_dict[arg_name] = arg_defaults[position]</span>
<span class="gi">+                except (IndexError, KeyError) as e:</span>
<span class="gi">+                    # Missing argument</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        &#39;Wrong number of arguments for %s:\n&#39;</span>
<span class="gi">+                        &#39;     %s was called.&#39;</span>
<span class="gi">+                        % (_signature_str(name, arg_sig),</span>
<span class="gi">+                           _function_called_str(name, args, kwargs))</span>
<span class="gi">+                    ) from e</span>
<span class="gi">+</span>
<span class="gi">+    varkwargs = dict()</span>
<span class="gi">+    for arg_name, arg_value in sorted(kwargs.items()):</span>
<span class="gi">+        if arg_name in arg_dict:</span>
<span class="gi">+            arg_dict[arg_name] = arg_value</span>
<span class="gi">+        elif arg_varkw is not None:</span>
<span class="gi">+            varkwargs[arg_name] = arg_value</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise TypeError(&quot;Ignore list for %s() contains an unexpected &quot;</span>
<span class="gi">+                            &quot;keyword argument &#39;%s&#39;&quot; % (name, arg_name))</span>
<span class="gi">+</span>
<span class="gi">+    if arg_varkw is not None:</span>
<span class="gi">+        arg_dict[&#39;**&#39;] = varkwargs</span>
<span class="gi">+    if arg_varargs is not None:</span>
<span class="gi">+        varargs = args[arg_position + 1:]</span>
<span class="gi">+        arg_dict[&#39;*&#39;] = varargs</span>
<span class="gi">+</span>
<span class="gi">+    # Now remove the arguments to be ignored</span>
<span class="gi">+    for item in ignore_lst:</span>
<span class="gi">+        if item in arg_dict:</span>
<span class="gi">+            arg_dict.pop(item)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(&quot;Ignore list: argument &#39;%s&#39; is not defined for &quot;</span>
<span class="gi">+                             &quot;function %s&quot;</span>
<span class="gi">+                             % (item,</span>
<span class="gi">+                                _signature_str(name, arg_sig))</span>
<span class="gi">+                             )</span>
<span class="gi">+    # XXX: Return a sorted list of pairs?</span>
<span class="gi">+    return arg_dict</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _format_arg(arg):</span>
<span class="gi">+    formatted_arg = pformat(arg, indent=2)</span>
<span class="gi">+    if len(formatted_arg) &gt; 1500:</span>
<span class="gi">+        formatted_arg = &#39;%s...&#39; % formatted_arg[:700]</span>
<span class="gi">+    return formatted_arg</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def format_signature(func, *args, **kwargs):</span>
<span class="gi">+    # XXX: Should this use inspect.formatargvalues/formatargspec?</span>
<span class="gi">+    module, name = get_func_name(func)</span>
<span class="gi">+    module = [m for m in module if m]</span>
<span class="gi">+    if module:</span>
<span class="gi">+        module.append(name)</span>
<span class="gi">+        module_path = &#39;.&#39;.join(module)</span>
<span class="gi">+    else:</span>
<span class="gi">+        module_path = name</span>
<span class="gi">+    arg_str = list()</span>
<span class="gi">+    previous_length = 0</span>
<span class="gi">+    for arg in args:</span>
<span class="gi">+        formatted_arg = _format_arg(arg)</span>
<span class="gi">+        if previous_length &gt; 80:</span>
<span class="gi">+            formatted_arg = &#39;\n%s&#39; % formatted_arg</span>
<span class="gi">+        previous_length = len(formatted_arg)</span>
<span class="gi">+        arg_str.append(formatted_arg)</span>
<span class="gi">+    arg_str.extend([&#39;%s=%s&#39; % (v, _format_arg(i)) for v, i in kwargs.items()])</span>
<span class="gi">+    arg_str = &#39;, &#39;.join(arg_str)</span>
<span class="gi">+</span>
<span class="gi">+    signature = &#39;%s(%s)&#39; % (name, arg_str)</span>
<span class="gi">+    return module_path, signature</span>


<span class="gd">-def format_call(func, args, kwargs, object_name=&#39;Memory&#39;):</span>
<span class="gi">+def format_call(func, args, kwargs, object_name=&quot;Memory&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot; Returns a nicely formatted statement displaying the function
<span class="w"> </span>        call with the given arguments.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    path, signature = format_signature(func, *args, **kwargs)</span>
<span class="gi">+    msg = &#39;%s\n[%s] Calling %s...\n%s&#39; % (80 * &#39;_&#39;, object_name,</span>
<span class="gi">+                                          path, signature)</span>
<span class="gi">+    return msg</span>
<span class="gi">+    # XXX: Not using logging framework</span>
<span class="gi">+    # self.debug(msg)</span>
<span class="gh">diff --git a/joblib/hashing.py b/joblib/hashing.py</span>
<span class="gh">index 7f57b88..6c081f0 100644</span>
<span class="gd">--- a/joblib/hashing.py</span>
<span class="gi">+++ b/joblib/hashing.py</span>
<span class="gu">@@ -2,6 +2,11 @@</span>
<span class="w"> </span>Fast cryptographic hash of Python objects, with a special case for fast
<span class="w"> </span>hashing of numpy arrays.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2009 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="w"> </span>import pickle
<span class="w"> </span>import hashlib
<span class="w"> </span>import sys
<span class="gu">@@ -9,6 +14,8 @@ import types</span>
<span class="w"> </span>import struct
<span class="w"> </span>import io
<span class="w"> </span>import decimal
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>Pickler = pickle._Pickler


<span class="gu">@@ -16,12 +23,18 @@ class _ConsistentSet(object):</span>
<span class="w"> </span>    &quot;&quot;&quot; Class used to ensure the hash of Sets is preserved
<span class="w"> </span>        whatever the order of its items.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, set_sequence):
<span class="gi">+        # Forces order of elements in set to ensure consistent hash.</span>
<span class="w"> </span>        try:
<span class="gi">+            # Trying first to order the set assuming the type of elements is</span>
<span class="gi">+            # consistent and orderable.</span>
<span class="gi">+            # This fails on python 3 when elements are unorderable</span>
<span class="gi">+            # but we keep it in a try as it&#39;s faster.</span>
<span class="w"> </span>            self._sequence = sorted(set_sequence)
<span class="w"> </span>        except (TypeError, decimal.InvalidOperation):
<span class="gd">-            self._sequence = sorted(hash(e) for e in set_sequence)</span>
<span class="gi">+            # If elements are unorderable, sorting them using their hash.</span>
<span class="gi">+            # This is slower but works in any case.</span>
<span class="gi">+            self._sequence = sorted((hash(e) for e in set_sequence))</span>


<span class="w"> </span>class _MyHash(object):
<span class="gu">@@ -38,14 +51,103 @@ class Hasher(Pickler):</span>

<span class="w"> </span>    def __init__(self, hash_name=&#39;md5&#39;):
<span class="w"> </span>        self.stream = io.BytesIO()
<span class="gi">+        # By default we want a pickle protocol that only changes with</span>
<span class="gi">+        # the major python version and not the minor one</span>
<span class="w"> </span>        protocol = 3
<span class="w"> </span>        Pickler.__init__(self, self.stream, protocol=protocol)
<span class="gi">+        # Initialise the hash obj</span>
<span class="w"> </span>        self._hash = hashlib.new(hash_name)
<span class="gi">+</span>
<span class="gi">+    def hash(self, obj, return_digest=True):</span>
<span class="gi">+        try:</span>
<span class="gi">+            self.dump(obj)</span>
<span class="gi">+        except pickle.PicklingError as e:</span>
<span class="gi">+            e.args += (&#39;PicklingError while hashing %r: %r&#39; % (obj, e),)</span>
<span class="gi">+            raise</span>
<span class="gi">+        dumps = self.stream.getvalue()</span>
<span class="gi">+        self._hash.update(dumps)</span>
<span class="gi">+        if return_digest:</span>
<span class="gi">+            return self._hash.hexdigest()</span>
<span class="gi">+</span>
<span class="gi">+    def save(self, obj):</span>
<span class="gi">+        if isinstance(obj, (types.MethodType, type({}.pop))):</span>
<span class="gi">+            # the Pickler cannot pickle instance methods; here we decompose</span>
<span class="gi">+            # them into components that make them uniquely identifiable</span>
<span class="gi">+            if hasattr(obj, &#39;__func__&#39;):</span>
<span class="gi">+                func_name = obj.__func__.__name__</span>
<span class="gi">+            else:</span>
<span class="gi">+                func_name = obj.__name__</span>
<span class="gi">+            inst = obj.__self__</span>
<span class="gi">+            if type(inst) is type(pickle):</span>
<span class="gi">+                obj = _MyHash(func_name, inst.__name__)</span>
<span class="gi">+            elif inst is None:</span>
<span class="gi">+                # type(None) or type(module) do not pickle</span>
<span class="gi">+                obj = _MyHash(func_name, inst)</span>
<span class="gi">+            else:</span>
<span class="gi">+                cls = obj.__self__.__class__</span>
<span class="gi">+                obj = _MyHash(func_name, inst, cls)</span>
<span class="gi">+        Pickler.save(self, obj)</span>
<span class="gi">+</span>
<span class="gi">+    def memoize(self, obj):</span>
<span class="gi">+        # We want hashing to be sensitive to value instead of reference.</span>
<span class="gi">+        # For example we want [&#39;aa&#39;, &#39;aa&#39;] and [&#39;aa&#39;, &#39;aaZ&#39;[:2]]</span>
<span class="gi">+        # to hash to the same value and that&#39;s why we disable memoization</span>
<span class="gi">+        # for strings</span>
<span class="gi">+        if isinstance(obj, (bytes, str)):</span>
<span class="gi">+            return</span>
<span class="gi">+        Pickler.memoize(self, obj)</span>
<span class="gi">+</span>
<span class="gi">+    # The dispatch table of the pickler is not accessible in Python</span>
<span class="gi">+    # 3, as these lines are only bugware for IPython, we skip them.</span>
<span class="gi">+    def save_global(self, obj, name=None, pack=struct.pack):</span>
<span class="gi">+        # We have to override this method in order to deal with objects</span>
<span class="gi">+        # defined interactively in IPython that are not injected in</span>
<span class="gi">+        # __main__</span>
<span class="gi">+        kwargs = dict(name=name, pack=pack)</span>
<span class="gi">+        del kwargs[&#39;pack&#39;]</span>
<span class="gi">+        try:</span>
<span class="gi">+            Pickler.save_global(self, obj, **kwargs)</span>
<span class="gi">+        except pickle.PicklingError:</span>
<span class="gi">+            Pickler.save_global(self, obj, **kwargs)</span>
<span class="gi">+            module = getattr(obj, &quot;__module__&quot;, None)</span>
<span class="gi">+            if module == &#39;__main__&#39;:</span>
<span class="gi">+                my_name = name</span>
<span class="gi">+                if my_name is None:</span>
<span class="gi">+                    my_name = obj.__name__</span>
<span class="gi">+                mod = sys.modules[module]</span>
<span class="gi">+                if not hasattr(mod, my_name):</span>
<span class="gi">+                    # IPython doesn&#39;t inject the variables define</span>
<span class="gi">+                    # interactively in __main__</span>
<span class="gi">+                    setattr(mod, my_name, obj)</span>
<span class="gi">+</span>
<span class="w"> </span>    dispatch = Pickler.dispatch.copy()
<span class="gi">+    # builtin</span>
<span class="w"> </span>    dispatch[type(len)] = save_global
<span class="gi">+    # type</span>
<span class="w"> </span>    dispatch[type(object)] = save_global
<span class="gi">+    # classobj</span>
<span class="w"> </span>    dispatch[type(Pickler)] = save_global
<span class="gi">+    # function</span>
<span class="w"> </span>    dispatch[type(pickle.dump)] = save_global
<span class="gi">+</span>
<span class="gi">+    def _batch_setitems(self, items):</span>
<span class="gi">+        # forces order of keys in dict to ensure consistent hash.</span>
<span class="gi">+        try:</span>
<span class="gi">+            # Trying first to compare dict assuming the type of keys is</span>
<span class="gi">+            # consistent and orderable.</span>
<span class="gi">+            # This fails on python 3 when keys are unorderable</span>
<span class="gi">+            # but we keep it in a try as it&#39;s faster.</span>
<span class="gi">+            Pickler._batch_setitems(self, iter(sorted(items)))</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            # If keys are unorderable, sorting them using their hash. This is</span>
<span class="gi">+            # slower but works in any case.</span>
<span class="gi">+            Pickler._batch_setitems(self, iter(sorted((hash(k), v)</span>
<span class="gi">+                                                      for k, v in items)))</span>
<span class="gi">+</span>
<span class="gi">+    def save_set(self, set_items):</span>
<span class="gi">+        # forces order of items in Set to ensure consistent hash</span>
<span class="gi">+        Pickler.save(self, _ConsistentSet(set_items))</span>
<span class="gi">+</span>
<span class="w"> </span>    dispatch[type(set())] = save_set


<span class="gu">@@ -65,6 +167,7 @@ class NumpyHasher(Hasher):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        self.coerce_mmap = coerce_mmap
<span class="w"> </span>        Hasher.__init__(self, hash_name=hash_name)
<span class="gi">+        # delayed import of numpy, to avoid tight coupling</span>
<span class="w"> </span>        import numpy as np
<span class="w"> </span>        self.np = np
<span class="w"> </span>        if hasattr(np, &#39;getbuffer&#39;):
<span class="gu">@@ -77,7 +180,65 @@ class NumpyHasher(Hasher):</span>
<span class="w"> </span>            than pickling them. Off course, this is a total abuse of
<span class="w"> </span>            the Pickler class.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(obj, self.np.ndarray) and not obj.dtype.hasobject:</span>
<span class="gi">+            # Compute a hash of the object</span>
<span class="gi">+            # The update function of the hash requires a c_contiguous buffer.</span>
<span class="gi">+            if obj.shape == ():</span>
<span class="gi">+                # 0d arrays need to be flattened because viewing them as bytes</span>
<span class="gi">+                # raises a ValueError exception.</span>
<span class="gi">+                obj_c_contiguous = obj.flatten()</span>
<span class="gi">+            elif obj.flags.c_contiguous:</span>
<span class="gi">+                obj_c_contiguous = obj</span>
<span class="gi">+            elif obj.flags.f_contiguous:</span>
<span class="gi">+                obj_c_contiguous = obj.T</span>
<span class="gi">+            else:</span>
<span class="gi">+                # Cater for non-single-segment arrays: this creates a</span>
<span class="gi">+                # copy, and thus alleviates this issue.</span>
<span class="gi">+                # XXX: There might be a more efficient way of doing this</span>
<span class="gi">+                obj_c_contiguous = obj.flatten()</span>
<span class="gi">+</span>
<span class="gi">+            # memoryview is not supported for some dtypes, e.g. datetime64, see</span>
<span class="gi">+            # https://github.com/numpy/numpy/issues/4983. The</span>
<span class="gi">+            # workaround is to view the array as bytes before</span>
<span class="gi">+            # taking the memoryview.</span>
<span class="gi">+            self._hash.update(</span>
<span class="gi">+                self._getbuffer(obj_c_contiguous.view(self.np.uint8)))</span>
<span class="gi">+</span>
<span class="gi">+            # We store the class, to be able to distinguish between</span>
<span class="gi">+            # Objects with the same binary content, but different</span>
<span class="gi">+            # classes.</span>
<span class="gi">+            if self.coerce_mmap and isinstance(obj, self.np.memmap):</span>
<span class="gi">+                # We don&#39;t make the difference between memmap and</span>
<span class="gi">+                # normal ndarrays, to be able to reload previously</span>
<span class="gi">+                # computed results with memmap.</span>
<span class="gi">+                klass = self.np.ndarray</span>
<span class="gi">+            else:</span>
<span class="gi">+                klass = obj.__class__</span>
<span class="gi">+            # We also return the dtype and the shape, to distinguish</span>
<span class="gi">+            # different views on the same data with different dtypes.</span>
<span class="gi">+</span>
<span class="gi">+            # The object will be pickled by the pickler hashed at the end.</span>
<span class="gi">+            obj = (klass, (&#39;HASHED&#39;, obj.dtype, obj.shape, obj.strides))</span>
<span class="gi">+        elif isinstance(obj, self.np.dtype):</span>
<span class="gi">+            # numpy.dtype consistent hashing is tricky to get right. This comes</span>
<span class="gi">+            # from the fact that atomic np.dtype objects are interned:</span>
<span class="gi">+            # ``np.dtype(&#39;f4&#39;) is np.dtype(&#39;f4&#39;)``. The situation is</span>
<span class="gi">+            # complicated by the fact that this interning does not resist a</span>
<span class="gi">+            # simple pickle.load/dump roundtrip:</span>
<span class="gi">+            # ``pickle.loads(pickle.dumps(np.dtype(&#39;f4&#39;))) is not</span>
<span class="gi">+            # np.dtype(&#39;f4&#39;) Because pickle relies on memoization during</span>
<span class="gi">+            # pickling, it is easy to</span>
<span class="gi">+            # produce different hashes for seemingly identical objects, such as</span>
<span class="gi">+            # ``[np.dtype(&#39;f4&#39;), np.dtype(&#39;f4&#39;)]``</span>
<span class="gi">+            # and ``[np.dtype(&#39;f4&#39;), pickle.loads(pickle.dumps(&#39;f4&#39;))]``.</span>
<span class="gi">+            # To prevent memoization from interfering with hashing, we isolate</span>
<span class="gi">+            # the serialization (and thus the pickle memoization) of each dtype</span>
<span class="gi">+            # using each time a different ``pickle.dumps`` call unrelated to</span>
<span class="gi">+            # the current Hasher instance.</span>
<span class="gi">+            self._hash.update(&quot;_HASHED_DTYPE&quot;.encode(&#39;utf-8&#39;))</span>
<span class="gi">+            self._hash.update(pickle.dumps(obj))</span>
<span class="gi">+            return</span>
<span class="gi">+        Hasher.save(self, obj)</span>


<span class="w"> </span>def hash(obj, hash_name=&#39;md5&#39;, coerce_mmap=False):
<span class="gu">@@ -92,4 +253,13 @@ def hash(obj, hash_name=&#39;md5&#39;, coerce_mmap=False):</span>
<span class="w"> </span>        coerce_mmap: boolean
<span class="w"> </span>            Make no difference between np.memmap and np.ndarray
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    valid_hash_names = (&#39;md5&#39;, &#39;sha1&#39;)</span>
<span class="gi">+    if hash_name not in valid_hash_names:</span>
<span class="gi">+        raise ValueError(&quot;Valid options for &#39;hash_name&#39; are {}. &quot;</span>
<span class="gi">+                         &quot;Got hash_name={!r} instead.&quot;</span>
<span class="gi">+                         .format(valid_hash_names, hash_name))</span>
<span class="gi">+    if &#39;numpy&#39; in sys.modules:</span>
<span class="gi">+        hasher = NumpyHasher(hash_name=hash_name, coerce_mmap=coerce_mmap)</span>
<span class="gi">+    else:</span>
<span class="gi">+        hasher = Hasher(hash_name=hash_name)</span>
<span class="gi">+    return hasher.hash(obj)</span>
<span class="gh">diff --git a/joblib/logger.py b/joblib/logger.py</span>
<span class="gh">index 4991108..cf9d258 100644</span>
<span class="gd">--- a/joblib/logger.py</span>
<span class="gi">+++ b/joblib/logger.py</span>
<span class="gu">@@ -3,13 +3,20 @@ Helpers for logging.</span>

<span class="w"> </span>This module needs much love to become useful.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2008 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="w"> </span>from __future__ import print_function
<span class="gi">+</span>
<span class="w"> </span>import time
<span class="w"> </span>import sys
<span class="w"> </span>import os
<span class="w"> </span>import shutil
<span class="w"> </span>import logging
<span class="w"> </span>import pprint
<span class="gi">+</span>
<span class="w"> </span>from .disk import mkdirp


<span class="gu">@@ -18,9 +25,41 @@ def _squeeze_time(t):</span>
<span class="w"> </span>    stat files. This is needed to make results similar to timings under
<span class="w"> </span>    Unix, for tests
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sys.platform.startswith(&#39;win&#39;):</span>
<span class="gi">+        return max(0, t - .1)</span>
<span class="gi">+    else:</span>
<span class="gi">+        return t</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def format_time(t):</span>
<span class="gi">+    t = _squeeze_time(t)</span>
<span class="gi">+    return &quot;%.1fs, %.1fmin&quot; % (t, t / 60.)</span>
<span class="gi">+</span>

<span class="gi">+def short_format_time(t):</span>
<span class="gi">+    t = _squeeze_time(t)</span>
<span class="gi">+    if t &gt; 60:</span>
<span class="gi">+        return &quot;%4.1fmin&quot; % (t / 60.)</span>
<span class="gi">+    else:</span>
<span class="gi">+        return &quot; %5.1fs&quot; % (t)</span>

<span class="gi">+</span>
<span class="gi">+def pformat(obj, indent=0, depth=3):</span>
<span class="gi">+    if &#39;numpy&#39; in sys.modules:</span>
<span class="gi">+        import numpy as np</span>
<span class="gi">+        print_options = np.get_printoptions()</span>
<span class="gi">+        np.set_printoptions(precision=6, threshold=64, edgeitems=1)</span>
<span class="gi">+    else:</span>
<span class="gi">+        print_options = None</span>
<span class="gi">+    out = pprint.pformat(obj, depth=depth, indent=indent)</span>
<span class="gi">+    if print_options:</span>
<span class="gi">+        np.set_printoptions(**print_options)</span>
<span class="gi">+    return out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# class `Logger`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class Logger(object):
<span class="w"> </span>    &quot;&quot;&quot; Base class for logging messages.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -37,11 +76,24 @@ class Logger(object):</span>
<span class="w"> </span>        self.depth = depth
<span class="w"> </span>        self._name = name if name else &#39;joblib&#39;

<span class="gi">+    def warn(self, msg):</span>
<span class="gi">+        logging.getLogger(self._name).warning(&quot;[%s]: %s&quot; % (self, msg))</span>
<span class="gi">+</span>
<span class="gi">+    def info(self, msg):</span>
<span class="gi">+        logging.info(&quot;[%s]: %s&quot; % (self, msg))</span>
<span class="gi">+</span>
<span class="gi">+    def debug(self, msg):</span>
<span class="gi">+        # XXX: This conflicts with the debug flag used in children class</span>
<span class="gi">+        logging.getLogger(self._name).debug(&quot;[%s]: %s&quot; % (self, msg))</span>
<span class="gi">+</span>
<span class="w"> </span>    def format(self, obj, indent=0):
<span class="w"> </span>        &quot;&quot;&quot;Return the formatted representation of the object.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return pformat(obj, indent=indent, depth=self.depth)</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# class `PrintTime`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class PrintTime(object):
<span class="w"> </span>    &quot;&quot;&quot; Print and log messages while keeping track of time.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -49,6 +101,7 @@ class PrintTime(object):</span>
<span class="w"> </span>    def __init__(self, logfile=None, logdir=None):
<span class="w"> </span>        if logfile is not None and logdir is not None:
<span class="w"> </span>            raise ValueError(&#39;Cannot specify both logfile and logdir&#39;)
<span class="gi">+        # XXX: Need argument docstring</span>
<span class="w"> </span>        self.last_time = time.time()
<span class="w"> </span>        self.start_time = self.last_time
<span class="w"> </span>        if logdir is not None:
<span class="gu">@@ -57,25 +110,30 @@ class PrintTime(object):</span>
<span class="w"> </span>        if logfile is not None:
<span class="w"> </span>            mkdirp(os.path.dirname(logfile))
<span class="w"> </span>            if os.path.exists(logfile):
<span class="gi">+                # Rotate the logs</span>
<span class="w"> </span>                for i in range(1, 9):
<span class="w"> </span>                    try:
<span class="gd">-                        shutil.move(logfile + &#39;.%i&#39; % i, logfile + &#39;.%i&#39; %</span>
<span class="gd">-                            (i + 1))</span>
<span class="gd">-                    except:</span>
<span class="gd">-                        &quot;&quot;&quot;No reason failing here&quot;&quot;&quot;</span>
<span class="gi">+                        shutil.move(logfile + &#39;.%i&#39; % i,</span>
<span class="gi">+                                    logfile + &#39;.%i&#39; % (i + 1))</span>
<span class="gi">+                    except:  # noqa: E722</span>
<span class="gi">+                        &quot;No reason failing here&quot;</span>
<span class="gi">+                # Use a copy rather than a move, so that a process</span>
<span class="gi">+                # monitoring this file does not get lost.</span>
<span class="w"> </span>                try:
<span class="w"> </span>                    shutil.copy(logfile, logfile + &#39;.1&#39;)
<span class="gd">-                except:</span>
<span class="gd">-                    &quot;&quot;&quot;No reason failing here&quot;&quot;&quot;</span>
<span class="gi">+                except:  # noqa: E722</span>
<span class="gi">+                    &quot;No reason failing here&quot;</span>
<span class="w"> </span>            try:
<span class="w"> </span>                with open(logfile, &#39;w&#39;) as logfile:
<span class="w"> </span>                    logfile.write(&#39;\nLogging joblib python script\n&#39;)
<span class="w"> </span>                    logfile.write(&#39;\n---%s---\n&#39; % time.ctime(self.last_time))
<span class="gd">-            except:</span>
<span class="gi">+            except:  # noqa: E722</span>
<span class="w"> </span>                &quot;&quot;&quot; Multiprocessing writing to files can create race
<span class="w"> </span>                    conditions. Rather fail silently than crash the
<span class="w"> </span>                    computation.
<span class="w"> </span>                &quot;&quot;&quot;
<span class="gi">+                # XXX: We actually need a debug flag to disable this</span>
<span class="gi">+                # silent failure.</span>

<span class="w"> </span>    def __call__(self, msg=&#39;&#39;, total=False):
<span class="w"> </span>        &quot;&quot;&quot; Print the time elapsed between the last call and the current
<span class="gu">@@ -83,19 +141,22 @@ class PrintTime(object):</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        if not total:
<span class="w"> </span>            time_lapse = time.time() - self.last_time
<span class="gd">-            full_msg = &#39;%s: %s&#39; % (msg, format_time(time_lapse))</span>
<span class="gi">+            full_msg = &quot;%s: %s&quot; % (msg, format_time(time_lapse))</span>
<span class="w"> </span>        else:
<span class="gi">+            # FIXME: Too much logic duplicated</span>
<span class="w"> </span>            time_lapse = time.time() - self.start_time
<span class="gd">-            full_msg = &#39;%s: %.2fs, %.1f min&#39; % (msg, time_lapse, time_lapse /</span>
<span class="gd">-                60)</span>
<span class="gi">+            full_msg = &quot;%s: %.2fs, %.1f min&quot; % (msg, time_lapse,</span>
<span class="gi">+                                                time_lapse / 60)</span>
<span class="w"> </span>        print(full_msg, file=sys.stderr)
<span class="w"> </span>        if self.logfile is not None:
<span class="w"> </span>            try:
<span class="w"> </span>                with open(self.logfile, &#39;a&#39;) as f:
<span class="w"> </span>                    print(full_msg, file=f)
<span class="gd">-            except:</span>
<span class="gi">+            except:  # noqa: E722</span>
<span class="w"> </span>                &quot;&quot;&quot; Multiprocessing writing to files can create race
<span class="w"> </span>                    conditions. Rather fail silently than crash the
<span class="w"> </span>                    calculation.
<span class="w"> </span>                &quot;&quot;&quot;
<span class="gi">+                # XXX: We actually need a debug flag to disable this</span>
<span class="gi">+                # silent failure.</span>
<span class="w"> </span>        self.last_time = time.time()
<span class="gh">diff --git a/joblib/memory.py b/joblib/memory.py</span>
<span class="gh">index 14f8456..b83a855 100644</span>
<span class="gd">--- a/joblib/memory.py</span>
<span class="gi">+++ b/joblib/memory.py</span>
<span class="gu">@@ -3,6 +3,12 @@ A context object for caching a function&#39;s return value each time it</span>
<span class="w"> </span>is called with the same input arguments.

<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2009 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>import asyncio
<span class="w"> </span>import datetime
<span class="w"> </span>import functools
<span class="gu">@@ -18,19 +24,40 @@ import tokenize</span>
<span class="w"> </span>import traceback
<span class="w"> </span>import warnings
<span class="w"> </span>import weakref
<span class="gi">+</span>
<span class="w"> </span>from . import hashing
<span class="gd">-from ._store_backends import CacheWarning</span>
<span class="gi">+from ._store_backends import CacheWarning  # noqa</span>
<span class="w"> </span>from ._store_backends import FileSystemStoreBackend, StoreBackendBase
<span class="gd">-from .func_inspect import filter_args, format_call, format_signature, get_func_code, get_func_name</span>
<span class="gi">+from .func_inspect import (filter_args, format_call, format_signature,</span>
<span class="gi">+                           get_func_code, get_func_name)</span>
<span class="w"> </span>from .logger import Logger, format_time, pformat
<span class="gd">-FIRST_LINE_TEXT = &#39;# first line:&#39;</span>
<span class="gi">+</span>
<span class="gi">+FIRST_LINE_TEXT = &quot;# first line:&quot;</span>
<span class="gi">+</span>
<span class="gi">+# TODO: The following object should have a data store object as a sub</span>
<span class="gi">+# object, and the interface to persist and query should be separated in</span>
<span class="gi">+# the data store.</span>
<span class="gi">+#</span>
<span class="gi">+# This would enable creating &#39;Memory&#39; objects with a different logic for</span>
<span class="gi">+# pickling that would simply span a MemorizedFunc with the same</span>
<span class="gi">+# store (or do we want to copy it to avoid cross-talks?), for instance to</span>
<span class="gi">+# implement HDF5 pickling.</span>
<span class="gi">+</span>
<span class="gi">+# TODO: Same remark for the logger, and probably use the Python logging</span>
<span class="gi">+# mechanism.</span>


<span class="w"> </span>def extract_first_line(func_code):
<span class="w"> </span>    &quot;&quot;&quot; Extract the first line information from the function code
<span class="w"> </span>        text if available.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if func_code.startswith(FIRST_LINE_TEXT):</span>
<span class="gi">+        func_code = func_code.split(&#39;\n&#39;)</span>
<span class="gi">+        first_line = int(func_code[0][len(FIRST_LINE_TEXT):])</span>
<span class="gi">+        func_code = &#39;\n&#39;.join(func_code[1:])</span>
<span class="gi">+    else:</span>
<span class="gi">+        first_line = -1</span>
<span class="gi">+    return func_code, first_line</span>


<span class="w"> </span>class JobLibCollisionWarning(UserWarning):
<span class="gu">@@ -59,22 +86,72 @@ def register_store_backend(backend_name, backend):</span>
<span class="w"> </span>        The name of a class that implements the StoreBackendBase interface.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not isinstance(backend_name, str):</span>
<span class="gi">+        raise ValueError(&quot;Store backend name should be a string, &quot;</span>
<span class="gi">+                         &quot;&#39;{0}&#39; given.&quot;.format(backend_name))</span>
<span class="gi">+    if backend is None or not issubclass(backend, StoreBackendBase):</span>
<span class="gi">+        raise ValueError(&quot;Store backend should inherit &quot;</span>
<span class="gi">+                         &quot;StoreBackendBase, &quot;</span>
<span class="gi">+                         &quot;&#39;{0}&#39; given.&quot;.format(backend))</span>
<span class="gi">+</span>
<span class="gi">+    _STORE_BACKENDS[backend_name] = backend</span>


<span class="w"> </span>def _store_backend_factory(backend, location, verbose=0, backend_options=None):
<span class="w"> </span>    &quot;&quot;&quot;Return the correct store object for the given location.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if backend_options is None:</span>
<span class="gi">+        backend_options = {}</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(location, pathlib.Path):</span>
<span class="gi">+        location = str(location)</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(location, StoreBackendBase):</span>
<span class="gi">+        return location</span>
<span class="gi">+    elif isinstance(location, str):</span>
<span class="gi">+        obj = None</span>
<span class="gi">+        location = os.path.expanduser(location)</span>
<span class="gi">+        # The location is not a local file system, we look in the</span>
<span class="gi">+        # registered backends if there&#39;s one matching the given backend</span>
<span class="gi">+        # name.</span>
<span class="gi">+        for backend_key, backend_obj in _STORE_BACKENDS.items():</span>
<span class="gi">+            if backend == backend_key:</span>
<span class="gi">+                obj = backend_obj()</span>
<span class="gi">+</span>
<span class="gi">+        # By default, we assume the FileSystemStoreBackend can be used if no</span>
<span class="gi">+        # matching backend could be found.</span>
<span class="gi">+        if obj is None:</span>
<span class="gi">+            raise TypeError(&#39;Unknown location {0} or backend {1}&#39;.format(</span>
<span class="gi">+                            location, backend))</span>
<span class="gi">+</span>
<span class="gi">+        # The store backend is configured with the extra named parameters,</span>
<span class="gi">+        # some of them are specific to the underlying store backend.</span>
<span class="gi">+        obj.configure(location, verbose=verbose,</span>
<span class="gi">+                      backend_options=backend_options)</span>
<span class="gi">+        return obj</span>
<span class="gi">+    elif location is not None:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;Instantiating a backend using a {} as a location is not &quot;</span>
<span class="gi">+            &quot;supported by joblib. Returning None instead.&quot;.format(</span>
<span class="gi">+                location.__class__.__name__), UserWarning)</span>
<span class="gi">+</span>
<span class="gi">+    return None</span>


<span class="w"> </span>def _build_func_identifier(func):
<span class="w"> </span>    &quot;&quot;&quot;Build a roughly unique identifier for the cached function.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    modules, funcname = get_func_name(func)</span>
<span class="gi">+    # We reuse historical fs-like way of building a function identifier</span>
<span class="gi">+    return os.path.join(*modules, funcname)</span>


<span class="gi">+# An in-memory store to avoid looking at the disk-based function</span>
<span class="gi">+# source code to check if a function definition has changed</span>
<span class="w"> </span>_FUNCTION_HASHES = weakref.WeakKeyDictionary()


<span class="gi">+###############################################################################</span>
<span class="gi">+# class `MemorizedResult`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class MemorizedResult(Logger):
<span class="w"> </span>    &quot;&quot;&quot;Object representing a cached value.

<span class="gu">@@ -105,33 +182,69 @@ class MemorizedResult(Logger):</span>
<span class="w"> </span>    timestamp, metadata: string
<span class="w"> </span>        for internal use only.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, location, call_id, backend=&#39;local&#39;, mmap_mode=None,
<span class="gd">-        verbose=0, timestamp=None, metadata=None):</span>
<span class="gi">+                 verbose=0, timestamp=None, metadata=None):</span>
<span class="w"> </span>        Logger.__init__(self)
<span class="w"> </span>        self._call_id = call_id
<span class="w"> </span>        self.store_backend = _store_backend_factory(backend, location,
<span class="gd">-            verbose=verbose)</span>
<span class="gi">+                                                    verbose=verbose)</span>
<span class="w"> </span>        self.mmap_mode = mmap_mode
<span class="gi">+</span>
<span class="w"> </span>        if metadata is not None:
<span class="w"> </span>            self.metadata = metadata
<span class="w"> </span>        else:
<span class="w"> </span>            self.metadata = self.store_backend.get_metadata(self._call_id)
<span class="gi">+</span>
<span class="w"> </span>        self.duration = self.metadata.get(&#39;duration&#39;, None)
<span class="w"> </span>        self.verbose = verbose
<span class="w"> </span>        self.timestamp = timestamp

<span class="gi">+    @property</span>
<span class="gi">+    def func(self):</span>
<span class="gi">+        return self.func_id</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def func_id(self):</span>
<span class="gi">+        return self._call_id[0]</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def args_id(self):</span>
<span class="gi">+        return self._call_id[1]</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def argument_hash(self):</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;The &#39;argument_hash&#39; attribute has been deprecated in version &quot;</span>
<span class="gi">+            &quot;0.12 and will be removed in version 0.14.\n&quot;</span>
<span class="gi">+            &quot;Use `args_id` attribute instead.&quot;,</span>
<span class="gi">+            DeprecationWarning, stacklevel=2)</span>
<span class="gi">+        return self.args_id</span>
<span class="gi">+</span>
<span class="w"> </span>    def get(self):
<span class="w"> </span>        &quot;&quot;&quot;Read value from cache and return it.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            return self.store_backend.load_item(</span>
<span class="gi">+                self._call_id,</span>
<span class="gi">+                timestamp=self.timestamp,</span>
<span class="gi">+                metadata=self.metadata,</span>
<span class="gi">+                verbose=self.verbose</span>
<span class="gi">+            )</span>
<span class="gi">+        except ValueError as exc:</span>
<span class="gi">+            new_exc = KeyError(</span>
<span class="gi">+                &quot;Error while trying to load a MemorizedResult&#39;s value. &quot;</span>
<span class="gi">+                &quot;It seems that this folder is corrupted : {}&quot;.format(</span>
<span class="gi">+                    os.path.join(self.store_backend.location, *self._call_id)))</span>
<span class="gi">+            raise new_exc from exc</span>

<span class="w"> </span>    def clear(self):
<span class="w"> </span>        &quot;&quot;&quot;Clear value from cache&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.store_backend.clear_item(self._call_id)</span>

<span class="w"> </span>    def __repr__(self):
<span class="gd">-        return &#39;{}(location=&quot;{}&quot;, func=&quot;{}&quot;, args_id=&quot;{}&quot;)&#39;.format(self.</span>
<span class="gd">-            __class__.__name__, self.store_backend.location, *self._call_id)</span>
<span class="gi">+        return &#39;{}(location=&quot;{}&quot;, func=&quot;{}&quot;, args_id=&quot;{}&quot;)&#39;.format(</span>
<span class="gi">+            self.__class__.__name__, self.store_backend.location,</span>
<span class="gi">+            *self._call_id</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        state = self.__dict__.copy()
<span class="gu">@@ -144,27 +257,42 @@ class NotMemorizedResult(object):</span>

<span class="w"> </span>    This class is a replacement for MemorizedResult when there is no cache.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    __slots__ = &#39;value&#39;, &#39;valid&#39;</span>
<span class="gi">+    __slots__ = (&#39;value&#39;, &#39;valid&#39;)</span>

<span class="w"> </span>    def __init__(self, value):
<span class="w"> </span>        self.value = value
<span class="w"> </span>        self.valid = True

<span class="gi">+    def get(self):</span>
<span class="gi">+        if self.valid:</span>
<span class="gi">+            return self.value</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise KeyError(&quot;No value stored.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def clear(self):</span>
<span class="gi">+        self.valid = False</span>
<span class="gi">+        self.value = None</span>
<span class="gi">+</span>
<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        if self.valid:
<span class="gd">-            return &#39;{class_name}({value})&#39;.format(class_name=self.__class__</span>
<span class="gd">-                .__name__, value=pformat(self.value))</span>
<span class="gi">+            return (&#39;{class_name}({value})&#39;</span>
<span class="gi">+                    .format(class_name=self.__class__.__name__,</span>
<span class="gi">+                            value=pformat(self.value)))</span>
<span class="w"> </span>        else:
<span class="w"> </span>            return self.__class__.__name__ + &#39; with no value&#39;

<span class="gi">+    # __getstate__ and __setstate__ are required because of __slots__</span>
<span class="w"> </span>    def __getstate__(self):
<span class="gd">-        return {&#39;valid&#39;: self.valid, &#39;value&#39;: self.value}</span>
<span class="gi">+        return {&quot;valid&quot;: self.valid, &quot;value&quot;: self.value}</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="gd">-        self.valid = state[&#39;valid&#39;]</span>
<span class="gd">-        self.value = state[&#39;value&#39;]</span>
<span class="gi">+        self.valid = state[&quot;valid&quot;]</span>
<span class="gi">+        self.value = state[&quot;value&quot;]</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# class `NotMemorizedFunc`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class NotMemorizedFunc(object):
<span class="w"> </span>    &quot;&quot;&quot;No-op object decorating a function.

<span class="gu">@@ -176,21 +304,41 @@ class NotMemorizedFunc(object):</span>
<span class="w"> </span>    func: callable
<span class="w"> </span>        Original undecorated function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-</span>
<span class="gi">+    # Should be a light as possible (for speed)</span>
<span class="w"> </span>    def __init__(self, func):
<span class="w"> </span>        self.func = func

<span class="w"> </span>    def __call__(self, *args, **kwargs):
<span class="w"> </span>        return self.func(*args, **kwargs)

<span class="gi">+    def call_and_shelve(self, *args, **kwargs):</span>
<span class="gi">+        return NotMemorizedResult(self.func(*args, **kwargs))</span>
<span class="gi">+</span>
<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        return &#39;{0}(func={1})&#39;.format(self.__class__.__name__, self.func)

<span class="gi">+    def clear(self, warn=True):</span>
<span class="gi">+        # Argument &quot;warn&quot; is for compatibility with MemorizedFunc.clear</span>
<span class="gi">+        pass</span>

<span class="gi">+    def call(self, *args, **kwargs):</span>
<span class="gi">+        return self.func(*args, **kwargs), {}</span>
<span class="gi">+</span>
<span class="gi">+    def check_call_in_cache(self, *args, **kwargs):</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# class `AsyncNotMemorizedFunc`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class AsyncNotMemorizedFunc(NotMemorizedFunc):
<span class="gd">-    pass</span>
<span class="gi">+    async def call_and_shelve(self, *args, **kwargs):</span>
<span class="gi">+        return NotMemorizedResult(await self.func(*args, **kwargs))</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# class `MemorizedFunc`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class MemorizedFunc(Logger):
<span class="w"> </span>    &quot;&quot;&quot;Callable object decorating a function for caching its return value
<span class="w"> </span>    each time it is called.
<span class="gu">@@ -236,10 +384,13 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>        argument. If it returns True, the cached result is returned, else the
<span class="w"> </span>        cache for these arguments is cleared and the result is recomputed.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+    # ------------------------------------------------------------------------</span>
<span class="gi">+    # Public interface</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>

<span class="w"> </span>    def __init__(self, func, location, backend=&#39;local&#39;, ignore=None,
<span class="gd">-        mmap_mode=None, compress=False, verbose=1, timestamp=None,</span>
<span class="gd">-        cache_validation_callback=None):</span>
<span class="gi">+                 mmap_mode=None, compress=False, verbose=1, timestamp=None,</span>
<span class="gi">+                 cache_validation_callback=None):</span>
<span class="w"> </span>        Logger.__init__(self)
<span class="w"> </span>        self.mmap_mode = mmap_mode
<span class="w"> </span>        self.compress = compress
<span class="gu">@@ -248,23 +399,34 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>        self.func_id = _build_func_identifier(func)
<span class="w"> </span>        self.ignore = ignore if ignore is not None else []
<span class="w"> </span>        self._verbose = verbose
<span class="gi">+</span>
<span class="gi">+        # retrieve store object from backend type and location.</span>
<span class="w"> </span>        self.store_backend = _store_backend_factory(backend, location,
<span class="gd">-            verbose=verbose, backend_options=dict(compress=compress,</span>
<span class="gd">-            mmap_mode=mmap_mode))</span>
<span class="gi">+                                                    verbose=verbose,</span>
<span class="gi">+                                                    backend_options=dict(</span>
<span class="gi">+                                                        compress=compress,</span>
<span class="gi">+                                                        mmap_mode=mmap_mode),</span>
<span class="gi">+                                                    )</span>
<span class="w"> </span>        if self.store_backend is not None:
<span class="gi">+            # Create func directory on demand.</span>
<span class="w"> </span>            self.store_backend.store_cached_func_code([self.func_id])
<span class="gi">+</span>
<span class="w"> </span>        self.timestamp = timestamp if timestamp is not None else time.time()
<span class="w"> </span>        try:
<span class="w"> </span>            functools.update_wrapper(self, func)
<span class="w"> </span>        except Exception:
<span class="gd">-            pass</span>
<span class="gi">+            pass  # Objects like ufunc don&#39;t like that</span>
<span class="w"> </span>        if inspect.isfunction(func):
<span class="w"> </span>            doc = pydoc.TextDoc().document(func)
<span class="gi">+            # Remove blank line</span>
<span class="w"> </span>            doc = doc.replace(&#39;\n&#39;, &#39;\n\n&#39;, 1)
<span class="gi">+            # Strip backspace-overprints for compatibility with autodoc</span>
<span class="w"> </span>            doc = re.sub(&#39;\x08.&#39;, &#39;&#39;, doc)
<span class="w"> </span>        else:
<span class="gi">+            # Pydoc does a poor job on other objects</span>
<span class="w"> </span>            doc = func.__doc__
<span class="w"> </span>        self.__doc__ = &#39;Memoized version of %s&#39; % doc
<span class="gi">+</span>
<span class="w"> </span>        self._func_code_info = None
<span class="w"> </span>        self._func_code_id = None

<span class="gu">@@ -279,7 +441,22 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>        Returns True if the function call is in cache and can be used, and
<span class="w"> </span>        returns False otherwise.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check if the code of the function has changed</span>
<span class="gi">+        if not self._check_previous_func_code(stacklevel=4):</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        # Check if this specific call is in the cache</span>
<span class="gi">+        if not self.store_backend.contains_item(call_id):</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        # Call the user defined cache validation callback</span>
<span class="gi">+        metadata = self.store_backend.get_metadata(call_id)</span>
<span class="gi">+        if (self.cache_validation_callback is not None and</span>
<span class="gi">+                not self.cache_validation_callback(metadata)):</span>
<span class="gi">+            self.store_backend.clear_item(call_id)</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        return True</span>

<span class="w"> </span>    def _cached_call(self, args, kwargs, shelving):
<span class="w"> </span>        &quot;&quot;&quot;Call wrapped function and cache result, or read cache if available.
<span class="gu">@@ -303,7 +480,79 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>            MemorizedResult reference to the value if shelving is true.
<span class="w"> </span>        metadata: dict containing the metadata associated with the call.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        args_id = self._get_args_id(*args, **kwargs)</span>
<span class="gi">+        call_id = (self.func_id, args_id)</span>
<span class="gi">+        _, func_name = get_func_name(self.func)</span>
<span class="gi">+        func_info = self.store_backend.get_cached_func_info([self.func_id])</span>
<span class="gi">+        location = func_info[&#39;location&#39;]</span>
<span class="gi">+</span>
<span class="gi">+        if self._verbose &gt;= 20:</span>
<span class="gi">+            logging.basicConfig(level=logging.INFO)</span>
<span class="gi">+            _, signature = format_signature(self.func, *args, **kwargs)</span>
<span class="gi">+            self.info(</span>
<span class="gi">+                textwrap.dedent(</span>
<span class="gi">+                    f&quot;&quot;&quot;</span>
<span class="gi">+                        Querying {func_name} with signature</span>
<span class="gi">+                        {signature}.</span>
<span class="gi">+</span>
<span class="gi">+                        (argument hash {args_id})</span>
<span class="gi">+</span>
<span class="gi">+                        The store location is {location}.</span>
<span class="gi">+                        &quot;&quot;&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Compare the function code with the previous to see if the</span>
<span class="gi">+        # function code has changed and check if the results are present in</span>
<span class="gi">+        # the cache.</span>
<span class="gi">+        if self._is_in_cache_and_valid(call_id):</span>
<span class="gi">+            if shelving:</span>
<span class="gi">+                return self._get_memorized_result(call_id), {}</span>
<span class="gi">+</span>
<span class="gi">+            try:</span>
<span class="gi">+                start_time = time.time()</span>
<span class="gi">+                output = self._load_item(call_id)</span>
<span class="gi">+                if self._verbose &gt; 4:</span>
<span class="gi">+                    self._print_duration(time.time() - start_time,</span>
<span class="gi">+                                         context=&#39;cache loaded &#39;)</span>
<span class="gi">+                return output, {}</span>
<span class="gi">+            except Exception:</span>
<span class="gi">+                # XXX: Should use an exception logger</span>
<span class="gi">+                _, signature = format_signature(self.func, *args, **kwargs)</span>
<span class="gi">+                self.warn(&#39;Exception while loading results for &#39;</span>
<span class="gi">+                          &#39;{}\n {}&#39;.format(signature, traceback.format_exc()))</span>
<span class="gi">+</span>
<span class="gi">+        if self._verbose &gt; 10:</span>
<span class="gi">+            self.warn(</span>
<span class="gi">+                f&quot;Computing func {func_name}, argument hash {args_id} &quot;</span>
<span class="gi">+                f&quot;in location {location}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Returns the output but not the metadata</span>
<span class="gi">+        return self._call(call_id, args, kwargs, shelving)</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def func_code_info(self):</span>
<span class="gi">+        # 3-tuple property containing: the function source code, source file,</span>
<span class="gi">+        # and first line of the code inside the source file</span>
<span class="gi">+        if hasattr(self.func, &#39;__code__&#39;):</span>
<span class="gi">+            if self._func_code_id is None:</span>
<span class="gi">+                self._func_code_id = id(self.func.__code__)</span>
<span class="gi">+            elif id(self.func.__code__) != self._func_code_id:</span>
<span class="gi">+                # Be robust to dynamic reassignments of self.func.__code__</span>
<span class="gi">+                self._func_code_info = None</span>
<span class="gi">+</span>
<span class="gi">+        if self._func_code_info is None:</span>
<span class="gi">+            # Cache the source code of self.func . Provided that get_func_code</span>
<span class="gi">+            # (which should be called once on self) gets called in the process</span>
<span class="gi">+            # in which self.func was defined, this caching mechanism prevents</span>
<span class="gi">+            # undesired cache clearing when the cached function is called in</span>
<span class="gi">+            # an environment where the introspection utilities get_func_code</span>
<span class="gi">+            # relies on do not work (typically, in joblib child processes).</span>
<span class="gi">+            # See #1035 for  more info</span>
<span class="gi">+            # TODO (pierreglaser): do the same with get_func_name?</span>
<span class="gi">+            self._func_code_info = get_func_code(self.func)</span>
<span class="gi">+        return self._func_code_info</span>

<span class="w"> </span>    def call_and_shelve(self, *args, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Call wrapped function, cache result and return a reference.
<span class="gu">@@ -320,16 +569,27 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>            class &quot;NotMemorizedResult&quot; is used when there is no cache
<span class="w"> </span>            activated (e.g. location=None in Memory).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Return the wrapped output, without the metadata</span>
<span class="gi">+        return self._cached_call(args, kwargs, shelving=True)[0]</span>

<span class="w"> </span>    def __call__(self, *args, **kwargs):
<span class="gi">+        # Return the output, without the metadata</span>
<span class="w"> </span>        return self._cached_call(args, kwargs, shelving=False)[0]

<span class="w"> </span>    def __getstate__(self):
<span class="gi">+        # Make sure self.func&#39;s source is introspected prior to being pickled -</span>
<span class="gi">+        # code introspection utilities typically do not work inside child</span>
<span class="gi">+        # processes</span>
<span class="w"> </span>        _ = self.func_code_info
<span class="gi">+</span>
<span class="gi">+        # We don&#39;t store the timestamp when pickling, to avoid the hash</span>
<span class="gi">+        # depending from it.</span>
<span class="w"> </span>        state = self.__dict__.copy()
<span class="w"> </span>        state[&#39;timestamp&#39;] = None
<span class="gi">+</span>
<span class="gi">+        # Invalidate the code id as id(obj) will be different in the child</span>
<span class="w"> </span>        state[&#39;_func_code_id&#39;] = None
<span class="gi">+</span>
<span class="w"> </span>        return state

<span class="w"> </span>    def check_call_in_cache(self, *args, **kwargs):
<span class="gu">@@ -344,31 +604,140 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>            Whether or not the result of the function has been cached
<span class="w"> </span>            for the input arguments that have been passed.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        call_id = (self.func_id, self._get_args_id(*args, **kwargs))</span>
<span class="gi">+        return self.store_backend.contains_item(call_id)</span>
<span class="gi">+</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>
<span class="gi">+    # Private interface</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>

<span class="w"> </span>    def _get_args_id(self, *args, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Return the input parameter hash of a result.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return hashing.hash(filter_args(self.func, self.ignore, args, kwargs),</span>
<span class="gi">+                            coerce_mmap=self.mmap_mode is not None)</span>

<span class="w"> </span>    def _hash_func(self):
<span class="w"> </span>        &quot;&quot;&quot;Hash a function to key the online cache&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        func_code_h = hash(getattr(self.func, &#39;__code__&#39;, None))</span>
<span class="gi">+        return id(self.func), hash(self.func), func_code_h</span>

<span class="w"> </span>    def _write_func_code(self, func_code, first_line):
<span class="w"> </span>        &quot;&quot;&quot; Write the function code and the filename to a file.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # We store the first line because the filename and the function</span>
<span class="gi">+        # name is not always enough to identify a function: people</span>
<span class="gi">+        # sometimes have several functions named the same way in a</span>
<span class="gi">+        # file. This is bad practice, but joblib should be robust to bad</span>
<span class="gi">+        # practice.</span>
<span class="gi">+        func_code = u&#39;%s %i\n%s&#39; % (FIRST_LINE_TEXT, first_line, func_code)</span>
<span class="gi">+        self.store_backend.store_cached_func_code([self.func_id], func_code)</span>
<span class="gi">+</span>
<span class="gi">+        # Also store in the in-memory store of function hashes</span>
<span class="gi">+        is_named_callable = (hasattr(self.func, &#39;__name__&#39;) and</span>
<span class="gi">+                             self.func.__name__ != &#39;&lt;lambda&gt;&#39;)</span>
<span class="gi">+        if is_named_callable:</span>
<span class="gi">+            # Don&#39;t do this for lambda functions or strange callable</span>
<span class="gi">+            # objects, as it ends up being too fragile</span>
<span class="gi">+            func_hash = self._hash_func()</span>
<span class="gi">+            try:</span>
<span class="gi">+                _FUNCTION_HASHES[self.func] = func_hash</span>
<span class="gi">+            except TypeError:</span>
<span class="gi">+                # Some callable are not hashable</span>
<span class="gi">+                pass</span>

<span class="w"> </span>    def _check_previous_func_code(self, stacklevel=2):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>            stacklevel is the depth a which this function is called, to
<span class="w"> </span>            issue useful warnings to the user.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # First check if our function is in the in-memory store.</span>
<span class="gi">+        # Using the in-memory store not only makes things faster, but it</span>
<span class="gi">+        # also renders us robust to variations of the files when the</span>
<span class="gi">+        # in-memory version of the code does not vary</span>
<span class="gi">+        try:</span>
<span class="gi">+            if self.func in _FUNCTION_HASHES:</span>
<span class="gi">+                # We use as an identifier the id of the function and its</span>
<span class="gi">+                # hash. This is more likely to falsely change than have hash</span>
<span class="gi">+                # collisions, thus we are on the safe side.</span>
<span class="gi">+                func_hash = self._hash_func()</span>
<span class="gi">+                if func_hash == _FUNCTION_HASHES[self.func]:</span>
<span class="gi">+                    return True</span>
<span class="gi">+        except TypeError:</span>
<span class="gi">+            # Some callables are not hashable</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+        # Here, we go through some effort to be robust to dynamically</span>
<span class="gi">+        # changing code and collision. We cannot inspect.getsource</span>
<span class="gi">+        # because it is not reliable when using IPython&#39;s magic &quot;%run&quot;.</span>
<span class="gi">+        func_code, source_file, first_line = self.func_code_info</span>
<span class="gi">+        try:</span>
<span class="gi">+            old_func_code, old_first_line = extract_first_line(</span>
<span class="gi">+                self.store_backend.get_cached_func_code([self.func_id]))</span>
<span class="gi">+        except (IOError, OSError):  # some backend can also raise OSError</span>
<span class="gi">+            self._write_func_code(func_code, first_line)</span>
<span class="gi">+            return False</span>
<span class="gi">+        if old_func_code == func_code:</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        # We have differing code, is this because we are referring to</span>
<span class="gi">+        # different functions, or because the function we are referring to has</span>
<span class="gi">+        # changed?</span>
<span class="gi">+</span>
<span class="gi">+        _, func_name = get_func_name(self.func, resolv_alias=False,</span>
<span class="gi">+                                     win_characters=False)</span>
<span class="gi">+        if old_first_line == first_line == -1 or func_name == &#39;&lt;lambda&gt;&#39;:</span>
<span class="gi">+            if not first_line == -1:</span>
<span class="gi">+                func_description = (&quot;{0} ({1}:{2})&quot;</span>
<span class="gi">+                                    .format(func_name, source_file,</span>
<span class="gi">+                                            first_line))</span>
<span class="gi">+            else:</span>
<span class="gi">+                func_description = func_name</span>
<span class="gi">+            warnings.warn(JobLibCollisionWarning(</span>
<span class="gi">+                &quot;Cannot detect name collisions for function &#39;{0}&#39;&quot;</span>
<span class="gi">+                .format(func_description)), stacklevel=stacklevel)</span>
<span class="gi">+</span>
<span class="gi">+        # Fetch the code at the old location and compare it. If it is the</span>
<span class="gi">+        # same than the code store, we have a collision: the code in the</span>
<span class="gi">+        # file has not changed, but the name we have is pointing to a new</span>
<span class="gi">+        # code block.</span>
<span class="gi">+        if not old_first_line == first_line and source_file is not None:</span>
<span class="gi">+            if os.path.exists(source_file):</span>
<span class="gi">+                _, func_name = get_func_name(self.func, resolv_alias=False)</span>
<span class="gi">+                num_lines = len(func_code.split(&#39;\n&#39;))</span>
<span class="gi">+                with tokenize.open(source_file) as f:</span>
<span class="gi">+                    on_disk_func_code = f.readlines()[</span>
<span class="gi">+                        old_first_line - 1:old_first_line - 1 + num_lines - 1]</span>
<span class="gi">+                on_disk_func_code = &#39;&#39;.join(on_disk_func_code)</span>
<span class="gi">+                possible_collision = (on_disk_func_code.rstrip() ==</span>
<span class="gi">+                                      old_func_code.rstrip())</span>
<span class="gi">+            else:</span>
<span class="gi">+                possible_collision = source_file.startswith(&#39;&lt;doctest &#39;)</span>
<span class="gi">+            if possible_collision:</span>
<span class="gi">+                warnings.warn(JobLibCollisionWarning(</span>
<span class="gi">+                    &#39;Possible name collisions between functions &#39;</span>
<span class="gi">+                    &quot;&#39;%s&#39; (%s:%i) and &#39;%s&#39; (%s:%i)&quot; %</span>
<span class="gi">+                    (func_name, source_file, old_first_line,</span>
<span class="gi">+                     func_name, source_file, first_line)),</span>
<span class="gi">+                    stacklevel=stacklevel)</span>
<span class="gi">+</span>
<span class="gi">+        # The function has changed, wipe the cache directory.</span>
<span class="gi">+        # XXX: Should be using warnings, and giving stacklevel</span>
<span class="gi">+        if self._verbose &gt; 10:</span>
<span class="gi">+            _, func_name = get_func_name(self.func, resolv_alias=False)</span>
<span class="gi">+            self.warn(&quot;Function {0} (identified by {1}) has changed&quot;</span>
<span class="gi">+                      &quot;.&quot;.format(func_name, self.func_id))</span>
<span class="gi">+        self.clear(warn=True)</span>
<span class="gi">+        return False</span>

<span class="w"> </span>    def clear(self, warn=True):
<span class="w"> </span>        &quot;&quot;&quot;Empty the function&#39;s cache.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        func_id = self.func_id</span>
<span class="gi">+        if self._verbose &gt; 0 and warn:</span>
<span class="gi">+            self.warn(&quot;Clearing function cache identified by %s&quot; % func_id)</span>
<span class="gi">+        self.store_backend.clear_path([func_id, ])</span>
<span class="gi">+</span>
<span class="gi">+        func_code, _, first_line = self.func_code_info</span>
<span class="gi">+        self._write_func_code(func_code, first_line)</span>

<span class="w"> </span>    def call(self, *args, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Force the execution of the function with the given arguments.
<span class="gu">@@ -390,10 +759,40 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>        metadata : dict
<span class="w"> </span>            The metadata associated with the call.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        call_id = (self.func_id, self._get_args_id(*args, **kwargs))</span>
<span class="gi">+</span>
<span class="gi">+        # Return the output and the metadata</span>
<span class="gi">+        return self._call(call_id, args, kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    def _call(self, call_id, args, kwargs, shelving=False):</span>
<span class="gi">+        # Return the output and the metadata</span>
<span class="gi">+        self._before_call(args, kwargs)</span>
<span class="gi">+        start_time = time.time()</span>
<span class="gi">+        output = self.func(*args, **kwargs)</span>
<span class="gi">+        return self._after_call(call_id, args, kwargs, shelving,</span>
<span class="gi">+                                output, start_time)</span>
<span class="gi">+</span>
<span class="gi">+    def _before_call(self, args, kwargs):</span>
<span class="gi">+        if self._verbose &gt; 0:</span>
<span class="gi">+            print(format_call(self.func, args, kwargs))</span>
<span class="gi">+</span>
<span class="gi">+    def _after_call(self, call_id, args, kwargs, shelving, output, start_time):</span>
<span class="gi">+        self.store_backend.dump_item(call_id, output, verbose=self._verbose)</span>
<span class="gi">+        duration = time.time() - start_time</span>
<span class="gi">+        if self._verbose &gt; 0:</span>
<span class="gi">+            self._print_duration(duration)</span>
<span class="gi">+        metadata = self._persist_input(duration, call_id, args, kwargs)</span>
<span class="gi">+        if shelving:</span>
<span class="gi">+            return self._get_memorized_result(call_id, metadata), metadata</span>
<span class="gi">+</span>
<span class="gi">+        if self.mmap_mode is not None:</span>
<span class="gi">+            # Memmap the output at the first call to be consistent with</span>
<span class="gi">+            # later calls</span>
<span class="gi">+            output = self._load_item(call_id, metadata)</span>
<span class="gi">+        return output, metadata</span>

<span class="w"> </span>    def _persist_input(self, duration, call_id, args, kwargs,
<span class="gd">-        this_duration_limit=0.5):</span>
<span class="gi">+                       this_duration_limit=0.5):</span>
<span class="w"> </span>        &quot;&quot;&quot; Save a small summary of the call using json format in the
<span class="w"> </span>            output directory.

<span class="gu">@@ -410,22 +809,92 @@ class MemorizedFunc(Logger):</span>
<span class="w"> </span>            this_duration_limit: float
<span class="w"> </span>                Max execution time for this function before issuing a warning.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        start_time = time.time()</span>
<span class="gi">+        argument_dict = filter_args(self.func, self.ignore,</span>
<span class="gi">+                                    args, kwargs)</span>
<span class="gi">+</span>
<span class="gi">+        input_repr = dict((k, repr(v)) for k, v in argument_dict.items())</span>
<span class="gi">+        # This can fail due to race-conditions with multiple</span>
<span class="gi">+        # concurrent joblibs removing the file or the directory</span>
<span class="gi">+        metadata = {</span>
<span class="gi">+            &quot;duration&quot;: duration, &quot;input_args&quot;: input_repr, &quot;time&quot;: start_time,</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+        self.store_backend.store_metadata(call_id, metadata)</span>
<span class="gi">+</span>
<span class="gi">+        this_duration = time.time() - start_time</span>
<span class="gi">+        if this_duration &gt; this_duration_limit:</span>
<span class="gi">+            # This persistence should be fast. It will not be if repr() takes</span>
<span class="gi">+            # time and its output is large, because json.dump will have to</span>
<span class="gi">+            # write a large file. This should not be an issue with numpy arrays</span>
<span class="gi">+            # for which repr() always output a short representation, but can</span>
<span class="gi">+            # be with complex dictionaries. Fixing the problem should be a</span>
<span class="gi">+            # matter of replacing repr() above by something smarter.</span>
<span class="gi">+            warnings.warn(&quot;Persisting input arguments took %.2fs to run.&quot;</span>
<span class="gi">+                          &quot;If this happens often in your code, it can cause &quot;</span>
<span class="gi">+                          &quot;performance problems &quot;</span>
<span class="gi">+                          &quot;(results will be correct in all cases). &quot;</span>
<span class="gi">+                          &quot;The reason for this is probably some large input &quot;</span>
<span class="gi">+                          &quot;arguments for a wrapped function.&quot;</span>
<span class="gi">+                          % this_duration, stacklevel=5)</span>
<span class="gi">+        return metadata</span>
<span class="gi">+</span>
<span class="gi">+    def _get_memorized_result(self, call_id, metadata=None):</span>
<span class="gi">+        return MemorizedResult(self.store_backend, call_id,</span>
<span class="gi">+                               metadata=metadata, timestamp=self.timestamp,</span>
<span class="gi">+                               verbose=self._verbose - 1)</span>
<span class="gi">+</span>
<span class="gi">+    def _load_item(self, call_id, metadata=None):</span>
<span class="gi">+        return self.store_backend.load_item(call_id, metadata=metadata,</span>
<span class="gi">+                                            timestamp=self.timestamp,</span>
<span class="gi">+                                            verbose=self._verbose)</span>
<span class="gi">+</span>
<span class="gi">+    def _print_duration(self, duration, context=&#39;&#39;):</span>
<span class="gi">+        _, name = get_func_name(self.func)</span>
<span class="gi">+        msg = f&quot;{name} {context}- {format_time(duration)}&quot;</span>
<span class="gi">+        print(max(0, (80 - len(msg))) * &#39;_&#39; + msg)</span>
<span class="gi">+</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>
<span class="gi">+    # Private `object` interface</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>

<span class="w"> </span>    def __repr__(self):
<span class="w"> </span>        return &#39;{class_name}(func={func}, location={location})&#39;.format(
<span class="gd">-            class_name=self.__class__.__name__, func=self.func, location=</span>
<span class="gd">-            self.store_backend.location)</span>
<span class="gi">+            class_name=self.__class__.__name__,</span>
<span class="gi">+            func=self.func,</span>
<span class="gi">+            location=self.store_backend.location,)</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# class `AsyncMemorizedFunc`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class AsyncMemorizedFunc(MemorizedFunc):
<span class="gd">-</span>
<span class="w"> </span>    async def __call__(self, *args, **kwargs):
<span class="w"> </span>        out = self._cached_call(args, kwargs, shelving=False)
<span class="w"> </span>        out = await out if asyncio.iscoroutine(out) else out
<span class="gd">-        return out[0]</span>
<span class="gi">+        return out[0]  # Don&#39;t return metadata</span>

<span class="gi">+    async def call_and_shelve(self, *args, **kwargs):</span>
<span class="gi">+        out = self._cached_call(args, kwargs, shelving=True)</span>
<span class="gi">+        out = await out if asyncio.iscoroutine(out) else out</span>
<span class="gi">+        return out[0]  # Don&#39;t return metadata</span>

<span class="gi">+    async def call(self, *args, **kwargs):</span>
<span class="gi">+        out = super().call(*args, **kwargs)</span>
<span class="gi">+        return await out if asyncio.iscoroutine(out) else out</span>
<span class="gi">+</span>
<span class="gi">+    async def _call(self, call_id, args, kwargs, shelving=False):</span>
<span class="gi">+        self._before_call(args, kwargs)</span>
<span class="gi">+        start_time = time.time()</span>
<span class="gi">+        output = await self.func(*args, **kwargs)</span>
<span class="gi">+        return self._after_call(</span>
<span class="gi">+            call_id, args, kwargs, shelving, output, start_time</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# class `Memory`</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class Memory(Logger):
<span class="w"> </span>    &quot;&quot;&quot; A context object for caching a function&#39;s return value each time it
<span class="w"> </span>        is called with the same input arguments.
<span class="gu">@@ -482,35 +951,46 @@ class Memory(Logger):</span>
<span class="w"> </span>            Contains a dictionary of named parameters used to configure
<span class="w"> </span>            the store backend.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+    # ------------------------------------------------------------------------</span>
<span class="gi">+    # Public interface</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>

<span class="gd">-    def __init__(self, location=None, backend=&#39;local&#39;, mmap_mode=None,</span>
<span class="gd">-        compress=False, verbose=1, bytes_limit=None, backend_options=None):</span>
<span class="gi">+    def __init__(self, location=None, backend=&#39;local&#39;,</span>
<span class="gi">+                 mmap_mode=None, compress=False, verbose=1, bytes_limit=None,</span>
<span class="gi">+                 backend_options=None):</span>
<span class="w"> </span>        Logger.__init__(self)
<span class="w"> </span>        self._verbose = verbose
<span class="w"> </span>        self.mmap_mode = mmap_mode
<span class="w"> </span>        self.timestamp = time.time()
<span class="w"> </span>        if bytes_limit is not None:
<span class="w"> </span>            warnings.warn(
<span class="gd">-                &#39;bytes_limit argument has been deprecated. It will be removed in version 1.5. Please pass its value directly to Memory.reduce_size.&#39;</span>
<span class="gd">-                , category=DeprecationWarning)</span>
<span class="gi">+                &quot;bytes_limit argument has been deprecated. It will be removed &quot;</span>
<span class="gi">+                &quot;in version 1.5. Please pass its value directly to &quot;</span>
<span class="gi">+                &quot;Memory.reduce_size.&quot;,</span>
<span class="gi">+                category=DeprecationWarning</span>
<span class="gi">+            )</span>
<span class="w"> </span>        self.bytes_limit = bytes_limit
<span class="w"> </span>        self.backend = backend
<span class="w"> </span>        self.compress = compress
<span class="w"> </span>        if backend_options is None:
<span class="w"> </span>            backend_options = {}
<span class="w"> </span>        self.backend_options = backend_options
<span class="gi">+</span>
<span class="w"> </span>        if compress and mmap_mode is not None:
<span class="w"> </span>            warnings.warn(&#39;Compressed results cannot be memmapped&#39;,
<span class="gd">-                stacklevel=2)</span>
<span class="gi">+                          stacklevel=2)</span>
<span class="gi">+</span>
<span class="w"> </span>        self.location = location
<span class="w"> </span>        if isinstance(location, str):
<span class="w"> </span>            location = os.path.join(location, &#39;joblib&#39;)
<span class="gd">-        self.store_backend = _store_backend_factory(backend, location,</span>
<span class="gd">-            verbose=self._verbose, backend_options=dict(compress=compress,</span>
<span class="gd">-            mmap_mode=mmap_mode, **backend_options))</span>
<span class="gi">+</span>
<span class="gi">+        self.store_backend = _store_backend_factory(</span>
<span class="gi">+            backend, location, verbose=self._verbose,</span>
<span class="gi">+            backend_options=dict(compress=compress, mmap_mode=mmap_mode,</span>
<span class="gi">+                                 **backend_options))</span>

<span class="w"> </span>    def cache(self, func=None, ignore=None, verbose=None, mmap_mode=False,
<span class="gd">-        cache_validation_callback=None):</span>
<span class="gi">+              cache_validation_callback=None):</span>
<span class="w"> </span>        &quot;&quot;&quot; Decorates the given function func to only compute its return
<span class="w"> </span>            value for input arguments not cached on disk.

<span class="gu">@@ -543,12 +1023,55 @@ class Memory(Logger):</span>
<span class="w"> </span>                methods for cache lookup and management. See the
<span class="w"> </span>                documentation for :class:`joblib.memory.MemorizedFunc`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if (cache_validation_callback is not None and</span>
<span class="gi">+                not callable(cache_validation_callback)):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;cache_validation_callback needs to be callable. &quot;</span>
<span class="gi">+                f&quot;Got {cache_validation_callback}.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        if func is None:</span>
<span class="gi">+            # Partial application, to be able to specify extra keyword</span>
<span class="gi">+            # arguments in decorators</span>
<span class="gi">+            return functools.partial(</span>
<span class="gi">+                self.cache, ignore=ignore,</span>
<span class="gi">+                mmap_mode=mmap_mode,</span>
<span class="gi">+                verbose=verbose,</span>
<span class="gi">+                cache_validation_callback=cache_validation_callback</span>
<span class="gi">+            )</span>
<span class="gi">+        if self.store_backend is None:</span>
<span class="gi">+            cls = (AsyncNotMemorizedFunc</span>
<span class="gi">+                   if asyncio.iscoroutinefunction(func)</span>
<span class="gi">+                   else NotMemorizedFunc)</span>
<span class="gi">+            return cls(func)</span>
<span class="gi">+        if verbose is None:</span>
<span class="gi">+            verbose = self._verbose</span>
<span class="gi">+        if mmap_mode is False:</span>
<span class="gi">+            mmap_mode = self.mmap_mode</span>
<span class="gi">+        if isinstance(func, MemorizedFunc):</span>
<span class="gi">+            func = func.func</span>
<span class="gi">+        cls = (AsyncMemorizedFunc</span>
<span class="gi">+               if asyncio.iscoroutinefunction(func)</span>
<span class="gi">+               else MemorizedFunc)</span>
<span class="gi">+        return cls(</span>
<span class="gi">+            func, location=self.store_backend, backend=self.backend,</span>
<span class="gi">+            ignore=ignore, mmap_mode=mmap_mode, compress=self.compress,</span>
<span class="gi">+            verbose=verbose, timestamp=self.timestamp,</span>
<span class="gi">+            cache_validation_callback=cache_validation_callback</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def clear(self, warn=True):
<span class="w"> </span>        &quot;&quot;&quot; Erase the complete cache directory.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if warn:</span>
<span class="gi">+            self.warn(&#39;Flushing completely the cache&#39;)</span>
<span class="gi">+        if self.store_backend is not None:</span>
<span class="gi">+            self.store_backend.clear()</span>
<span class="gi">+</span>
<span class="gi">+            # As the cache is completely clear, make sure the _FUNCTION_HASHES</span>
<span class="gi">+            # cache is also reset. Else, for a function that is present in this</span>
<span class="gi">+            # table, results cached after this clear will be have cache miss</span>
<span class="gi">+            # as the function code is not re-written.</span>
<span class="gi">+            _FUNCTION_HASHES.clear()</span>

<span class="w"> </span>    def reduce_size(self, bytes_limit=None, items_limit=None, age_limit=None):
<span class="w"> </span>        &quot;&quot;&quot;Remove cache elements to make the cache fit its limits.
<span class="gu">@@ -576,7 +1099,21 @@ class Memory(Logger):</span>
<span class="w"> </span>            of the cache, any items last accessed more than the given length of
<span class="w"> </span>            time ago are deleted.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if bytes_limit is None:</span>
<span class="gi">+            bytes_limit = self.bytes_limit</span>
<span class="gi">+</span>
<span class="gi">+        if self.store_backend is None:</span>
<span class="gi">+            # No cached results, this function does nothing.</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        if bytes_limit is None and items_limit is None and age_limit is None:</span>
<span class="gi">+            # No limitation to impose, returning</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        # Defers the actual limits enforcing to the store backend.</span>
<span class="gi">+        self.store_backend.enforce_store_limits(</span>
<span class="gi">+            bytes_limit, items_limit, age_limit</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def eval(self, func, *args, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot; Eval function func with arguments `*args` and `**kwargs`,
<span class="gu">@@ -587,12 +1124,19 @@ class Memory(Logger):</span>
<span class="w"> </span>            up to date.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.store_backend is None:</span>
<span class="gi">+            return func(*args, **kwargs)</span>
<span class="gi">+        return self.cache(func)(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>
<span class="gi">+    # Private `object` interface</span>
<span class="gi">+    # ------------------------------------------------------------------------</span>

<span class="w"> </span>    def __repr__(self):
<span class="gd">-        return &#39;{class_name}(location={location})&#39;.format(class_name=self.</span>
<span class="gd">-            __class__.__name__, location=None if self.store_backend is None</span>
<span class="gd">-             else self.store_backend.location)</span>
<span class="gi">+        return &#39;{class_name}(location={location})&#39;.format(</span>
<span class="gi">+            class_name=self.__class__.__name__,</span>
<span class="gi">+            location=(None if self.store_backend is None</span>
<span class="gi">+                      else self.store_backend.location))</span>

<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        &quot;&quot;&quot; We don&#39;t store the timestamp when pickling, to avoid the hash
<span class="gu">@@ -603,8 +1147,12 @@ class Memory(Logger):</span>
<span class="w"> </span>        return state


<span class="gd">-def expires_after(days=0, seconds=0, microseconds=0, milliseconds=0,</span>
<span class="gd">-    minutes=0, hours=0, weeks=0):</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# cache_validation_callback helpers</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+</span>
<span class="gi">+def expires_after(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0,</span>
<span class="gi">+                  hours=0, weeks=0):</span>
<span class="w"> </span>    &quot;&quot;&quot;Helper cache_validation_callback to force recompute after a duration.

<span class="w"> </span>    Parameters
<span class="gu">@@ -612,4 +1160,13 @@ def expires_after(days=0, seconds=0, microseconds=0, milliseconds=0,</span>
<span class="w"> </span>    days, seconds, microseconds, milliseconds, minutes, hours, weeks: numbers
<span class="w"> </span>        argument passed to a timedelta.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    delta = datetime.timedelta(</span>
<span class="gi">+        days=days, seconds=seconds, microseconds=microseconds,</span>
<span class="gi">+        milliseconds=milliseconds, minutes=minutes, hours=hours, weeks=weeks</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    def cache_validation_callback(metadata):</span>
<span class="gi">+        computation_age = time.time() - metadata[&#39;time&#39;]</span>
<span class="gi">+        return computation_age &lt; delta.total_seconds()</span>
<span class="gi">+</span>
<span class="gi">+    return cache_validation_callback</span>
<span class="gh">diff --git a/joblib/numpy_pickle.py b/joblib/numpy_pickle.py</span>
<span class="gh">index 6cc4089..bf83bb0 100644</span>
<span class="gd">--- a/joblib/numpy_pickle.py</span>
<span class="gi">+++ b/joblib/numpy_pickle.py</span>
<span class="gu">@@ -1,26 +1,48 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utilities for fast persistence of big data, with optional compression.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2009 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="w"> </span>import pickle
<span class="w"> </span>import os
<span class="w"> </span>import warnings
<span class="w"> </span>import io
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>from .compressor import lz4, LZ4_NOT_INSTALLED_ERROR
<span class="w"> </span>from .compressor import _COMPRESSORS, register_compressor, BinaryZlibFile
<span class="gd">-from .compressor import ZlibCompressorWrapper, GzipCompressorWrapper, BZ2CompressorWrapper, LZMACompressorWrapper, XZCompressorWrapper, LZ4CompressorWrapper</span>
<span class="gi">+from .compressor import (ZlibCompressorWrapper, GzipCompressorWrapper,</span>
<span class="gi">+                         BZ2CompressorWrapper, LZMACompressorWrapper,</span>
<span class="gi">+                         XZCompressorWrapper, LZ4CompressorWrapper)</span>
<span class="w"> </span>from .numpy_pickle_utils import Unpickler, Pickler
<span class="w"> </span>from .numpy_pickle_utils import _read_fileobject, _write_fileobject
<span class="w"> </span>from .numpy_pickle_utils import _read_bytes, BUFFER_SIZE
<span class="w"> </span>from .numpy_pickle_utils import _ensure_native_byte_order
<span class="w"> </span>from .numpy_pickle_compat import load_compatibility
<span class="w"> </span>from .numpy_pickle_compat import NDArrayWrapper
<span class="gd">-from .numpy_pickle_compat import ZNDArrayWrapper</span>
<span class="gi">+# For compatibility with old versions of joblib, we need ZNDArrayWrapper</span>
<span class="gi">+# to be visible in the current namespace.</span>
<span class="gi">+# Explicitly skipping next line from flake8 as it triggers an F401 warning</span>
<span class="gi">+# which we don&#39;t care.</span>
<span class="gi">+from .numpy_pickle_compat import ZNDArrayWrapper  # noqa</span>
<span class="w"> </span>from .backports import make_memmap
<span class="gi">+</span>
<span class="gi">+# Register supported compressors</span>
<span class="w"> </span>register_compressor(&#39;zlib&#39;, ZlibCompressorWrapper())
<span class="w"> </span>register_compressor(&#39;gzip&#39;, GzipCompressorWrapper())
<span class="w"> </span>register_compressor(&#39;bz2&#39;, BZ2CompressorWrapper())
<span class="w"> </span>register_compressor(&#39;lzma&#39;, LZMACompressorWrapper())
<span class="w"> </span>register_compressor(&#39;xz&#39;, XZCompressorWrapper())
<span class="w"> </span>register_compressor(&#39;lz4&#39;, LZ4CompressorWrapper())
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Utility objects for persistence.</span>
<span class="gi">+</span>
<span class="gi">+# For convenience, 16 bytes are used to be sure to cover all the possible</span>
<span class="gi">+# dtypes&#39; alignments. For reference, see:</span>
<span class="gi">+# https://numpy.org/devdocs/dev/alignment.html</span>
<span class="w"> </span>NUMPY_ARRAY_ALIGNMENT_BYTES = 16


<span class="gu">@@ -55,22 +77,61 @@ class NumpyArrayWrapper(object):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, subclass, shape, order, dtype, allow_mmap=False,
<span class="gd">-        numpy_array_alignment_bytes=NUMPY_ARRAY_ALIGNMENT_BYTES):</span>
<span class="gi">+                 numpy_array_alignment_bytes=NUMPY_ARRAY_ALIGNMENT_BYTES):</span>
<span class="w"> </span>        &quot;&quot;&quot;Constructor. Store the useful information for later.&quot;&quot;&quot;
<span class="w"> </span>        self.subclass = subclass
<span class="w"> </span>        self.shape = shape
<span class="w"> </span>        self.order = order
<span class="w"> </span>        self.dtype = dtype
<span class="w"> </span>        self.allow_mmap = allow_mmap
<span class="gi">+        # We make numpy_array_alignment_bytes an instance attribute to allow us</span>
<span class="gi">+        # to change our mind about the default alignment and still load the old</span>
<span class="gi">+        # pickles (with the previous alignment) correctly</span>
<span class="w"> </span>        self.numpy_array_alignment_bytes = numpy_array_alignment_bytes

<span class="gi">+    def safe_get_numpy_array_alignment_bytes(self):</span>
<span class="gi">+        # NumpyArrayWrapper instances loaded from joblib &lt;= 1.1 pickles don&#39;t</span>
<span class="gi">+        # have an numpy_array_alignment_bytes attribute</span>
<span class="gi">+        return getattr(self, &#39;numpy_array_alignment_bytes&#39;, None)</span>
<span class="gi">+</span>
<span class="w"> </span>    def write_array(self, array, pickler):
<span class="w"> </span>        &quot;&quot;&quot;Write array bytes to pickler file handle.

<span class="w"> </span>        This function is an adaptation of the numpy write_array function
<span class="w"> </span>        available in version 1.10.1 in numpy/lib/format.py.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Set buffer size to 16 MiB to hide the Python loop overhead.</span>
<span class="gi">+        buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)</span>
<span class="gi">+        if array.dtype.hasobject:</span>
<span class="gi">+            # We contain Python objects so we cannot write out the data</span>
<span class="gi">+            # directly. Instead, we will pickle it out with version 2 of the</span>
<span class="gi">+            # pickle protocol.</span>
<span class="gi">+            pickle.dump(array, pickler.file_handle, protocol=2)</span>
<span class="gi">+        else:</span>
<span class="gi">+            numpy_array_alignment_bytes = \</span>
<span class="gi">+                self.safe_get_numpy_array_alignment_bytes()</span>
<span class="gi">+            if numpy_array_alignment_bytes is not None:</span>
<span class="gi">+                current_pos = pickler.file_handle.tell()</span>
<span class="gi">+                pos_after_padding_byte = current_pos + 1</span>
<span class="gi">+                padding_length = numpy_array_alignment_bytes - (</span>
<span class="gi">+                    pos_after_padding_byte % numpy_array_alignment_bytes)</span>
<span class="gi">+                # A single byte is written that contains the padding length in</span>
<span class="gi">+                # bytes</span>
<span class="gi">+                padding_length_byte = int.to_bytes(</span>
<span class="gi">+                    padding_length, length=1, byteorder=&#39;little&#39;)</span>
<span class="gi">+                pickler.file_handle.write(padding_length_byte)</span>
<span class="gi">+</span>
<span class="gi">+                if padding_length != 0:</span>
<span class="gi">+                    padding = b&#39;\xff&#39; * padding_length</span>
<span class="gi">+                    pickler.file_handle.write(padding)</span>
<span class="gi">+</span>
<span class="gi">+            for chunk in pickler.np.nditer(array,</span>
<span class="gi">+                                           flags=[&#39;external_loop&#39;,</span>
<span class="gi">+                                                  &#39;buffered&#39;,</span>
<span class="gi">+                                                  &#39;zerosize_ok&#39;],</span>
<span class="gi">+                                           buffersize=buffersize,</span>
<span class="gi">+                                           order=self.order):</span>
<span class="gi">+                pickler.file_handle.write(chunk.tobytes(&#39;C&#39;))</span>

<span class="w"> </span>    def read_array(self, unpickler):
<span class="w"> </span>        &quot;&quot;&quot;Read array from unpickler file handle.
<span class="gu">@@ -78,11 +139,97 @@ class NumpyArrayWrapper(object):</span>
<span class="w"> </span>        This function is an adaptation of the numpy read_array function
<span class="w"> </span>        available in version 1.10.1 in numpy/lib/format.py.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if len(self.shape) == 0:</span>
<span class="gi">+            count = 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            # joblib issue #859: we cast the elements of self.shape to int64 to</span>
<span class="gi">+            # prevent a potential overflow when computing their product.</span>
<span class="gi">+            shape_int64 = [unpickler.np.int64(x) for x in self.shape]</span>
<span class="gi">+            count = unpickler.np.multiply.reduce(shape_int64)</span>
<span class="gi">+        # Now read the actual data.</span>
<span class="gi">+        if self.dtype.hasobject:</span>
<span class="gi">+            # The array contained Python objects. We need to unpickle the data.</span>
<span class="gi">+            array = pickle.load(unpickler.file_handle)</span>
<span class="gi">+        else:</span>
<span class="gi">+            numpy_array_alignment_bytes = \</span>
<span class="gi">+                self.safe_get_numpy_array_alignment_bytes()</span>
<span class="gi">+            if numpy_array_alignment_bytes is not None:</span>
<span class="gi">+                padding_byte = unpickler.file_handle.read(1)</span>
<span class="gi">+                padding_length = int.from_bytes(</span>
<span class="gi">+                    padding_byte, byteorder=&#39;little&#39;)</span>
<span class="gi">+                if padding_length != 0:</span>
<span class="gi">+                    unpickler.file_handle.read(padding_length)</span>
<span class="gi">+</span>
<span class="gi">+            # This is not a real file. We have to read it the</span>
<span class="gi">+            # memory-intensive way.</span>
<span class="gi">+            # crc32 module fails on reads greater than 2 ** 32 bytes,</span>
<span class="gi">+            # breaking large reads from gzip streams. Chunk reads to</span>
<span class="gi">+            # BUFFER_SIZE bytes to avoid issue and reduce memory overhead</span>
<span class="gi">+            # of the read. In non-chunked case count &lt; max_read_count, so</span>
<span class="gi">+            # only one read is performed.</span>
<span class="gi">+            max_read_count = BUFFER_SIZE // min(BUFFER_SIZE,</span>
<span class="gi">+                                                self.dtype.itemsize)</span>
<span class="gi">+</span>
<span class="gi">+            array = unpickler.np.empty(count, dtype=self.dtype)</span>
<span class="gi">+            for i in range(0, count, max_read_count):</span>
<span class="gi">+                read_count = min(max_read_count, count - i)</span>
<span class="gi">+                read_size = int(read_count * self.dtype.itemsize)</span>
<span class="gi">+                data = _read_bytes(unpickler.file_handle,</span>
<span class="gi">+                                   read_size, &quot;array data&quot;)</span>
<span class="gi">+                array[i:i + read_count] = \</span>
<span class="gi">+                    unpickler.np.frombuffer(data, dtype=self.dtype,</span>
<span class="gi">+                                            count=read_count)</span>
<span class="gi">+                del data</span>
<span class="gi">+</span>
<span class="gi">+            if self.order == &#39;F&#39;:</span>
<span class="gi">+                array.shape = self.shape[::-1]</span>
<span class="gi">+                array = array.transpose()</span>
<span class="gi">+            else:</span>
<span class="gi">+                array.shape = self.shape</span>
<span class="gi">+</span>
<span class="gi">+        # Detect byte order mismatch and swap as needed.</span>
<span class="gi">+        return _ensure_native_byte_order(array)</span>

<span class="w"> </span>    def read_mmap(self, unpickler):
<span class="w"> </span>        &quot;&quot;&quot;Read an array using numpy memmap.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        current_pos = unpickler.file_handle.tell()</span>
<span class="gi">+        offset = current_pos</span>
<span class="gi">+        numpy_array_alignment_bytes = \</span>
<span class="gi">+            self.safe_get_numpy_array_alignment_bytes()</span>
<span class="gi">+</span>
<span class="gi">+        if numpy_array_alignment_bytes is not None:</span>
<span class="gi">+            padding_byte = unpickler.file_handle.read(1)</span>
<span class="gi">+            padding_length = int.from_bytes(padding_byte, byteorder=&#39;little&#39;)</span>
<span class="gi">+            # + 1 is for the padding byte</span>
<span class="gi">+            offset += padding_length + 1</span>
<span class="gi">+</span>
<span class="gi">+        if unpickler.mmap_mode == &#39;w+&#39;:</span>
<span class="gi">+            unpickler.mmap_mode = &#39;r+&#39;</span>
<span class="gi">+</span>
<span class="gi">+        marray = make_memmap(unpickler.filename,</span>
<span class="gi">+                             dtype=self.dtype,</span>
<span class="gi">+                             shape=self.shape,</span>
<span class="gi">+                             order=self.order,</span>
<span class="gi">+                             mode=unpickler.mmap_mode,</span>
<span class="gi">+                             offset=offset)</span>
<span class="gi">+        # update the offset so that it corresponds to the end of the read array</span>
<span class="gi">+        unpickler.file_handle.seek(offset + marray.nbytes)</span>
<span class="gi">+</span>
<span class="gi">+        if (numpy_array_alignment_bytes is None and</span>
<span class="gi">+                current_pos % NUMPY_ARRAY_ALIGNMENT_BYTES != 0):</span>
<span class="gi">+            message = (</span>
<span class="gi">+                f&#39;The memmapped array {marray} loaded from the file &#39;</span>
<span class="gi">+                f&#39;{unpickler.file_handle.name} is not byte aligned. &#39;</span>
<span class="gi">+                &#39;This may cause segmentation faults if this memmapped array &#39;</span>
<span class="gi">+                &#39;is used in some libraries like BLAS or PyTorch. &#39;</span>
<span class="gi">+                &#39;To get rid of this warning, regenerate your pickle file &#39;</span>
<span class="gi">+                &#39;with joblib &gt;= 1.2.0. &#39;</span>
<span class="gi">+                &#39;See https://github.com/joblib/joblib/issues/563 &#39;</span>
<span class="gi">+                &#39;for more details&#39;</span>
<span class="gi">+            )</span>
<span class="gi">+            warnings.warn(message)</span>
<span class="gi">+</span>
<span class="gi">+        return _ensure_native_byte_order(marray)</span>

<span class="w"> </span>    def read(self, unpickler):
<span class="w"> </span>        &quot;&quot;&quot;Read the array corresponding to this wrapper.
<span class="gu">@@ -98,7 +245,25 @@ class NumpyArrayWrapper(object):</span>
<span class="w"> </span>        array: numpy.ndarray

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # When requested, only use memmap mode if allowed.</span>
<span class="gi">+        if unpickler.mmap_mode is not None and self.allow_mmap:</span>
<span class="gi">+            array = self.read_mmap(unpickler)</span>
<span class="gi">+        else:</span>
<span class="gi">+            array = self.read_array(unpickler)</span>
<span class="gi">+</span>
<span class="gi">+        # Manage array subclass case</span>
<span class="gi">+        if (hasattr(array, &#39;__array_prepare__&#39;) and</span>
<span class="gi">+            self.subclass not in (unpickler.np.ndarray,</span>
<span class="gi">+                                  unpickler.np.memmap)):</span>
<span class="gi">+            # We need to reconstruct another subclass</span>
<span class="gi">+            new_array = unpickler.np.core.multiarray._reconstruct(</span>
<span class="gi">+                self.subclass, (0,), &#39;b&#39;)</span>
<span class="gi">+            return new_array.__array_prepare__(array)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return array</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Pickler classes</span>


<span class="w"> </span>class NumpyPickler(Pickler):
<span class="gu">@@ -115,14 +280,20 @@ class NumpyPickler(Pickler):</span>
<span class="w"> </span>    protocol: int, optional
<span class="w"> </span>        Pickle protocol used. Default is pickle.DEFAULT_PROTOCOL.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    dispatch = Pickler.dispatch.copy()

<span class="w"> </span>    def __init__(self, fp, protocol=None):
<span class="w"> </span>        self.file_handle = fp
<span class="w"> </span>        self.buffered = isinstance(self.file_handle, BinaryZlibFile)
<span class="gi">+</span>
<span class="gi">+        # By default we want a pickle protocol that only changes with</span>
<span class="gi">+        # the major python version and not the minor one</span>
<span class="w"> </span>        if protocol is None:
<span class="w"> </span>            protocol = pickle.DEFAULT_PROTOCOL
<span class="gi">+</span>
<span class="w"> </span>        Pickler.__init__(self, self.file_handle, protocol=protocol)
<span class="gi">+        # delayed import of numpy, to avoid tight coupling</span>
<span class="w"> </span>        try:
<span class="w"> </span>            import numpy as np
<span class="w"> </span>        except ImportError:
<span class="gu">@@ -131,7 +302,22 @@ class NumpyPickler(Pickler):</span>

<span class="w"> </span>    def _create_array_wrapper(self, array):
<span class="w"> </span>        &quot;&quot;&quot;Create and returns a numpy array wrapper from a numpy array.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        order = &#39;F&#39; if (array.flags.f_contiguous and</span>
<span class="gi">+                        not array.flags.c_contiguous) else &#39;C&#39;</span>
<span class="gi">+        allow_mmap = not self.buffered and not array.dtype.hasobject</span>
<span class="gi">+</span>
<span class="gi">+        kwargs = {}</span>
<span class="gi">+        try:</span>
<span class="gi">+            self.file_handle.tell()</span>
<span class="gi">+        except io.UnsupportedOperation:</span>
<span class="gi">+            kwargs = {&#39;numpy_array_alignment_bytes&#39;: None}</span>
<span class="gi">+</span>
<span class="gi">+        wrapper = NumpyArrayWrapper(type(array),</span>
<span class="gi">+                                    array.shape, order, array.dtype,</span>
<span class="gi">+                                    allow_mmap=allow_mmap,</span>
<span class="gi">+                                    **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+        return wrapper</span>

<span class="w"> </span>    def save(self, obj):
<span class="w"> </span>        &quot;&quot;&quot;Subclass the Pickler `save` method.
<span class="gu">@@ -143,7 +329,30 @@ class NumpyPickler(Pickler):</span>
<span class="w"> </span>        after in the file. Warning: the file produced does not follow the
<span class="w"> </span>        pickle format. As such it can not be read with `pickle.load`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.np is not None and type(obj) in (self.np.ndarray,</span>
<span class="gi">+                                                 self.np.matrix,</span>
<span class="gi">+                                                 self.np.memmap):</span>
<span class="gi">+            if type(obj) is self.np.memmap:</span>
<span class="gi">+                # Pickling doesn&#39;t work with memmapped arrays</span>
<span class="gi">+                obj = self.np.asanyarray(obj)</span>
<span class="gi">+</span>
<span class="gi">+            # The array wrapper is pickled instead of the real array.</span>
<span class="gi">+            wrapper = self._create_array_wrapper(obj)</span>
<span class="gi">+            Pickler.save(self, wrapper)</span>
<span class="gi">+</span>
<span class="gi">+            # A framer was introduced with pickle protocol 4 and we want to</span>
<span class="gi">+            # ensure the wrapper object is written before the numpy array</span>
<span class="gi">+            # buffer in the pickle file.</span>
<span class="gi">+            # See https://www.python.org/dev/peps/pep-3154/#framing to get</span>
<span class="gi">+            # more information on the framer behavior.</span>
<span class="gi">+            if self.proto &gt;= 4:</span>
<span class="gi">+                self.framer.commit_frame(force=True)</span>
<span class="gi">+</span>
<span class="gi">+            # And then array bytes are written right after the wrapper.</span>
<span class="gi">+            wrapper.write_array(obj, self)</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        return Pickler.save(self, obj)</span>


<span class="w"> </span>class NumpyUnpickler(Unpickler):
<span class="gu">@@ -162,12 +371,17 @@ class NumpyUnpickler(Unpickler):</span>
<span class="w"> </span>        Reference to numpy module if numpy is installed else None.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    dispatch = Unpickler.dispatch.copy()

<span class="w"> </span>    def __init__(self, filename, file_handle, mmap_mode=None):
<span class="gi">+        # The next line is for backward compatibility with pickle generated</span>
<span class="gi">+        # with joblib versions less than 0.10.</span>
<span class="w"> </span>        self._dirname = os.path.dirname(filename)
<span class="gi">+</span>
<span class="w"> </span>        self.mmap_mode = mmap_mode
<span class="w"> </span>        self.file_handle = file_handle
<span class="gi">+        # filename is required for numpy mmap mode.</span>
<span class="w"> </span>        self.filename = filename
<span class="w"> </span>        self.compat_mode = False
<span class="w"> </span>        Unpickler.__init__(self, self.file_handle)
<span class="gu">@@ -185,10 +399,28 @@ class NumpyUnpickler(Unpickler):</span>
<span class="w"> </span>        replace them directly in the stack of pickler.
<span class="w"> </span>        NDArrayWrapper is used for backward compatibility with joblib &lt;= 0.9.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        Unpickler.load_build(self)</span>
<span class="gi">+</span>
<span class="gi">+        # For backward compatibility, we support NDArrayWrapper objects.</span>
<span class="gi">+        if isinstance(self.stack[-1], (NDArrayWrapper, NumpyArrayWrapper)):</span>
<span class="gi">+            if self.np is None:</span>
<span class="gi">+                raise ImportError(&quot;Trying to unpickle an ndarray, &quot;</span>
<span class="gi">+                                  &quot;but numpy didn&#39;t import correctly&quot;)</span>
<span class="gi">+            array_wrapper = self.stack.pop()</span>
<span class="gi">+            # If any NDArrayWrapper is found, we switch to compatibility mode,</span>
<span class="gi">+            # this will be used to raise a DeprecationWarning to the user at</span>
<span class="gi">+            # the end of the unpickling.</span>
<span class="gi">+            if isinstance(array_wrapper, NDArrayWrapper):</span>
<span class="gi">+                self.compat_mode = True</span>
<span class="gi">+            self.stack.append(array_wrapper.read(self))</span>
<span class="gi">+</span>
<span class="gi">+    # Be careful to register our new method.</span>
<span class="w"> </span>    dispatch[pickle.BUILD[0]] = load_build


<span class="gi">+###############################################################################</span>
<span class="gi">+# Utility functions</span>
<span class="gi">+</span>
<span class="w"> </span>def dump(value, filename, compress=0, protocol=None, cache_size=None):
<span class="w"> </span>    &quot;&quot;&quot;Persist an arbitrary Python object into one file.

<span class="gu">@@ -236,12 +468,137 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):</span>
<span class="w"> </span>    dump and load.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-def _unpickle(fobj, filename=&#39;&#39;, mmap_mode=None):</span>
<span class="gi">+    if Path is not None and isinstance(filename, Path):</span>
<span class="gi">+        filename = str(filename)</span>
<span class="gi">+</span>
<span class="gi">+    is_filename = isinstance(filename, str)</span>
<span class="gi">+    is_fileobj = hasattr(filename, &quot;write&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    compress_method = &#39;zlib&#39;  # zlib is the default compression method.</span>
<span class="gi">+    if compress is True:</span>
<span class="gi">+        # By default, if compress is enabled, we want the default compress</span>
<span class="gi">+        # level of the compressor.</span>
<span class="gi">+        compress_level = None</span>
<span class="gi">+    elif isinstance(compress, tuple):</span>
<span class="gi">+        # a 2-tuple was set in compress</span>
<span class="gi">+        if len(compress) != 2:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &#39;Compress argument tuple should contain exactly 2 elements: &#39;</span>
<span class="gi">+                &#39;(compress method, compress level), you passed {}&#39;</span>
<span class="gi">+                .format(compress))</span>
<span class="gi">+        compress_method, compress_level = compress</span>
<span class="gi">+    elif isinstance(compress, str):</span>
<span class="gi">+        compress_method = compress</span>
<span class="gi">+        compress_level = None  # Use default compress level</span>
<span class="gi">+        compress = (compress_method, compress_level)</span>
<span class="gi">+    else:</span>
<span class="gi">+        compress_level = compress</span>
<span class="gi">+</span>
<span class="gi">+    if compress_method == &#39;lz4&#39; and lz4 is None:</span>
<span class="gi">+        raise ValueError(LZ4_NOT_INSTALLED_ERROR)</span>
<span class="gi">+</span>
<span class="gi">+    if (compress_level is not None and</span>
<span class="gi">+            compress_level is not False and</span>
<span class="gi">+            compress_level not in range(10)):</span>
<span class="gi">+        # Raising an error if a non valid compress level is given.</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &#39;Non valid compress level given: &quot;{}&quot;. Possible values are &#39;</span>
<span class="gi">+            &#39;{}.&#39;.format(compress_level, list(range(10))))</span>
<span class="gi">+</span>
<span class="gi">+    if compress_method not in _COMPRESSORS:</span>
<span class="gi">+        # Raising an error if an unsupported compression method is given.</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &#39;Non valid compression method given: &quot;{}&quot;. Possible values are &#39;</span>
<span class="gi">+            &#39;{}.&#39;.format(compress_method, _COMPRESSORS))</span>
<span class="gi">+</span>
<span class="gi">+    if not is_filename and not is_fileobj:</span>
<span class="gi">+        # People keep inverting arguments, and the resulting error is</span>
<span class="gi">+        # incomprehensible</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &#39;Second argument should be a filename or a file-like object, &#39;</span>
<span class="gi">+            &#39;%s (type %s) was given.&#39;</span>
<span class="gi">+            % (filename, type(filename))</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if is_filename and not isinstance(compress, tuple):</span>
<span class="gi">+        # In case no explicit compression was requested using both compression</span>
<span class="gi">+        # method and level in a tuple and the filename has an explicit</span>
<span class="gi">+        # extension, we select the corresponding compressor.</span>
<span class="gi">+</span>
<span class="gi">+        # unset the variable to be sure no compression level is set afterwards.</span>
<span class="gi">+        compress_method = None</span>
<span class="gi">+        for name, compressor in _COMPRESSORS.items():</span>
<span class="gi">+            if filename.endswith(compressor.extension):</span>
<span class="gi">+                compress_method = name</span>
<span class="gi">+</span>
<span class="gi">+        if compress_method in _COMPRESSORS and compress_level == 0:</span>
<span class="gi">+            # we choose the default compress_level in case it was not given</span>
<span class="gi">+            # as an argument (using compress).</span>
<span class="gi">+            compress_level = None</span>
<span class="gi">+</span>
<span class="gi">+    if cache_size is not None:</span>
<span class="gi">+        # Cache size is deprecated starting from version 0.10</span>
<span class="gi">+        warnings.warn(&quot;Please do not set &#39;cache_size&#39; in joblib.dump, &quot;</span>
<span class="gi">+                      &quot;this parameter has no effect and will be removed. &quot;</span>
<span class="gi">+                      &quot;You used &#39;cache_size={}&#39;&quot;.format(cache_size),</span>
<span class="gi">+                      DeprecationWarning, stacklevel=2)</span>
<span class="gi">+</span>
<span class="gi">+    if compress_level != 0:</span>
<span class="gi">+        with _write_fileobject(filename, compress=(compress_method,</span>
<span class="gi">+                                                   compress_level)) as f:</span>
<span class="gi">+            NumpyPickler(f, protocol=protocol).dump(value)</span>
<span class="gi">+    elif is_filename:</span>
<span class="gi">+        with open(filename, &#39;wb&#39;) as f:</span>
<span class="gi">+            NumpyPickler(f, protocol=protocol).dump(value)</span>
<span class="gi">+    else:</span>
<span class="gi">+        NumpyPickler(filename, protocol=protocol).dump(value)</span>
<span class="gi">+</span>
<span class="gi">+    # If the target container is a file object, nothing is returned.</span>
<span class="gi">+    if is_fileobj:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    # For compatibility, the list of created filenames (e.g with one element</span>
<span class="gi">+    # after 0.10.0) is returned by default.</span>
<span class="gi">+    return [filename]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _unpickle(fobj, filename=&quot;&quot;, mmap_mode=None):</span>
<span class="w"> </span>    &quot;&quot;&quot;Internal unpickling function.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # We are careful to open the file handle early and keep it open to</span>
<span class="gi">+    # avoid race-conditions on renames.</span>
<span class="gi">+    # That said, if data is stored in companion files, which can be</span>
<span class="gi">+    # the case with the old persistence format, moving the directory</span>
<span class="gi">+    # will create a race when joblib tries to access the companion</span>
<span class="gi">+    # files.</span>
<span class="gi">+    unpickler = NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)</span>
<span class="gi">+    obj = None</span>
<span class="gi">+    try:</span>
<span class="gi">+        obj = unpickler.load()</span>
<span class="gi">+        if unpickler.compat_mode:</span>
<span class="gi">+            warnings.warn(&quot;The file &#39;%s&#39; has been generated with a &quot;</span>
<span class="gi">+                          &quot;joblib version less than 0.10. &quot;</span>
<span class="gi">+                          &quot;Please regenerate this pickle file.&quot;</span>
<span class="gi">+                          % filename,</span>
<span class="gi">+                          DeprecationWarning, stacklevel=3)</span>
<span class="gi">+    except UnicodeDecodeError as exc:</span>
<span class="gi">+        # More user-friendly error message</span>
<span class="gi">+        new_exc = ValueError(</span>
<span class="gi">+            &#39;You may be trying to read with &#39;</span>
<span class="gi">+            &#39;python 3 a joblib pickle generated with python 2. &#39;</span>
<span class="gi">+            &#39;This feature is not supported by joblib.&#39;)</span>
<span class="gi">+        new_exc.__cause__ = exc</span>
<span class="gi">+        raise new_exc</span>
<span class="gi">+    return obj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def load_temporary_memmap(filename, mmap_mode, unlink_on_gc_collect):</span>
<span class="gi">+    from ._memmapping_reducer import JOBLIB_MMAPS, add_maybe_unlink_finalizer</span>
<span class="gi">+    obj = load(filename, mmap_mode)</span>
<span class="gi">+    JOBLIB_MMAPS.add(obj.filename)</span>
<span class="gi">+    if unlink_on_gc_collect:</span>
<span class="gi">+        add_maybe_unlink_finalizer(obj)</span>
<span class="gi">+    return obj</span>


<span class="w"> </span>def load(filename, mmap_mode=None):
<span class="gu">@@ -281,4 +638,22 @@ def load(filename, mmap_mode=None):</span>
<span class="w"> </span>    object might not match the original pickled object. Note that if the
<span class="w"> </span>    file was saved with compression, the arrays cannot be memmapped.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if Path is not None and isinstance(filename, Path):</span>
<span class="gi">+        filename = str(filename)</span>
<span class="gi">+</span>
<span class="gi">+    if hasattr(filename, &quot;read&quot;):</span>
<span class="gi">+        fobj = filename</span>
<span class="gi">+        filename = getattr(fobj, &#39;name&#39;, &#39;&#39;)</span>
<span class="gi">+        with _read_fileobject(fobj, filename, mmap_mode) as fobj:</span>
<span class="gi">+            obj = _unpickle(fobj)</span>
<span class="gi">+    else:</span>
<span class="gi">+        with open(filename, &#39;rb&#39;) as f:</span>
<span class="gi">+            with _read_fileobject(f, filename, mmap_mode) as fobj:</span>
<span class="gi">+                if isinstance(fobj, str):</span>
<span class="gi">+                    # if the returned file object is a string, this means we</span>
<span class="gi">+                    # try to load a pickle file generated with an version of</span>
<span class="gi">+                    # Joblib so we load it with joblib compatibility function.</span>
<span class="gi">+                    return load_compatibility(fobj)</span>
<span class="gi">+</span>
<span class="gi">+                obj = _unpickle(fobj, filename, mmap_mode)</span>
<span class="gi">+    return obj</span>
<span class="gh">diff --git a/joblib/numpy_pickle_compat.py b/joblib/numpy_pickle_compat.py</span>
<span class="gh">index 4ccffb6..3261284 100644</span>
<span class="gd">--- a/joblib/numpy_pickle_compat.py</span>
<span class="gi">+++ b/joblib/numpy_pickle_compat.py</span>
<span class="gu">@@ -1,9 +1,12 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Numpy pickle compatibility functions.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import pickle
<span class="w"> </span>import os
<span class="w"> </span>import zlib
<span class="w"> </span>import inspect
<span class="gi">+</span>
<span class="w"> </span>from io import BytesIO
<span class="gi">+</span>
<span class="w"> </span>from .numpy_pickle_utils import _ZFILE_PREFIX
<span class="w"> </span>from .numpy_pickle_utils import Unpickler
<span class="w"> </span>from .numpy_pickle_utils import _ensure_native_byte_order
<span class="gu">@@ -11,7 +14,13 @@ from .numpy_pickle_utils import _ensure_native_byte_order</span>

<span class="w"> </span>def hex_str(an_int):
<span class="w"> </span>    &quot;&quot;&quot;Convert an int to an hexadecimal string.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return &#39;{:#x}&#39;.format(an_int)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def asbytes(s):</span>
<span class="gi">+    if isinstance(s, bytes):</span>
<span class="gi">+        return s</span>
<span class="gi">+    return s.encode(&#39;latin1&#39;)</span>


<span class="w"> </span>_MAX_LEN = len(hex_str(2 ** 64))
<span class="gu">@@ -25,7 +34,30 @@ def read_zfile(file_handle):</span>
<span class="w"> </span>    for persistence. Backward compatibility is not guaranteed. Do not
<span class="w"> </span>    use for external purposes.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    file_handle.seek(0)</span>
<span class="gi">+    header_length = len(_ZFILE_PREFIX) + _MAX_LEN</span>
<span class="gi">+    length = file_handle.read(header_length)</span>
<span class="gi">+    length = length[len(_ZFILE_PREFIX):]</span>
<span class="gi">+    length = int(length, 16)</span>
<span class="gi">+</span>
<span class="gi">+    # With python2 and joblib version &lt;= 0.8.4 compressed pickle header is one</span>
<span class="gi">+    # character wider so we need to ignore an additional space if present.</span>
<span class="gi">+    # Note: the first byte of the zlib data is guaranteed not to be a</span>
<span class="gi">+    # space according to</span>
<span class="gi">+    # https://tools.ietf.org/html/rfc6713#section-2.1</span>
<span class="gi">+    next_byte = file_handle.read(1)</span>
<span class="gi">+    if next_byte != b&#39; &#39;:</span>
<span class="gi">+        # The zlib compressed data has started and we need to go back</span>
<span class="gi">+        # one byte</span>
<span class="gi">+        file_handle.seek(header_length)</span>
<span class="gi">+</span>
<span class="gi">+    # We use the known length of the data to tell Zlib the size of the</span>
<span class="gi">+    # buffer to allocate.</span>
<span class="gi">+    data = zlib.decompress(file_handle.read(), 15, length)</span>
<span class="gi">+    assert len(data) == length, (</span>
<span class="gi">+        &quot;Incorrect data length while decompressing %s.&quot;</span>
<span class="gi">+        &quot;The file could be corrupted.&quot; % file_handle)</span>
<span class="gi">+    return data</span>


<span class="w"> </span>def write_zfile(file_handle, data, compress=1):
<span class="gu">@@ -35,7 +67,14 @@ def write_zfile(file_handle, data, compress=1):</span>
<span class="w"> </span>    for persistence. Backward compatibility is not guaranteed. Do not
<span class="w"> </span>    use for external purposes.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    file_handle.write(_ZFILE_PREFIX)</span>
<span class="gi">+    length = hex_str(len(data))</span>
<span class="gi">+    # Store the length of the data</span>
<span class="gi">+    file_handle.write(asbytes(length.ljust(_MAX_LEN)))</span>
<span class="gi">+    file_handle.write(zlib.compress(asbytes(data), compress))</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Utility objects for persistence.</span>


<span class="w"> </span>class NDArrayWrapper(object):
<span class="gu">@@ -53,7 +92,34 @@ class NDArrayWrapper(object):</span>

<span class="w"> </span>    def read(self, unpickler):
<span class="w"> </span>        &quot;&quot;&quot;Reconstruct the array.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        filename = os.path.join(unpickler._dirname, self.filename)</span>
<span class="gi">+        # Load the array from the disk</span>
<span class="gi">+        # use getattr instead of self.allow_mmap to ensure backward compat</span>
<span class="gi">+        # with NDArrayWrapper instances pickled with joblib &lt; 0.9.0</span>
<span class="gi">+        allow_mmap = getattr(self, &#39;allow_mmap&#39;, True)</span>
<span class="gi">+        kwargs = {}</span>
<span class="gi">+        if allow_mmap:</span>
<span class="gi">+            kwargs[&#39;mmap_mode&#39;] = unpickler.mmap_mode</span>
<span class="gi">+        if &quot;allow_pickle&quot; in inspect.signature(unpickler.np.load).parameters:</span>
<span class="gi">+            # Required in numpy 1.16.3 and later to aknowledge the security</span>
<span class="gi">+            # risk.</span>
<span class="gi">+            kwargs[&quot;allow_pickle&quot;] = True</span>
<span class="gi">+        array = unpickler.np.load(filename, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+        # Detect byte order mismatch and swap as needed.</span>
<span class="gi">+        array = _ensure_native_byte_order(array)</span>
<span class="gi">+</span>
<span class="gi">+        # Reconstruct subclasses. This does not work with old</span>
<span class="gi">+        # versions of numpy</span>
<span class="gi">+        if (hasattr(array, &#39;__array_prepare__&#39;) and</span>
<span class="gi">+            self.subclass not in (unpickler.np.ndarray,</span>
<span class="gi">+                                  unpickler.np.memmap)):</span>
<span class="gi">+            # We need to reconstruct another subclass</span>
<span class="gi">+            new_array = unpickler.np.core.multiarray._reconstruct(</span>
<span class="gi">+                self.subclass, (0,), &#39;b&#39;)</span>
<span class="gi">+            return new_array.__array_prepare__(array)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return array</span>


<span class="w"> </span>class ZNDArrayWrapper(NDArrayWrapper):
<span class="gu">@@ -79,11 +145,20 @@ class ZNDArrayWrapper(NDArrayWrapper):</span>

<span class="w"> </span>    def read(self, unpickler):
<span class="w"> </span>        &quot;&quot;&quot;Reconstruct the array from the meta-information and the z-file.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Here we a simply reproducing the unpickling mechanism for numpy</span>
<span class="gi">+        # arrays</span>
<span class="gi">+        filename = os.path.join(unpickler._dirname, self.filename)</span>
<span class="gi">+        array = unpickler.np.core.multiarray._reconstruct(*self.init_args)</span>
<span class="gi">+        with open(filename, &#39;rb&#39;) as f:</span>
<span class="gi">+            data = read_zfile(f)</span>
<span class="gi">+        state = self.state + (data,)</span>
<span class="gi">+        array.__setstate__(state)</span>
<span class="gi">+        return array</span>


<span class="w"> </span>class ZipNumpyUnpickler(Unpickler):
<span class="w"> </span>    &quot;&quot;&quot;A subclass of the Unpickler to unpickle our numpy pickles.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    dispatch = Unpickler.dispatch.copy()

<span class="w"> </span>    def __init__(self, filename, file_handle, mmap_mode=None):
<span class="gu">@@ -99,6 +174,9 @@ class ZipNumpyUnpickler(Unpickler):</span>
<span class="w"> </span>            np = None
<span class="w"> </span>        self.np = np

<span class="gi">+    def _open_pickle(self, file_handle):</span>
<span class="gi">+        return BytesIO(read_zfile(file_handle))</span>
<span class="gi">+</span>
<span class="w"> </span>    def load_build(self):
<span class="w"> </span>        &quot;&quot;&quot;Set the state of a newly created object.

<span class="gu">@@ -106,7 +184,15 @@ class ZipNumpyUnpickler(Unpickler):</span>
<span class="w"> </span>        NDArrayWrapper, by the array we are interested in. We
<span class="w"> </span>        replace them directly in the stack of pickler.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        Unpickler.load_build(self)</span>
<span class="gi">+        if isinstance(self.stack[-1], NDArrayWrapper):</span>
<span class="gi">+            if self.np is None:</span>
<span class="gi">+                raise ImportError(&quot;Trying to unpickle an ndarray, &quot;</span>
<span class="gi">+                                  &quot;but numpy didn&#39;t import correctly&quot;)</span>
<span class="gi">+            nd_array_wrapper = self.stack.pop()</span>
<span class="gi">+            array = nd_array_wrapper.read(self)</span>
<span class="gi">+            self.stack.append(array)</span>
<span class="gi">+</span>
<span class="w"> </span>    dispatch[pickle.BUILD[0]] = load_build


<span class="gu">@@ -136,4 +222,23 @@ def load_compatibility(filename):</span>
<span class="w"> </span>    This function can load numpy array files saved separately during the
<span class="w"> </span>    dump.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with open(filename, &#39;rb&#39;) as file_handle:</span>
<span class="gi">+        # We are careful to open the file handle early and keep it open to</span>
<span class="gi">+        # avoid race-conditions on renames. That said, if data is stored in</span>
<span class="gi">+        # companion files, moving the directory will create a race when</span>
<span class="gi">+        # joblib tries to access the companion files.</span>
<span class="gi">+        unpickler = ZipNumpyUnpickler(filename, file_handle=file_handle)</span>
<span class="gi">+        try:</span>
<span class="gi">+            obj = unpickler.load()</span>
<span class="gi">+        except UnicodeDecodeError as exc:</span>
<span class="gi">+            # More user-friendly error message</span>
<span class="gi">+            new_exc = ValueError(</span>
<span class="gi">+                &#39;You may be trying to read with &#39;</span>
<span class="gi">+                &#39;python 3 a joblib pickle generated with python 2. &#39;</span>
<span class="gi">+                &#39;This feature is not supported by joblib.&#39;)</span>
<span class="gi">+            new_exc.__cause__ = exc</span>
<span class="gi">+            raise new_exc</span>
<span class="gi">+        finally:</span>
<span class="gi">+            if hasattr(unpickler, &#39;file_handle&#39;):</span>
<span class="gi">+                unpickler.file_handle.close()</span>
<span class="gi">+        return obj</span>
<span class="gh">diff --git a/joblib/numpy_pickle_utils.py b/joblib/numpy_pickle_utils.py</span>
<span class="gh">index e79528e..23cfb34 100644</span>
<span class="gd">--- a/joblib/numpy_pickle_utils.py</span>
<span class="gi">+++ b/joblib/numpy_pickle_utils.py</span>
<span class="gu">@@ -1,33 +1,66 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utilities for fast persistence of big data, with optional compression.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="gi">+# Copyright (c) 2009 Gael Varoquaux</span>
<span class="gi">+# License: BSD Style, 3 clauses.</span>
<span class="gi">+</span>
<span class="w"> </span>import pickle
<span class="w"> </span>import io
<span class="w"> </span>import sys
<span class="w"> </span>import warnings
<span class="w"> </span>import contextlib
<span class="gi">+</span>
<span class="w"> </span>from .compressor import _ZFILE_PREFIX
<span class="w"> </span>from .compressor import _COMPRESSORS
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import numpy as np
<span class="w"> </span>except ImportError:
<span class="w"> </span>    np = None
<span class="gi">+</span>
<span class="w"> </span>Unpickler = pickle._Unpickler
<span class="w"> </span>Pickler = pickle._Pickler
<span class="w"> </span>xrange = range
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # The python standard library can be built without bz2 so we make bz2</span>
<span class="gi">+    # usage optional.</span>
<span class="gi">+    # see https://github.com/scikit-learn/scikit-learn/issues/7526 for more</span>
<span class="gi">+    # details.</span>
<span class="w"> </span>    import bz2
<span class="w"> </span>except ImportError:
<span class="w"> </span>    bz2 = None
<span class="gi">+</span>
<span class="gi">+# Buffer size used in io.BufferedReader and io.BufferedWriter</span>
<span class="w"> </span>_IO_BUFFER_SIZE = 1024 ** 2


<span class="w"> </span>def _is_raw_file(fileobj):
<span class="w"> </span>    &quot;&quot;&quot;Check if fileobj is a raw file object, e.g created with open.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    fileobj = getattr(fileobj, &#39;raw&#39;, fileobj)</span>
<span class="gi">+    return isinstance(fileobj, io.FileIO)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_prefixes_max_len():</span>
<span class="gi">+    # Compute the max prefix len of registered compressors.</span>
<span class="gi">+    prefixes = [len(compressor.prefix) for compressor in _COMPRESSORS.values()]</span>
<span class="gi">+    prefixes += [len(_ZFILE_PREFIX)]</span>
<span class="gi">+    return max(prefixes)</span>


<span class="w"> </span>def _is_numpy_array_byte_order_mismatch(array):
<span class="w"> </span>    &quot;&quot;&quot;Check if numpy array is having byte order mismatch&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return ((sys.byteorder == &#39;big&#39; and</span>
<span class="gi">+             (array.dtype.byteorder == &#39;&lt;&#39; or</span>
<span class="gi">+              (array.dtype.byteorder == &#39;|&#39; and array.dtype.fields and</span>
<span class="gi">+               all(e[0].byteorder == &#39;&lt;&#39;</span>
<span class="gi">+                   for e in array.dtype.fields.values())))) or</span>
<span class="gi">+            (sys.byteorder == &#39;little&#39; and</span>
<span class="gi">+             (array.dtype.byteorder == &#39;&gt;&#39; or</span>
<span class="gi">+              (array.dtype.byteorder == &#39;|&#39; and array.dtype.fields and</span>
<span class="gi">+               all(e[0].byteorder == &#39;&gt;&#39;</span>
<span class="gi">+                   for e in array.dtype.fields.values())))))</span>


<span class="w"> </span>def _ensure_native_byte_order(array):
<span class="gu">@@ -35,9 +68,13 @@ def _ensure_native_byte_order(array):</span>

<span class="w"> </span>    Does nothing if array already uses the system byte order.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if _is_numpy_array_byte_order_mismatch(array):</span>
<span class="gi">+        array = array.byteswap().view(array.dtype.newbyteorder(&#39;=&#39;))</span>
<span class="gi">+    return array</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# Cache file utilities</span>
<span class="w"> </span>def _detect_compressor(fileobj):
<span class="w"> </span>    &quot;&quot;&quot;Return the compressor matching fileobj.

<span class="gu">@@ -49,17 +86,35 @@ def _detect_compressor(fileobj):</span>
<span class="w"> </span>    -------
<span class="w"> </span>    str in {&#39;zlib&#39;, &#39;gzip&#39;, &#39;bz2&#39;, &#39;lzma&#39;, &#39;xz&#39;, &#39;compat&#39;, &#39;not-compressed&#39;}
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Read the magic number in the first bytes of the file.</span>
<span class="gi">+    max_prefix_len = _get_prefixes_max_len()</span>
<span class="gi">+    if hasattr(fileobj, &#39;peek&#39;):</span>
<span class="gi">+        # Peek allows to read those bytes without moving the cursor in the</span>
<span class="gi">+        # file whic.</span>
<span class="gi">+        first_bytes = fileobj.peek(max_prefix_len)</span>
<span class="gi">+    else:</span>
<span class="gi">+        # Fallback to seek if the fileobject is not peekable.</span>
<span class="gi">+        first_bytes = fileobj.read(max_prefix_len)</span>
<span class="gi">+        fileobj.seek(0)</span>
<span class="gi">+</span>
<span class="gi">+    if first_bytes.startswith(_ZFILE_PREFIX):</span>
<span class="gi">+        return &quot;compat&quot;</span>
<span class="gi">+    else:</span>
<span class="gi">+        for name, compressor in _COMPRESSORS.items():</span>
<span class="gi">+            if first_bytes.startswith(compressor.prefix):</span>
<span class="gi">+                return name</span>
<span class="gi">+</span>
<span class="gi">+    return &quot;not-compressed&quot;</span>


<span class="w"> </span>def _buffered_read_file(fobj):
<span class="w"> </span>    &quot;&quot;&quot;Return a buffered version of a read file object.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return io.BufferedReader(fobj, buffer_size=_IO_BUFFER_SIZE)</span>


<span class="w"> </span>def _buffered_write_file(fobj):
<span class="w"> </span>    &quot;&quot;&quot;Return a buffered version of a write file object.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return io.BufferedWriter(fobj, buffer_size=_IO_BUFFER_SIZE)</span>


<span class="w"> </span>@contextlib.contextmanager
<span class="gu">@@ -90,18 +145,70 @@ def _read_fileobject(fileobj, filename, mmap_mode=None):</span>
<span class="w"> </span>        a file like object

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def _write_fileobject(filename, compress=(&#39;zlib&#39;, 3)):</span>
<span class="gi">+    # Detect if the fileobj contains compressed data.</span>
<span class="gi">+    compressor = _detect_compressor(fileobj)</span>
<span class="gi">+</span>
<span class="gi">+    if compressor == &#39;compat&#39;:</span>
<span class="gi">+        # Compatibility with old pickle mode: simply return the input</span>
<span class="gi">+        # filename &quot;as-is&quot; and let the compatibility function be called by the</span>
<span class="gi">+        # caller.</span>
<span class="gi">+        warnings.warn(&quot;The file &#39;%s&#39; has been generated with a joblib &quot;</span>
<span class="gi">+                      &quot;version less than 0.10. &quot;</span>
<span class="gi">+                      &quot;Please regenerate this pickle file.&quot; % filename,</span>
<span class="gi">+                      DeprecationWarning, stacklevel=2)</span>
<span class="gi">+        yield filename</span>
<span class="gi">+    else:</span>
<span class="gi">+        if compressor in _COMPRESSORS:</span>
<span class="gi">+            # based on the compressor detected in the file, we open the</span>
<span class="gi">+            # correct decompressor file object, wrapped in a buffer.</span>
<span class="gi">+            compressor_wrapper = _COMPRESSORS[compressor]</span>
<span class="gi">+            inst = compressor_wrapper.decompressor_file(fileobj)</span>
<span class="gi">+            fileobj = _buffered_read_file(inst)</span>
<span class="gi">+</span>
<span class="gi">+        # Checking if incompatible load parameters with the type of file:</span>
<span class="gi">+        # mmap_mode cannot be used with compressed file or in memory buffers</span>
<span class="gi">+        # such as io.BytesIO.</span>
<span class="gi">+        if mmap_mode is not None:</span>
<span class="gi">+            if isinstance(fileobj, io.BytesIO):</span>
<span class="gi">+                warnings.warn(&#39;In memory persistence is not compatible with &#39;</span>
<span class="gi">+                              &#39;mmap_mode &quot;%(mmap_mode)s&quot; flag passed. &#39;</span>
<span class="gi">+                              &#39;mmap_mode option will be ignored.&#39;</span>
<span class="gi">+                              % locals(), stacklevel=2)</span>
<span class="gi">+            elif compressor != &#39;not-compressed&#39;:</span>
<span class="gi">+                warnings.warn(&#39;mmap_mode &quot;%(mmap_mode)s&quot; is not compatible &#39;</span>
<span class="gi">+                              &#39;with compressed file %(filename)s. &#39;</span>
<span class="gi">+                              &#39;&quot;%(mmap_mode)s&quot; flag will be ignored.&#39;</span>
<span class="gi">+                              % locals(), stacklevel=2)</span>
<span class="gi">+            elif not _is_raw_file(fileobj):</span>
<span class="gi">+                warnings.warn(&#39;&quot;%(fileobj)r&quot; is not a raw file, mmap_mode &#39;</span>
<span class="gi">+                              &#39;&quot;%(mmap_mode)s&quot; flag will be ignored.&#39;</span>
<span class="gi">+                              % locals(), stacklevel=2)</span>
<span class="gi">+</span>
<span class="gi">+        yield fileobj</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _write_fileobject(filename, compress=(&quot;zlib&quot;, 3)):</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the right compressor file object in write mode.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    compressmethod = compress[0]</span>
<span class="gi">+    compresslevel = compress[1]</span>
<span class="gi">+</span>
<span class="gi">+    if compressmethod in _COMPRESSORS.keys():</span>
<span class="gi">+        file_instance = _COMPRESSORS[compressmethod].compressor_file(</span>
<span class="gi">+            filename, compresslevel=compresslevel)</span>
<span class="gi">+        return _buffered_write_file(file_instance)</span>
<span class="gi">+    else:</span>
<span class="gi">+        file_instance = _COMPRESSORS[&#39;zlib&#39;].compressor_file(</span>
<span class="gi">+            filename, compresslevel=compresslevel)</span>
<span class="gi">+        return _buffered_write_file(file_instance)</span>


<span class="gd">-BUFFER_SIZE = 2 ** 18</span>
<span class="gi">+# Utility functions/variables from numpy required for writing arrays.</span>
<span class="gi">+# We need at least the functions introduced in version 1.9 of numpy. Here,</span>
<span class="gi">+# we use the ones from numpy 1.10.2.</span>
<span class="gi">+BUFFER_SIZE = 2 ** 18  # size of buffer for reading npz files in bytes</span>


<span class="gd">-def _read_bytes(fp, size, error_template=&#39;ran out of data&#39;):</span>
<span class="gi">+def _read_bytes(fp, size, error_template=&quot;ran out of data&quot;):</span>
<span class="w"> </span>    &quot;&quot;&quot;Read from file-like object until size bytes are read.

<span class="w"> </span>    TODO python2_drop: is it still needed? The docstring mentions python 2.6
<span class="gu">@@ -127,4 +234,20 @@ def _read_bytes(fp, size, error_template=&#39;ran out of data&#39;):</span>
<span class="w"> </span>        The data read in bytes.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    data = bytes()</span>
<span class="gi">+    while True:</span>
<span class="gi">+        # io files (default in python3) return None or raise on</span>
<span class="gi">+        # would-block, python2 file will truncate, probably nothing can be</span>
<span class="gi">+        # done about that.  note that regular files can&#39;t be non-blocking</span>
<span class="gi">+        try:</span>
<span class="gi">+            r = fp.read(size - len(data))</span>
<span class="gi">+            data += r</span>
<span class="gi">+            if len(r) == 0 or len(data) == size:</span>
<span class="gi">+                break</span>
<span class="gi">+        except io.BlockingIOError:</span>
<span class="gi">+            pass</span>
<span class="gi">+    if len(data) != size:</span>
<span class="gi">+        msg = &quot;EOF: reading %s, expected %d bytes got %d&quot;</span>
<span class="gi">+        raise ValueError(msg % (error_template, size, len(data)))</span>
<span class="gi">+    else:</span>
<span class="gi">+        return data</span>
<span class="gh">diff --git a/joblib/parallel.py b/joblib/parallel.py</span>
<span class="gh">index f141ee0..fa4fd3c 100644</span>
<span class="gd">--- a/joblib/parallel.py</span>
<span class="gi">+++ b/joblib/parallel.py</span>
<span class="gu">@@ -1,7 +1,12 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Helpers for embarrassingly parallel code.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Author: Gael Varoquaux &lt; gael dot varoquaux at normalesup dot org &gt;</span>
<span class="gi">+# Copyright: 2010, Gael Varoquaux</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>from __future__ import division
<span class="gi">+</span>
<span class="w"> </span>import os
<span class="w"> </span>import sys
<span class="w"> </span>from math import sqrt
<span class="gu">@@ -16,41 +21,90 @@ import warnings</span>
<span class="w"> </span>import queue
<span class="w"> </span>import weakref
<span class="w"> </span>from contextlib import nullcontext
<span class="gi">+</span>
<span class="w"> </span>from multiprocessing import TimeoutError
<span class="gi">+</span>
<span class="w"> </span>from ._multiprocessing_helpers import mp
<span class="gi">+</span>
<span class="w"> </span>from .logger import Logger, short_format_time
<span class="w"> </span>from .disk import memstr_to_bytes
<span class="gd">-from ._parallel_backends import FallbackToBackend, MultiprocessingBackend, ThreadingBackend, SequentialBackend, LokyBackend</span>
<span class="gi">+from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,</span>
<span class="gi">+                                 ThreadingBackend, SequentialBackend,</span>
<span class="gi">+                                 LokyBackend)</span>
<span class="w"> </span>from ._utils import eval_expr, _Sentinel
<span class="gd">-from ._parallel_backends import AutoBatchingMixin</span>
<span class="gd">-from ._parallel_backends import ParallelBackendBase</span>
<span class="gd">-IS_PYPY = hasattr(sys, &#39;pypy_version_info&#39;)</span>
<span class="gd">-BACKENDS = {&#39;threading&#39;: ThreadingBackend, &#39;sequential&#39;: SequentialBackend}</span>
<span class="gi">+</span>
<span class="gi">+# Make sure that those two classes are part of the public joblib.parallel API</span>
<span class="gi">+# so that 3rd party backend implementers can import them from here.</span>
<span class="gi">+from ._parallel_backends import AutoBatchingMixin  # noqa</span>
<span class="gi">+from ._parallel_backends import ParallelBackendBase  # noqa</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+IS_PYPY = hasattr(sys, &quot;pypy_version_info&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+BACKENDS = {</span>
<span class="gi">+    &#39;threading&#39;: ThreadingBackend,</span>
<span class="gi">+    &#39;sequential&#39;: SequentialBackend,</span>
<span class="gi">+}</span>
<span class="gi">+# name of the backend used by default by Parallel outside of any context</span>
<span class="gi">+# managed by ``parallel_config`` or ``parallel_backend``.</span>
<span class="gi">+</span>
<span class="gi">+# threading is the only backend that is always everywhere</span>
<span class="w"> </span>DEFAULT_BACKEND = &#39;threading&#39;
<span class="gi">+</span>
<span class="w"> </span>MAYBE_AVAILABLE_BACKENDS = {&#39;multiprocessing&#39;, &#39;loky&#39;}
<span class="gi">+</span>
<span class="gi">+# if multiprocessing is available, so is loky, we set it as the default</span>
<span class="gi">+# backend</span>
<span class="w"> </span>if mp is not None:
<span class="w"> </span>    BACKENDS[&#39;multiprocessing&#39;] = MultiprocessingBackend
<span class="w"> </span>    from .externals import loky
<span class="w"> </span>    BACKENDS[&#39;loky&#39;] = LokyBackend
<span class="w"> </span>    DEFAULT_BACKEND = &#39;loky&#39;
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>DEFAULT_THREAD_BACKEND = &#39;threading&#39;
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Thread local value that can be overridden by the ``parallel_config`` context</span>
<span class="gi">+# manager</span>
<span class="w"> </span>_backend = threading.local()


<span class="w"> </span>def _register_dask():
<span class="w"> </span>    &quot;&quot;&quot;Register Dask Backend if called with parallel_config(backend=&quot;dask&quot;)&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        from ._dask import DaskDistributedBackend</span>
<span class="gi">+        register_parallel_backend(&#39;dask&#39;, DaskDistributedBackend)</span>
<span class="gi">+    except ImportError as e:</span>
<span class="gi">+        msg = (&quot;To use the dask.distributed backend you must install both &quot;</span>
<span class="gi">+               &quot;the `dask` and distributed modules.\n\n&quot;</span>
<span class="gi">+               &quot;See https://dask.pydata.org/en/latest/install.html for more &quot;</span>
<span class="gi">+               &quot;information.&quot;)</span>
<span class="gi">+        raise ImportError(msg) from e</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+EXTERNAL_BACKENDS = {</span>
<span class="gi">+    &#39;dask&#39;: _register_dask,</span>
<span class="gi">+}</span>
<span class="gi">+</span>

<span class="gi">+# Sentinels for the default values of the Parallel constructor and</span>
<span class="gi">+# the parallel_config and parallel_backend context managers</span>
<span class="gi">+default_parallel_config = {</span>
<span class="gi">+    &quot;backend&quot;: _Sentinel(default_value=None),</span>
<span class="gi">+    &quot;n_jobs&quot;: _Sentinel(default_value=None),</span>
<span class="gi">+    &quot;verbose&quot;: _Sentinel(default_value=0),</span>
<span class="gi">+    &quot;temp_folder&quot;: _Sentinel(default_value=None),</span>
<span class="gi">+    &quot;max_nbytes&quot;: _Sentinel(default_value=&quot;1M&quot;),</span>
<span class="gi">+    &quot;mmap_mode&quot;: _Sentinel(default_value=&quot;r&quot;),</span>
<span class="gi">+    &quot;prefer&quot;: _Sentinel(default_value=None),</span>
<span class="gi">+    &quot;require&quot;: _Sentinel(default_value=None),</span>
<span class="gi">+}</span>

<span class="gd">-EXTERNAL_BACKENDS = {&#39;dask&#39;: _register_dask}</span>
<span class="gd">-default_parallel_config = {&#39;backend&#39;: _Sentinel(default_value=None),</span>
<span class="gd">-    &#39;n_jobs&#39;: _Sentinel(default_value=None), &#39;verbose&#39;: _Sentinel(</span>
<span class="gd">-    default_value=0), &#39;temp_folder&#39;: _Sentinel(default_value=None),</span>
<span class="gd">-    &#39;max_nbytes&#39;: _Sentinel(default_value=&#39;1M&#39;), &#39;mmap_mode&#39;: _Sentinel(</span>
<span class="gd">-    default_value=&#39;r&#39;), &#39;prefer&#39;: _Sentinel(default_value=None), &#39;require&#39;:</span>
<span class="gd">-    _Sentinel(default_value=None)}</span>
<span class="gd">-VALID_BACKEND_HINTS = &#39;processes&#39;, &#39;threads&#39;, None</span>
<span class="gd">-VALID_BACKEND_CONSTRAINTS = &#39;sharedmem&#39;, None</span>
<span class="gi">+</span>
<span class="gi">+VALID_BACKEND_HINTS = (&#39;processes&#39;, &#39;threads&#39;, None)</span>
<span class="gi">+VALID_BACKEND_CONSTRAINTS = (&#39;sharedmem&#39;, None)</span>


<span class="w"> </span>def _get_config_param(param, context_config, key):
<span class="gu">@@ -59,21 +113,105 @@ def _get_config_param(param, context_config, key):</span>
<span class="w"> </span>    Explicitly setting it in Parallel has priority over setting in a
<span class="w"> </span>    parallel_(config/backend) context manager.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if param is not default_parallel_config[key]:</span>
<span class="gi">+        # param is explicitly set, return it</span>
<span class="gi">+        return param</span>

<span class="gi">+    if context_config[key] is not default_parallel_config[key]:</span>
<span class="gi">+        # there&#39;s a context manager and the key is set, return it</span>
<span class="gi">+        return context_config[key]</span>

<span class="gd">-def get_active_backend(prefer=default_parallel_config[&#39;prefer&#39;], require=</span>
<span class="gd">-    default_parallel_config[&#39;require&#39;], verbose=default_parallel_config[</span>
<span class="gd">-    &#39;verbose&#39;]):</span>
<span class="gd">-    &quot;&quot;&quot;Return the active default backend&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+    # Otherwise, we are in the default_parallel_config,</span>
<span class="gi">+    # return the default value</span>
<span class="gi">+    return param.default_value</span>


<span class="gd">-def _get_active_backend(prefer=default_parallel_config[&#39;prefer&#39;], require=</span>
<span class="gd">-    default_parallel_config[&#39;require&#39;], verbose=default_parallel_config[</span>
<span class="gd">-    &#39;verbose&#39;]):</span>
<span class="gi">+def get_active_backend(</span>
<span class="gi">+    prefer=default_parallel_config[&quot;prefer&quot;],</span>
<span class="gi">+    require=default_parallel_config[&quot;require&quot;],</span>
<span class="gi">+    verbose=default_parallel_config[&quot;verbose&quot;],</span>
<span class="gi">+):</span>
<span class="gi">+    &quot;&quot;&quot;Return the active default backend&quot;&quot;&quot;</span>
<span class="gi">+    backend, config = _get_active_backend(prefer, require, verbose)</span>
<span class="gi">+    n_jobs = _get_config_param(</span>
<span class="gi">+        default_parallel_config[&#39;n_jobs&#39;], config, &quot;n_jobs&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    return backend, n_jobs</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _get_active_backend(</span>
<span class="gi">+    prefer=default_parallel_config[&quot;prefer&quot;],</span>
<span class="gi">+    require=default_parallel_config[&quot;require&quot;],</span>
<span class="gi">+    verbose=default_parallel_config[&quot;verbose&quot;],</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Return the active default backend&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    backend_config = getattr(_backend, &quot;config&quot;, default_parallel_config)</span>
<span class="gi">+</span>
<span class="gi">+    backend = _get_config_param(</span>
<span class="gi">+        default_parallel_config[&#39;backend&#39;], backend_config, &quot;backend&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    prefer = _get_config_param(prefer, backend_config, &quot;prefer&quot;)</span>
<span class="gi">+    require = _get_config_param(require, backend_config, &quot;require&quot;)</span>
<span class="gi">+    verbose = _get_config_param(verbose, backend_config, &quot;verbose&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if prefer not in VALID_BACKEND_HINTS:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;prefer={prefer} is not a valid backend hint, &quot;</span>
<span class="gi">+            f&quot;expected one of {VALID_BACKEND_HINTS}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if require not in VALID_BACKEND_CONSTRAINTS:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;require={require} is not a valid backend constraint, &quot;</span>
<span class="gi">+            f&quot;expected one of {VALID_BACKEND_CONSTRAINTS}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if prefer == &#39;processes&#39; and require == &#39;sharedmem&#39;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;prefer == &#39;processes&#39; and require == &#39;sharedmem&#39;&quot;</span>
<span class="gi">+            &quot; are inconsistent settings&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    explicit_backend = True</span>
<span class="gi">+    if backend is None:</span>
<span class="gi">+</span>
<span class="gi">+        # We are either outside of the scope of any parallel_(config/backend)</span>
<span class="gi">+        # context manager or the context manager did not set a backend.</span>
<span class="gi">+        # create the default backend instance now.</span>
<span class="gi">+        backend = BACKENDS[DEFAULT_BACKEND](nesting_level=0)</span>
<span class="gi">+        explicit_backend = False</span>
<span class="gi">+</span>
<span class="gi">+    # Try to use the backend set by the user with the context manager.</span>
<span class="gi">+</span>
<span class="gi">+    nesting_level = backend.nesting_level</span>
<span class="gi">+    uses_threads = getattr(backend, &#39;uses_threads&#39;, False)</span>
<span class="gi">+    supports_sharedmem = getattr(backend, &#39;supports_sharedmem&#39;, False)</span>
<span class="gi">+    # Force to use thread-based backend if the provided backend does not</span>
<span class="gi">+    # match the shared memory constraint or if the backend is not explicitly</span>
<span class="gi">+    # given and threads are preferred.</span>
<span class="gi">+    force_threads = (require == &#39;sharedmem&#39; and not supports_sharedmem)</span>
<span class="gi">+    force_threads |= (</span>
<span class="gi">+        not explicit_backend and prefer == &#39;threads&#39; and not uses_threads</span>
<span class="gi">+    )</span>
<span class="gi">+    if force_threads:</span>
<span class="gi">+        # This backend does not match the shared memory constraint:</span>
<span class="gi">+        # fallback to the default thead-based backend.</span>
<span class="gi">+        sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND](</span>
<span class="gi">+            nesting_level=nesting_level</span>
<span class="gi">+        )</span>
<span class="gi">+        # Warn the user if we forced the backend to thread-based, while the</span>
<span class="gi">+        # user explicitly specified a non-thread-based backend.</span>
<span class="gi">+        if verbose &gt;= 10 and explicit_backend:</span>
<span class="gi">+            print(</span>
<span class="gi">+                f&quot;Using {sharedmem_backend.__class__.__name__} as &quot;</span>
<span class="gi">+                f&quot;joblib backend instead of {backend.__class__.__name__} &quot;</span>
<span class="gi">+                &quot;as the latter does not provide shared memory semantics.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        # Force to n_jobs=1 by default</span>
<span class="gi">+        thread_config = backend_config.copy()</span>
<span class="gi">+        thread_config[&#39;n_jobs&#39;] = 1</span>
<span class="gi">+        return sharedmem_backend, thread_config</span>
<span class="gi">+</span>
<span class="gi">+    return backend, backend_config</span>


<span class="w"> </span>class parallel_config:
<span class="gu">@@ -215,26 +353,98 @@ class parallel_config:</span>
<span class="w"> </span>    [-1, -2, -3, -4, -5]

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-</span>
<span class="gd">-    def __init__(self, backend=default_parallel_config[&#39;backend&#39;], *,</span>
<span class="gd">-        n_jobs=default_parallel_config[&#39;n_jobs&#39;], verbose=</span>
<span class="gd">-        default_parallel_config[&#39;verbose&#39;], temp_folder=</span>
<span class="gd">-        default_parallel_config[&#39;temp_folder&#39;], max_nbytes=</span>
<span class="gd">-        default_parallel_config[&#39;max_nbytes&#39;], mmap_mode=</span>
<span class="gd">-        default_parallel_config[&#39;mmap_mode&#39;], prefer=</span>
<span class="gd">-        default_parallel_config[&#39;prefer&#39;], require=default_parallel_config[</span>
<span class="gd">-        &#39;require&#39;], inner_max_num_threads=None, **backend_params):</span>
<span class="gd">-        self.old_parallel_config = getattr(_backend, &#39;config&#39;,</span>
<span class="gd">-            default_parallel_config)</span>
<span class="gd">-        backend = self._check_backend(backend, inner_max_num_threads, **</span>
<span class="gd">-            backend_params)</span>
<span class="gd">-        new_config = {&#39;n_jobs&#39;: n_jobs, &#39;verbose&#39;: verbose, &#39;temp_folder&#39;:</span>
<span class="gd">-            temp_folder, &#39;max_nbytes&#39;: max_nbytes, &#39;mmap_mode&#39;: mmap_mode,</span>
<span class="gd">-            &#39;prefer&#39;: prefer, &#39;require&#39;: require, &#39;backend&#39;: backend}</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        backend=default_parallel_config[&quot;backend&quot;],</span>
<span class="gi">+        *,</span>
<span class="gi">+        n_jobs=default_parallel_config[&quot;n_jobs&quot;],</span>
<span class="gi">+        verbose=default_parallel_config[&quot;verbose&quot;],</span>
<span class="gi">+        temp_folder=default_parallel_config[&quot;temp_folder&quot;],</span>
<span class="gi">+        max_nbytes=default_parallel_config[&quot;max_nbytes&quot;],</span>
<span class="gi">+        mmap_mode=default_parallel_config[&quot;mmap_mode&quot;],</span>
<span class="gi">+        prefer=default_parallel_config[&quot;prefer&quot;],</span>
<span class="gi">+        require=default_parallel_config[&quot;require&quot;],</span>
<span class="gi">+        inner_max_num_threads=None,</span>
<span class="gi">+        **backend_params</span>
<span class="gi">+    ):</span>
<span class="gi">+        # Save the parallel info and set the active parallel config</span>
<span class="gi">+        self.old_parallel_config = getattr(</span>
<span class="gi">+            _backend, &quot;config&quot;, default_parallel_config</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        backend = self._check_backend(</span>
<span class="gi">+            backend, inner_max_num_threads, **backend_params</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        new_config = {</span>
<span class="gi">+            &quot;n_jobs&quot;: n_jobs,</span>
<span class="gi">+            &quot;verbose&quot;: verbose,</span>
<span class="gi">+            &quot;temp_folder&quot;: temp_folder,</span>
<span class="gi">+            &quot;max_nbytes&quot;: max_nbytes,</span>
<span class="gi">+            &quot;mmap_mode&quot;: mmap_mode,</span>
<span class="gi">+            &quot;prefer&quot;: prefer,</span>
<span class="gi">+            &quot;require&quot;: require,</span>
<span class="gi">+            &quot;backend&quot;: backend</span>
<span class="gi">+        }</span>
<span class="w"> </span>        self.parallel_config = self.old_parallel_config.copy()
<span class="gd">-        self.parallel_config.update({k: v for k, v in new_config.items() if</span>
<span class="gd">-            not isinstance(v, _Sentinel)})</span>
<span class="gd">-        setattr(_backend, &#39;config&#39;, self.parallel_config)</span>
<span class="gi">+        self.parallel_config.update({</span>
<span class="gi">+            k: v for k, v in new_config.items()</span>
<span class="gi">+            if not isinstance(v, _Sentinel)</span>
<span class="gi">+        })</span>
<span class="gi">+</span>
<span class="gi">+        setattr(_backend, &quot;config&quot;, self.parallel_config)</span>
<span class="gi">+</span>
<span class="gi">+    def _check_backend(self, backend, inner_max_num_threads, **backend_params):</span>
<span class="gi">+        if backend is default_parallel_config[&#39;backend&#39;]:</span>
<span class="gi">+            if inner_max_num_threads is not None or len(backend_params) &gt; 0:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;inner_max_num_threads and other constructor &quot;</span>
<span class="gi">+                    &quot;parameters backend_params are only supported &quot;</span>
<span class="gi">+                    &quot;when backend is not None.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return backend</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(backend, str):</span>
<span class="gi">+            # Handle non-registered or missing backends</span>
<span class="gi">+            if backend not in BACKENDS:</span>
<span class="gi">+                if backend in EXTERNAL_BACKENDS:</span>
<span class="gi">+                    register = EXTERNAL_BACKENDS[backend]</span>
<span class="gi">+                    register()</span>
<span class="gi">+                elif backend in MAYBE_AVAILABLE_BACKENDS:</span>
<span class="gi">+                    warnings.warn(</span>
<span class="gi">+                        f&quot;joblib backend &#39;{backend}&#39; is not available on &quot;</span>
<span class="gi">+                        f&quot;your system, falling back to {DEFAULT_BACKEND}.&quot;,</span>
<span class="gi">+                        UserWarning,</span>
<span class="gi">+                        stacklevel=2</span>
<span class="gi">+                    )</span>
<span class="gi">+                    BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]</span>
<span class="gi">+                else:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;Invalid backend: {backend}, expected one of &quot;</span>
<span class="gi">+                        f&quot;{sorted(BACKENDS.keys())}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+            backend = BACKENDS[backend](**backend_params)</span>
<span class="gi">+</span>
<span class="gi">+        if inner_max_num_threads is not None:</span>
<span class="gi">+            msg = (</span>
<span class="gi">+                f&quot;{backend.__class__.__name__} does not accept setting the &quot;</span>
<span class="gi">+                &quot;inner_max_num_threads argument.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            assert backend.supports_inner_max_num_threads, msg</span>
<span class="gi">+            backend.inner_max_num_threads = inner_max_num_threads</span>
<span class="gi">+</span>
<span class="gi">+        # If the nesting_level of the backend is not set previously, use the</span>
<span class="gi">+        # nesting level from the previous active_backend to set it</span>
<span class="gi">+        if backend.nesting_level is None:</span>
<span class="gi">+            parent_backend = self.old_parallel_config[&#39;backend&#39;]</span>
<span class="gi">+            if parent_backend is default_parallel_config[&#39;backend&#39;]:</span>
<span class="gi">+                nesting_level = 0</span>
<span class="gi">+            else:</span>
<span class="gi">+                nesting_level = parent_backend.nesting_level</span>
<span class="gi">+            backend.nesting_level = nesting_level</span>
<span class="gi">+</span>
<span class="gi">+        return backend</span>

<span class="w"> </span>    def __enter__(self):
<span class="w"> </span>        return self.parallel_config
<span class="gu">@@ -242,6 +452,9 @@ class parallel_config:</span>
<span class="w"> </span>    def __exit__(self, type, value, traceback):
<span class="w"> </span>        self.unregister()

<span class="gi">+    def unregister(self):</span>
<span class="gi">+        setattr(_backend, &quot;config&quot;, self.old_parallel_config)</span>
<span class="gi">+</span>

<span class="w"> </span>class parallel_backend(parallel_config):
<span class="w"> </span>    &quot;&quot;&quot;Change the default backend used by Parallel inside a with block.
<span class="gu">@@ -324,23 +537,37 @@ class parallel_backend(parallel_config):</span>
<span class="w"> </span>    joblib.parallel_config: context manager to change the backend
<span class="w"> </span>        configuration.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+    def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None,</span>
<span class="gi">+                 **backend_params):</span>
<span class="gi">+</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            backend=backend,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+            inner_max_num_threads=inner_max_num_threads,</span>
<span class="gi">+            **backend_params</span>
<span class="gi">+        )</span>

<span class="gd">-    def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None, **</span>
<span class="gd">-        backend_params):</span>
<span class="gd">-        super().__init__(backend=backend, n_jobs=n_jobs,</span>
<span class="gd">-            inner_max_num_threads=inner_max_num_threads, **backend_params)</span>
<span class="w"> </span>        if self.old_parallel_config is None:
<span class="w"> </span>            self.old_backend_and_jobs = None
<span class="w"> </span>        else:
<span class="gd">-            self.old_backend_and_jobs = self.old_parallel_config[&#39;backend&#39;</span>
<span class="gd">-                ], self.old_parallel_config[&#39;n_jobs&#39;]</span>
<span class="gd">-        self.new_backend_and_jobs = self.parallel_config[&#39;backend&#39;</span>
<span class="gd">-            ], self.parallel_config[&#39;n_jobs&#39;]</span>
<span class="gi">+            self.old_backend_and_jobs = (</span>
<span class="gi">+                self.old_parallel_config[&quot;backend&quot;],</span>
<span class="gi">+                self.old_parallel_config[&quot;n_jobs&quot;],</span>
<span class="gi">+            )</span>
<span class="gi">+        self.new_backend_and_jobs = (</span>
<span class="gi">+            self.parallel_config[&quot;backend&quot;],</span>
<span class="gi">+            self.parallel_config[&quot;n_jobs&quot;],</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __enter__(self):
<span class="w"> </span>        return self.new_backend_and_jobs


<span class="gi">+# Under Linux or OS X the default start method of multiprocessing</span>
<span class="gi">+# can cause third party libraries to crash. Under Python 3.4+ it is possible</span>
<span class="gi">+# to set an environment variable to switch the default start method from</span>
<span class="gi">+# &#39;fork&#39; to &#39;forkserver&#39; or &#39;spawn&#39; to avoid this issue albeit at the cost</span>
<span class="gi">+# of causing semantic changes and some additional pool instantiation overhead.</span>
<span class="w"> </span>DEFAULT_MP_CONTEXT = None
<span class="w"> </span>if hasattr(mp, &#39;get_context&#39;):
<span class="w"> </span>    method = os.environ.get(&#39;JOBLIB_START_METHOD&#39;, &#39;&#39;).strip() or None
<span class="gu">@@ -351,36 +578,49 @@ if hasattr(mp, &#39;get_context&#39;):</span>
<span class="w"> </span>class BatchedCalls(object):
<span class="w"> </span>    &quot;&quot;&quot;Wrap a sequence of (func, args, kwargs) tuples as a single callable&quot;&quot;&quot;

<span class="gd">-    def __init__(self, iterator_slice, backend_and_jobs, reducer_callback=</span>
<span class="gd">-        None, pickle_cache=None):</span>
<span class="gi">+    def __init__(self, iterator_slice, backend_and_jobs, reducer_callback=None,</span>
<span class="gi">+                 pickle_cache=None):</span>
<span class="w"> </span>        self.items = list(iterator_slice)
<span class="w"> </span>        self._size = len(self.items)
<span class="w"> </span>        self._reducer_callback = reducer_callback
<span class="w"> </span>        if isinstance(backend_and_jobs, tuple):
<span class="w"> </span>            self._backend, self._n_jobs = backend_and_jobs
<span class="w"> </span>        else:
<span class="gi">+            # this is for backward compatibility purposes. Before 0.12.6,</span>
<span class="gi">+            # nested backends were returned without n_jobs indications.</span>
<span class="w"> </span>            self._backend, self._n_jobs = backend_and_jobs, None
<span class="w"> </span>        self._pickle_cache = pickle_cache if pickle_cache is not None else {}

<span class="w"> </span>    def __call__(self):
<span class="gi">+        # Set the default nested backend to self._backend but do not set the</span>
<span class="gi">+        # change the default number of processes to -1</span>
<span class="w"> </span>        with parallel_config(backend=self._backend, n_jobs=self._n_jobs):
<span class="gd">-            return [func(*args, **kwargs) for func, args, kwargs in self.items]</span>
<span class="gi">+            return [func(*args, **kwargs)</span>
<span class="gi">+                    for func, args, kwargs in self.items]</span>

<span class="w"> </span>    def __reduce__(self):
<span class="w"> </span>        if self._reducer_callback is not None:
<span class="w"> </span>            self._reducer_callback()
<span class="gd">-        return BatchedCalls, (self.items, (self._backend, self._n_jobs),</span>
<span class="gd">-            None, self._pickle_cache)</span>
<span class="gi">+        # no need to pickle the callback.</span>
<span class="gi">+        return (</span>
<span class="gi">+            BatchedCalls,</span>
<span class="gi">+            (self.items, (self._backend, self._n_jobs), None,</span>
<span class="gi">+             self._pickle_cache)</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def __len__(self):
<span class="w"> </span>        return self._size


<span class="gd">-TASK_DONE = &#39;Done&#39;</span>
<span class="gd">-TASK_ERROR = &#39;Error&#39;</span>
<span class="gd">-TASK_PENDING = &#39;Pending&#39;</span>
<span class="gi">+# Possible exit status for a task</span>
<span class="gi">+TASK_DONE = &quot;Done&quot;</span>
<span class="gi">+TASK_ERROR = &quot;Error&quot;</span>
<span class="gi">+TASK_PENDING = &quot;Pending&quot;</span>


<span class="gi">+###############################################################################</span>
<span class="gi">+# CPU count that works also when multiprocessing has been disabled via</span>
<span class="gi">+# the JOBLIB_MULTIPROCESSING environment variable</span>
<span class="w"> </span>def cpu_count(only_physical_cores=False):
<span class="w"> </span>    &quot;&quot;&quot;Return the number of CPUs.

<span class="gu">@@ -392,8 +632,14 @@ def cpu_count(only_physical_cores=False):</span>
<span class="w"> </span>    If only_physical_cores is True, do not take hyperthreading / SMT logical
<span class="w"> </span>    cores into account.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if mp is None:</span>
<span class="gi">+        return 1</span>
<span class="gi">+</span>
<span class="gi">+    return loky.cpu_count(only_physical_cores=only_physical_cores)</span>
<span class="gi">+</span>

<span class="gi">+###############################################################################</span>
<span class="gi">+# For verbosity</span>

<span class="w"> </span>def _verbosity_filter(index, verbose):
<span class="w"> </span>    &quot;&quot;&quot; Returns False for indices increasingly apart, the distance
<span class="gu">@@ -401,14 +647,32 @@ def _verbosity_filter(index, verbose):</span>

<span class="w"> </span>        We use a lag increasing as the square of index
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gi">+    if not verbose:</span>
<span class="gi">+        return True</span>
<span class="gi">+    elif verbose &gt; 10:</span>
<span class="gi">+        return False</span>
<span class="gi">+    if index == 0:</span>
<span class="gi">+        return False</span>
<span class="gi">+    verbose = .5 * (11 - verbose) ** 2</span>
<span class="gi">+    scale = sqrt(index / verbose)</span>
<span class="gi">+    next_scale = sqrt((index + 1) / verbose)</span>
<span class="gi">+    return (int(next_scale) == int(scale))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>def delayed(function):
<span class="w"> </span>    &quot;&quot;&quot;Decorator used to capture the arguments of a function.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def delayed_function(*args, **kwargs):</span>
<span class="gi">+        return function, args, kwargs</span>
<span class="gi">+    try:</span>
<span class="gi">+        delayed_function = functools.wraps(function)(delayed_function)</span>
<span class="gi">+    except AttributeError:</span>
<span class="gi">+        &quot; functools.wraps fails on some callable objects &quot;</span>
<span class="gi">+    return delayed_function</span>


<span class="gi">+###############################################################################</span>
<span class="w"> </span>class BatchCompletionCallBack(object):
<span class="w"> </span>    &quot;&quot;&quot;Callback to keep track of completed results and schedule the next tasks.

<span class="gu">@@ -424,20 +688,35 @@ class BatchCompletionCallBack(object):</span>
<span class="w"> </span>    failure.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    ##########################################################################</span>
<span class="gi">+    #                   METHODS CALLED BY THE MAIN THREAD                    #</span>
<span class="gi">+    ##########################################################################</span>
<span class="w"> </span>    def __init__(self, dispatch_timestamp, batch_size, parallel):
<span class="w"> </span>        self.dispatch_timestamp = dispatch_timestamp
<span class="w"> </span>        self.batch_size = batch_size
<span class="w"> </span>        self.parallel = parallel
<span class="w"> </span>        self.parallel_call_id = parallel._call_id
<span class="gi">+</span>
<span class="gi">+        # Internals to keep track of the status and outcome of the task.</span>
<span class="gi">+</span>
<span class="gi">+        # Used to hold a reference to the future-like object returned by the</span>
<span class="gi">+        # backend after launching this task</span>
<span class="gi">+        # This will be set later when calling `register_job`, as it is only</span>
<span class="gi">+        # created once the task has been submitted.</span>
<span class="w"> </span>        self.job = None
<span class="gi">+</span>
<span class="w"> </span>        if not parallel._backend.supports_retrieve_callback:
<span class="gi">+            # The status is only used for asynchronous result retrieval in the</span>
<span class="gi">+            # callback.</span>
<span class="w"> </span>            self.status = None
<span class="w"> </span>        else:
<span class="gi">+            # The initial status for the job is TASK_PENDING.</span>
<span class="gi">+            # Once it is done, it will be either TASK_DONE, or TASK_ERROR.</span>
<span class="w"> </span>            self.status = TASK_PENDING

<span class="w"> </span>    def register_job(self, job):
<span class="w"> </span>        &quot;&quot;&quot;Register the object returned by `apply_async`.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.job = job</span>

<span class="w"> </span>    def get_result(self, timeout):
<span class="w"> </span>        &quot;&quot;&quot;Returns the raw result of the task that was submitted.
<span class="gu">@@ -456,7 +735,35 @@ class BatchCompletionCallBack(object):</span>
<span class="w"> </span>        return it or raise. It will block at most `self.timeout` seconds
<span class="w"> </span>        waiting for retrieval to complete, after that it raises a TimeoutError.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        backend = self.parallel._backend</span>
<span class="gi">+</span>
<span class="gi">+        if backend.supports_retrieve_callback:</span>
<span class="gi">+            # We assume that the result has already been retrieved by the</span>
<span class="gi">+            # callback thread, and is stored internally. It&#39;s just waiting to</span>
<span class="gi">+            # be returned.</span>
<span class="gi">+            return self._return_or_raise()</span>
<span class="gi">+</span>
<span class="gi">+        # For other backends, the main thread needs to run the retrieval step.</span>
<span class="gi">+        try:</span>
<span class="gi">+            if backend.supports_timeout:</span>
<span class="gi">+                result = self.job.get(timeout=timeout)</span>
<span class="gi">+            else:</span>
<span class="gi">+                result = self.job.get()</span>
<span class="gi">+            outcome = dict(result=result, status=TASK_DONE)</span>
<span class="gi">+        except BaseException as e:</span>
<span class="gi">+            outcome = dict(result=e, status=TASK_ERROR)</span>
<span class="gi">+        self._register_outcome(outcome)</span>
<span class="gi">+</span>
<span class="gi">+        return self._return_or_raise()</span>
<span class="gi">+</span>
<span class="gi">+    def _return_or_raise(self):</span>
<span class="gi">+        try:</span>
<span class="gi">+            if self.status == TASK_ERROR:</span>
<span class="gi">+                raise self._result</span>
<span class="gi">+            return self._result</span>
<span class="gi">+        finally:</span>
<span class="gi">+            del self._result</span>

<span class="w"> </span>    def get_status(self, timeout):
<span class="w"> </span>        &quot;&quot;&quot;Get the status of the task.
<span class="gu">@@ -464,27 +771,76 @@ class BatchCompletionCallBack(object):</span>
<span class="w"> </span>        This function also checks if the timeout has been reached and register
<span class="w"> </span>        the TimeoutError outcome when it is the case.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if timeout is None or self.status != TASK_PENDING:</span>
<span class="gi">+            return self.status</span>
<span class="gi">+</span>
<span class="gi">+        # The computation are running and the status is pending.</span>
<span class="gi">+        # Check that we did not wait for this jobs more than `timeout`.</span>
<span class="gi">+        now = time.time()</span>
<span class="gi">+        if not hasattr(self, &quot;_completion_timeout_counter&quot;):</span>
<span class="gi">+            self._completion_timeout_counter = now</span>
<span class="gi">+</span>
<span class="gi">+        if (now - self._completion_timeout_counter) &gt; timeout:</span>
<span class="gi">+            outcome = dict(result=TimeoutError(), status=TASK_ERROR)</span>
<span class="gi">+            self._register_outcome(outcome)</span>
<span class="gi">+</span>
<span class="gi">+        return self.status</span>

<span class="gi">+    ##########################################################################</span>
<span class="gi">+    #                     METHODS CALLED BY CALLBACK THREADS                 #</span>
<span class="gi">+    ##########################################################################</span>
<span class="w"> </span>    def __call__(self, out):
<span class="w"> </span>        &quot;&quot;&quot;Function called by the callback thread after a job is completed.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+        # If the backend doesn&#39;t support callback retrievals, the next batch of</span>
<span class="gi">+        # tasks is dispatched regardless. The result will be retrieved by the</span>
<span class="gi">+        # main thread when calling `get_result`.</span>
<span class="w"> </span>        if not self.parallel._backend.supports_retrieve_callback:
<span class="w"> </span>            self._dispatch_new()
<span class="w"> </span>            return
<span class="gi">+</span>
<span class="gi">+        # If the backend supports retrieving the result in the callback, it</span>
<span class="gi">+        # registers the task outcome (TASK_ERROR or TASK_DONE), and schedules</span>
<span class="gi">+        # the next batch if needed.</span>
<span class="w"> </span>        with self.parallel._lock:
<span class="gi">+            # Edge case where while the task was processing, the `parallel`</span>
<span class="gi">+            # instance has been reset and a new call has been issued, but the</span>
<span class="gi">+            # worker managed to complete the task and trigger this callback</span>
<span class="gi">+            # call just before being aborted by the reset.</span>
<span class="w"> </span>            if self.parallel._call_id != self.parallel_call_id:
<span class="w"> </span>                return
<span class="gi">+</span>
<span class="gi">+            # When aborting, stop as fast as possible and do not retrieve the</span>
<span class="gi">+            # result as it won&#39;t be returned by the Parallel call.</span>
<span class="w"> </span>            if self.parallel._aborting:
<span class="w"> </span>                return
<span class="gi">+</span>
<span class="gi">+            # Retrieves the result of the task in the main process and dispatch</span>
<span class="gi">+            # a new batch if needed.</span>
<span class="w"> </span>            job_succeeded = self._retrieve_result(out)
<span class="gi">+</span>
<span class="w"> </span>            if not self.parallel.return_ordered:
<span class="gi">+                # Append the job to the queue in the order of completion</span>
<span class="gi">+                # instead of submission.</span>
<span class="w"> </span>                self.parallel._jobs.append(self)
<span class="gi">+</span>
<span class="w"> </span>        if job_succeeded:
<span class="w"> </span>            self._dispatch_new()

<span class="w"> </span>    def _dispatch_new(self):
<span class="w"> </span>        &quot;&quot;&quot;Schedule the next batch of tasks to be processed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # This steps ensure that auto-batching works as expected.</span>
<span class="gi">+        this_batch_duration = time.time() - self.dispatch_timestamp</span>
<span class="gi">+        self.parallel._backend.batch_completed(self.batch_size,</span>
<span class="gi">+                                               this_batch_duration)</span>
<span class="gi">+</span>
<span class="gi">+        # Schedule the next batch of tasks.</span>
<span class="gi">+        with self.parallel._lock:</span>
<span class="gi">+            self.parallel.n_completed_tasks += self.batch_size</span>
<span class="gi">+            self.parallel.print_progress()</span>
<span class="gi">+            if self.parallel._original_iterator is not None:</span>
<span class="gi">+                self.parallel.dispatch_next()</span>

<span class="w"> </span>    def _retrieve_result(self, out):
<span class="w"> </span>        &quot;&quot;&quot;Fetch and register the outcome of a task.
<span class="gu">@@ -493,16 +849,48 @@ class BatchCompletionCallBack(object):</span>
<span class="w"> </span>        This function is only called by backends that support retrieving
<span class="w"> </span>        the task result in the callback thread.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        try:</span>
<span class="gi">+            result = self.parallel._backend.retrieve_result_callback(out)</span>
<span class="gi">+            outcome = dict(status=TASK_DONE, result=result)</span>
<span class="gi">+        except BaseException as e:</span>
<span class="gi">+            # Avoid keeping references to parallel in the error.</span>
<span class="gi">+            e.__traceback__ = None</span>
<span class="gi">+            outcome = dict(result=e, status=TASK_ERROR)</span>
<span class="gi">+</span>
<span class="gi">+        self._register_outcome(outcome)</span>
<span class="gi">+        return outcome[&#39;status&#39;] != TASK_ERROR</span>
<span class="gi">+</span>
<span class="gi">+    ##########################################################################</span>
<span class="gi">+    #            This method can be called either in the main thread         #</span>
<span class="gi">+    #                        or in the callback thread.                      #</span>
<span class="gi">+    ##########################################################################</span>
<span class="w"> </span>    def _register_outcome(self, outcome):
<span class="w"> </span>        &quot;&quot;&quot;Register the outcome of a task.

<span class="w"> </span>        This method can be called only once, future calls will be ignored.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Covers the edge case where the main thread tries to register a</span>
<span class="gi">+        # `TimeoutError` while the callback thread tries to register a result</span>
<span class="gi">+        # at the same time.</span>
<span class="gi">+        with self.parallel._lock:</span>
<span class="gi">+            if self.status not in (TASK_PENDING, None):</span>
<span class="gi">+                return</span>
<span class="gi">+            self.status = outcome[&quot;status&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        self._result = outcome[&quot;result&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        # Once the result and the status are extracted, the last reference to</span>
<span class="gi">+        # the job can be deleted.</span>
<span class="gi">+        self.job = None</span>
<span class="gi">+</span>
<span class="gi">+        # As soon as an error as been spotted, early stopping flags are sent to</span>
<span class="gi">+        # the `parallel` instance.</span>
<span class="gi">+        if self.status == TASK_ERROR:</span>
<span class="gi">+            self.parallel._exception = True</span>
<span class="gi">+            self.parallel._aborting = True</span>


<span class="gi">+###############################################################################</span>
<span class="w"> </span>def register_parallel_backend(name, factory, make_default=False):
<span class="w"> </span>    &quot;&quot;&quot;Register a new Parallel backend factory.

<span class="gu">@@ -518,7 +906,10 @@ def register_parallel_backend(name, factory, make_default=False):</span>

<span class="w"> </span>    .. versionadded:: 0.10
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    BACKENDS[name] = factory</span>
<span class="gi">+    if make_default:</span>
<span class="gi">+        global DEFAULT_BACKEND</span>
<span class="gi">+        DEFAULT_BACKEND = name</span>


<span class="w"> </span>def effective_n_jobs(n_jobs=-1):
<span class="gu">@@ -542,11 +933,18 @@ def effective_n_jobs(n_jobs=-1):</span>

<span class="w"> </span>    .. versionadded:: 0.10
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if n_jobs == 1:</span>
<span class="gi">+        return 1</span>

<span class="gi">+    backend, backend_n_jobs = get_active_backend()</span>
<span class="gi">+    if n_jobs is None:</span>
<span class="gi">+        n_jobs = backend_n_jobs</span>
<span class="gi">+    return backend.effective_n_jobs(n_jobs=n_jobs)</span>

<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="w"> </span>class Parallel(Logger):
<span class="gd">-    &quot;&quot;&quot; Helper class for readable parallel mapping.</span>
<span class="gi">+    &#39;&#39;&#39; Helper class for readable parallel mapping.</span>

<span class="w"> </span>        Read more in the :ref:`User Guide &lt;parallel&gt;`.

<span class="gu">@@ -795,95 +1193,148 @@ class Parallel(Logger):</span>
<span class="w"> </span>        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s
<span class="w"> </span>        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished

<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, n_jobs=default_parallel_config[&#39;n_jobs&#39;], backend=</span>
<span class="gd">-        default_parallel_config[&#39;backend&#39;], return_as=&#39;list&#39;, verbose=</span>
<span class="gd">-        default_parallel_config[&#39;verbose&#39;], timeout=None, pre_dispatch=</span>
<span class="gd">-        &#39;2 * n_jobs&#39;, batch_size=&#39;auto&#39;, temp_folder=</span>
<span class="gd">-        default_parallel_config[&#39;temp_folder&#39;], max_nbytes=</span>
<span class="gd">-        default_parallel_config[&#39;max_nbytes&#39;], mmap_mode=</span>
<span class="gd">-        default_parallel_config[&#39;mmap_mode&#39;], prefer=</span>
<span class="gd">-        default_parallel_config[&#39;prefer&#39;], require=default_parallel_config[</span>
<span class="gd">-        &#39;require&#39;]):</span>
<span class="gi">+    &#39;&#39;&#39;  # noqa: E501</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        n_jobs=default_parallel_config[&quot;n_jobs&quot;],</span>
<span class="gi">+        backend=default_parallel_config[&#39;backend&#39;],</span>
<span class="gi">+        return_as=&quot;list&quot;,</span>
<span class="gi">+        verbose=default_parallel_config[&quot;verbose&quot;],</span>
<span class="gi">+        timeout=None,</span>
<span class="gi">+        pre_dispatch=&#39;2 * n_jobs&#39;,</span>
<span class="gi">+        batch_size=&#39;auto&#39;,</span>
<span class="gi">+        temp_folder=default_parallel_config[&quot;temp_folder&quot;],</span>
<span class="gi">+        max_nbytes=default_parallel_config[&quot;max_nbytes&quot;],</span>
<span class="gi">+        mmap_mode=default_parallel_config[&quot;mmap_mode&quot;],</span>
<span class="gi">+        prefer=default_parallel_config[&quot;prefer&quot;],</span>
<span class="gi">+        require=default_parallel_config[&quot;require&quot;],</span>
<span class="gi">+    ):</span>
<span class="gi">+        # Initiate parent Logger class state</span>
<span class="w"> </span>        super().__init__()
<span class="gi">+</span>
<span class="gi">+        # Interpret n_jobs=None as &#39;unset&#39;</span>
<span class="w"> </span>        if n_jobs is None:
<span class="gd">-            n_jobs = default_parallel_config[&#39;n_jobs&#39;]</span>
<span class="gd">-        active_backend, context_config = _get_active_backend(prefer=prefer,</span>
<span class="gd">-            require=require, verbose=verbose)</span>
<span class="gi">+            n_jobs = default_parallel_config[&quot;n_jobs&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        active_backend, context_config = _get_active_backend(</span>
<span class="gi">+            prefer=prefer, require=require, verbose=verbose</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>        nesting_level = active_backend.nesting_level
<span class="gd">-        self.verbose = _get_config_param(verbose, context_config, &#39;verbose&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        self.verbose = _get_config_param(verbose, context_config, &quot;verbose&quot;)</span>
<span class="w"> </span>        self.timeout = timeout
<span class="w"> </span>        self.pre_dispatch = pre_dispatch
<span class="gd">-        if return_as not in {&#39;list&#39;, &#39;generator&#39;, &#39;generator_unordered&#39;}:</span>
<span class="gi">+</span>
<span class="gi">+        if return_as not in {&quot;list&quot;, &quot;generator&quot;, &quot;generator_unordered&quot;}:</span>
<span class="w"> </span>            raise ValueError(
<span class="gd">-                f&#39;Expected `return_as` parameter to be a string equal to &quot;list&quot;,&quot;generator&quot; or &quot;generator_unordered&quot;, but got {return_as} instead.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                &#39;Expected `return_as` parameter to be a string equal to &quot;list&quot;&#39;</span>
<span class="gi">+                f&#39;,&quot;generator&quot; or &quot;generator_unordered&quot;, but got {return_as} &#39;</span>
<span class="gi">+                &quot;instead.&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>        self.return_as = return_as
<span class="gd">-        self.return_generator = return_as != &#39;list&#39;</span>
<span class="gd">-        self.return_ordered = return_as != &#39;generator_unordered&#39;</span>
<span class="gd">-        self._backend_args = {k: _get_config_param(param, context_config, k</span>
<span class="gd">-            ) for param, k in [(max_nbytes, &#39;max_nbytes&#39;), (temp_folder,</span>
<span class="gd">-            &#39;temp_folder&#39;), (mmap_mode, &#39;mmap_mode&#39;), (prefer, &#39;prefer&#39;), (</span>
<span class="gd">-            require, &#39;require&#39;), (verbose, &#39;verbose&#39;)]}</span>
<span class="gd">-        if isinstance(self._backend_args[&#39;max_nbytes&#39;], str):</span>
<span class="gd">-            self._backend_args[&#39;max_nbytes&#39;] = memstr_to_bytes(self.</span>
<span class="gd">-                _backend_args[&#39;max_nbytes&#39;])</span>
<span class="gd">-        self._backend_args[&#39;verbose&#39;] = max(0, self._backend_args[&#39;verbose&#39;</span>
<span class="gd">-            ] - 50)</span>
<span class="gi">+        self.return_generator = return_as != &quot;list&quot;</span>
<span class="gi">+        self.return_ordered = return_as != &quot;generator_unordered&quot;</span>
<span class="gi">+</span>
<span class="gi">+        # Check if we are under a parallel_config or parallel_backend</span>
<span class="gi">+        # context manager and use the config from the context manager</span>
<span class="gi">+        # for arguments that are not explicitly set.</span>
<span class="gi">+        self._backend_args = {</span>
<span class="gi">+            k: _get_config_param(param, context_config, k) for param, k in [</span>
<span class="gi">+                (max_nbytes, &quot;max_nbytes&quot;),</span>
<span class="gi">+                (temp_folder, &quot;temp_folder&quot;),</span>
<span class="gi">+                (mmap_mode, &quot;mmap_mode&quot;),</span>
<span class="gi">+                (prefer, &quot;prefer&quot;),</span>
<span class="gi">+                (require, &quot;require&quot;),</span>
<span class="gi">+                (verbose, &quot;verbose&quot;),</span>
<span class="gi">+            ]</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self._backend_args[&quot;max_nbytes&quot;], str):</span>
<span class="gi">+            self._backend_args[&quot;max_nbytes&quot;] = memstr_to_bytes(</span>
<span class="gi">+                self._backend_args[&quot;max_nbytes&quot;]</span>
<span class="gi">+            )</span>
<span class="gi">+        self._backend_args[&quot;verbose&quot;] = max(</span>
<span class="gi">+            0, self._backend_args[&quot;verbose&quot;] - 50</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>        if DEFAULT_MP_CONTEXT is not None:
<span class="w"> </span>            self._backend_args[&#39;context&#39;] = DEFAULT_MP_CONTEXT
<span class="gd">-        elif hasattr(mp, &#39;get_context&#39;):</span>
<span class="gi">+        elif hasattr(mp, &quot;get_context&quot;):</span>
<span class="w"> </span>            self._backend_args[&#39;context&#39;] = mp.get_context()
<span class="gi">+</span>
<span class="w"> </span>        if backend is default_parallel_config[&#39;backend&#39;] or backend is None:
<span class="w"> </span>            backend = active_backend
<span class="gi">+</span>
<span class="w"> </span>        elif isinstance(backend, ParallelBackendBase):
<span class="gi">+            # Use provided backend as is, with the current nesting_level if it</span>
<span class="gi">+            # is not set yet.</span>
<span class="w"> </span>            if backend.nesting_level is None:
<span class="w"> </span>                backend.nesting_level = nesting_level
<span class="gi">+</span>
<span class="w"> </span>        elif hasattr(backend, &#39;Pool&#39;) and hasattr(backend, &#39;Lock&#39;):
<span class="gi">+            # Make it possible to pass a custom multiprocessing context as</span>
<span class="gi">+            # backend to change the start method to forkserver or spawn or</span>
<span class="gi">+            # preload modules on the forkserver helper process.</span>
<span class="w"> </span>            self._backend_args[&#39;context&#39;] = backend
<span class="w"> </span>            backend = MultiprocessingBackend(nesting_level=nesting_level)
<span class="gi">+</span>
<span class="w"> </span>        elif backend not in BACKENDS and backend in MAYBE_AVAILABLE_BACKENDS:
<span class="w"> </span>            warnings.warn(
<span class="gd">-                f&quot;joblib backend &#39;{backend}&#39; is not available on your system, falling back to {DEFAULT_BACKEND}.&quot;</span>
<span class="gd">-                , UserWarning, stacklevel=2)</span>
<span class="gi">+                f&quot;joblib backend &#39;{backend}&#39; is not available on &quot;</span>
<span class="gi">+                f&quot;your system, falling back to {DEFAULT_BACKEND}.&quot;,</span>
<span class="gi">+                UserWarning,</span>
<span class="gi">+                stacklevel=2)</span>
<span class="w"> </span>            BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]
<span class="w"> </span>            backend = BACKENDS[DEFAULT_BACKEND](nesting_level=nesting_level)
<span class="gi">+</span>
<span class="w"> </span>        else:
<span class="w"> </span>            try:
<span class="w"> </span>                backend_factory = BACKENDS[backend]
<span class="w"> </span>            except KeyError as e:
<span class="gd">-                raise ValueError(&#39;Invalid backend: %s, expected one of %r&#39; %</span>
<span class="gd">-                    (backend, sorted(BACKENDS.keys()))) from e</span>
<span class="gi">+                raise ValueError(&quot;Invalid backend: %s, expected one of %r&quot;</span>
<span class="gi">+                                 % (backend, sorted(BACKENDS.keys()))) from e</span>
<span class="w"> </span>            backend = backend_factory(nesting_level=nesting_level)
<span class="gd">-        n_jobs = _get_config_param(n_jobs, context_config, &#39;n_jobs&#39;)</span>
<span class="gi">+</span>
<span class="gi">+        n_jobs = _get_config_param(n_jobs, context_config, &quot;n_jobs&quot;)</span>
<span class="w"> </span>        if n_jobs is None:
<span class="gi">+            # No specific context override and no specific value request:</span>
<span class="gi">+            # default to the default of the backend.</span>
<span class="w"> </span>            n_jobs = backend.default_n_jobs
<span class="w"> </span>        try:
<span class="w"> </span>            n_jobs = int(n_jobs)
<span class="w"> </span>        except ValueError:
<span class="gd">-            raise ValueError(&#39;n_jobs could not be converted to int&#39;)</span>
<span class="gi">+            raise ValueError(&quot;n_jobs could not be converted to int&quot;)</span>
<span class="w"> </span>        self.n_jobs = n_jobs
<span class="gd">-        if require == &#39;sharedmem&#39; and not getattr(backend,</span>
<span class="gd">-            &#39;supports_sharedmem&#39;, False):</span>
<span class="gd">-            raise ValueError(&#39;Backend %s does not support shared memory&#39; %</span>
<span class="gd">-                backend)</span>
<span class="gd">-        if batch_size == &#39;auto&#39; or isinstance(batch_size, Integral</span>
<span class="gd">-            ) and batch_size &gt; 0:</span>
<span class="gi">+</span>
<span class="gi">+        if (require == &#39;sharedmem&#39; and</span>
<span class="gi">+                not getattr(backend, &#39;supports_sharedmem&#39;, False)):</span>
<span class="gi">+            raise ValueError(&quot;Backend %s does not support shared memory&quot;</span>
<span class="gi">+                             % backend)</span>
<span class="gi">+</span>
<span class="gi">+        if (batch_size == &#39;auto&#39; or isinstance(batch_size, Integral) and</span>
<span class="gi">+                batch_size &gt; 0):</span>
<span class="w"> </span>            self.batch_size = batch_size
<span class="w"> </span>        else:
<span class="w"> </span>            raise ValueError(
<span class="gd">-                &quot;batch_size must be &#39;auto&#39; or a positive integer, got: %r&quot; %</span>
<span class="gd">-                batch_size)</span>
<span class="gi">+                &quot;batch_size must be &#39;auto&#39; or a positive integer, got: %r&quot;</span>
<span class="gi">+                % batch_size)</span>
<span class="gi">+</span>
<span class="w"> </span>        if not isinstance(backend, SequentialBackend):
<span class="w"> </span>            if self.return_generator and not backend.supports_return_generator:
<span class="gd">-                raise ValueError(&#39;Backend {} does not support return_as={}&#39;</span>
<span class="gd">-                    .format(backend, return_as))</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;Backend {} does not support &quot;</span>
<span class="gi">+                    &quot;return_as={}&quot;.format(backend, return_as)</span>
<span class="gi">+                )</span>
<span class="gi">+            # This lock is used to coordinate the main thread of this process</span>
<span class="gi">+            # with the async callback thread of our the pool.</span>
<span class="w"> </span>            self._lock = threading.RLock()
<span class="w"> </span>            self._jobs = collections.deque()
<span class="w"> </span>            self._pending_outputs = list()
<span class="w"> </span>            self._ready_batches = queue.Queue()
<span class="w"> </span>            self._reducer_callback = None
<span class="gi">+</span>
<span class="gi">+        # Internal variables</span>
<span class="w"> </span>        self._backend = backend
<span class="w"> </span>        self._running = False
<span class="w"> </span>        self._managed_backend = False
<span class="gu">@@ -904,7 +1355,35 @@ class Parallel(Logger):</span>

<span class="w"> </span>    def _initialize_backend(self):
<span class="w"> </span>        &quot;&quot;&quot;Build a process or thread pool and return the number of workers&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,</span>
<span class="gi">+                                             **self._backend_args)</span>
<span class="gi">+            if self.timeout is not None and not self._backend.supports_timeout:</span>
<span class="gi">+                warnings.warn(</span>
<span class="gi">+                    &#39;The backend class {!r} does not support timeout. &#39;</span>
<span class="gi">+                    &quot;You have set &#39;timeout={}&#39; in Parallel but &quot;</span>
<span class="gi">+                    &quot;the &#39;timeout&#39; parameter will not be used.&quot;.format(</span>
<span class="gi">+                        self._backend.__class__.__name__,</span>
<span class="gi">+                        self.timeout))</span>
<span class="gi">+</span>
<span class="gi">+        except FallbackToBackend as e:</span>
<span class="gi">+            # Recursively initialize the backend in case of requested fallback.</span>
<span class="gi">+            self._backend = e.backend</span>
<span class="gi">+            n_jobs = self._initialize_backend()</span>
<span class="gi">+</span>
<span class="gi">+        return n_jobs</span>
<span class="gi">+</span>
<span class="gi">+    def _effective_n_jobs(self):</span>
<span class="gi">+        if self._backend:</span>
<span class="gi">+            return self._backend.effective_n_jobs(self.n_jobs)</span>
<span class="gi">+        return 1</span>
<span class="gi">+</span>
<span class="gi">+    def _terminate_and_reset(self):</span>
<span class="gi">+        if hasattr(self._backend, &#39;stop_call&#39;) and self._calling:</span>
<span class="gi">+            self._backend.stop_call()</span>
<span class="gi">+        self._calling = False</span>
<span class="gi">+        if not self._managed_backend:</span>
<span class="gi">+            self._backend.terminate()</span>

<span class="w"> </span>    def _dispatch(self, batch):
<span class="w"> </span>        &quot;&quot;&quot;Queue the batch for computing, with or without multiprocessing
<span class="gu">@@ -913,7 +1392,31 @@ class Parallel(Logger):</span>
<span class="w"> </span>        indirectly via dispatch_one_batch.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # If job.get() catches an exception, it closes the queue:</span>
<span class="gi">+        if self._aborting:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        batch_size = len(batch)</span>
<span class="gi">+</span>
<span class="gi">+        self.n_dispatched_tasks += batch_size</span>
<span class="gi">+        self.n_dispatched_batches += 1</span>
<span class="gi">+</span>
<span class="gi">+        dispatch_timestamp = time.time()</span>
<span class="gi">+</span>
<span class="gi">+        batch_tracker = BatchCompletionCallBack(</span>
<span class="gi">+            dispatch_timestamp, batch_size, self</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        if self.return_ordered:</span>
<span class="gi">+            self._jobs.append(batch_tracker)</span>
<span class="gi">+</span>
<span class="gi">+        # If return_ordered is False, the batch_tracker is not stored in the</span>
<span class="gi">+        # jobs queue at the time of submission. Instead, it will be appended to</span>
<span class="gi">+        # the queue by itself as soon as the callback is triggered to be able</span>
<span class="gi">+        # to return the results in the order of completion.</span>
<span class="gi">+</span>
<span class="gi">+        job = self._backend.apply_async(batch, callback=batch_tracker)</span>
<span class="gi">+        batch_tracker.register_job(job)</span>

<span class="w"> </span>    def dispatch_next(self):
<span class="w"> </span>        &quot;&quot;&quot;Dispatch more data for parallel processing
<span class="gu">@@ -923,7 +1426,9 @@ class Parallel(Logger):</span>
<span class="w"> </span>        against concurrent consumption of the unprotected iterator.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not self.dispatch_one_batch(self._original_iterator):</span>
<span class="gi">+            self._iterating = False</span>
<span class="gi">+            self._original_iterator = None</span>

<span class="w"> </span>    def dispatch_one_batch(self, iterator):
<span class="w"> </span>        &quot;&quot;&quot;Prefetch the tasks for the next batch and dispatch them.
<span class="gu">@@ -935,41 +1440,381 @@ class Parallel(Logger):</span>
<span class="w"> </span>        lock so calling this function should be thread safe.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        if self._aborting:</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        batch_size = self._get_batch_size()</span>
<span class="gi">+</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            # to ensure an even distribution of the workload between workers,</span>
<span class="gi">+            # we look ahead in the original iterators more than batch_size</span>
<span class="gi">+            # tasks - However, we keep consuming only one batch at each</span>
<span class="gi">+            # dispatch_one_batch call. The extra tasks are stored in a local</span>
<span class="gi">+            # queue, _ready_batches, that is looked-up prior to re-consuming</span>
<span class="gi">+            # tasks from the origal iterator.</span>
<span class="gi">+            try:</span>
<span class="gi">+                tasks = self._ready_batches.get(block=False)</span>
<span class="gi">+            except queue.Empty:</span>
<span class="gi">+                # slice the iterator n_jobs * batchsize items at a time. If the</span>
<span class="gi">+                # slice returns less than that, then the current batchsize puts</span>
<span class="gi">+                # too much weight on a subset of workers, while other may end</span>
<span class="gi">+                # up starving. So in this case, re-scale the batch size</span>
<span class="gi">+                # accordingly to distribute evenly the last items between all</span>
<span class="gi">+                # workers.</span>
<span class="gi">+                n_jobs = self._cached_effective_n_jobs</span>
<span class="gi">+                big_batch_size = batch_size * n_jobs</span>
<span class="gi">+</span>
<span class="gi">+                try:</span>
<span class="gi">+                    islice = list(itertools.islice(iterator, big_batch_size))</span>
<span class="gi">+                except Exception as e:</span>
<span class="gi">+                    # Handle the fact that the generator of task raised an</span>
<span class="gi">+                    # exception. As this part of the code can be executed in</span>
<span class="gi">+                    # a thread internal to the backend, register a task with</span>
<span class="gi">+                    # an error that will be raised in the user&#39;s thread.</span>
<span class="gi">+                    if isinstance(e.__context__, queue.Empty):</span>
<span class="gi">+                        # Suppress the cause of the exception if it is</span>
<span class="gi">+                        # queue.Empty to avoid cluttered traceback. Only do it</span>
<span class="gi">+                        # if the __context__ is really empty to avoid messing</span>
<span class="gi">+                        # with causes of the original error.</span>
<span class="gi">+                        e.__cause__ = None</span>
<span class="gi">+                    batch_tracker = BatchCompletionCallBack(</span>
<span class="gi">+                        0, batch_size, self</span>
<span class="gi">+                    )</span>
<span class="gi">+                    self._jobs.append(batch_tracker)</span>
<span class="gi">+                    batch_tracker._register_outcome(dict(</span>
<span class="gi">+                        result=e, status=TASK_ERROR</span>
<span class="gi">+                    ))</span>
<span class="gi">+                    return True</span>
<span class="gi">+</span>
<span class="gi">+                if len(islice) == 0:</span>
<span class="gi">+                    return False</span>
<span class="gi">+                elif (iterator is self._original_iterator and</span>
<span class="gi">+                      len(islice) &lt; big_batch_size):</span>
<span class="gi">+                    # We reached the end of the original iterator (unless</span>
<span class="gi">+                    # iterator is the ``pre_dispatch``-long initial slice of</span>
<span class="gi">+                    # the original iterator) -- decrease the batch size to</span>
<span class="gi">+                    # account for potential variance in the batches running</span>
<span class="gi">+                    # time.</span>
<span class="gi">+                    final_batch_size = max(1, len(islice) // (10 * n_jobs))</span>
<span class="gi">+                else:</span>
<span class="gi">+                    final_batch_size = max(1, len(islice) // n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+                # enqueue n_jobs batches in a local queue</span>
<span class="gi">+                for i in range(0, len(islice), final_batch_size):</span>
<span class="gi">+                    tasks = BatchedCalls(islice[i:i + final_batch_size],</span>
<span class="gi">+                                         self._backend.get_nested_backend(),</span>
<span class="gi">+                                         self._reducer_callback,</span>
<span class="gi">+                                         self._pickle_cache)</span>
<span class="gi">+                    self._ready_batches.put(tasks)</span>
<span class="gi">+</span>
<span class="gi">+                # finally, get one task.</span>
<span class="gi">+                tasks = self._ready_batches.get(block=False)</span>
<span class="gi">+            if len(tasks) == 0:</span>
<span class="gi">+                # No more tasks available in the iterator: tell caller to stop.</span>
<span class="gi">+                return False</span>
<span class="gi">+            else:</span>
<span class="gi">+                self._dispatch(tasks)</span>
<span class="gi">+                return True</span>

<span class="w"> </span>    def _get_batch_size(self):
<span class="w"> </span>        &quot;&quot;&quot;Returns the effective batch size for dispatch&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.batch_size == &#39;auto&#39;:</span>
<span class="gi">+            return self._backend.compute_batch_size()</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Fixed batch size strategy</span>
<span class="gi">+            return self.batch_size</span>

<span class="w"> </span>    def _print(self, msg):
<span class="w"> </span>        &quot;&quot;&quot;Display the message on stout or stderr depending on verbosity&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # XXX: Not using the logger framework: need to</span>
<span class="gi">+        # learn to use logger better.</span>
<span class="gi">+        if not self.verbose:</span>
<span class="gi">+            return</span>
<span class="gi">+        if self.verbose &lt; 50:</span>
<span class="gi">+            writer = sys.stderr.write</span>
<span class="gi">+        else:</span>
<span class="gi">+            writer = sys.stdout.write</span>
<span class="gi">+        writer(f&quot;[{self}]: {msg}\n&quot;)</span>

<span class="w"> </span>    def _is_completed(self):
<span class="w"> </span>        &quot;&quot;&quot;Check if all tasks have been completed&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.n_completed_tasks == self.n_dispatched_tasks and not (</span>
<span class="gi">+            self._iterating or self._aborting</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def print_progress(self):
<span class="w"> </span>        &quot;&quot;&quot;Display the process of the parallel execution only a fraction
<span class="w"> </span>           of time, controlled by self.verbose.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        if not self.verbose:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        elapsed_time = time.time() - self._start_time</span>
<span class="gi">+</span>
<span class="gi">+        if self._is_completed():</span>
<span class="gi">+            # Make sure that we get a last message telling us we are done</span>
<span class="gi">+            self._print(</span>
<span class="gi">+                f&quot;Done {self.n_completed_tasks:3d} out of &quot;</span>
<span class="gi">+                f&quot;{self.n_completed_tasks:3d} | elapsed: &quot;</span>
<span class="gi">+                f&quot;{short_format_time(elapsed_time)} finished&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        # Original job iterator becomes None once it has been fully</span>
<span class="gi">+        # consumed: at this point we know the total number of jobs and we are</span>
<span class="gi">+        # able to display an estimation of the remaining time based on already</span>
<span class="gi">+        # completed jobs. Otherwise, we simply display the number of completed</span>
<span class="gi">+        # tasks.</span>
<span class="gi">+        elif self._original_iterator is not None:</span>
<span class="gi">+            if _verbosity_filter(self.n_dispatched_batches, self.verbose):</span>
<span class="gi">+                return</span>
<span class="gi">+            self._print(</span>
<span class="gi">+                f&quot;Done {self.n_completed_tasks:3d} tasks      | elapsed: &quot;</span>
<span class="gi">+                f&quot;{short_format_time(elapsed_time)}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            index = self.n_completed_tasks</span>
<span class="gi">+            # We are finished dispatching</span>
<span class="gi">+            total_tasks = self.n_dispatched_tasks</span>
<span class="gi">+            # We always display the first loop</span>
<span class="gi">+            if not index == 0:</span>
<span class="gi">+                # Display depending on the number of remaining items</span>
<span class="gi">+                # A message as soon as we finish dispatching, cursor is 0</span>
<span class="gi">+                cursor = (total_tasks - index + 1 -</span>
<span class="gi">+                          self._pre_dispatch_amount)</span>
<span class="gi">+                frequency = (total_tasks // self.verbose) + 1</span>
<span class="gi">+                is_last_item = (index + 1 == total_tasks)</span>
<span class="gi">+                if (is_last_item or cursor % frequency):</span>
<span class="gi">+                    return</span>
<span class="gi">+            remaining_time = (elapsed_time / index) * \</span>
<span class="gi">+                             (self.n_dispatched_tasks - index * 1.0)</span>
<span class="gi">+            # only display status if remaining time is greater or equal to 0</span>
<span class="gi">+            self._print(</span>
<span class="gi">+                f&quot;Done {index:3d} out of {total_tasks:3d} | elapsed: &quot;</span>
<span class="gi">+                f&quot;{short_format_time(elapsed_time)} remaining: &quot;</span>
<span class="gi">+                f&quot;{short_format_time(remaining_time)}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _abort(self):</span>
<span class="gi">+        # Stop dispatching new jobs in the async callback thread</span>
<span class="gi">+        self._aborting = True</span>
<span class="gi">+</span>
<span class="gi">+        # If the backend allows it, cancel or kill remaining running</span>
<span class="gi">+        # tasks without waiting for the results as we will raise</span>
<span class="gi">+        # the exception we got back to the caller instead of returning</span>
<span class="gi">+        # any result.</span>
<span class="gi">+        backend = self._backend</span>
<span class="gi">+        if (not self._aborted and hasattr(backend, &#39;abort_everything&#39;)):</span>
<span class="gi">+            # If the backend is managed externally we need to make sure</span>
<span class="gi">+            # to leave it in a working state to allow for future jobs</span>
<span class="gi">+            # scheduling.</span>
<span class="gi">+            ensure_ready = self._managed_backend</span>
<span class="gi">+            backend.abort_everything(ensure_ready=ensure_ready)</span>
<span class="gi">+        self._aborted = True</span>
<span class="gi">+</span>
<span class="gi">+    def _start(self, iterator, pre_dispatch):</span>
<span class="gi">+        # Only set self._iterating to True if at least a batch</span>
<span class="gi">+        # was dispatched. In particular this covers the edge</span>
<span class="gi">+        # case of Parallel used with an exhausted iterator. If</span>
<span class="gi">+        # self._original_iterator is None, then this means either</span>
<span class="gi">+        # that pre_dispatch == &quot;all&quot;, n_jobs == 1 or that the first batch</span>
<span class="gi">+        # was very quick and its callback already dispatched all the</span>
<span class="gi">+        # remaining jobs.</span>
<span class="gi">+        self._iterating = False</span>
<span class="gi">+        if self.dispatch_one_batch(iterator):</span>
<span class="gi">+            self._iterating = self._original_iterator is not None</span>
<span class="gi">+</span>
<span class="gi">+        while self.dispatch_one_batch(iterator):</span>
<span class="gi">+            pass</span>
<span class="gi">+</span>
<span class="gi">+        if pre_dispatch == &quot;all&quot;:</span>
<span class="gi">+            # The iterable was consumed all at once by the above for loop.</span>
<span class="gi">+            # No need to wait for async callbacks to trigger to</span>
<span class="gi">+            # consumption.</span>
<span class="gi">+            self._iterating = False</span>

<span class="w"> </span>    def _get_outputs(self, iterator, pre_dispatch):
<span class="w"> </span>        &quot;&quot;&quot;Iterator returning the tasks&#39; output as soon as they are ready.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        dispatch_thread_id = threading.get_ident()</span>
<span class="gi">+        detach_generator_exit = False</span>
<span class="gi">+        try:</span>
<span class="gi">+            self._start(iterator, pre_dispatch)</span>
<span class="gi">+            # first yield returns None, for internal use only. This ensures</span>
<span class="gi">+            # that we enter the try/except block and start dispatching the</span>
<span class="gi">+            # tasks.</span>
<span class="gi">+            yield</span>
<span class="gi">+</span>
<span class="gi">+            with self._backend.retrieval_context():</span>
<span class="gi">+                yield from self._retrieve()</span>
<span class="gi">+</span>
<span class="gi">+        except GeneratorExit:</span>
<span class="gi">+            # The generator has been garbage collected before being fully</span>
<span class="gi">+            # consumed. This aborts the remaining tasks if possible and warn</span>
<span class="gi">+            # the user if necessary.</span>
<span class="gi">+            self._exception = True</span>
<span class="gi">+</span>
<span class="gi">+            # In some interpreters such as PyPy, GeneratorExit can be raised in</span>
<span class="gi">+            # a different thread than the one used to start the dispatch of the</span>
<span class="gi">+            # parallel tasks. This can lead to hang when a thread attempts to</span>
<span class="gi">+            # join itself. As workaround, we detach the execution of the</span>
<span class="gi">+            # aborting code to a dedicated thread. We then need to make sure</span>
<span class="gi">+            # the rest of the function does not call `_terminate_and_reset`</span>
<span class="gi">+            # in finally.</span>
<span class="gi">+            if dispatch_thread_id != threading.get_ident():</span>
<span class="gi">+                if not IS_PYPY:</span>
<span class="gi">+                    warnings.warn(</span>
<span class="gi">+                        &quot;A generator produced by joblib.Parallel has been &quot;</span>
<span class="gi">+                        &quot;gc&#39;ed in an unexpected thread. This behavior should &quot;</span>
<span class="gi">+                        &quot;not cause major -issues but to make sure, please &quot;</span>
<span class="gi">+                        &quot;report this warning and your use case at &quot;</span>
<span class="gi">+                        &quot;https://github.com/joblib/joblib/issues so it can &quot;</span>
<span class="gi">+                        &quot;be investigated.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+                detach_generator_exit = True</span>
<span class="gi">+                _parallel = self</span>
<span class="gi">+</span>
<span class="gi">+                class _GeneratorExitThread(threading.Thread):</span>
<span class="gi">+                    def run(self):</span>
<span class="gi">+                        _parallel._abort()</span>
<span class="gi">+                        if _parallel.return_generator:</span>
<span class="gi">+                            _parallel._warn_exit_early()</span>
<span class="gi">+                        _parallel._terminate_and_reset()</span>
<span class="gi">+</span>
<span class="gi">+                _GeneratorExitThread(</span>
<span class="gi">+                    name=&quot;GeneratorExitThread&quot;</span>
<span class="gi">+                ).start()</span>
<span class="gi">+                return</span>
<span class="gi">+</span>
<span class="gi">+            # Otherwise, we are in the thread that started the dispatch: we can</span>
<span class="gi">+            # safely abort the execution and warn the user.</span>
<span class="gi">+            self._abort()</span>
<span class="gi">+            if self.return_generator:</span>
<span class="gi">+                self._warn_exit_early()</span>
<span class="gi">+</span>
<span class="gi">+            raise</span>
<span class="gi">+</span>
<span class="gi">+        # Note: we catch any BaseException instead of just Exception instances</span>
<span class="gi">+        # to also include KeyboardInterrupt</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            self._exception = True</span>
<span class="gi">+            self._abort()</span>
<span class="gi">+            raise</span>
<span class="gi">+        finally:</span>
<span class="gi">+            # Store the unconsumed tasks and terminate the workers if necessary</span>
<span class="gi">+            _remaining_outputs = ([] if self._exception else self._jobs)</span>
<span class="gi">+            self._jobs = collections.deque()</span>
<span class="gi">+            self._running = False</span>
<span class="gi">+            if not detach_generator_exit:</span>
<span class="gi">+                self._terminate_and_reset()</span>
<span class="gi">+</span>
<span class="gi">+        while len(_remaining_outputs) &gt; 0:</span>
<span class="gi">+            batched_results = _remaining_outputs.popleft()</span>
<span class="gi">+            batched_results = batched_results.get_result(self.timeout)</span>
<span class="gi">+            for result in batched_results:</span>
<span class="gi">+                yield result</span>

<span class="w"> </span>    def _wait_retrieval(self):
<span class="w"> </span>        &quot;&quot;&quot;Return True if we need to continue retrieving some tasks.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # If the input load is still being iterated over, it means that tasks</span>
<span class="gi">+        # are still on the dispatch waitlist and their results will need to</span>
<span class="gi">+        # be retrieved later on.</span>
<span class="gi">+        if self._iterating:</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        # If some of the dispatched tasks are still being processed by the</span>
<span class="gi">+        # workers, wait for the compute to finish before starting retrieval</span>
<span class="gi">+        if self.n_completed_tasks &lt; self.n_dispatched_tasks:</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        # For backends that does not support retrieving asynchronously the</span>
<span class="gi">+        # result to the main process, all results must be carefully retrieved</span>
<span class="gi">+        # in the _retrieve loop in the main thread while the backend is alive.</span>
<span class="gi">+        # For other backends, the actual retrieval is done asynchronously in</span>
<span class="gi">+        # the callback thread, and we can terminate the backend before the</span>
<span class="gi">+        # `self._jobs` result list has been emptied. The remaining results</span>
<span class="gi">+        # will be collected in the `finally` step of the generator.</span>
<span class="gi">+        if not self._backend.supports_retrieve_callback:</span>
<span class="gi">+            if len(self._jobs) &gt; 0:</span>
<span class="gi">+                return True</span>
<span class="gi">+</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    def _retrieve(self):</span>
<span class="gi">+        while self._wait_retrieval():</span>
<span class="gi">+</span>
<span class="gi">+            # If the callback thread of a worker has signaled that its task</span>
<span class="gi">+            # triggered an exception, or if the retrieval loop has raised an</span>
<span class="gi">+            # exception (e.g. `GeneratorExit`), exit the loop and surface the</span>
<span class="gi">+            # worker traceback.</span>
<span class="gi">+            if self._aborting:</span>
<span class="gi">+                self._raise_error_fast()</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+            # If the next job is not ready for retrieval yet, we just wait for</span>
<span class="gi">+            # async callbacks to progress.</span>
<span class="gi">+            if ((len(self._jobs) == 0) or</span>
<span class="gi">+                (self._jobs[0].get_status(</span>
<span class="gi">+                    timeout=self.timeout) == TASK_PENDING)):</span>
<span class="gi">+                time.sleep(0.01)</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            # We need to be careful: the job list can be filling up as</span>
<span class="gi">+            # we empty it and Python list are not thread-safe by</span>
<span class="gi">+            # default hence the use of the lock</span>
<span class="gi">+            with self._lock:</span>
<span class="gi">+                batched_results = self._jobs.popleft()</span>
<span class="gi">+</span>
<span class="gi">+            # Flatten the batched results to output one output at a time</span>
<span class="gi">+            batched_results = batched_results.get_result(self.timeout)</span>
<span class="gi">+            for result in batched_results:</span>
<span class="gi">+                self._nb_consumed += 1</span>
<span class="gi">+                yield result</span>

<span class="w"> </span>    def _raise_error_fast(self):
<span class="w"> </span>        &quot;&quot;&quot;If we are aborting, raise if a job caused an error.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # Find the first job whose status is TASK_ERROR if it exists.</span>
<span class="gi">+        with self._lock:</span>
<span class="gi">+            error_job = next((job for job in self._jobs</span>
<span class="gi">+                              if job.status == TASK_ERROR), None)</span>
<span class="gi">+</span>
<span class="gi">+        # If this error job exists, immediately raise the error by</span>
<span class="gi">+        # calling get_result. This job might not exists if abort has been</span>
<span class="gi">+        # called directly or if the generator is gc&#39;ed.</span>
<span class="gi">+        if error_job is not None:</span>
<span class="gi">+            error_job.get_result(self.timeout)</span>

<span class="w"> </span>    def _warn_exit_early(self):
<span class="w"> </span>        &quot;&quot;&quot;Warn the user if the generator is gc&#39;ed before being consumned.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ready_outputs = self.n_completed_tasks - self._nb_consumed</span>
<span class="gi">+        is_completed = self._is_completed()</span>
<span class="gi">+        msg = &quot;&quot;</span>
<span class="gi">+        if ready_outputs:</span>
<span class="gi">+            msg += (</span>
<span class="gi">+                f&quot;{ready_outputs} tasks have been successfully executed &quot;</span>
<span class="gi">+                &quot; but not used.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            if not is_completed:</span>
<span class="gi">+                msg += &quot; Additionally, &quot;</span>
<span class="gi">+</span>
<span class="gi">+        if not is_completed:</span>
<span class="gi">+            msg += (</span>
<span class="gi">+                f&quot;{self.n_dispatched_tasks - self.n_completed_tasks} tasks &quot;</span>
<span class="gi">+                &quot;which were still being processed by the workers have been &quot;</span>
<span class="gi">+                &quot;cancelled.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if msg:</span>
<span class="gi">+            msg += (</span>
<span class="gi">+                &quot; You could benefit from adjusting the input task &quot;</span>
<span class="gi">+                &quot;iterator to limit unnecessary computation time.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            warnings.warn(msg)</span>

<span class="w"> </span>    def _get_sequential_output(self, iterable):
<span class="w"> </span>        &quot;&quot;&quot;Separate loop for sequential output.
<span class="gu">@@ -977,58 +1822,188 @@ class Parallel(Logger):</span>
<span class="w"> </span>        This simplifies the traceback in case of errors and reduces the
<span class="w"> </span>        overhead of calling sequential tasks with `joblib`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            self._iterating = True</span>
<span class="gi">+            self._original_iterator = iterable</span>
<span class="gi">+            batch_size = self._get_batch_size()</span>
<span class="gi">+</span>
<span class="gi">+            if batch_size != 1:</span>
<span class="gi">+                it = iter(iterable)</span>
<span class="gi">+                iterable_batched = iter(</span>
<span class="gi">+                    lambda: tuple(itertools.islice(it, batch_size)), ()</span>
<span class="gi">+                )</span>
<span class="gi">+                iterable = (</span>
<span class="gi">+                    task for batch in iterable_batched for task in batch</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # first yield returns None, for internal use only. This ensures</span>
<span class="gi">+            # that we enter the try/except block and setup the generator.</span>
<span class="gi">+            yield None</span>
<span class="gi">+</span>
<span class="gi">+            # Sequentially call the tasks and yield the results.</span>
<span class="gi">+            for func, args, kwargs in iterable:</span>
<span class="gi">+                self.n_dispatched_batches += 1</span>
<span class="gi">+                self.n_dispatched_tasks += 1</span>
<span class="gi">+                res = func(*args, **kwargs)</span>
<span class="gi">+                self.n_completed_tasks += 1</span>
<span class="gi">+                self.print_progress()</span>
<span class="gi">+                yield res</span>
<span class="gi">+                self._nb_consumed += 1</span>
<span class="gi">+        except BaseException:</span>
<span class="gi">+            self._exception = True</span>
<span class="gi">+            self._aborting = True</span>
<span class="gi">+            self._aborted = True</span>
<span class="gi">+            raise</span>
<span class="gi">+        finally:</span>
<span class="gi">+            self.print_progress()</span>
<span class="gi">+            self._running = False</span>
<span class="gi">+            self._iterating = False</span>
<span class="gi">+            self._original_iterator = None</span>

<span class="w"> </span>    def _reset_run_tracking(self):
<span class="w"> </span>        &quot;&quot;&quot;Reset the counters and flags used to track the execution.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # Makes sur the parallel instance was not previously running in a</span>
<span class="gi">+        # thread-safe way.</span>
<span class="gi">+        with getattr(self, &#39;_lock&#39;, nullcontext()):</span>
<span class="gi">+            if self._running:</span>
<span class="gi">+                msg = &#39;This Parallel instance is already running !&#39;</span>
<span class="gi">+                if self.return_generator is True:</span>
<span class="gi">+                    msg += (</span>
<span class="gi">+                        &quot; Before submitting new tasks, you must wait for the &quot;</span>
<span class="gi">+                        &quot;completion of all the previous tasks, or clean all &quot;</span>
<span class="gi">+                        &quot;references to the output generator.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                raise RuntimeError(msg)</span>
<span class="gi">+            self._running = True</span>
<span class="gi">+</span>
<span class="gi">+        # Counter to keep track of the task dispatched and completed.</span>
<span class="gi">+        self.n_dispatched_batches = 0</span>
<span class="gi">+        self.n_dispatched_tasks = 0</span>
<span class="gi">+        self.n_completed_tasks = 0</span>
<span class="gi">+</span>
<span class="gi">+        # Following count is incremented by one each time the user iterates</span>
<span class="gi">+        # on the output generator, it is used to prepare an informative</span>
<span class="gi">+        # warning message in case the generator is deleted before all the</span>
<span class="gi">+        # dispatched tasks have been consumed.</span>
<span class="gi">+        self._nb_consumed = 0</span>
<span class="gi">+</span>
<span class="gi">+        # Following flags are used to synchronize the threads in case one of</span>
<span class="gi">+        # the tasks error-out to ensure that all workers abort fast and that</span>
<span class="gi">+        # the backend terminates properly.</span>
<span class="gi">+</span>
<span class="gi">+        # Set to True as soon as a worker signals that a task errors-out</span>
<span class="gi">+        self._exception = False</span>
<span class="gi">+        # Set to True in case of early termination following an incident</span>
<span class="gi">+        self._aborting = False</span>
<span class="gi">+        # Set to True after abortion is complete</span>
<span class="gi">+        self._aborted = False</span>

<span class="w"> </span>    def __call__(self, iterable):
<span class="w"> </span>        &quot;&quot;&quot;Main function to dispatch parallel tasks.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>        self._reset_run_tracking()
<span class="w"> </span>        self._start_time = time.time()
<span class="gi">+</span>
<span class="w"> </span>        if not self._managed_backend:
<span class="w"> </span>            n_jobs = self._initialize_backend()
<span class="w"> </span>        else:
<span class="w"> </span>            n_jobs = self._effective_n_jobs()
<span class="gi">+</span>
<span class="w"> </span>        if n_jobs == 1:
<span class="gi">+            # If n_jobs==1, run the computation sequentially and return</span>
<span class="gi">+            # immediately to avoid overheads.</span>
<span class="w"> </span>            output = self._get_sequential_output(iterable)
<span class="w"> </span>            next(output)
<span class="w"> </span>            return output if self.return_generator else list(output)
<span class="gi">+</span>
<span class="gi">+        # Let&#39;s create an ID that uniquely identifies the current call. If the</span>
<span class="gi">+        # call is interrupted early and that the same instance is immediately</span>
<span class="gi">+        # re-used, this id will be used to prevent workers that were</span>
<span class="gi">+        # concurrently finalizing a task from the previous call to run the</span>
<span class="gi">+        # callback.</span>
<span class="w"> </span>        with self._lock:
<span class="w"> </span>            self._call_id = uuid4().hex
<span class="gi">+</span>
<span class="gi">+        # self._effective_n_jobs should be called in the Parallel.__call__</span>
<span class="gi">+        # thread only -- store its value in an attribute for further queries.</span>
<span class="w"> </span>        self._cached_effective_n_jobs = n_jobs
<span class="gi">+</span>
<span class="w"> </span>        if isinstance(self._backend, LokyBackend):
<span class="gi">+            # For the loky backend, we add a callback executed when reducing</span>
<span class="gi">+            # BatchCalls, that makes the loky executor use a temporary folder</span>
<span class="gi">+            # specific to this Parallel object when pickling temporary memmaps.</span>
<span class="gi">+            # This callback is necessary to ensure that several Parallel</span>
<span class="gi">+            # objects using the same reusable executor don&#39;t use the same</span>
<span class="gi">+            # temporary resources.</span>

<span class="w"> </span>            def _batched_calls_reducer_callback():
<span class="gd">-                self._backend._workers._temp_folder_manager.set_current_context(</span>
<span class="gd">-                    self._id)</span>
<span class="gi">+                # Relevant implementation detail: the following lines, called</span>
<span class="gi">+                # when reducing BatchedCalls, are called in a thread-safe</span>
<span class="gi">+                # situation, meaning that the context of the temporary folder</span>
<span class="gi">+                # manager will not be changed in between the callback execution</span>
<span class="gi">+                # and the end of the BatchedCalls pickling. The reason is that</span>
<span class="gi">+                # pickling (the only place where set_current_context is used)</span>
<span class="gi">+                # is done from a single thread (the queue_feeder_thread).</span>
<span class="gi">+                self._backend._workers._temp_folder_manager.set_current_context(  # noqa</span>
<span class="gi">+                    self._id</span>
<span class="gi">+                )</span>
<span class="w"> </span>            self._reducer_callback = _batched_calls_reducer_callback
<span class="gi">+</span>
<span class="gi">+        # self._effective_n_jobs should be called in the Parallel.__call__</span>
<span class="gi">+        # thread only -- store its value in an attribute for further queries.</span>
<span class="w"> </span>        self._cached_effective_n_jobs = n_jobs
<span class="gi">+</span>
<span class="w"> </span>        backend_name = self._backend.__class__.__name__
<span class="w"> </span>        if n_jobs == 0:
<span class="gd">-            raise RuntimeError(&#39;%s has no active worker.&#39; % backend_name)</span>
<span class="gi">+            raise RuntimeError(&quot;%s has no active worker.&quot; % backend_name)</span>
<span class="gi">+</span>
<span class="w"> </span>        self._print(
<span class="gd">-            f&#39;Using backend {backend_name} with {n_jobs} concurrent workers.&#39;)</span>
<span class="gi">+            f&quot;Using backend {backend_name} with {n_jobs} concurrent workers.&quot;</span>
<span class="gi">+        )</span>
<span class="w"> </span>        if hasattr(self._backend, &#39;start_call&#39;):
<span class="w"> </span>            self._backend.start_call()
<span class="gi">+</span>
<span class="gi">+        # Following flag prevents double calls to `backend.stop_call`.</span>
<span class="w"> </span>        self._calling = True
<span class="gi">+</span>
<span class="w"> </span>        iterator = iter(iterable)
<span class="w"> </span>        pre_dispatch = self.pre_dispatch
<span class="gi">+</span>
<span class="w"> </span>        if pre_dispatch == &#39;all&#39;:
<span class="gi">+            # prevent further dispatch via multiprocessing callback thread</span>
<span class="w"> </span>            self._original_iterator = None
<span class="w"> </span>            self._pre_dispatch_amount = 0
<span class="w"> </span>        else:
<span class="w"> </span>            self._original_iterator = iterator
<span class="w"> </span>            if hasattr(pre_dispatch, &#39;endswith&#39;):
<span class="gd">-                pre_dispatch = eval_expr(pre_dispatch.replace(&#39;n_jobs&#39;, str</span>
<span class="gd">-                    (n_jobs)))</span>
<span class="gi">+                pre_dispatch = eval_expr(</span>
<span class="gi">+                    pre_dispatch.replace(&quot;n_jobs&quot;, str(n_jobs))</span>
<span class="gi">+                )</span>
<span class="w"> </span>            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)
<span class="gi">+</span>
<span class="gi">+            # The main thread will consume the first pre_dispatch items and</span>
<span class="gi">+            # the remaining items will later be lazily dispatched by async</span>
<span class="gi">+            # callbacks upon task completions.</span>
<span class="gi">+</span>
<span class="gi">+            # TODO: this iterator should be batch_size * n_jobs</span>
<span class="w"> </span>            iterator = itertools.islice(iterator, self._pre_dispatch_amount)
<span class="gi">+</span>
<span class="gi">+        # Use a caching dict for callables that are pickled with cloudpickle to</span>
<span class="gi">+        # improve performances. This cache is used only in the case of</span>
<span class="gi">+        # functions that are defined in the __main__ module, functions that</span>
<span class="gi">+        # are defined locally (inside another function) and lambda expressions.</span>
<span class="w"> </span>        self._pickle_cache = dict()
<span class="gi">+</span>
<span class="w"> </span>        output = self._get_outputs(iterator, pre_dispatch)
<span class="w"> </span>        self._call_ref = weakref.ref(output)
<span class="gi">+</span>
<span class="gi">+        # The first item from the output is blank, but it makes the interpreter</span>
<span class="gi">+        # progress until it enters the Try/Except block of the generator and</span>
<span class="gi">+        # reaches the first `yield` statement. This starts the asynchronous</span>
<span class="gi">+        # dispatch of the tasks to the workers.</span>
<span class="w"> </span>        next(output)
<span class="gi">+</span>
<span class="w"> </span>        return output if self.return_generator else list(output)

<span class="w"> </span>    def __repr__(self):
<span class="gh">diff --git a/joblib/pool.py b/joblib/pool.py</span>
<span class="gh">index a5c2643..c0c3549 100644</span>
<span class="gd">--- a/joblib/pool.py</span>
<span class="gi">+++ b/joblib/pool.py</span>
<span class="gu">@@ -9,27 +9,42 @@ available as it implements subclasses of multiprocessing Pool</span>
<span class="w"> </span>that uses a custom alternative to SimpleQueue.

<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Author: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="gi">+# Copyright: 2012, Olivier Grisel</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>import copyreg
<span class="w"> </span>import sys
<span class="w"> </span>import warnings
<span class="w"> </span>from time import sleep
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    WindowsError
<span class="w"> </span>except NameError:
<span class="w"> </span>    WindowsError = type(None)
<span class="gi">+</span>
<span class="w"> </span>from pickle import Pickler
<span class="gi">+</span>
<span class="w"> </span>from pickle import HIGHEST_PROTOCOL
<span class="w"> </span>from io import BytesIO
<span class="gi">+</span>
<span class="w"> </span>from ._memmapping_reducer import get_memmapping_reducers
<span class="w"> </span>from ._memmapping_reducer import TemporaryResourcesManager
<span class="w"> </span>from ._multiprocessing_helpers import mp, assert_spawning
<span class="gi">+</span>
<span class="gi">+# We need the class definition to derive from it, not the multiprocessing.Pool</span>
<span class="gi">+# factory function</span>
<span class="w"> </span>from multiprocessing.pool import Pool
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    import numpy as np
<span class="w"> </span>except ImportError:
<span class="w"> </span>    np = None


<span class="gi">+###############################################################################</span>
<span class="gi">+# Enable custom pickling in Pool queues</span>
<span class="gi">+</span>
<span class="w"> </span>class CustomizablePickler(Pickler):
<span class="w"> </span>    &quot;&quot;&quot;Pickler that accepts custom reducers.

<span class="gu">@@ -48,20 +63,38 @@ class CustomizablePickler(Pickler):</span>

<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    # We override the pure Python pickler as its the only way to be able to</span>
<span class="gi">+    # customize the dispatch table without side effects in Python 2.7</span>
<span class="gi">+    # to 3.2. For Python 3.3+ leverage the new dispatch_table</span>
<span class="gi">+    # feature from https://bugs.python.org/issue14166 that makes it possible</span>
<span class="gi">+    # to use the C implementation of the Pickler which is faster.</span>
<span class="gi">+</span>
<span class="w"> </span>    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
<span class="w"> </span>        Pickler.__init__(self, writer, protocol=protocol)
<span class="w"> </span>        if reducers is None:
<span class="w"> </span>            reducers = {}
<span class="w"> </span>        if hasattr(Pickler, &#39;dispatch&#39;):
<span class="gi">+            # Make the dispatch registry an instance level attribute instead of</span>
<span class="gi">+            # a reference to the class dictionary under Python 2</span>
<span class="w"> </span>            self.dispatch = Pickler.dispatch.copy()
<span class="w"> </span>        else:
<span class="gi">+            # Under Python 3 initialize the dispatch table with a copy of the</span>
<span class="gi">+            # default registry</span>
<span class="w"> </span>            self.dispatch_table = copyreg.dispatch_table.copy()
<span class="w"> </span>        for type, reduce_func in reducers.items():
<span class="w"> </span>            self.register(type, reduce_func)

<span class="w"> </span>    def register(self, type, reduce_func):
<span class="w"> </span>        &quot;&quot;&quot;Attach a reducer function to a given type in the dispatch table.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(Pickler, &#39;dispatch&#39;):</span>
<span class="gi">+            # Python 2 pickler dispatching is not explicitly customizable.</span>
<span class="gi">+            # Let us use a closure to workaround this limitation.</span>
<span class="gi">+            def dispatcher(self, obj):</span>
<span class="gi">+                reduced = reduce_func(obj)</span>
<span class="gi">+                self.save_reduce(obj=obj, *reduced)</span>
<span class="gi">+            self.dispatch[type] = dispatcher</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.dispatch_table[type] = reduce_func</span>


<span class="w"> </span>class CustomizablePicklingQueue(object):
<span class="gu">@@ -93,14 +126,54 @@ class CustomizablePicklingQueue(object):</span>

<span class="w"> </span>    def __getstate__(self):
<span class="w"> </span>        assert_spawning(self)
<span class="gd">-        return (self._reader, self._writer, self._rlock, self._wlock, self.</span>
<span class="gd">-            _reducers)</span>
<span class="gi">+        return (self._reader, self._writer, self._rlock, self._wlock,</span>
<span class="gi">+                self._reducers)</span>

<span class="w"> </span>    def __setstate__(self, state):
<span class="gd">-        (self._reader, self._writer, self._rlock, self._wlock, self._reducers</span>
<span class="gd">-            ) = state</span>
<span class="gi">+        (self._reader, self._writer, self._rlock, self._wlock,</span>
<span class="gi">+         self._reducers) = state</span>
<span class="w"> </span>        self._make_methods()

<span class="gi">+    def empty(self):</span>
<span class="gi">+        return not self._reader.poll()</span>
<span class="gi">+</span>
<span class="gi">+    def _make_methods(self):</span>
<span class="gi">+        self._recv = recv = self._reader.recv</span>
<span class="gi">+        racquire, rrelease = self._rlock.acquire, self._rlock.release</span>
<span class="gi">+</span>
<span class="gi">+        def get():</span>
<span class="gi">+            racquire()</span>
<span class="gi">+            try:</span>
<span class="gi">+                return recv()</span>
<span class="gi">+            finally:</span>
<span class="gi">+                rrelease()</span>
<span class="gi">+</span>
<span class="gi">+        self.get = get</span>
<span class="gi">+</span>
<span class="gi">+        if self._reducers:</span>
<span class="gi">+            def send(obj):</span>
<span class="gi">+                buffer = BytesIO()</span>
<span class="gi">+                CustomizablePickler(buffer, self._reducers).dump(obj)</span>
<span class="gi">+                self._writer.send_bytes(buffer.getvalue())</span>
<span class="gi">+            self._send = send</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._send = send = self._writer.send</span>
<span class="gi">+        if self._wlock is None:</span>
<span class="gi">+            # writes to a message oriented win32 pipe are atomic</span>
<span class="gi">+            self.put = send</span>
<span class="gi">+        else:</span>
<span class="gi">+            wlock_acquire, wlock_release = (</span>
<span class="gi">+                self._wlock.acquire, self._wlock.release)</span>
<span class="gi">+</span>
<span class="gi">+            def put(obj):</span>
<span class="gi">+                wlock_acquire()</span>
<span class="gi">+                try:</span>
<span class="gi">+                    return send(obj)</span>
<span class="gi">+                finally:</span>
<span class="gi">+                    wlock_release()</span>
<span class="gi">+</span>
<span class="gi">+            self.put = put</span>
<span class="gi">+</span>

<span class="w"> </span>class PicklingPool(Pool):
<span class="w"> </span>    &quot;&quot;&quot;Pool implementation with customizable pickling reducers.
<span class="gu">@@ -120,7 +193,7 @@ class PicklingPool(Pool):</span>
<span class="w"> </span>    &quot;&quot;&quot;

<span class="w"> </span>    def __init__(self, processes=None, forward_reducers=None,
<span class="gd">-        backward_reducers=None, **kwargs):</span>
<span class="gi">+                 backward_reducers=None, **kwargs):</span>
<span class="w"> </span>        if forward_reducers is None:
<span class="w"> </span>            forward_reducers = dict()
<span class="w"> </span>        if backward_reducers is None:
<span class="gu">@@ -131,6 +204,15 @@ class PicklingPool(Pool):</span>
<span class="w"> </span>        poolargs.update(kwargs)
<span class="w"> </span>        super(PicklingPool, self).__init__(**poolargs)

<span class="gi">+    def _setup_queues(self):</span>
<span class="gi">+        context = getattr(self, &#39;_ctx&#39;, mp)</span>
<span class="gi">+        self._inqueue = CustomizablePicklingQueue(context,</span>
<span class="gi">+                                                  self._forward_reducers)</span>
<span class="gi">+        self._outqueue = CustomizablePicklingQueue(context,</span>
<span class="gi">+                                                   self._backward_reducers)</span>
<span class="gi">+        self._quick_put = self._inqueue._send</span>
<span class="gi">+        self._quick_get = self._outqueue._recv</span>
<span class="gi">+</span>

<span class="w"> </span>class MemmappingPool(PicklingPool):
<span class="w"> </span>    &quot;&quot;&quot;Process pool that shares large arrays to avoid memory copy.
<span class="gu">@@ -209,21 +291,64 @@ class MemmappingPool(PicklingPool):</span>

<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, processes=None, temp_folder=None, max_nbytes=</span>
<span class="gd">-        1000000.0, mmap_mode=&#39;r&#39;, forward_reducers=None, backward_reducers=</span>
<span class="gd">-        None, verbose=0, context_id=None, prewarm=False, **kwargs):</span>
<span class="gi">+    def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,</span>
<span class="gi">+                 mmap_mode=&#39;r&#39;, forward_reducers=None, backward_reducers=None,</span>
<span class="gi">+                 verbose=0, context_id=None, prewarm=False, **kwargs):</span>
<span class="gi">+</span>
<span class="w"> </span>        if context_id is not None:
<span class="gd">-            warnings.warn(</span>
<span class="gd">-                &#39;context_id is deprecated and ignored in joblib 0.9.4 and will be removed in 0.11&#39;</span>
<span class="gd">-                , DeprecationWarning)</span>
<span class="gi">+            warnings.warn(&#39;context_id is deprecated and ignored in joblib&#39;</span>
<span class="gi">+                          &#39; 0.9.4 and will be removed in 0.11&#39;,</span>
<span class="gi">+                          DeprecationWarning)</span>
<span class="gi">+</span>
<span class="w"> </span>        manager = TemporaryResourcesManager(temp_folder)
<span class="w"> </span>        self._temp_folder_manager = manager
<span class="gd">-        forward_reducers, backward_reducers = get_memmapping_reducers(</span>
<span class="gd">-            temp_folder_resolver=manager.resolve_temp_folder_name,</span>
<span class="gd">-            max_nbytes=max_nbytes, mmap_mode=mmap_mode, forward_reducers=</span>
<span class="gd">-            forward_reducers, backward_reducers=backward_reducers, verbose=</span>
<span class="gd">-            verbose, unlink_on_gc_collect=False, prewarm=prewarm)</span>
<span class="gd">-        poolargs = dict(processes=processes, forward_reducers=</span>
<span class="gd">-            forward_reducers, backward_reducers=backward_reducers)</span>
<span class="gi">+</span>
<span class="gi">+        # The usage of a temp_folder_resolver over a simple temp_folder is</span>
<span class="gi">+        # superfluous for multiprocessing pools, as they don&#39;t get reused, see</span>
<span class="gi">+        # get_memmapping_executor for more details. We still use it for code</span>
<span class="gi">+        # simplicity.</span>
<span class="gi">+        forward_reducers, backward_reducers = \</span>
<span class="gi">+            get_memmapping_reducers(</span>
<span class="gi">+                temp_folder_resolver=manager.resolve_temp_folder_name,</span>
<span class="gi">+                max_nbytes=max_nbytes, mmap_mode=mmap_mode,</span>
<span class="gi">+                forward_reducers=forward_reducers,</span>
<span class="gi">+                backward_reducers=backward_reducers, verbose=verbose,</span>
<span class="gi">+                unlink_on_gc_collect=False, prewarm=prewarm)</span>
<span class="gi">+</span>
<span class="gi">+        poolargs = dict(</span>
<span class="gi">+            processes=processes,</span>
<span class="gi">+            forward_reducers=forward_reducers,</span>
<span class="gi">+            backward_reducers=backward_reducers)</span>
<span class="w"> </span>        poolargs.update(kwargs)
<span class="w"> </span>        super(MemmappingPool, self).__init__(**poolargs)
<span class="gi">+</span>
<span class="gi">+    def terminate(self):</span>
<span class="gi">+        n_retries = 10</span>
<span class="gi">+        for i in range(n_retries):</span>
<span class="gi">+            try:</span>
<span class="gi">+                super(MemmappingPool, self).terminate()</span>
<span class="gi">+                break</span>
<span class="gi">+            except OSError as e:</span>
<span class="gi">+                if isinstance(e, WindowsError):</span>
<span class="gi">+                    # Workaround  occasional &quot;[Error 5] Access is denied&quot; issue</span>
<span class="gi">+                    # when trying to terminate a process under windows.</span>
<span class="gi">+                    sleep(0.1)</span>
<span class="gi">+                    if i + 1 == n_retries:</span>
<span class="gi">+                        warnings.warn(&quot;Failed to terminate worker processes in&quot;</span>
<span class="gi">+                                      &quot; multiprocessing pool: %r&quot; % e)</span>
<span class="gi">+</span>
<span class="gi">+        # Clean up the temporary resources as the workers should now be off.</span>
<span class="gi">+        self._temp_folder_manager._clean_temporary_resources()</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def _temp_folder(self):</span>
<span class="gi">+        # Legacy property in tests. could be removed if we refactored the</span>
<span class="gi">+        # memmapping tests. SHOULD ONLY BE USED IN TESTS!</span>
<span class="gi">+        # We cache this property because it is called late in the tests - at</span>
<span class="gi">+        # this point, all context have been unregistered, and</span>
<span class="gi">+        # resolve_temp_folder_name raises an error.</span>
<span class="gi">+        if getattr(self, &#39;_cached_temp_folder&#39;, None) is not None:</span>
<span class="gi">+            return self._cached_temp_folder</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._cached_temp_folder = self._temp_folder_manager.resolve_temp_folder_name()  # noqa</span>
<span class="gi">+            return self._cached_temp_folder</span>
<span class="gh">diff --git a/joblib/testing.py b/joblib/testing.py</span>
<span class="gh">index e20431c..caab7d2 100644</span>
<span class="gd">--- a/joblib/testing.py</span>
<span class="gi">+++ b/joblib/testing.py</span>
<span class="gu">@@ -1,14 +1,18 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Helper for testing.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import sys
<span class="w"> </span>import warnings
<span class="w"> </span>import os.path
<span class="w"> </span>import re
<span class="w"> </span>import subprocess
<span class="w"> </span>import threading
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="w"> </span>import _pytest
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>raises = pytest.raises
<span class="w"> </span>warns = pytest.warns
<span class="w"> </span>SkipTest = _pytest.runner.Skipped
<span class="gu">@@ -23,11 +27,17 @@ param = pytest.param</span>
<span class="w"> </span>def warnings_to_stdout():
<span class="w"> </span>    &quot;&quot;&quot; Redirect all warnings to stdout.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    showwarning_orig = warnings.showwarning</span>
<span class="gi">+</span>
<span class="gi">+    def showwarning(msg, cat, fname, lno, file=None, line=0):</span>
<span class="gi">+        showwarning_orig(msg, cat, os.path.basename(fname), line, sys.stdout)</span>
<span class="gi">+</span>
<span class="gi">+    warnings.showwarning = showwarning</span>
<span class="gi">+    # warnings.simplefilter(&#39;always&#39;)</span>


<span class="gd">-def check_subprocess_call(cmd, timeout=5, stdout_regex=None, stderr_regex=None</span>
<span class="gd">-    ):</span>
<span class="gi">+def check_subprocess_call(cmd, timeout=5, stdout_regex=None,</span>
<span class="gi">+                          stderr_regex=None):</span>
<span class="w"> </span>    &quot;&quot;&quot;Runs a command in a subprocess with timeout in seconds.

<span class="w"> </span>    A SIGTERM is sent after `timeout` and if it does not terminate, a
<span class="gu">@@ -36,4 +46,54 @@ def check_subprocess_call(cmd, timeout=5, stdout_regex=None, stderr_regex=None</span>
<span class="w"> </span>    Also checks returncode is zero, stdout if stdout_regex is set, and
<span class="w"> </span>    stderr if stderr_regex is set.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,</span>
<span class="gi">+                            stderr=subprocess.PIPE)</span>
<span class="gi">+</span>
<span class="gi">+    def terminate_process():  # pragma: no cover</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Attempt to terminate a leftover process spawned during test execution:</span>
<span class="gi">+        ideally this should not be needed but can help avoid clogging the CI</span>
<span class="gi">+        workers in case of deadlocks.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        warnings.warn(f&quot;Timeout running {cmd}&quot;)</span>
<span class="gi">+        proc.terminate()</span>
<span class="gi">+</span>
<span class="gi">+    def kill_process():  # pragma: no cover</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        Kill a leftover process spawned during test execution: ideally this</span>
<span class="gi">+        should not be needed but can help avoid clogging the CI workers in</span>
<span class="gi">+        case of deadlocks.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        warnings.warn(f&quot;Timeout running {cmd}&quot;)</span>
<span class="gi">+        proc.kill()</span>
<span class="gi">+</span>
<span class="gi">+    try:</span>
<span class="gi">+        if timeout is not None:</span>
<span class="gi">+            terminate_timer = threading.Timer(timeout, terminate_process)</span>
<span class="gi">+            terminate_timer.start()</span>
<span class="gi">+            kill_timer = threading.Timer(2 * timeout, kill_process)</span>
<span class="gi">+            kill_timer.start()</span>
<span class="gi">+        stdout, stderr = proc.communicate()</span>
<span class="gi">+        stdout, stderr = stdout.decode(), stderr.decode()</span>
<span class="gi">+        if proc.returncode != 0:</span>
<span class="gi">+            message = (</span>
<span class="gi">+                &#39;Non-zero return code: {}.\nStdout:\n{}\n&#39;</span>
<span class="gi">+                &#39;Stderr:\n{}&#39;).format(</span>
<span class="gi">+                    proc.returncode, stdout, stderr)</span>
<span class="gi">+            raise ValueError(message)</span>
<span class="gi">+</span>
<span class="gi">+        if (stdout_regex is not None and</span>
<span class="gi">+                not re.search(stdout_regex, stdout)):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Unexpected stdout: {!r} does not match:\n{!r}&quot;.format(</span>
<span class="gi">+                    stdout_regex, stdout))</span>
<span class="gi">+        if (stderr_regex is not None and</span>
<span class="gi">+                not re.search(stderr_regex, stderr)):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Unexpected stderr: {!r} does not match:\n{!r}&quot;.format(</span>
<span class="gi">+                    stderr_regex, stderr))</span>
<span class="gi">+</span>
<span class="gi">+    finally:</span>
<span class="gi">+        if timeout is not None:</span>
<span class="gi">+            terminate_timer.cancel()</span>
<span class="gi">+            kill_timer.cancel()</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
        <script src="../javascripts/tablesort.number.js"></script>
      
    
  </body>
</html>