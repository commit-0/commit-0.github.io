
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis commit0 all plain fillin imbalanced learn - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#claude-sonnet-35-fill-in-imbalanced-learn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis commit0 all plain fillin imbalanced learn
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#failed-to-run-pytests-for-test-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed to run pytests for test tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_commit0-all-plain_fillin">back to Claude Sonnet 3.5 - Fill-in summary</a></p>
<h1 id="claude-sonnet-35-fill-in-imbalanced-learn"><strong>Claude Sonnet 3.5 - Fill-in</strong>: imbalanced-learn</h1>
<h2 id="failed-to-run-pytests-for-test-tests">Failed to run pytests for test <code>tests</code></h2>
<div class="highlight"><pre><span></span><code>Pytest collection failure.
</code></pre></div>
<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/imblearn/_config.py b/imblearn/_config.py</span>
<span class="gh">index 88884fe..35f4445 100644</span>
<span class="gd">--- a/imblearn/_config.py</span>
<span class="gi">+++ b/imblearn/_config.py</span>
<span class="gu">@@ -21,7 +21,9 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>    def _get_threadlocal_config():
<span class="w"> </span>        &quot;&quot;&quot;Get a threadlocal **mutable** configuration. If the configuration
<span class="w"> </span>        does not exist, copy the default global configuration.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not hasattr(_threadlocal, &#39;config&#39;):</span>
<span class="gi">+            _threadlocal.config = _global_config.copy()</span>
<span class="gi">+        return _threadlocal.config</span>

<span class="w"> </span>    def get_config():
<span class="w"> </span>        &quot;&quot;&quot;Retrieve current values for configuration set by :func:`set_config`.
<span class="gu">@@ -36,7 +38,7 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        config_context : Context manager for global scikit-learn configuration.
<span class="w"> </span>        set_config : Set global scikit-learn configuration.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return {k: getattr(_get_threadlocal_config(), k) for k in _global_config.keys()}</span>

<span class="w"> </span>    def set_config(assume_finite=None, working_memory=None,
<span class="w"> </span>        print_changed_only=None, display=None, pairwise_dist_chunk_size=
<span class="gu">@@ -142,7 +144,26 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        config_context : Context manager for global scikit-learn configuration.
<span class="w"> </span>        get_config : Retrieve current values of the global configuration.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        local_config = _get_threadlocal_config()</span>
<span class="gi">+</span>
<span class="gi">+        if assume_finite is not None:</span>
<span class="gi">+            local_config[&#39;assume_finite&#39;] = assume_finite</span>
<span class="gi">+        if working_memory is not None:</span>
<span class="gi">+            local_config[&#39;working_memory&#39;] = working_memory</span>
<span class="gi">+        if print_changed_only is not None:</span>
<span class="gi">+            local_config[&#39;print_changed_only&#39;] = print_changed_only</span>
<span class="gi">+        if display is not None:</span>
<span class="gi">+            local_config[&#39;display&#39;] = display</span>
<span class="gi">+        if pairwise_dist_chunk_size is not None:</span>
<span class="gi">+            local_config[&#39;pairwise_dist_chunk_size&#39;] = pairwise_dist_chunk_size</span>
<span class="gi">+        if enable_cython_pairwise_dist is not None:</span>
<span class="gi">+            local_config[&#39;enable_cython_pairwise_dist&#39;] = enable_cython_pairwise_dist</span>
<span class="gi">+        if transform_output is not None:</span>
<span class="gi">+            local_config[&#39;transform_output&#39;] = transform_output</span>
<span class="gi">+        if enable_metadata_routing is not None:</span>
<span class="gi">+            local_config[&#39;enable_metadata_routing&#39;] = enable_metadata_routing</span>
<span class="gi">+        if skip_parameter_validation is not None:</span>
<span class="gi">+            local_config[&#39;skip_parameter_validation&#39;] = skip_parameter_validation</span>

<span class="w"> </span>    @contextmanager
<span class="w"> </span>    def config_context(*, assume_finite=None, working_memory=None,
<span class="gu">@@ -270,6 +291,19 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        ...
<span class="w"> </span>        ValueError: Input contains NaN...
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        old_config = get_config().copy()</span>
<span class="gi">+        set_config(assume_finite=assume_finite,</span>
<span class="gi">+                   working_memory=working_memory,</span>
<span class="gi">+                   print_changed_only=print_changed_only,</span>
<span class="gi">+                   display=display,</span>
<span class="gi">+                   pairwise_dist_chunk_size=pairwise_dist_chunk_size,</span>
<span class="gi">+                   enable_cython_pairwise_dist=enable_cython_pairwise_dist,</span>
<span class="gi">+                   transform_output=transform_output,</span>
<span class="gi">+                   enable_metadata_routing=enable_metadata_routing,</span>
<span class="gi">+                   skip_parameter_validation=skip_parameter_validation)</span>
<span class="gi">+        try:</span>
<span class="gi">+            yield</span>
<span class="gi">+        finally:</span>
<span class="gi">+            set_config(**old_config)</span>
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn._config import _get_threadlocal_config, _global_config, config_context, get_config
<span class="gh">diff --git a/imblearn/base.py b/imblearn/base.py</span>
<span class="gh">index b4c50a8..b2727a8 100644</span>
<span class="gd">--- a/imblearn/base.py</span>
<span class="gi">+++ b/imblearn/base.py</span>
<span class="gu">@@ -27,7 +27,12 @@ class _ParamsValidationMixin:</span>
<span class="w"> </span>        the docstring of `validate_parameter_constraints` for a description of the
<span class="w"> </span>        accepted constraints.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(self, &quot;_parameter_constraints&quot;):</span>
<span class="gi">+            validate_parameter_constraints(</span>
<span class="gi">+                self._parameter_constraints,</span>
<span class="gi">+                self.get_params(deep=False),</span>
<span class="gi">+                caller_name=self.__class__.__name__,</span>
<span class="gi">+            )</span>


<span class="w"> </span>class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):
<span class="gu">@@ -56,7 +61,11 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        X, y = self._validate_data(X, y, reset=True)</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.
<span class="gu">@@ -77,7 +86,21 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>
<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_classification_targets(y)</span>
<span class="gi">+        arrays_transformer = ArraysTransformer(X, y)</span>
<span class="gi">+        X, y, binarize_y = self._check_X_y(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        output = self._fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        y_ = (label_binarize(output[1], classes=np.unique(y))</span>
<span class="gi">+              if binarize_y else output[1])</span>
<span class="gi">+</span>
<span class="gi">+        X_, y_ = arrays_transformer.transform(output[0], y_)</span>
<span class="gi">+        return (X_, y_) if len(output) == 2 else (X_, y_, output[2])</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def _fit_resample(self, X, y):
<span class="gu">@@ -132,7 +155,11 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        X, y = self._validate_data(X, y, reset=True)</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.
<span class="gu">@@ -153,7 +180,21 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>
<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_classification_targets(y)</span>
<span class="gi">+        arrays_transformer = ArraysTransformer(X, y)</span>
<span class="gi">+        X, y, binarize_y = self._check_X_y(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        output = self._fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        y_ = (label_binarize(output[1], classes=np.unique(y))</span>
<span class="gi">+              if binarize_y else output[1])</span>
<span class="gi">+</span>
<span class="gi">+        X_, y_ = arrays_transformer.transform(output[0], y_)</span>
<span class="gi">+        return (X_, y_) if len(output) == 2 else (X_, y_, output[2])</span>


<span class="w"> </span>def is_sampler(estimator):
<span class="gu">@@ -169,7 +210,8 @@ def is_sampler(estimator):</span>
<span class="w"> </span>    is_sampler : bool
<span class="w"> </span>        True if estimator is a sampler, otherwise False.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return (hasattr(estimator, &#39;fit_resample&#39;) and</span>
<span class="gi">+            hasattr(estimator, &#39;_sampling_type&#39;))</span>


<span class="w"> </span>class FunctionSampler(BaseSampler):
<span class="gu">@@ -291,7 +333,13 @@ class FunctionSampler(BaseSampler):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        </span>
<span class="gi">+        if self.validate:</span>
<span class="gi">+            X, y = self._validate_data(X, y, reset=True)</span>
<span class="gi">+        </span>
<span class="gi">+        self.sampling_strategy_ = &quot;bypass&quot;</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.
<span class="gu">@@ -312,4 +360,22 @@ class FunctionSampler(BaseSampler):</span>
<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        </span>
<span class="gi">+        if self.validate:</span>
<span class="gi">+            X, y = self._validate_data(X, y, reset=True)</span>
<span class="gi">+        </span>
<span class="gi">+        if self.func is None:</span>
<span class="gi">+            return X, y</span>
<span class="gi">+        </span>
<span class="gi">+        func_params = self.func.__code__.co_varnames</span>
<span class="gi">+        if &#39;y&#39; in func_params:</span>
<span class="gi">+            if self.kw_args is None:</span>
<span class="gi">+                return self.func(X, y)</span>
<span class="gi">+            else:</span>
<span class="gi">+                return self.func(X, y, **self.kw_args)</span>
<span class="gi">+        else:</span>
<span class="gi">+            if self.kw_args is None:</span>
<span class="gi">+                return self.func(X)</span>
<span class="gi">+            else:</span>
<span class="gi">+                return self.func(X, **self.kw_args)</span>
<span class="gh">diff --git a/imblearn/combine/_smote_enn.py b/imblearn/combine/_smote_enn.py</span>
<span class="gh">index 451604e..9d7655f 100644</span>
<span class="gd">--- a/imblearn/combine/_smote_enn.py</span>
<span class="gi">+++ b/imblearn/combine/_smote_enn.py</span>
<span class="gu">@@ -114,4 +114,22 @@ class SMOTEENN(BaseSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to validate SMOTE and ENN objects&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.smote is None:</span>
<span class="gi">+            self.smote_ = SMOTE(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.smote_ = clone(self.smote)</span>
<span class="gi">+</span>
<span class="gi">+        if self.enn is None:</span>
<span class="gi">+            self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+                sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.enn_ = clone(self.enn)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.sampling_strategy, dict):</span>
<span class="gi">+            self.smote_.sampling_strategy = self.sampling_strategy</span>
<span class="gh">diff --git a/imblearn/combine/_smote_tomek.py b/imblearn/combine/_smote_tomek.py</span>
<span class="gh">index 2bbf9bf..94e7ec8 100644</span>
<span class="gd">--- a/imblearn/combine/_smote_tomek.py</span>
<span class="gi">+++ b/imblearn/combine/_smote_tomek.py</span>
<span class="gu">@@ -111,5 +111,20 @@ class SMOTETomek(BaseSampler):</span>
<span class="w"> </span>        self.n_jobs = n_jobs

<span class="w"> </span>    def _validate_estimator(self):
<span class="gd">-        &quot;&quot;&quot;Private function to validate SMOTE and ENN objects&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+        &quot;&quot;&quot;Private function to validate SMOTE and Tomek objects&quot;&quot;&quot;</span>
<span class="gi">+        if self.smote is None:</span>
<span class="gi">+            self.smote_ = SMOTE(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.smote_ = clone(self.smote)</span>
<span class="gi">+</span>
<span class="gi">+        if self.tomek is None:</span>
<span class="gi">+            self.tomek_ = TomekLinks(</span>
<span class="gi">+                sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.tomek_ = clone(self.tomek)</span>
<span class="gh">diff --git a/imblearn/datasets/_imbalance.py b/imblearn/datasets/_imbalance.py</span>
<span class="gh">index 53e40de..f78e9db 100644</span>
<span class="gd">--- a/imblearn/datasets/_imbalance.py</span>
<span class="gi">+++ b/imblearn/datasets/_imbalance.py</span>
<span class="gu">@@ -82,4 +82,20 @@ def make_imbalance(X, y, *, sampling_strategy=None, random_state=None,</span>
<span class="w"> </span>    &gt;&gt;&gt; print(f&#39;Distribution after imbalancing: {Counter(y_res)}&#39;)
<span class="w"> </span>    Distribution after imbalancing: Counter({2: 30, 1: 20, 0: 10})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Check and validate the sampling_strategy</span>
<span class="gi">+    sampling_strategy_ = check_sampling_strategy(sampling_strategy, y, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    # Create a RandomUnderSampler instance</span>
<span class="gi">+    undersampler = RandomUnderSampler(</span>
<span class="gi">+        sampling_strategy=sampling_strategy_,</span>
<span class="gi">+        random_state=random_state,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    # Fit and resample the data</span>
<span class="gi">+    X_resampled, y_resampled = undersampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        print(f&quot;Original dataset shape: {Counter(y)}&quot;)</span>
<span class="gi">+        print(f&quot;Resampled dataset shape: {Counter(y_resampled)}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/datasets/_zenodo.py b/imblearn/datasets/_zenodo.py</span>
<span class="gh">index f9062b9..115a77a 100644</span>
<span class="gd">--- a/imblearn/datasets/_zenodo.py</span>
<span class="gi">+++ b/imblearn/datasets/_zenodo.py</span>
<span class="gu">@@ -185,4 +185,49 @@ def fetch_datasets(*, data_home=None, filter_data=None, download_if_missing</span>
<span class="w"> </span>       Imbalanced Data Learning and their Application in Bioinformatics.&quot;
<span class="w"> </span>       Dissertation, Georgia State University, (2011).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    data_home = get_data_home(data_home=data_home)</span>
<span class="gi">+    zenodo_dir = join(data_home, &quot;zenodo&quot;)</span>
<span class="gi">+    datasets = OrderedDict()</span>
<span class="gi">+</span>
<span class="gi">+    if not isfile(join(zenodo_dir, &quot;tar_data.npz&quot;)):</span>
<span class="gi">+        if download_if_missing:</span>
<span class="gi">+            if verbose:</span>
<span class="gi">+                print(&quot;Downloading dataset from %s (14.2 MB)&quot; % URL)</span>
<span class="gi">+            _fetch_remote(URL, zenodo_dir)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise IOError(&quot;Data not found and `download_if_missing` is False&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # Load the data from the Zenodo archive</span>
<span class="gi">+    with tarfile.open(join(zenodo_dir, &quot;tar_data.npz&quot;), &quot;r:gz&quot;) as tar:</span>
<span class="gi">+        for member in tar.getmembers():</span>
<span class="gi">+            if member.name.endswith(POST_FILENAME):</span>
<span class="gi">+                f = tar.extractfile(member)</span>
<span class="gi">+                if f is None:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                data = np.load(BytesIO(f.read()))</span>
<span class="gi">+                X = data[&quot;data&quot;]</span>
<span class="gi">+                y = data[&quot;label&quot;]</span>
<span class="gi">+                name = member.name.split(&quot;/&quot;)[1]</span>
<span class="gi">+                datasets[name] = Bunch(data=X, target=y, DESCR=data[&quot;DESCR&quot;].item())</span>
<span class="gi">+</span>
<span class="gi">+    # Filter the data if requested</span>
<span class="gi">+    if filter_data is not None:</span>
<span class="gi">+        filtered_datasets = OrderedDict()</span>
<span class="gi">+        for key in filter_data:</span>
<span class="gi">+            if isinstance(key, str):</span>
<span class="gi">+                if key in MAP_NAME_ID:</span>
<span class="gi">+                    filtered_datasets[key] = datasets[MAP_NAME_ID_KEYS[MAP_NAME_ID[key] - 1]]</span>
<span class="gi">+            elif isinstance(key, int):</span>
<span class="gi">+                if key in MAP_ID_NAME:</span>
<span class="gi">+                    filtered_datasets[MAP_ID_NAME[key]] = datasets[MAP_NAME_ID_KEYS[key - 1]]</span>
<span class="gi">+        datasets = filtered_datasets</span>
<span class="gi">+</span>
<span class="gi">+    # Shuffle the data if requested</span>
<span class="gi">+    if shuffle:</span>
<span class="gi">+        random_state = check_random_state(random_state)</span>
<span class="gi">+        for name in datasets:</span>
<span class="gi">+            datasets[name].data, datasets[name].target = shuffle_method(</span>
<span class="gi">+                datasets[name].data, datasets[name].target, random_state=random_state</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    return datasets</span>
<span class="gh">diff --git a/imblearn/ensemble/_bagging.py b/imblearn/ensemble/_bagging.py</span>
<span class="gh">index b1905ed..e3463e7 100644</span>
<span class="gd">--- a/imblearn/ensemble/_bagging.py</span>
<span class="gi">+++ b/imblearn/ensemble/_bagging.py</span>
<span class="gu">@@ -266,12 +266,29 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>    def _validate_estimator(self, default=DecisionTreeClassifier()):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            self.estimator_ = default</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.estimator_, (Pipeline, BaseUnderSampler)):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;&#39;{type(self.estimator_).__name__}&#39; is not supported for the&quot;</span>
<span class="gi">+                &quot; &#39;estimator&#39; parameter. Use a classifier instead.&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # TODO: Remove this property once the minimum supported scikit-learn version is 1.2</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;The `n_features_` attribute is deprecated in scikit-learn 1.0 and &quot;</span>
<span class="gi">+            &quot;will be removed in version 1.2. When the minimum version of &quot;</span>
<span class="gi">+            &quot;scikit-learn supported by imbalanced-learn reaches 1.2, this &quot;</span>
<span class="gi">+            &quot;attribute will be removed.&quot;,</span>
<span class="gi">+            FutureWarning</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.n_features_in_</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=False)
<span class="w"> </span>    def fit(self, X, y):
<span class="gu">@@ -292,7 +309,63 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Fitted estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check parameters</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+</span>
<span class="gi">+        # Check data</span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X, y, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;], dtype=None,</span>
<span class="gi">+            force_all_finite=False, multi_output=True</span>
<span class="gi">+        )</span>
<span class="gi">+        </span>
<span class="gi">+        # Check target type</span>
<span class="gi">+        y = check_target_type(y)</span>
<span class="gi">+        self.classes_ = np.unique(y)</span>
<span class="gi">+        self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        # Check sampling_strategy</span>
<span class="gi">+        self._sampling_strategy = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, is_ensemble=True</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Validate the estimator</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        # Initialize and validate the sampler</span>
<span class="gi">+        if self.sampler is None:</span>
<span class="gi">+            self.sampler_ = RandomUnderSampler(</span>
<span class="gi">+                sampling_strategy=self._sampling_strategy,</span>
<span class="gi">+                replacement=self.replacement,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.sampler_ = clone(self.sampler)</span>
<span class="gi">+</span>
<span class="gi">+        # Remap y for numpy indexing</span>
<span class="gi">+        self._y = np.searchsorted(self.classes_, y)</span>
<span class="gi">+</span>
<span class="gi">+        # Initialize attributes</span>
<span class="gi">+        self.estimators_ = []</span>
<span class="gi">+        self.estimators_samples_ = []</span>
<span class="gi">+        self.estimators_features_ = []</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_more_estimators = self.n_estimators - len(self.estimators_)</span>
<span class="gi">+        n_jobs = effective_n_jobs(self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+        if n_jobs == 1:</span>
<span class="gi">+            for _ in range(n_more_estimators):</span>
<span class="gi">+                self._parallel_build_estimators(X, self._y)</span>
<span class="gi">+        else:</span>
<span class="gi">+            # TODO: Implement parallel processing using joblib or sklearn&#39;s Parallel</span>
<span class="gi">+</span>
<span class="gi">+        # Set attributes</span>
<span class="gi">+        self.n_features_in_ = X.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+        if hasattr(X, &quot;columns&quot;):</span>
<span class="gi">+            self.feature_names_in_ = np.asarray(X.columns)</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    @available_if(_estimator_has(&#39;decision_function&#39;))
<span class="w"> </span>    def decision_function(self, X):
<span class="gu">@@ -312,9 +385,37 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>            ``classes_``. Regression and binary classification are special
<span class="w"> </span>            cases with ``k == 1``, otherwise ``k==n_classes``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+        </span>
<span class="gi">+        # Check data</span>
<span class="gi">+        X = self._validate_data(</span>
<span class="gi">+            X, accept_sparse=[&#39;csr&#39;, &#39;csc&#39;], dtype=None,</span>
<span class="gi">+            force_all_finite=False, reset=False</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(</span>
<span class="gi">+            delayed(_parallel_decision_function)(</span>
<span class="gi">+                self.estimators_[starts[i]:starts[i + 1]],</span>
<span class="gi">+                self.estimators_features_[starts[i]:starts[i + 1]],</span>
<span class="gi">+                X</span>
<span class="gi">+            )</span>
<span class="gi">+            for i in range(n_jobs)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Reduce</span>
<span class="gi">+        decisions = sum(all_decisions) / self.n_estimators</span>
<span class="gi">+</span>
<span class="gi">+        return decisions</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def base_estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Attribute for older sklearn version compatibility.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;The `base_estimator_` attribute is deprecated in scikit-learn 1.2 &quot;</span>
<span class="gi">+            &quot;and will be removed in 1.4. Use `estimator_` instead.&quot;,</span>
<span class="gi">+            FutureWarning</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.estimator_</span>
<span class="gh">diff --git a/imblearn/ensemble/_common.py b/imblearn/ensemble/_common.py</span>
<span class="gh">index f7dcb6e..7a28f55 100644</span>
<span class="gd">--- a/imblearn/ensemble/_common.py</span>
<span class="gi">+++ b/imblearn/ensemble/_common.py</span>
<span class="gu">@@ -8,7 +8,13 @@ def _estimator_has(attr):</span>
<span class="w"> </span>    First, we check the first fitted estimator if available, otherwise we
<span class="w"> </span>    check the estimator attribute.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    def check(estimator):</span>
<span class="gi">+        return hasattr(estimator, attr)</span>
<span class="gi">+</span>
<span class="gi">+    return lambda self: (</span>
<span class="gi">+        check(self.estimators_[0]) if hasattr(self, &quot;estimators_&quot;) and self.estimators_</span>
<span class="gi">+        else check(self.estimator)</span>
<span class="gi">+    )</span>


<span class="w"> </span>_bagging_parameter_constraints = {&#39;estimator&#39;: [HasMethods([&#39;fit&#39;,
<span class="gh">diff --git a/imblearn/ensemble/_easy_ensemble.py b/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gh">index 6d79bd6..afbbd18 100644</span>
<span class="gd">--- a/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gi">+++ b/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gu">@@ -187,16 +187,26 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.replacement = replacement

<span class="gd">-    def _validate_estimator(self, default=AdaBoostClassifier(algorithm=&#39;SAMME&#39;)</span>
<span class="gd">-        ):</span>
<span class="gi">+    def _validate_estimator(self, default=AdaBoostClassifier(algorithm=&#39;SAMME&#39;)):</span>
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            self.estimator_ = default</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.estimator_, (AdaBoostClassifier)):</span>
<span class="gi">+            if not hasattr(self.estimator_, &quot;n_estimators&quot;):</span>
<span class="gi">+                self.estimator_.n_estimators = self.n_estimators</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(&quot;estimator must be an AdaBoostClassifier&quot;)</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check if the estimator is fitted</span>
<span class="gi">+        check_is_fitted(self, &quot;estimators_&quot;)</span>
<span class="gi">+        return self.estimators_[0].n_features_in_</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=False)
<span class="w"> </span>    def fit(self, X, y):
<span class="gu">@@ -217,7 +227,49 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Fitted estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check that X and y have correct shape</span>
<span class="gi">+        X, y = self._validate_data(X, y, accept_sparse=[&#39;csr&#39;, &#39;csc&#39;])</span>
<span class="gi">+</span>
<span class="gi">+        # Check target type</span>
<span class="gi">+        y = check_target_type(y)</span>
<span class="gi">+        self.classes_ = np.unique(y)</span>
<span class="gi">+        self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        # Check parameters</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        # Validate sampling_strategy</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, &#39;under-sampling&#39;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Initialize random state</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        # Initialize estimators_</span>
<span class="gi">+        self.estimators_ = []</span>
<span class="gi">+        self.estimators_features_ = []</span>
<span class="gi">+        self.estimators_samples_ = []</span>
<span class="gi">+</span>
<span class="gi">+        for i in range(self.n_estimators):</span>
<span class="gi">+            # Apply random undersampling</span>
<span class="gi">+            rus = RandomUnderSampler(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy_,</span>
<span class="gi">+                replacement=self.replacement,</span>
<span class="gi">+                random_state=random_state.randint(MAX_INT)</span>
<span class="gi">+            )</span>
<span class="gi">+            X_resampled, y_resampled = rus.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+            # Train AdaBoost on the resampled data</span>
<span class="gi">+            estimator = clone(self.estimator_)</span>
<span class="gi">+            estimator.random_state = random_state.randint(MAX_INT)</span>
<span class="gi">+            estimator.fit(X_resampled, y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+            self.estimators_.append(estimator)</span>
<span class="gi">+            self.estimators_features_.append(np.arange(X.shape[1]))</span>
<span class="gi">+            self.estimators_samples_.append(rus.sample_indices_)</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    @available_if(_estimator_has(&#39;decision_function&#39;))
<span class="w"> </span>    def decision_function(self, X):
<span class="gu">@@ -237,9 +289,29 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>            ``classes_``. Regression and binary classification are special
<span class="w"> </span>            cases with ``k == 1``, otherwise ``k==n_classes``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+</span>
<span class="gi">+        # Check data</span>
<span class="gi">+        X = self._validate_data(X, accept_sparse=[&#39;csr&#39;, &#39;csc&#39;], reset=False)</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(</span>
<span class="gi">+            delayed(_parallel_decision_function)(</span>
<span class="gi">+                self.estimators_[starts[i]:starts[i + 1]],</span>
<span class="gi">+                self.estimators_features_[starts[i]:starts[i + 1]],</span>
<span class="gi">+                X</span>
<span class="gi">+            )</span>
<span class="gi">+            for i in range(n_jobs)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Reduce</span>
<span class="gi">+        decisions = sum(all_decisions) / self.n_estimators</span>
<span class="gi">+</span>
<span class="gi">+        return decisions</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def base_estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Attribute for older sklearn version compatibility.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.estimator_</span>
<span class="gh">diff --git a/imblearn/ensemble/_forest.py b/imblearn/ensemble/_forest.py</span>
<span class="gh">index 6a8de5d..1d9b486 100644</span>
<span class="gd">--- a/imblearn/ensemble/_forest.py</span>
<span class="gi">+++ b/imblearn/ensemble/_forest.py</span>
<span class="gu">@@ -400,14 +400,22 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>    def _validate_estimator(self, default=DecisionTreeClassifier()):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not isinstance(self.estimator, DecisionTreeClassifier):</span>
<span class="gi">+            raise ValueError(&quot;estimator must be a DecisionTreeClassifier&quot;)</span>
<span class="gi">+        self.estimator_ = clone(self.estimator)</span>

<span class="w"> </span>    def _make_sampler_estimator(self, random_state=None):
<span class="w"> </span>        &quot;&quot;&quot;Make and configure a copy of the `base_estimator_` attribute.
<span class="w"> </span>        Warning: This method should be used to properly instantiate new
<span class="w"> </span>        sub-estimators.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = clone(self.estimator_)</span>
<span class="gi">+        estimator.set_params(**{p: getattr(self, p)</span>
<span class="gi">+                                for p in self.estimator_params</span>
<span class="gi">+                                if p != &#39;random_state&#39;})</span>
<span class="gi">+        if random_state is not None:</span>
<span class="gi">+            estimator.set_params(random_state=random_state)</span>
<span class="gi">+        return estimator</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=True)
<span class="w"> </span>    def fit(self, X, y, sample_weight=None):
<span class="gu">@@ -436,7 +444,114 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            The fitted instance.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check input</span>
<span class="gi">+        X, y = self._validate_data(X, y, multi_output=True,</span>
<span class="gi">+                                   accept_sparse=&quot;csc&quot;, dtype=DTYPE)</span>
<span class="gi">+        if sample_weight is not None:</span>
<span class="gi">+            sample_weight = _check_sample_weight(sample_weight, X)</span>
<span class="gi">+</span>
<span class="gi">+        if issparse(X):</span>
<span class="gi">+            # Pre-sort indices to avoid that each individual tree of the</span>
<span class="gi">+            # ensemble sorts the indices.</span>
<span class="gi">+            X.sort_indices()</span>
<span class="gi">+</span>
<span class="gi">+        # Remap output</span>
<span class="gi">+        self.n_features_in_ = X.shape[1]</span>
<span class="gi">+        </span>
<span class="gi">+        y = np.atleast_1d(y)</span>
<span class="gi">+        if y.ndim == 2 and y.shape[1] == 1:</span>
<span class="gi">+            warn(&quot;A column-vector y was passed when a 1d array was&quot;</span>
<span class="gi">+                 &quot; expected. Please change the shape of y to &quot;</span>
<span class="gi">+                 &quot;(n_samples,), for example using ravel().&quot;,</span>
<span class="gi">+                 DataConversionWarning, stacklevel=2)</span>
<span class="gi">+</span>
<span class="gi">+        if y.ndim == 1:</span>
<span class="gi">+            # reshape is necessary to preserve the data contiguity against vs</span>
<span class="gi">+            # [:, np.newaxis] that does not.</span>
<span class="gi">+            y = np.reshape(y, (-1, 1))</span>
<span class="gi">+</span>
<span class="gi">+        self.n_outputs_ = y.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+        y, expanded_class_weight = self._validate_y_class_weight(y)</span>
<span class="gi">+</span>
<span class="gi">+        if getattr(y, &quot;dtype&quot;, None) != DOUBLE or not y.flags.contiguous:</span>
<span class="gi">+            y = np.ascontiguousarray(y, dtype=DOUBLE)</span>
<span class="gi">+</span>
<span class="gi">+        if expanded_class_weight is not None:</span>
<span class="gi">+            if sample_weight is not None:</span>
<span class="gi">+                sample_weight = sample_weight * expanded_class_weight</span>
<span class="gi">+            else:</span>
<span class="gi">+                sample_weight = expanded_class_weight</span>
<span class="gi">+</span>
<span class="gi">+        # Get bootstrap sample size</span>
<span class="gi">+        n_samples_bootstrap = _get_n_samples_bootstrap(</span>
<span class="gi">+            n_samples=X.shape[0],</span>
<span class="gi">+            max_samples=self.max_samples</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Check parameters</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        if not self.bootstrap and self.oob_score:</span>
<span class="gi">+            raise ValueError(&quot;Out of bag estimation only available&quot;</span>
<span class="gi">+                             &quot; if bootstrap=True&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        if not self.warm_start or not hasattr(self, &quot;estimators_&quot;):</span>
<span class="gi">+            # Free allocated memory, if any</span>
<span class="gi">+            self.estimators_ = []</span>
<span class="gi">+            self.estimators_features_ = []</span>
<span class="gi">+</span>
<span class="gi">+        n_more_estimators = self.n_estimators - len(self.estimators_)</span>
<span class="gi">+</span>
<span class="gi">+        if n_more_estimators &lt; 0:</span>
<span class="gi">+            raise ValueError(&#39;n_estimators=%d must be larger or equal to &#39;</span>
<span class="gi">+                             &#39;len(estimators_)=%d when warm_start==True&#39;</span>
<span class="gi">+                             % (self.n_estimators, len(self.estimators_)))</span>
<span class="gi">+</span>
<span class="gi">+        elif n_more_estimators == 0:</span>
<span class="gi">+            warn(&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
<span class="gi">+                 &quot;fit new trees.&quot;)</span>
<span class="gi">+            return self</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,</span>
<span class="gi">+                                                             self.n_jobs)</span>
<span class="gi">+        total_n_estimators = sum(n_estimators)</span>
<span class="gi">+</span>
<span class="gi">+        # Advance random state to state after training</span>
<span class="gi">+        # the first n_estimators</span>
<span class="gi">+        if self.warm_start and len(self.estimators_) &gt; 0:</span>
<span class="gi">+            random_state.randint(MAX_INT, size=len(self.estimators_))</span>
<span class="gi">+</span>
<span class="gi">+        trees = []</span>
<span class="gi">+        for i in range(total_n_estimators):</span>
<span class="gi">+            tree = self._make_sampler_estimator(random_state=random_state.randint(MAX_INT))</span>
<span class="gi">+            trees.append(tree)</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,</span>
<span class="gi">+                               **self._parallel_args())(</span>
<span class="gi">+            delayed(_parallel_build_trees)(</span>
<span class="gi">+                t, self, X, y, sample_weight, i, len(trees),</span>
<span class="gi">+                verbose=self.verbose, class_weight=self.class_weight,</span>
<span class="gi">+                n_samples_bootstrap=n_samples_bootstrap)</span>
<span class="gi">+            for i, t in enumerate(trees))</span>
<span class="gi">+</span>
<span class="gi">+        # Collect newly grown trees</span>
<span class="gi">+        self.estimators_.extend(tree for tree, _, _ in all_results)</span>
<span class="gi">+        self.estimators_features_.extend(features for _, features, _ in all_results)</span>
<span class="gi">+</span>
<span class="gi">+        if self.oob_score:</span>
<span class="gi">+            self._set_oob_score_and_attributes(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        # Decapsulate classes_ attributes</span>
<span class="gi">+        if hasattr(self, &quot;classes_&quot;) and self.n_outputs_ == 1:</span>
<span class="gi">+            self.n_classes_ = self.n_classes_[0]</span>
<span class="gi">+            self.classes_ = self.classes_[0]</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def _set_oob_score_and_attributes(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Compute and set the OOB score and attributes.
<span class="gu">@@ -448,7 +563,14 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        y : ndarray of shape (n_samples, n_outputs)
<span class="w"> </span>            The target matrix.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.oob_decision_function_ = self._compute_oob_predictions(X, y)</span>
<span class="gi">+        if self.oob_decision_function_.shape[-1] == 1:</span>
<span class="gi">+            # For binary problems, we need to normalize the OOB score</span>
<span class="gi">+            self.oob_decision_function_ = self.oob_decision_function_.ravel()</span>
<span class="gi">+        </span>
<span class="gi">+        self.oob_score_ = accuracy_score(</span>
<span class="gi">+            y, np.argmax(self.oob_decision_function_, axis=1)</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _compute_oob_predictions(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Compute and set the OOB score.
<span class="gu">@@ -462,12 +584,50 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or                 (n_samples, 1, n_outputs)</span>
<span class="gi">+        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or</span>
<span class="gi">+                   (n_samples, 1, n_outputs)</span>
<span class="w"> </span>            The OOB predictions.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        n_samples = y.shape[0]</span>
<span class="gi">+        n_classes = self.n_classes_</span>
<span class="gi">+        n_outputs = self.n_outputs_</span>
<span class="gi">+</span>
<span class="gi">+        oob_pred = np.zeros((n_samples, n_classes, n_outputs))</span>
<span class="gi">+        n_oob_pred = np.zeros((n_samples, n_outputs))</span>
<span class="gi">+</span>
<span class="gi">+        for estimator, features in zip(self.estimators_, self.estimators_features_):</span>
<span class="gi">+            unsampled_indices = _generate_unsampled_indices(</span>
<span class="gi">+                estimator.random_state, n_samples</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            y_pred = estimator.predict_proba(</span>
<span class="gi">+                X[unsampled_indices, :][:, features]</span>
<span class="gi">+            )</span>
<span class="gi">+            y_pred = np.array(y_pred, copy=False)</span>
<span class="gi">+</span>
<span class="gi">+            if n_outputs == 1:</span>
<span class="gi">+                y_pred = y_pred[..., np.newaxis]</span>
<span class="gi">+</span>
<span class="gi">+            oob_pred[unsampled_indices] += y_pred</span>
<span class="gi">+            n_oob_pred[unsampled_indices] += 1</span>
<span class="gi">+</span>
<span class="gi">+        for k in range(n_outputs):</span>
<span class="gi">+            if (n_oob_pred == 0).any():</span>
<span class="gi">+                warn(&quot;Some inputs do not have OOB scores. This probably means &quot;</span>
<span class="gi">+                     &quot;too few trees were used to compute any reliable OOB &quot;</span>
<span class="gi">+                     &quot;estimates.&quot;)</span>
<span class="gi">+                n_oob_pred[n_oob_pred == 0] = 1</span>
<span class="gi">+            oob_pred[..., k] /= n_oob_pred[..., k, np.newaxis]</span>
<span class="gi">+</span>
<span class="gi">+        if n_outputs == 1:</span>
<span class="gi">+            oob_pred = oob_pred[..., 0]</span>
<span class="gi">+</span>
<span class="gi">+        return oob_pred</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warn(&quot;The `n_features_` attribute is deprecated in 1.0 and will be &quot;</span>
<span class="gi">+             &quot;removed in 1.2. Use `n_features_in_` instead.&quot;,</span>
<span class="gi">+             FutureWarning)</span>
<span class="gi">+        return self.n_features_in_</span>
<span class="gh">diff --git a/imblearn/ensemble/_weight_boosting.py b/imblearn/ensemble/_weight_boosting.py</span>
<span class="gh">index 26f43c4..30c6004 100644</span>
<span class="gd">--- a/imblearn/ensemble/_weight_boosting.py</span>
<span class="gi">+++ b/imblearn/ensemble/_weight_boosting.py</span>
<span class="gu">@@ -193,26 +193,224 @@ class RUSBoostClassifier(_ParamsValidationMixin, AdaBoostClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Returns self.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # Check that algorithm is supported</span>
<span class="gi">+        if self.algorithm not in (&#39;SAMME&#39;, &#39;SAMME.R&#39;):</span>
<span class="gi">+            raise ValueError(&quot;algorithm %s is not supported&quot; % self.algorithm)</span>
<span class="gi">+</span>
<span class="gi">+        # Check parameters</span>
<span class="gi">+        if self.learning_rate &lt;= 0:</span>
<span class="gi">+            raise ValueError(&quot;learning_rate must be greater than zero&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        if (self.estimator is None or</span>
<span class="gi">+                isinstance(self.estimator, (DecisionTreeClassifier,</span>
<span class="gi">+                                            sklearn.tree.DecisionTreeClassifier))):</span>
<span class="gi">+            DTYPE = np.float64</span>
<span class="gi">+            dtype = DTYPE</span>
<span class="gi">+            accept_sparse = &#39;csc&#39;</span>
<span class="gi">+        else:</span>
<span class="gi">+            dtype = None</span>
<span class="gi">+            accept_sparse = [&#39;csr&#39;, &#39;csc&#39;]</span>
<span class="gi">+</span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X, y, accept_sparse=accept_sparse, dtype=dtype, y_numeric=False,</span>
<span class="gi">+            force_all_finite=False</span>
<span class="gi">+        )</span>
<span class="gi">+        check_target_type(y)</span>
<span class="gi">+        self.classes_ = np.unique(y)</span>
<span class="gi">+        n_classes = len(self.classes_)</span>
<span class="gi">+        n_samples = X.shape[0]</span>
<span class="gi">+</span>
<span class="gi">+        # Check sample_weight</span>
<span class="gi">+        if sample_weight is None:</span>
<span class="gi">+            # Initialize weights to 1 / n_samples</span>
<span class="gi">+            sample_weight = np.empty(n_samples, dtype=DTYPE)</span>
<span class="gi">+            sample_weight[:] = 1. / n_samples</span>
<span class="gi">+        else:</span>
<span class="gi">+            sample_weight = _check_sample_weight(sample_weight, X, DTYPE)</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        # Check random state</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        # Clear any previous fit results</span>
<span class="gi">+        self.estimators_ = []</span>
<span class="gi">+        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=DTYPE)</span>
<span class="gi">+        self.estimator_errors_ = np.ones(self.n_estimators, dtype=DTYPE)</span>
<span class="gi">+</span>
<span class="gi">+        # Create and fit base sampler</span>
<span class="gi">+        self.base_sampler_ = RandomUnderSampler(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            replacement=self.replacement,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        for iboost in range(self.n_estimators):</span>
<span class="gi">+            # Random undersampling</span>
<span class="gi">+            X_resampled, y_resampled = self.base_sampler_.fit_resample(X, y)</span>
<span class="gi">+            sample_weight_resampled = _safe_indexing(sample_weight, self.base_sampler_.sample_indices_)</span>
<span class="gi">+</span>
<span class="gi">+            # Boosting step</span>
<span class="gi">+            sample_weight_resampled, estimator_weight, estimator_error = self._boost(</span>
<span class="gi">+                iboost,</span>
<span class="gi">+                X_resampled, y_resampled,</span>
<span class="gi">+                sample_weight_resampled,</span>
<span class="gi">+                random_state</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            # Early termination</span>
<span class="gi">+            if sample_weight_resampled is None:</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+            self.estimator_weights_[iboost] = estimator_weight</span>
<span class="gi">+            self.estimator_errors_[iboost] = estimator_error</span>
<span class="gi">+</span>
<span class="gi">+            # Stop if error is zero</span>
<span class="gi">+            if estimator_error == 0:</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+            sample_weight_sum = np.sum(sample_weight_resampled)</span>
<span class="gi">+</span>
<span class="gi">+            # Stop if the sum of sample weights has become non-positive</span>
<span class="gi">+            if sample_weight_sum &lt;= 0:</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+            if iboost &lt; self.n_estimators - 1:</span>
<span class="gi">+                # Normalize</span>
<span class="gi">+                sample_weight_resampled /= sample_weight_sum</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute.

<span class="w"> </span>        Sets the `estimator_` attributes.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        if not has_fit_parameter(self.estimator_, &quot;sample_weight&quot;):</span>
<span class="gi">+            raise ValueError(&quot;%s doesn&#39;t support sample_weight.&quot;</span>
<span class="gi">+                             % self.estimator_.__class__.__name__)</span>

<span class="w"> </span>    def _make_sampler_estimator(self, append=True, random_state=None):
<span class="w"> </span>        &quot;&quot;&quot;Make and configure a copy of the `base_estimator_` attribute.
<span class="w"> </span>        Warning: This method should be used to properly instantiate new
<span class="w"> </span>        sub-estimators.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = clone(self.estimator_)</span>
<span class="gi">+        estimator.set_params(**{p: getattr(self, p)</span>
<span class="gi">+                                for p in self.estimator_params})</span>
<span class="gi">+</span>
<span class="gi">+        if random_state is not None:</span>
<span class="gi">+            _set_random_states(estimator, random_state)</span>
<span class="gi">+</span>
<span class="gi">+        if append:</span>
<span class="gi">+            self.estimators_.append(estimator)</span>
<span class="gi">+</span>
<span class="gi">+        return estimator</span>

<span class="w"> </span>    def _boost_real(self, iboost, X, y, sample_weight, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Implement a single boost using the SAMME.R real algorithm.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = self._make_sampler_estimator(random_state=random_state)</span>
<span class="gi">+</span>
<span class="gi">+        estimator.fit(X, y, sample_weight=sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict_proba = estimator.predict_proba(X)</span>
<span class="gi">+</span>
<span class="gi">+        if iboost == 0:</span>
<span class="gi">+            self.classes_ = getattr(estimator, &#39;classes_&#39;, None)</span>
<span class="gi">+            self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),</span>
<span class="gi">+                                       axis=0)</span>
<span class="gi">+</span>
<span class="gi">+        # Instances incorrectly classified</span>
<span class="gi">+        incorrect = y_predict != y</span>
<span class="gi">+</span>
<span class="gi">+        # Error fraction</span>
<span class="gi">+        estimator_error = np.mean(</span>
<span class="gi">+            np.average(incorrect, weights=sample_weight, axis=0))</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if classification is perfect</span>
<span class="gi">+        if estimator_error &lt;= 0:</span>
<span class="gi">+            return sample_weight, 1., 0.</span>
<span class="gi">+</span>
<span class="gi">+        # Construct y coding as described in Zhu et al [2]:</span>
<span class="gi">+        #</span>
<span class="gi">+        #    y_k = 1 if c == k else -1 / (K - 1)</span>
<span class="gi">+        #</span>
<span class="gi">+        # where K == n_classes_ and c, k in [0, K) are indices along the second</span>
<span class="gi">+        # axis of the y coding with c being the index corresponding to the true</span>
<span class="gi">+        # class label.</span>
<span class="gi">+        n_classes = self.n_classes_</span>
<span class="gi">+        classes = self.classes_</span>
<span class="gi">+        y_codes = np.array([-1. / (n_classes - 1), 1.])</span>
<span class="gi">+        y_coding = y_codes.take(classes == y[:, np.newaxis])</span>
<span class="gi">+</span>
<span class="gi">+        # Displace zero probabilities so the log is defined.</span>
<span class="gi">+        # Also fix negative elements which may occur with</span>
<span class="gi">+        # negative sample weights.</span>
<span class="gi">+        proba = y_predict_proba  # alias for readability</span>
<span class="gi">+        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)</span>
<span class="gi">+</span>
<span class="gi">+        # Boost weight using multi-class AdaBoost SAMME.R alg</span>
<span class="gi">+        estimator_weight = (-1. * self.learning_rate</span>
<span class="gi">+                            * (((n_classes - 1.) / n_classes) *</span>
<span class="gi">+                               inner1d(y_coding, np.log(y_predict_proba))))</span>
<span class="gi">+</span>
<span class="gi">+        # Only boost the weights if it will fit again</span>
<span class="gi">+        if not iboost == self.n_estimators - 1:</span>
<span class="gi">+            # Only boost positive weights</span>
<span class="gi">+            sample_weight *= np.exp(estimator_weight *</span>
<span class="gi">+                                    ((sample_weight &gt; 0) |</span>
<span class="gi">+                                     (estimator_weight &lt; 0)))</span>
<span class="gi">+</span>
<span class="gi">+        return sample_weight, 1., estimator_error</span>

<span class="w"> </span>    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Implement a single boost using the SAMME discrete algorithm.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = self._make_sampler_estimator(random_state=random_state)</span>
<span class="gi">+</span>
<span class="gi">+        estimator.fit(X, y, sample_weight=sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict = estimator.predict(X)</span>
<span class="gi">+</span>
<span class="gi">+        if iboost == 0:</span>
<span class="gi">+            self.classes_ = getattr(estimator, &#39;classes_&#39;, None)</span>
<span class="gi">+            self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        # Instances incorrectly classified</span>
<span class="gi">+        incorrect = y_predict != y</span>
<span class="gi">+</span>
<span class="gi">+        # Error fraction</span>
<span class="gi">+        estimator_error = np.mean(</span>
<span class="gi">+            np.average(incorrect, weights=sample_weight, axis=0))</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if classification is perfect</span>
<span class="gi">+        if estimator_error &lt;= 0:</span>
<span class="gi">+            return sample_weight, 1., 0.</span>
<span class="gi">+</span>
<span class="gi">+        n_classes = self.n_classes_</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if the error is at least as bad as random guessing</span>
<span class="gi">+        if estimator_error &gt;= 1. - (1. / n_classes):</span>
<span class="gi">+            self.estimators_.pop(-1)</span>
<span class="gi">+            if len(self.estimators_) == 0:</span>
<span class="gi">+                raise ValueError(&#39;BaseClassifier in AdaBoostClassifier &#39;</span>
<span class="gi">+                                 &#39;ensemble is worse than random, ensemble &#39;</span>
<span class="gi">+                                 &#39;can not be fit.&#39;)</span>
<span class="gi">+            return None, None, None</span>
<span class="gi">+</span>
<span class="gi">+        # Boost weight using multi-class AdaBoost SAMME alg</span>
<span class="gi">+        estimator_weight = self.learning_rate * (</span>
<span class="gi">+            np.log((1. - estimator_error) / estimator_error) +</span>
<span class="gi">+            np.log(n_classes - 1.))</span>
<span class="gi">+</span>
<span class="gi">+        # Only boost the weights if I will fit again</span>
<span class="gi">+        if not iboost == self.n_estimators - 1:</span>
<span class="gi">+            # Only boost positive weights</span>
<span class="gi">+            sample_weight *= np.exp(estimator_weight * incorrect *</span>
<span class="gi">+                                    ((sample_weight &gt; 0) |</span>
<span class="gi">+                                     (estimator_weight &lt; 0)))</span>
<span class="gi">+</span>
<span class="gi">+        return sample_weight, estimator_weight, estimator_error</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_bagging.py b/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gh">index 02d90ed..36ab0aa 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gu">@@ -31,4 +31,10 @@ class CountDecisionTreeClassifier(DecisionTreeClassifier):</span>

<span class="w"> </span>def test_balanced_bagging_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_features=4, n_informative=2, n_redundant=0, n_classes=2, random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    bbc = BalancedBaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)</span>
<span class="gi">+    bbc.fit(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;Attribute n_features_ was deprecated&quot;):</span>
<span class="gi">+        _ = bbc.n_features_</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_easy_ensemble.py b/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gh">index 3b667a8..4041f40 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gu">@@ -25,4 +25,16 @@ Y = np.array([1, 2, 2, 2, 1, 0, 1, 1, 1, 0])</span>

<span class="w"> </span>def test_easy_ensemble_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data, iris.target, sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=RND_SEED</span>
<span class="gi">+    )</span>
<span class="gi">+    eec = EasyEnsembleClassifier(random_state=RND_SEED)</span>
<span class="gi">+    eec.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`n_features_` attribute is deprecated&quot;):</span>
<span class="gi">+        _ = eec.n_features_</span>
<span class="gi">+</span>
<span class="gi">+    # Check that n_features_in_ is available and correct</span>
<span class="gi">+    assert hasattr(eec, &quot;n_features_in_&quot;)</span>
<span class="gi">+    assert eec.n_features_in_ == X.shape[1]</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_forest.py b/imblearn/ensemble/tests/test_forest.py</span>
<span class="gh">index 2742293..bb9d1a4 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_forest.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_forest.py</span>
<span class="gu">@@ -11,21 +11,45 @@ sklearn_version = parse_version(sklearn.__version__)</span>

<span class="w"> </span>def test_balanced_bagging_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_features=4, n_informative=2, n_redundant=0, n_classes=2, random_state=0)</span>
<span class="gi">+    clf = BalancedRandomForestClassifier(n_estimators=5, random_state=0)</span>
<span class="gi">+    clf.fit(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`n_features_` is deprecated&quot;):</span>
<span class="gi">+        _ = clf.n_features_</span>


<span class="w"> </span>def test_balanced_random_forest_change_behaviour(imbalanced_dataset):
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a change of behaviour for the parameters `sampling_strategy`
<span class="w"> </span>    and `replacement`.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    clf = BalancedRandomForestClassifier(n_estimators=5, random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The default value of `sampling_strategy` will change&quot;):</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The default value of `replacement` will change&quot;):</span>
<span class="gi">+        BalancedRandomForestClassifier(n_estimators=5, random_state=0, sampling_strategy=&#39;auto&#39;).fit(X, y)</span>


<span class="w"> </span>@pytest.mark.skipif(parse_version(sklearn_version.base_version) &lt;
<span class="w"> </span>    parse_version(&#39;1.4&#39;), reason=&#39;scikit-learn should be &gt;= 1.4&#39;)
<span class="w"> </span>def test_missing_values_is_resilient():
<span class="w"> </span>    &quot;&quot;&quot;Check that forest can deal with missing values and has decent performance.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = load_iris(return_X_y=True)</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    # Introduce missing values</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    mask = rng.binomial(1, 0.2, X_train.shape).astype(bool)</span>
<span class="gi">+    X_train[mask] = np.nan</span>
<span class="gi">+    </span>
<span class="gi">+    clf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+    </span>
<span class="gi">+    score = clf.score(X_test, y_test)</span>
<span class="gi">+    assert score &gt; 0.8, f&quot;Performance with missing values is too low: {score}&quot;</span>


<span class="w"> </span>@pytest.mark.skipif(parse_version(sklearn_version.base_version) &lt;
<span class="gu">@@ -33,4 +57,25 @@ def test_missing_values_is_resilient():</span>
<span class="w"> </span>def test_missing_value_is_predictive():
<span class="w"> </span>    &quot;&quot;&quot;Check that the forest learns when missing values are only present for
<span class="w"> </span>    a predictive feature.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=1000, n_features=4, n_informative=2,</span>
<span class="gi">+                               n_redundant=0, n_repeated=0, n_classes=2,</span>
<span class="gi">+                               random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    # Make the first feature highly predictive</span>
<span class="gi">+    X[:, 0] = y + np.random.normal(0, 0.1, size=y.shape)</span>
<span class="gi">+    </span>
<span class="gi">+    # Introduce missing values only in the first feature</span>
<span class="gi">+    mask = np.random.RandomState(0).binomial(1, 0.5, X.shape[0]).astype(bool)</span>
<span class="gi">+    X[mask, 0] = np.nan</span>
<span class="gi">+    </span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    clf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+    </span>
<span class="gi">+    score = clf.score(X_test, y_test)</span>
<span class="gi">+    assert score &gt; 0.8, f&quot;Performance with predictive missing values is too low: {score}&quot;</span>
<span class="gi">+    </span>
<span class="gi">+    # Check feature importances</span>
<span class="gi">+    importances = clf.feature_importances_</span>
<span class="gi">+    assert importances[0] &gt; np.mean(importances[1:]), &quot;First feature should be most important&quot;</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_weight_boosting.py b/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gh">index c1d0d1c..03e82bf 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gu">@@ -3,7 +3,74 @@ import pytest</span>
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.model_selection import train_test_split
<span class="gd">-from sklearn.utils._testing import assert_array_equal</span>
<span class="gi">+from sklearn.tree import DecisionTreeClassifier</span>
<span class="gi">+from sklearn.utils._testing import assert_array_equal, assert_array_almost_equal</span>
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from imblearn.ensemble import RUSBoostClassifier
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gi">+</span>
<span class="gi">+class TestRUSBoostClassifier:</span>
<span class="gi">+    @pytest.mark.parametrize(</span>
<span class="gi">+        &quot;algorithm&quot;, [&quot;SAMME&quot;, &quot;SAMME.R&quot;]</span>
<span class="gi">+    )</span>
<span class="gi">+    def test_rusboost(self, algorithm):</span>
<span class="gi">+        X, y = make_classification(</span>
<span class="gi">+            n_samples=1000, n_classes=3, n_informative=4, weights=[0.2, 0.3, 0.5], random_state=0</span>
<span class="gi">+        )</span>
<span class="gi">+        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+        clf = RUSBoostClassifier(algorithm=algorithm, random_state=0)</span>
<span class="gi">+        clf.fit(X_train, y_train)</span>
<span class="gi">+        </span>
<span class="gi">+        assert hasattr(clf, &#39;estimators_&#39;)</span>
<span class="gi">+        assert len(clf.estimators_) &lt;= clf.n_estimators</span>
<span class="gi">+        assert clf.n_classes_ == 3</span>
<span class="gi">+</span>
<span class="gi">+        y_pred = clf.predict(X_test)</span>
<span class="gi">+        assert y_pred.shape == y_test.shape</span>
<span class="gi">+</span>
<span class="gi">+    def test_sample_weight(self):</span>
<span class="gi">+        X, y = make_classification(n_samples=1000, n_classes=3, random_state=0)</span>
<span class="gi">+        sample_weight = np.ones_like(y)</span>
<span class="gi">+        clf = RUSBoostClassifier(random_state=0)</span>
<span class="gi">+        clf.fit(X, y, sample_weight=sample_weight)</span>
<span class="gi">+        y_pred_weighted = clf.predict(X)</span>
<span class="gi">+</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+        y_pred_unweighted = clf.predict(X)</span>
<span class="gi">+</span>
<span class="gi">+        assert_array_equal(y_pred_weighted, y_pred_unweighted)</span>
<span class="gi">+</span>
<span class="gi">+    def test_staged_predict(self):</span>
<span class="gi">+        X, y = make_classification(n_samples=1000, n_classes=3, random_state=0)</span>
<span class="gi">+        clf = RUSBoostClassifier(n_estimators=10, random_state=0)</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        predictions = np.array([p for p in clf.staged_predict(X)])</span>
<span class="gi">+        assert predictions.shape == (10, X.shape[0])</span>
<span class="gi">+</span>
<span class="gi">+    def test_feature_importances(self):</span>
<span class="gi">+        X, y = make_classification(n_samples=1000, n_classes=3, n_informative=4, random_state=0)</span>
<span class="gi">+        clf = RUSBoostClassifier(random_state=0)</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+        </span>
<span class="gi">+        assert hasattr(clf, &#39;feature_importances_&#39;)</span>
<span class="gi">+        assert clf.feature_importances_.shape == (X.shape[1],)</span>
<span class="gi">+</span>
<span class="gi">+    @pytest.mark.parametrize(</span>
<span class="gi">+        &quot;sampling_strategy&quot;, [&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, 0.5]</span>
<span class="gi">+    )</span>
<span class="gi">+    def test_sampling_strategy(self, sampling_strategy):</span>
<span class="gi">+        X, y = make_classification(n_samples=1000, n_classes=3, weights=[0.2, 0.3, 0.5], random_state=0)</span>
<span class="gi">+        clf = RUSBoostClassifier(sampling_strategy=sampling_strategy, random_state=0)</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+        assert clf.base_sampler_.sampling_strategy == sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+    def test_custom_estimator(self):</span>
<span class="gi">+        X, y = make_classification(n_samples=1000, n_classes=3, random_state=0)</span>
<span class="gi">+        base_estimator = DecisionTreeClassifier(max_depth=2)</span>
<span class="gi">+        clf = RUSBoostClassifier(estimator=base_estimator, random_state=0)</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+        assert isinstance(clf.estimators_[0], DecisionTreeClassifier)</span>
<span class="gi">+        assert clf.estimators_[0].max_depth == 2</span>
<span class="gh">diff --git a/imblearn/exceptions.py b/imblearn/exceptions.py</span>
<span class="gh">index a78c1ad..c1ee98e 100644</span>
<span class="gd">--- a/imblearn/exceptions.py</span>
<span class="gi">+++ b/imblearn/exceptions.py</span>
<span class="gu">@@ -23,4 +23,8 @@ def raise_isinstance_error(variable_name, possible_type, variable):</span>
<span class="w"> </span>    ValueError
<span class="w"> </span>        If the instance is not of the possible type.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if not isinstance(variable, possible_type):</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;{variable_name} must be an instance of {possible_type.__name__}, &quot;</span>
<span class="gi">+            f&quot;got {type(variable).__name__} instead.&quot;</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/imblearn/keras/_generator.py b/imblearn/keras/_generator.py</span>
<span class="gh">index f3de87c..e47c7d3 100644</span>
<span class="gd">--- a/imblearn/keras/_generator.py</span>
<span class="gi">+++ b/imblearn/keras/_generator.py</span>
<span class="gu">@@ -6,7 +6,15 @@ def import_keras():</span>

<span class="w"> </span>    This is possible to import the sequence from keras or tensorflow.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    try:</span>
<span class="gi">+        from keras.utils import Sequence</span>
<span class="gi">+        return Sequence, True</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        try:</span>
<span class="gi">+            from tensorflow.keras.utils import Sequence</span>
<span class="gi">+            return Sequence, True</span>
<span class="gi">+        except ImportError:</span>
<span class="gi">+            return object, False</span>


<span class="w"> </span>ParentClass, HAS_KERAS = import_keras()
<span class="gu">@@ -204,4 +212,39 @@ def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="w"> </span>    ...                              steps_per_epoch=steps_per_epoch,
<span class="w"> </span>    ...                              epochs=10, verbose=0)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampler is None:</span>
<span class="gi">+        sampler = RandomUnderSampler(random_state=random_state)</span>
<span class="gi">+    </span>
<span class="gi">+    if not hasattr(sampler, &#39;fit_resample&#39;):</span>
<span class="gi">+        raise ValueError(&quot;&#39;sampler&#39; should have a &#39;fit_resample&#39; method.&quot;)</span>
<span class="gi">+    </span>
<span class="gi">+    sampler_ = clone(sampler)</span>
<span class="gi">+    X_resampled, y_resampled = sampler_.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    if sample_weight is not None:</span>
<span class="gi">+        sample_weight_resampled = _safe_indexing(sample_weight, sampler_.sample_indices_)</span>
<span class="gi">+    else:</span>
<span class="gi">+        sample_weight_resampled = None</span>
<span class="gi">+    </span>
<span class="gi">+    n_samples = X_resampled.shape[0]</span>
<span class="gi">+    steps_per_epoch = int(n_samples // batch_size)</span>
<span class="gi">+    </span>
<span class="gi">+    def generator():</span>
<span class="gi">+        while True:</span>
<span class="gi">+            indices = check_random_state(random_state).permutation(n_samples)</span>
<span class="gi">+            for start in range(0, n_samples, batch_size):</span>
<span class="gi">+                end = min(start + batch_size, n_samples)</span>
<span class="gi">+                batch_indices = indices[start:end]</span>
<span class="gi">+                X_batch = _safe_indexing(X_resampled, batch_indices)</span>
<span class="gi">+                y_batch = _safe_indexing(y_resampled, batch_indices)</span>
<span class="gi">+                </span>
<span class="gi">+                if not keep_sparse and issparse(X_batch):</span>
<span class="gi">+                    X_batch = X_batch.toarray()</span>
<span class="gi">+                </span>
<span class="gi">+                if sample_weight is None:</span>
<span class="gi">+                    yield X_batch, y_batch</span>
<span class="gi">+                else:</span>
<span class="gi">+                    sw_batch = _safe_indexing(sample_weight_resampled, batch_indices)</span>
<span class="gi">+                    yield X_batch, y_batch, sw_batch</span>
<span class="gi">+    </span>
<span class="gi">+    return generator(), steps_per_epoch</span>
<span class="gh">diff --git a/imblearn/metrics/_classification.py b/imblearn/metrics/_classification.py</span>
<span class="gh">index 6723b08..46f54c2 100644</span>
<span class="gd">--- a/imblearn/metrics/_classification.py</span>
<span class="gi">+++ b/imblearn/metrics/_classification.py</span>
<span class="gu">@@ -133,7 +133,55 @@ def sensitivity_specificity_support(y_true, y_pred, *, labels=None,</span>
<span class="w"> </span>    &gt;&gt;&gt; sensitivity_specificity_support(y_true, y_pred, average=&#39;weighted&#39;)
<span class="w"> </span>    (0.33..., 0.66..., None)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+    </span>
<span class="gi">+    if labels is None:</span>
<span class="gi">+        labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+    else:</span>
<span class="gi">+        labels = np.asarray(labels)</span>
<span class="gi">+</span>
<span class="gi">+    if y_type.startswith(&#39;multilabel&#39;):</span>
<span class="gi">+        raise ValueError(&quot;sensitivity_specificity_support is not defined for multilabel classification&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if pos_label is None:</span>
<span class="gi">+        if average == &#39;binary&#39;:</span>
<span class="gi">+            raise ValueError(&quot;Please specify the positive label explicitly &quot;</span>
<span class="gi">+                             &quot;in binary classification problems&quot;)</span>
<span class="gi">+        pos_label = 1</span>
<span class="gi">+</span>
<span class="gi">+    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels)</span>
<span class="gi">+    </span>
<span class="gi">+    with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):</span>
<span class="gi">+        per_class_sensitivity = np.diag(cm) / cm.sum(axis=1)</span>
<span class="gi">+        per_class_specificity = np.diag(cm) / cm.sum(axis=0)</span>
<span class="gi">+</span>
<span class="gi">+    if average == &#39;micro&#39;:</span>
<span class="gi">+        sensitivity = specificity = np.trace(cm) / cm.sum()</span>
<span class="gi">+        support = cm.sum()</span>
<span class="gi">+    elif average == &#39;macro&#39;:</span>
<span class="gi">+        sensitivity = np.mean(per_class_sensitivity)</span>
<span class="gi">+        specificity = np.mean(per_class_specificity)</span>
<span class="gi">+        support = None</span>
<span class="gi">+    elif average == &#39;weighted&#39;:</span>
<span class="gi">+        weights = cm.sum(axis=1)</span>
<span class="gi">+        sensitivity = np.average(per_class_sensitivity, weights=weights)</span>
<span class="gi">+        specificity = np.average(per_class_specificity, weights=weights)</span>
<span class="gi">+        support = None</span>
<span class="gi">+    elif average == &#39;binary&#39;:</span>
<span class="gi">+        if len(labels) != 2:</span>
<span class="gi">+            raise ValueError(&quot;Binary classification is only valid for two classes&quot;)</span>
<span class="gi">+        pos_label_idx = np.where(labels == pos_label)[0][0]</span>
<span class="gi">+        sensitivity = per_class_sensitivity[pos_label_idx]</span>
<span class="gi">+        specificity = per_class_specificity[pos_label_idx]</span>
<span class="gi">+        support = cm.sum(axis=1)[pos_label_idx]</span>
<span class="gi">+    elif average is None:</span>
<span class="gi">+        sensitivity = per_class_sensitivity</span>
<span class="gi">+        specificity = per_class_specificity</span>
<span class="gi">+        support = cm.sum(axis=1)</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(&quot;Unsupported &#39;average&#39; parameter&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return sensitivity, specificity, support</span>


<span class="w"> </span>@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],
<span class="gu">@@ -204,8 +252,8 @@ def sensitivity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>

<span class="w"> </span>    Returns
<span class="w"> </span>    -------
<span class="gd">-    specificity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gd">-        The specifcity metric.</span>
<span class="gi">+    sensitivity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+        The sensitivity metric.</span>

<span class="w"> </span>    Examples
<span class="w"> </span>    --------
<span class="gu">@@ -222,7 +270,15 @@ def sensitivity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="w"> </span>    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([1., 0., 0.])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    sensitivity, _, _ = sensitivity_specificity_support(</span>
<span class="gi">+        y_true, y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        pos_label=pos_label,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+        warn_for=(&#39;sensitivity&#39;,),</span>
<span class="gi">+        sample_weight=sample_weight</span>
<span class="gi">+    )</span>
<span class="gi">+    return sensitivity</span>


<span class="w"> </span>@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],
<span class="gu">@@ -311,7 +367,15 @@ def specificity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="w"> </span>    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([0.75, 0.5 , 0.75])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    _, specificity, _ = sensitivity_specificity_support(</span>
<span class="gi">+        y_true, y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        pos_label=pos_label,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+        warn_for=(&#39;specificity&#39;,),</span>
<span class="gi">+        sample_weight=sample_weight</span>
<span class="gi">+    )</span>
<span class="gi">+    return specificity</span>


<span class="w"> </span>@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],
<span class="gu">@@ -435,7 +499,31 @@ def geometric_mean_score(y_true, y_pred, *, labels=None, pos_label=1,</span>
<span class="w"> </span>    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([0.866...,  0.       ,  0.       ])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    sensitivity, _, support = sensitivity_specificity_support(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        pos_label=pos_label,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+        warn_for=(&#39;sensitivity&#39;,),</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    if average == &#39;multiclass&#39;:</span>
<span class="gi">+        if correction == 0:</span>
<span class="gi">+            return sp.stats.gmean(sensitivity)</span>
<span class="gi">+        sensitivity = np.where(sensitivity == 0, correction, sensitivity)</span>
<span class="gi">+        return sp.stats.gmean(sensitivity)</span>
<span class="gi">+    elif average == &#39;micro&#39;:</span>
<span class="gi">+        return sensitivity</span>
<span class="gi">+    elif average == &#39;macro&#39;:</span>
<span class="gi">+        return sp.stats.gmean(sensitivity)</span>
<span class="gi">+    elif average == &#39;weighted&#39;:</span>
<span class="gi">+        return np.average(sensitivity, weights=support)</span>
<span class="gi">+    elif average is None or average == &#39;binary&#39;:</span>
<span class="gi">+        return sensitivity</span>
<span class="gi">+</span>
<span class="gi">+    raise ValueError(&quot;Unsupported &#39;average&#39; parameter&quot;)</span>


<span class="w"> </span>@validate_params({&#39;alpha&#39;: [numbers.Real], &#39;squared&#39;: [&#39;boolean&#39;]},
<span class="gu">@@ -489,7 +577,41 @@ def make_index_balanced_accuracy(*, alpha=0.1, squared=True):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(gmean(y_true, y_pred, average=None))
<span class="w"> </span>    [0.44...  0.44...]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    def iba_scoring_func(scoring_func):</span>
<span class="gi">+        @functools.wraps(scoring_func)</span>
<span class="gi">+        def wrapped_scoring_func(y_true, y_pred, **kwargs):</span>
<span class="gi">+            score = scoring_func(y_true, y_pred, **kwargs)</span>
<span class="gi">+            y_true, y_pred = check_consistent_length(y_true, y_pred)</span>
<span class="gi">+            y_type, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+            if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):</span>
<span class="gi">+                raise ValueError(f&quot;{y_type} is not supported&quot;)</span>
<span class="gi">+</span>
<span class="gi">+            lb = LabelEncoder()</span>
<span class="gi">+            y_true = lb.fit_transform(y_true)</span>
<span class="gi">+            y_pred = lb.transform(y_pred)</span>
<span class="gi">+            classes = unique_labels(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+            dominance = np.zeros_like(classes, dtype=float)</span>
<span class="gi">+            for class_i in classes:</span>
<span class="gi">+                mask_class = y_true == class_i</span>
<span class="gi">+                n_class = np.count_nonzero(mask_class)</span>
<span class="gi">+                n_errors = np.count_nonzero(y_pred[mask_class] != class_i)</span>
<span class="gi">+                dominance[class_i] = (n_class - n_errors) / n_class - (1 - n_errors / (len(y_true) - n_class))</span>
<span class="gi">+</span>
<span class="gi">+            if squared:</span>
<span class="gi">+                dominance = dominance ** 2</span>
<span class="gi">+</span>
<span class="gi">+            final_score = score * (1 + alpha * dominance)</span>
<span class="gi">+</span>
<span class="gi">+            if isinstance(score, np.ndarray):</span>
<span class="gi">+                return final_score</span>
<span class="gi">+            else:</span>
<span class="gi">+                return np.mean(final_score)</span>
<span class="gi">+</span>
<span class="gi">+        return wrapped_scoring_func</span>
<span class="gi">+</span>
<span class="gi">+    return iba_scoring_func</span>


<span class="w"> </span>@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],
<span class="gu">@@ -581,7 +703,88 @@ def classification_report_imbalanced(y_true, y_pred, *, labels=None,</span>
<span class="w"> </span>    avg / total       0.70      0.60      0.90      0.61      0.66      0.54         5
<span class="w"> </span>    &lt;BLANKLINE&gt;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    if labels is None:</span>
<span class="gi">+        labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+    else:</span>
<span class="gi">+        labels = np.asarray(labels)</span>
<span class="gi">+</span>
<span class="gi">+    target_names = [str(label) for label in labels] if target_names is None else target_names</span>
<span class="gi">+</span>
<span class="gi">+    # Compute the different metrics</span>
<span class="gi">+    precision, recall, f1, support = precision_recall_fscore_support(</span>
<span class="gi">+        y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight,</span>
<span class="gi">+        zero_division=zero_division</span>
<span class="gi">+    )</span>
<span class="gi">+    specificity = specificity_score(</span>
<span class="gi">+        y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight</span>
<span class="gi">+    )</span>
<span class="gi">+    geo_mean = geometric_mean_score(</span>
<span class="gi">+        y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight</span>
<span class="gi">+    )</span>
<span class="gi">+    iba_gmean = make_index_balanced_accuracy(alpha=alpha, squared=True)(</span>
<span class="gi">+        geometric_mean_score</span>
<span class="gi">+    )</span>
<span class="gi">+    iba = iba_gmean(y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+    # Build the output dictionary</span>
<span class="gi">+    results = {}</span>
<span class="gi">+    for i, label in enumerate(labels):</span>
<span class="gi">+        results[label] = {</span>
<span class="gi">+            &quot;precision&quot;: precision[i],</span>
<span class="gi">+            &quot;recall&quot;: recall[i],</span>
<span class="gi">+            &quot;specificity&quot;: specificity[i],</span>
<span class="gi">+            &quot;f1-score&quot;: f1[i],</span>
<span class="gi">+            &quot;geo_mean&quot;: geo_mean[i],</span>
<span class="gi">+            &quot;iba&quot;: iba[i],</span>
<span class="gi">+            &quot;support&quot;: support[i]</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    # Compute the averages</span>
<span class="gi">+    average_precision = np.average(precision, weights=support)</span>
<span class="gi">+    average_recall = np.average(recall, weights=support)</span>
<span class="gi">+    average_specificity = np.average(specificity, weights=support)</span>
<span class="gi">+    average_f1 = np.average(f1, weights=support)</span>
<span class="gi">+    average_geo_mean = np.average(geo_mean, weights=support)</span>
<span class="gi">+    average_iba = np.average(iba, weights=support)</span>
<span class="gi">+    total_support = np.sum(support)</span>
<span class="gi">+</span>
<span class="gi">+    # Store the averages in the dictionary</span>
<span class="gi">+    results[&quot;macro avg&quot;] = {</span>
<span class="gi">+        &quot;precision&quot;: average_precision,</span>
<span class="gi">+        &quot;recall&quot;: average_recall,</span>
<span class="gi">+        &quot;specificity&quot;: average_specificity,</span>
<span class="gi">+        &quot;f1-score&quot;: average_f1,</span>
<span class="gi">+        &quot;geo_mean&quot;: average_geo_mean,</span>
<span class="gi">+        &quot;iba&quot;: average_iba,</span>
<span class="gi">+        &quot;support&quot;: total_support</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    if output_dict:</span>
<span class="gi">+        return results</span>
<span class="gi">+</span>
<span class="gi">+    # Build the output string</span>
<span class="gi">+    headers = [&quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;, &quot;f1-score&quot;, &quot;geo_mean&quot;, &quot;iba&quot;, &quot;support&quot;]</span>
<span class="gi">+    name_width = max(len(cn) for cn in target_names)</span>
<span class="gi">+    width = max(name_width, len(&quot;macro avg&quot;), digits)</span>
<span class="gi">+</span>
<span class="gi">+    head_fmt = &quot;{:&gt;{width}s} &quot; + &quot; {:&gt;9}&quot; * len(headers)</span>
<span class="gi">+    report = head_fmt.format(&quot;&quot;, *headers, width=width)</span>
<span class="gi">+    report += &quot;\n\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    row_fmt = &quot;{:&gt;{width}s} &quot; + &quot; {:&gt;9.{digits}f}&quot; * (len(headers) - 1) + &quot; {:&gt;9}\n&quot;</span>
<span class="gi">+    for row in (target_names + [&quot;macro avg&quot;]):</span>
<span class="gi">+        if row == &quot;macro avg&quot;:</span>
<span class="gi">+            report += &quot;\n&quot;</span>
<span class="gi">+        scores = results[row]</span>
<span class="gi">+        report += row_fmt.format(row, scores[&quot;precision&quot;], scores[&quot;recall&quot;],</span>
<span class="gi">+                                 scores[&quot;specificity&quot;], scores[&quot;f1-score&quot;],</span>
<span class="gi">+                                 scores[&quot;geo_mean&quot;], scores[&quot;iba&quot;],</span>
<span class="gi">+                                 int(scores[&quot;support&quot;]),</span>
<span class="gi">+                                 width=width, digits=digits)</span>
<span class="gi">+</span>
<span class="gi">+    return report</span>


<span class="w"> </span>@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],
<span class="gu">@@ -630,4 +833,27 @@ def macro_averaged_mean_absolute_error(y_true, y_pred, *, sample_weight=None):</span>
<span class="w"> </span>    &gt;&gt;&gt; macro_averaged_mean_absolute_error(y_true_imbalanced, y_pred)
<span class="w"> </span>    0.16...
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+    check_consistent_length(y_true, y_pred, sample_weight)</span>
<span class="gi">+    </span>
<span class="gi">+    if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):</span>
<span class="gi">+        raise ValueError(f&quot;{y_type} is not supported&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    y_true = column_or_1d(y_true)</span>
<span class="gi">+    y_pred = column_or_1d(y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    classes = unique_labels(y_true)</span>
<span class="gi">+    n_classes = len(classes)</span>
<span class="gi">+</span>
<span class="gi">+    mae_per_class = []</span>
<span class="gi">+    for cls in classes:</span>
<span class="gi">+        cls_mask = y_true == cls</span>
<span class="gi">+        if np.any(cls_mask):</span>
<span class="gi">+            mae = mean_absolute_error(</span>
<span class="gi">+                y_true[cls_mask],</span>
<span class="gi">+                y_pred[cls_mask],</span>
<span class="gi">+                sample_weight=sample_weight[cls_mask] if sample_weight is not None else None</span>
<span class="gi">+            )</span>
<span class="gi">+            mae_per_class.append(mae)</span>
<span class="gi">+</span>
<span class="gi">+    return np.mean(mae_per_class)</span>
<span class="gh">diff --git a/imblearn/metrics/pairwise.py b/imblearn/metrics/pairwise.py</span>
<span class="gh">index d73edae..732e9ea 100644</span>
<span class="gd">--- a/imblearn/metrics/pairwise.py</span>
<span class="gi">+++ b/imblearn/metrics/pairwise.py</span>
<span class="gu">@@ -137,7 +137,29 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        X, y = self._validate_data(X, y, dtype=np.int32)</span>
<span class="gi">+        check_consistent_length(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        self.n_features_in_ = X.shape[1]</span>
<span class="gi">+        self.classes_ = unique_labels(y)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.n_categories, str) and self.n_categories == &quot;auto&quot;:</span>
<span class="gi">+            self.n_categories_ = np.max(X, axis=0) + 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.n_categories_ = np.asarray(self.n_categories)</span>
<span class="gi">+            if self.n_categories_.shape[0] != self.n_features_in_:</span>
<span class="gi">+                raise ValueError(&quot;n_categories must have length equal to n_features&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        self.proba_per_class_ = []</span>
<span class="gi">+        for feature_idx in range(self.n_features_in_):</span>
<span class="gi">+            feature_proba = np.zeros((self.n_categories_[feature_idx], len(self.classes_)))</span>
<span class="gi">+            for class_idx, class_label in enumerate(self.classes_):</span>
<span class="gi">+                class_mask = y == class_label</span>
<span class="gi">+                feature_values, counts = np.unique(X[class_mask, feature_idx], return_counts=True)</span>
<span class="gi">+                feature_proba[feature_values, class_idx] = counts / np.sum(class_mask)</span>
<span class="gi">+            self.proba_per_class_.append(feature_proba)</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def pairwise(self, X, Y=None):
<span class="w"> </span>        &quot;&quot;&quot;Compute the VDM distance pairwise.
<span class="gu">@@ -157,4 +179,27 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>        distance_matrix : ndarray of shape (n_samples, n_samples)
<span class="w"> </span>            The VDM pairwise distance.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+        X = self._validate_data(X, reset=False, dtype=np.int32)</span>
<span class="gi">+</span>
<span class="gi">+        if Y is None:</span>
<span class="gi">+            Y = X</span>
<span class="gi">+        else:</span>
<span class="gi">+            Y = self._validate_data(Y, reset=False, dtype=np.int32)</span>
<span class="gi">+</span>
<span class="gi">+        if X.shape[1] != self.n_features_in_ or Y.shape[1] != self.n_features_in_:</span>
<span class="gi">+            raise ValueError(&quot;X and Y must have the same number of features as the fitted data&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        n_samples_X, n_samples_Y = X.shape[0], Y.shape[0]</span>
<span class="gi">+        distance_matrix = np.zeros((n_samples_X, n_samples_Y))</span>
<span class="gi">+</span>
<span class="gi">+        for i in range(n_samples_X):</span>
<span class="gi">+            for j in range(n_samples_Y):</span>
<span class="gi">+                feature_distances = np.zeros(self.n_features_in_)</span>
<span class="gi">+                for f in range(self.n_features_in_):</span>
<span class="gi">+                    x_val, y_val = X[i, f], Y[j, f]</span>
<span class="gi">+                    proba_diff = np.abs(self.proba_per_class_[f][x_val] - self.proba_per_class_[f][y_val])</span>
<span class="gi">+                    feature_distances[f] = np.sum(proba_diff ** self.k)</span>
<span class="gi">+                distance_matrix[i, j] = np.sum(feature_distances ** self.r) ** (1 / self.r)</span>
<span class="gi">+</span>
<span class="gi">+        return distance_matrix</span>
<span class="gh">diff --git a/imblearn/metrics/tests/test_classification.py b/imblearn/metrics/tests/test_classification.py</span>
<span class="gh">index 7fce78f..9866dbc 100644</span>
<span class="gd">--- a/imblearn/metrics/tests/test_classification.py</span>
<span class="gi">+++ b/imblearn/metrics/tests/test_classification.py</span>
<span class="gu">@@ -17,4 +17,35 @@ def make_prediction(dataset=None, binary=False):</span>
<span class="w"> </span>    If binary is True restrict to a binary classification problem instead of a
<span class="w"> </span>    multiclass classification problem
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if dataset is None:</span>
<span class="gi">+        # Load iris dataset</span>
<span class="gi">+        dataset = datasets.load_iris()</span>
<span class="gi">+    </span>
<span class="gi">+    X = dataset.data</span>
<span class="gi">+    y = dataset.target</span>
<span class="gi">+</span>
<span class="gi">+    if binary:</span>
<span class="gi">+        # Keep only two classes for binary classification</span>
<span class="gi">+        mask = y &lt; 2</span>
<span class="gi">+        X = X[mask]</span>
<span class="gi">+        y = y[mask]</span>
<span class="gi">+</span>
<span class="gi">+    # Split the data into training and testing sets</span>
<span class="gi">+    random_state = check_random_state(RND_SEED)</span>
<span class="gi">+    n_samples = X.shape[0]</span>
<span class="gi">+    permutation = random_state.permutation(n_samples)</span>
<span class="gi">+    n_train = int(0.8 * n_samples)</span>
<span class="gi">+    </span>
<span class="gi">+    X_train = X[permutation[:n_train]]</span>
<span class="gi">+    y_train = y[permutation[:n_train]]</span>
<span class="gi">+    X_test = X[permutation[n_train:]]</span>
<span class="gi">+    y_test = y[permutation[n_train:]]</span>
<span class="gi">+</span>
<span class="gi">+    # Train a Support Vector Classifier</span>
<span class="gi">+    clf = svm.SVC(random_state=RND_SEED)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    # Make predictions</span>
<span class="gi">+    y_pred = clf.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    return y_test, y_pred, X_test</span>
<span class="gh">diff --git a/imblearn/metrics/tests/test_pairwise.py b/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gh">index ccbfede..d68203c 100644</span>
<span class="gd">--- a/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gi">+++ b/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gu">@@ -5,3 +5,71 @@ from sklearn.exceptions import NotFittedError</span>
<span class="w"> </span>from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
<span class="w"> </span>from sklearn.utils._testing import _convert_container
<span class="w"> </span>from imblearn.metrics.pairwise import ValueDifferenceMetric
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_fit():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric()</span>
<span class="gi">+    vdm.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert vdm.n_features_in_ == 2</span>
<span class="gi">+    assert np.array_equal(vdm.classes_, [0, 1])</span>
<span class="gi">+    assert np.array_equal(vdm.n_categories_, [3, 3])</span>
<span class="gi">+    assert len(vdm.proba_per_class_) == 2</span>
<span class="gi">+    assert vdm.proba_per_class_[0].shape == (3, 2)</span>
<span class="gi">+    assert vdm.proba_per_class_[1].shape == (3, 2)</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_pairwise():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric()</span>
<span class="gi">+    vdm.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    distances = vdm.pairwise(X)</span>
<span class="gi">+    assert distances.shape == (4, 4)</span>
<span class="gi">+    assert np.allclose(distances, distances.T)  # Check symmetry</span>
<span class="gi">+    assert np.allclose(np.diag(distances), 0)  # Check diagonal is zero</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_not_fitted():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    vdm = ValueDifferenceMetric()</span>
<span class="gi">+    with pytest.raises(NotFittedError):</span>
<span class="gi">+        vdm.pairwise(X)</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_different_n_features():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric()</span>
<span class="gi">+    vdm.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    X_invalid = np.array([[0, 1, 2], [1, 1, 2]])</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        vdm.pairwise(X_invalid)</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_custom_parameters():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric(n_categories=[3, 3], k=2, r=1)</span>
<span class="gi">+    vdm.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert np.array_equal(vdm.n_categories_, [3, 3])</span>
<span class="gi">+    assert vdm.k == 2</span>
<span class="gi">+    assert vdm.r == 1</span>
<span class="gi">+</span>
<span class="gi">+    distances = vdm.pairwise(X)</span>
<span class="gi">+    assert distances.shape == (4, 4)</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_auto_n_categories():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric(n_categories=&quot;auto&quot;)</span>
<span class="gi">+    vdm.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert np.array_equal(vdm.n_categories_, [3, 3])</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_invalid_n_categories():</span>
<span class="gi">+    X = np.array([[0, 1], [1, 1], [2, 2], [2, 2]])</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+    vdm = ValueDifferenceMetric(n_categories=[3])</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        vdm.fit(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_adasyn.py b/imblearn/over_sampling/_adasyn.py</span>
<span class="gh">index d8159df..d85c8d9 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_adasyn.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_adasyn.py</span>
<span class="gu">@@ -126,4 +126,14 @@ class ADASYN(BaseOverSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create the necessary objects for ADASYN&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(</span>
<span class="gi">+            &#39;n_neighbors&#39;, self.n_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an estimator where `n_jobs` is &quot;</span>
<span class="gi">+                &quot;already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            self.nn_.set_params(n_jobs=self.n_jobs)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/base.py b/imblearn/over_sampling/_smote/base.py</span>
<span class="gh">index 968941c..d3c56cc 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/base.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/base.py</span>
<span class="gu">@@ -44,7 +44,8 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        &quot;&quot;&quot;Check the NN estimators shared across the different SMOTE
<span class="w"> </span>        algorithms.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_k_ = check_neighbors_object(&#39;k_neighbors&#39;, self.k_neighbors, additional_neighbor=1)</span>
<span class="gi">+        self.nn_k_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>

<span class="w"> </span>    def _make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples,
<span class="w"> </span>        step_size=1.0, y=None):
<span class="gu">@@ -88,7 +89,20 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        y_new : ndarray of shape (n_samples_new,)
<span class="w"> </span>            Target values for synthetic samples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        samples_indices = random_state.randint(low=0, high=nn_num.size, size=n_samples)</span>
<span class="gi">+</span>
<span class="gi">+        # Generate synthetic samples</span>
<span class="gi">+        X_new = self._generate_samples(X, nn_data, nn_num, samples_indices, step_size)</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_new = sparse.vstack([X, X_new])</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_new = np.vstack([X, X_new])</span>
<span class="gi">+</span>
<span class="gi">+        y_new = np.array([y_type] * n_samples, dtype=y_dtype)</span>
<span class="gi">+</span>
<span class="gi">+        return X_new, y_new</span>

<span class="w"> </span>    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps,
<span class="w"> </span>        y_type=None, y=None):
<span class="gu">@@ -139,7 +153,22 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        X_new : {ndarray, sparse matrix} of shape (n_samples, n_features)
<span class="w"> </span>            Synthetically generated samples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        X_new = np.zeros((rows.size, X.shape[1]))</span>
<span class="gi">+</span>
<span class="gi">+        for i, (row, col, step) in enumerate(zip(rows, cols, steps)):</span>
<span class="gi">+            if y is None:</span>
<span class="gi">+                weight = random_state.uniform(0, 1)</span>
<span class="gi">+            else:</span>
<span class="gi">+                weight = random_state.uniform(0, 1) if y[row] == y[col] else random_state.uniform(0, 0.5)</span>
<span class="gi">+</span>
<span class="gi">+            if sparse.issparse(X):</span>
<span class="gi">+                X_new[i] = X[row].toarray() + weight * (nn_data[col].toarray() - X[row].toarray())</span>
<span class="gi">+            else:</span>
<span class="gi">+                X_new[i] = X[row] + weight * (nn_data[col] - X[row])</span>
<span class="gi">+</span>
<span class="gi">+        return X_new</span>

<span class="w"> </span>    def _in_danger_noise(self, nn_estimator, samples, target_class, y, kind
<span class="w"> </span>        =&#39;danger&#39;):
<span class="gu">@@ -174,7 +203,18 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        output : ndarray of shape (n_samples,)
<span class="w"> </span>            A boolean array where True refer to samples in danger or noise.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]</span>
<span class="gi">+        nn_label = (y[x] != target_class).astype(int).sum(axis=1)</span>
<span class="gi">+</span>
<span class="gi">+        if kind == &#39;danger&#39;:</span>
<span class="gi">+            # Samples are in danger for m/2 &lt; m&#39; &lt; m</span>
<span class="gi">+            return np.logical_and(nn_label &gt; (nn_estimator.n_neighbors - 1) // 2,</span>
<span class="gi">+                                  nn_label &lt; nn_estimator.n_neighbors - 1)</span>
<span class="gi">+        elif kind == &#39;noise&#39;:</span>
<span class="gi">+            # Samples are noise for m = m&#39;</span>
<span class="gi">+            return nn_label == nn_estimator.n_neighbors - 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(&quot;&#39;kind&#39; should be &#39;danger&#39; or &#39;noise&#39;.&quot;)</span>


<span class="w"> </span>@Substitution(sampling_strategy=BaseOverSampler.
<span class="gu">@@ -462,11 +502,58 @@ class SMOTENC(SMOTE):</span>
<span class="w"> </span>        &quot;&quot;&quot;Overwrite the checking to let pass some string for categorical
<span class="w"> </span>        features.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if _is_pandas_df(X):</span>
<span class="gi">+            X_ = X.values</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_ = X</span>
<span class="gi">+        if not (hasattr(X_, &quot;dtype&quot;) and np.issubdtype(X_.dtype, np.number)):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;SMOTENC requires all features to be numeric. &quot;</span>
<span class="gi">+                &quot;When trying to fit, X and y were:\n&quot;</span>
<span class="gi">+                f&quot;X type: {type(X)}\n&quot;</span>
<span class="gi">+                f&quot;y type: {type(y)}\n&quot;</span>
<span class="gi">+                f&quot;X dtype: {X_.dtype}\n&quot;</span>
<span class="gi">+                f&quot;y dtype: {y.dtype}&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        return X, y</span>

<span class="w"> </span>    def _validate_column_types(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Compute the indices of the categorical and continuous features.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.categorical_features == &quot;auto&quot;:</span>
<span class="gi">+            if not _is_pandas_df(X):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;When `categorical_features=&#39;auto&#39;`, X must be a pandas DataFrame&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            self.categorical_features_ = [</span>
<span class="gi">+                i for i, dtype in enumerate(X.dtypes) if dtype.name == &quot;category&quot;</span>
<span class="gi">+            ]</span>
<span class="gi">+        elif isinstance(self.categorical_features, str):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;If `categorical_features` is a string, it must be &#39;auto&#39;. &quot;</span>
<span class="gi">+                f&quot;Got {self.categorical_features!r} instead.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            if _is_pandas_df(X):</span>
<span class="gi">+                if isinstance(self.categorical_features[0], str):</span>
<span class="gi">+                    self.categorical_features_ = [</span>
<span class="gi">+                        i for i, col in enumerate(X.columns) if col in self.categorical_features</span>
<span class="gi">+                    ]</span>
<span class="gi">+                else:</span>
<span class="gi">+                    self.categorical_features_ = self.categorical_features</span>
<span class="gi">+            else:</span>
<span class="gi">+                self.categorical_features_ = self.categorical_features</span>
<span class="gi">+</span>
<span class="gi">+        if not isinstance(self.categorical_features_, list):</span>
<span class="gi">+            self.categorical_features_ = list(self.categorical_features_)</span>
<span class="gi">+</span>
<span class="gi">+        self.continuous_features_ = [</span>
<span class="gi">+            i for i in range(X.shape[1]) if i not in self.categorical_features_</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+        if len(self.categorical_features_) == X.shape[1]:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;SMOTE-NC cannot be applied when all features are categorical.&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps,
<span class="w"> </span>        y_type, y=None):
<span class="gu">@@ -477,12 +564,30 @@ class SMOTENC(SMOTE):</span>
<span class="w"> </span>        categorical features are mapped to the most frequent nearest neighbors
<span class="w"> </span>        of the majority class.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        X_new = np.zeros((steps.size, X.shape[1]))</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        for i, (row, col, step) in enumerate(zip(rows, cols, steps)):</span>
<span class="gi">+            X_new[i, self.continuous_features_] = (</span>
<span class="gi">+                X[row, self.continuous_features_]</span>
<span class="gi">+                + step * (nn_data[col, self.continuous_features_] - X[row, self.continuous_features_])</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            for feature in self.categorical_features_:</span>
<span class="gi">+                neighbor_categories = nn_data[nn_num[row], feature]</span>
<span class="gi">+                X_new[i, feature] = _mode(neighbor_categories)</span>
<span class="gi">+</span>
<span class="gi">+        return X_new</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def ohe_(self):
<span class="w"> </span>        &quot;&quot;&quot;One-hot encoder used to encode the categorical features.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`ohe_` attribute is deprecated in 0.11 and will be removed in 0.13. &quot;</span>
<span class="gi">+            &quot;Use `categorical_encoder_` instead.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.categorical_encoder_</span>


<span class="w"> </span>@Substitution(sampling_strategy=BaseOverSampler.
<span class="gu">@@ -607,8 +712,28 @@ class SMOTEN(SMOTE):</span>

<span class="w"> </span>    def _check_X_y(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Check should accept strings and not sparse matrices.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            raise TypeError(&quot;SMOTEN does not support sparse input.&quot;)</span>
<span class="gi">+        </span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X, y, reset=True, accept_sparse=False, dtype=None, force_all_finite=False</span>
<span class="gi">+        )</span>
<span class="gi">+        </span>
<span class="gi">+        if not np.all([isinstance(x, (str, int, np.integer)) for x in X.ravel()]):</span>
<span class="gi">+            raise ValueError(&quot;SMOTEN expects all features to be categorical.&quot;)</span>
<span class="gi">+        </span>
<span class="gi">+        return X, y</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Force to use precomputed distance matrix.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_k_ = check_neighbors_object(</span>
<span class="gi">+            &quot;k_neighbors&quot;, self.k_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+        self.nn_k_.set_params(**{&quot;metric&quot;: &quot;precomputed&quot;})</span>
<span class="gi">+</span>
<span class="gi">+        if self.categorical_encoder is None:</span>
<span class="gi">+            self.categorical_encoder_ = OrdinalEncoder(</span>
<span class="gi">+                handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.categorical_encoder_ = clone(self.categorical_encoder)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/cluster.py b/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gh">index 31fb344..26ccff4 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gu">@@ -152,4 +152,22 @@ class KMeansSMOTE(BaseSMOTE):</span>

<span class="w"> </span>    def _find_cluster_sparsity(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Compute the cluster sparsity.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        n_samples, n_features = X.shape</span>
<span class="gi">+        </span>
<span class="gi">+        if self.density_exponent == &quot;auto&quot;:</span>
<span class="gi">+            exponent = 1 - (1 / n_features) if n_features &gt; 1 else 0.5</span>
<span class="gi">+        else:</span>
<span class="gi">+            exponent = self.density_exponent</span>
<span class="gi">+        </span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            # For sparse matrices, use efficient sparse operations</span>
<span class="gi">+            distances = pairwise_distances(X, metric=&#39;euclidean&#39;, n_jobs=self.n_jobs)</span>
<span class="gi">+            distances_sum = distances.sum(axis=1)</span>
<span class="gi">+        else:</span>
<span class="gi">+            # For dense matrices, use numpy operations</span>
<span class="gi">+            distances_sum = np.sum(pairwise_distances(X, metric=&#39;euclidean&#39;, n_jobs=self.n_jobs), axis=1)</span>
<span class="gi">+        </span>
<span class="gi">+        volumes = np.power(distances_sum / n_samples, exponent)</span>
<span class="gi">+        sparsity = np.mean(volumes)</span>
<span class="gi">+        </span>
<span class="gi">+        return sparsity</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_borderline_smote.py b/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gh">index b11e0ea..9b708ec 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gu">@@ -11,7 +11,25 @@ def test_borderline_smote_no_in_danger_samples(kind):</span>
<span class="w"> </span>    &quot;&quot;&quot;Check that the algorithm behave properly even on a dataset without any sample
<span class="w"> </span>    in danger.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=100,</span>
<span class="gi">+        n_classes=2,</span>
<span class="gi">+        weights=[0.9, 0.1],</span>
<span class="gi">+        random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    </span>
<span class="gi">+    smote = BorderlineSMOTE(kind=kind, random_state=42)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples has increased</span>
<span class="gi">+    assert len(X_res) &gt; len(X)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the class balance has improved</span>
<span class="gi">+    assert Counter(y_res)[0] == Counter(y_res)[1]</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the original samples are preserved</span>
<span class="gi">+    assert_array_equal(X, X_res[:len(X)])</span>
<span class="gi">+    assert_array_equal(y, y_res[:len(y)])</span>


<span class="w"> </span>def test_borderline_smote_kind():
<span class="gu">@@ -21,4 +39,30 @@ def test_borderline_smote_kind():</span>
<span class="w"> </span>    &quot;borderline-1&quot;. We generate an example where a logistic regression will perform
<span class="w"> </span>    worse on &quot;borderline-2&quot; than on &quot;borderline-1&quot;.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=100,</span>
<span class="gi">+        n_classes=2,</span>
<span class="gi">+        weights=[0.9, 0.1],</span>
<span class="gi">+        random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    </span>
<span class="gi">+    smote_1 = BorderlineSMOTE(kind=&#39;borderline-1&#39;, random_state=42)</span>
<span class="gi">+    X_res_1, y_res_1 = smote_1.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    smote_2 = BorderlineSMOTE(kind=&#39;borderline-2&#39;, random_state=42)</span>
<span class="gi">+    X_res_2, y_res_2 = smote_2.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Train logistic regression models</span>
<span class="gi">+    lr_1 = LogisticRegression(random_state=42)</span>
<span class="gi">+    lr_1.fit(X_res_1, y_res_1)</span>
<span class="gi">+    score_1 = lr_1.score(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    lr_2 = LogisticRegression(random_state=42)</span>
<span class="gi">+    lr_2.fit(X_res_2, y_res_2)</span>
<span class="gi">+    score_2 = lr_2.score(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that borderline-2 performs worse than borderline-1</span>
<span class="gi">+    assert score_2 &lt; score_1</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the synthetic samples are different</span>
<span class="gi">+    assert not assert_allclose(X_res_1, X_res_2)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_smote_nc.py b/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gh">index 06080ca..e5afa3c 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gu">@@ -15,19 +15,58 @@ sklearn_version = parse_version(sklearn.__version__)</span>

<span class="w"> </span>def test_smotenc_categorical_encoder():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can pass our own categorical encoder.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_classes=2, weights=[0.9, 0.1],</span>
<span class="gi">+                               n_informative=3, n_redundant=1, n_repeated=0,</span>
<span class="gi">+                               n_features=5, n_clusters_per_class=1,</span>
<span class="gi">+                               random_state=0)</span>
<span class="gi">+    X[:, [0, 2]] = X[:, [0, 2]].astype(int)</span>
<span class="gi">+    categorical_features = [0, 2]</span>
<span class="gi">+    </span>
<span class="gi">+    encoder = OneHotEncoder(sparse=False, handle_unknown=&quot;ignore&quot;)</span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=categorical_features,</span>
<span class="gi">+                       categorical_encoder=encoder, random_state=0)</span>
<span class="gi">+    X_res, y_res = smote_nc.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert isinstance(smote_nc.categorical_encoder_, OneHotEncoder)</span>
<span class="gi">+    assert smote_nc.categorical_encoder_ is encoder</span>
<span class="gi">+    assert X_res.shape[0] &gt; X.shape[0]</span>
<span class="gi">+    assert Counter(y_res)[0] == Counter(y_res)[1]</span>


<span class="w"> </span>def test_smotenc_deprecation_ohe_():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a deprecation warning when using `ohe_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_classes=2, weights=[0.9, 0.1],</span>
<span class="gi">+                               n_informative=3, n_redundant=1, n_repeated=0,</span>
<span class="gi">+                               n_features=5, n_clusters_per_class=1,</span>
<span class="gi">+                               random_state=0)</span>
<span class="gi">+    X[:, [0, 2]] = X[:, [0, 2]].astype(int)</span>
<span class="gi">+    categorical_features = [0, 2]</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    smote_nc.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The attribute `ohe_` is deprecated&quot;):</span>
<span class="gi">+        _ = smote_nc.ohe_</span>


<span class="w"> </span>def test_smotenc_param_validation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we validate the parameters correctly since this estimator requires
<span class="w"> </span>    a specific parameter.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_features=5, n_classes=2,</span>
<span class="gi">+                               weights=[0.9, 0.1], random_state=0)</span>
<span class="gi">+    categorical_features = [0, 2]</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=categorical_features)</span>
<span class="gi">+    check_param_validation(smote_nc, X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with invalid categorical_features</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;categorical_features should be&quot;):</span>
<span class="gi">+        SMOTENC(categorical_features=&quot;invalid&quot;)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with out of range categorical_features</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;categorical_features is out of range&quot;):</span>
<span class="gi">+        SMOTENC(categorical_features=[5])</span>


<span class="w"> </span>def test_smotenc_bool_categorical():
<span class="gu">@@ -37,23 +76,79 @@ def test_smotenc_bool_categorical():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/974
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    </span>
<span class="gi">+    X = pd.DataFrame({</span>
<span class="gi">+        &quot;num&quot;: np.random.randn(100),</span>
<span class="gi">+        &quot;cat&quot;: np.random.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=100),</span>
<span class="gi">+        &quot;bool&quot;: np.random.choice([True, False], size=100),</span>
<span class="gi">+    })</span>
<span class="gi">+    y = np.random.randint(0, 2, size=100)</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=[1, 2], random_state=0)</span>
<span class="gi">+    X_res, y_res = smote_nc.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert isinstance(X_res, pd.DataFrame)</span>
<span class="gi">+    assert X_res.dtypes[&quot;bool&quot;] == bool</span>
<span class="gi">+    assert X_res.shape[0] &gt; X.shape[0]</span>
<span class="gi">+    assert Counter(y_res)[0] == Counter(y_res)[1]</span>


<span class="w"> </span>def test_smotenc_categorical_features_str():
<span class="w"> </span>    &quot;&quot;&quot;Check that we support array-like of strings for `categorical_features` using
<span class="w"> </span>    pandas dataframe.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    </span>
<span class="gi">+    X = pd.DataFrame({</span>
<span class="gi">+        &quot;num1&quot;: np.random.randn(100),</span>
<span class="gi">+        &quot;cat1&quot;: np.random.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=100),</span>
<span class="gi">+        &quot;num2&quot;: np.random.randn(100),</span>
<span class="gi">+        &quot;cat2&quot;: np.random.choice([&quot;x&quot;, &quot;y&quot;, &quot;z&quot;], size=100),</span>
<span class="gi">+    })</span>
<span class="gi">+    y = np.random.randint(0, 2, size=100)</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=[&quot;cat1&quot;, &quot;cat2&quot;], random_state=0)</span>
<span class="gi">+    X_res, y_res = smote_nc.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert isinstance(X_res, pd.DataFrame)</span>
<span class="gi">+    assert X_res.shape[0] &gt; X.shape[0]</span>
<span class="gi">+    assert Counter(y_res)[0] == Counter(y_res)[1]</span>
<span class="gi">+    assert set(X_res[&quot;cat1&quot;].unique()) == set(X[&quot;cat1&quot;].unique())</span>
<span class="gi">+    assert set(X_res[&quot;cat2&quot;].unique()) == set(X[&quot;cat2&quot;].unique())</span>


<span class="w"> </span>def test_smotenc_categorical_features_auto():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can automatically detect categorical features based on pandas
<span class="w"> </span>    dataframe.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    </span>
<span class="gi">+    X = pd.DataFrame({</span>
<span class="gi">+        &quot;num1&quot;: np.random.randn(100),</span>
<span class="gi">+        &quot;cat1&quot;: pd.Categorical(np.random.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=100)),</span>
<span class="gi">+        &quot;num2&quot;: np.random.randn(100),</span>
<span class="gi">+        &quot;cat2&quot;: pd.Categorical(np.random.choice([&quot;x&quot;, &quot;y&quot;, &quot;z&quot;], size=100)),</span>
<span class="gi">+    })</span>
<span class="gi">+    y = np.random.randint(0, 2, size=100)</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=&quot;auto&quot;, random_state=0)</span>
<span class="gi">+    X_res, y_res = smote_nc.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert isinstance(X_res, pd.DataFrame)</span>
<span class="gi">+    assert X_res.shape[0] &gt; X.shape[0]</span>
<span class="gi">+    assert Counter(y_res)[0] == Counter(y_res)[1]</span>
<span class="gi">+    assert set(X_res[&quot;cat1&quot;].unique()) == set(X[&quot;cat1&quot;].unique())</span>
<span class="gi">+    assert set(X_res[&quot;cat2&quot;].unique()) == set(X[&quot;cat2&quot;].unique())</span>
<span class="gi">+    assert smote_nc.categorical_features_ == [1, 3]</span>


<span class="w"> </span>def test_smote_nc_categorical_features_auto_error():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a proper error when we cannot use the `&#39;auto&#39;` mode.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_features=5, n_classes=2,</span>
<span class="gi">+                               weights=[0.9, 0.1], random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    smote_nc = SMOTENC(categorical_features=&quot;auto&quot;, random_state=0)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;The &#39;auto&#39; option for categorical_features&quot;):</span>
<span class="gi">+        smote_nc.fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_smoten.py b/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gh">index b4fceeb..2dd21c4 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gu">@@ -13,9 +13,32 @@ def test_smoten_sparse_input(data, sparse_format):</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/971
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    X_sparse = _convert_container(X, constructor_name=sparse_format)</span>
<span class="gi">+    </span>
<span class="gi">+    smoten = SMOTEN(random_state=42)</span>
<span class="gi">+    X_res, y_res = smoten.fit_resample(X_sparse, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert X_res.format == X_sparse.format</span>
<span class="gi">+    assert X_res.shape[0] &gt; X.shape[0]</span>
<span class="gi">+    assert y_res.shape[0] == X_res.shape[0]</span>


<span class="w"> </span>def test_smoten_categorical_encoder(data):
<span class="w"> </span>    &quot;&quot;&quot;Check that `categorical_encoder` is used when provided.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    </span>
<span class="gi">+    # Create a custom categorical encoder</span>
<span class="gi">+    custom_encoder = OrdinalEncoder()</span>
<span class="gi">+    </span>
<span class="gi">+    smoten = SMOTEN(categorical_encoder=custom_encoder, random_state=42)</span>
<span class="gi">+    X_res, y_res = smoten.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the custom encoder was used</span>
<span class="gi">+    assert smoten.categorical_encoder_ == custom_encoder</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the result is different from using the default encoder</span>
<span class="gi">+    smoten_default = SMOTEN(random_state=42)</span>
<span class="gi">+    X_res_default, y_res_default = smoten_default.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert not np.array_equal(X_res, X_res_default)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_svm_smote.py b/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gh">index dd43004..3655163 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gu">@@ -11,7 +11,10 @@ from imblearn.over_sampling import SVMSMOTE</span>
<span class="w"> </span>def test_svm_smote_not_svm(data):
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a proper error if passing an estimator that does not
<span class="w"> </span>    expose a `support_` fitted attribute.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    svm_smote = SVMSMOTE(svm_estimator=LogisticRegression())</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;The svm_estimator doesn&#39;t have a support_ attribute.&quot;):</span>
<span class="gi">+        svm_smote.fit_resample(X, y)</span>


<span class="w"> </span>def test_svm_smote_all_noise(data):
<span class="gu">@@ -21,4 +24,10 @@ def test_svm_smote_all_noise(data):</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/742
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=50, n_classes=2, weights=[0.9, 0.1], random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    svm = SVC(kernel=&quot;rbf&quot;, gamma=1000)  # High gamma to make all points noise</span>
<span class="gi">+    svm_smote = SVMSMOTE(svm_estimator=svm, n_neighbors=1)</span>
<span class="gi">+    with pytest.raises(RuntimeError, match=&quot;No support vectors found in the SVM model.&quot;):</span>
<span class="gi">+        svm_smote.fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/tests/test_random_over_sampler.py b/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gh">index c239b21..2f32bbb 100644</span>
<span class="gd">--- a/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gi">+++ b/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gu">@@ -14,12 +14,33 @@ RND_SEED = 0</span>
<span class="w"> </span>def test_random_over_sampler_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseOverSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(n_samples=100, n_classes=2, weights=[0.9, 0.1], random_state=RND_SEED)</span>
<span class="gi">+    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    if sampling_strategy == &#39;auto&#39; or sampling_strategy == &#39;minority&#39;:</span>
<span class="gi">+        assert Counter(y_resampled)[1] == Counter(y_resampled)[0]</span>
<span class="gi">+    elif sampling_strategy == &#39;not minority&#39;:</span>
<span class="gi">+        assert Counter(y_resampled)[1] &gt; Counter(y)[1]</span>
<span class="gi">+    elif sampling_strategy == &#39;not majority&#39;:</span>
<span class="gi">+        assert Counter(y_resampled)[1] &gt; Counter(y)[1]</span>
<span class="gi">+    elif sampling_strategy == &#39;all&#39;:</span>
<span class="gi">+        assert Counter(y_resampled)[0] == Counter(y_resampled)[1]</span>


<span class="w"> </span>def test_random_over_sampling_datetime():
<span class="w"> </span>    &quot;&quot;&quot;Check that we don&#39;t convert input data and only sample from it.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X = np.array([datetime(2015, 2, 1), datetime(2015, 2, 2), datetime(2015, 2, 3),</span>
<span class="gi">+                  datetime(2015, 2, 4), datetime(2015, 2, 5)]).reshape(-1, 1)</span>
<span class="gi">+    y = np.array([0, 0, 0, 1, 1])</span>
<span class="gi">+</span>
<span class="gi">+    ros = RandomOverSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_resampled.dtype == X.dtype</span>
<span class="gi">+    assert len(X_resampled) == 6</span>
<span class="gi">+    assert Counter(y_resampled) == {0: 3, 1: 3}</span>
<span class="gi">+    assert all(isinstance(x[0], datetime) for x in X_resampled)</span>


<span class="w"> </span>def test_random_over_sampler_full_nat():
<span class="gu">@@ -28,4 +49,14 @@ def test_random_over_sampler_full_nat():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X = np.array([np.timedelta64(&#39;NaT&#39;), np.timedelta64(&#39;NaT&#39;),</span>
<span class="gi">+                  np.timedelta64(&#39;NaT&#39;), np.timedelta64(&#39;NaT&#39;)]).reshape(-1, 1)</span>
<span class="gi">+    y = np.array([0, 0, 1, 1])</span>
<span class="gi">+</span>
<span class="gi">+    ros = RandomOverSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_resampled.dtype == X.dtype</span>
<span class="gi">+    assert len(X_resampled) == 4</span>
<span class="gi">+    assert Counter(y_resampled) == {0: 2, 1: 2}</span>
<span class="gi">+    assert np.all(np.isnat(X_resampled))</span>
<span class="gh">diff --git a/imblearn/tensorflow/_generator.py b/imblearn/tensorflow/_generator.py</span>
<span class="gh">index c55dd52..e606d54 100644</span>
<span class="gd">--- a/imblearn/tensorflow/_generator.py</span>
<span class="gi">+++ b/imblearn/tensorflow/_generator.py</span>
<span class="gu">@@ -53,4 +53,43 @@ def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="w"> </span>    steps_per_epoch : int
<span class="w"> </span>        The number of samples per epoch.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    random_state = check_random_state(random_state)</span>
<span class="gi">+    </span>
<span class="gi">+    if sampler is None:</span>
<span class="gi">+        sampler = RandomUnderSampler(random_state=random_state)</span>
<span class="gi">+    else:</span>
<span class="gi">+        sampler = clone(sampler)</span>
<span class="gi">+        if random_state is not None:</span>
<span class="gi">+            sampler.set_params(random_state=random_state)</span>
<span class="gi">+    </span>
<span class="gi">+    # Fit the sampler</span>
<span class="gi">+    sampler.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Get the sample indices</span>
<span class="gi">+    indices = sampler.sample_indices_</span>
<span class="gi">+    </span>
<span class="gi">+    # Calculate steps per epoch</span>
<span class="gi">+    steps_per_epoch = len(indices) // batch_size</span>
<span class="gi">+    </span>
<span class="gi">+    def generator():</span>
<span class="gi">+        while True:</span>
<span class="gi">+            # Shuffle indices</span>
<span class="gi">+            random_state.shuffle(indices)</span>
<span class="gi">+            </span>
<span class="gi">+            for start in range(0, len(indices), batch_size):</span>
<span class="gi">+                end = start + batch_size</span>
<span class="gi">+                batch_indices = indices[start:end]</span>
<span class="gi">+                </span>
<span class="gi">+                X_batch = _safe_indexing(X, batch_indices)</span>
<span class="gi">+                y_batch = _safe_indexing(y, batch_indices)</span>
<span class="gi">+                </span>
<span class="gi">+                if not keep_sparse and issparse(X_batch):</span>
<span class="gi">+                    X_batch = X_batch.toarray()</span>
<span class="gi">+                </span>
<span class="gi">+                if sample_weight is not None:</span>
<span class="gi">+                    sw_batch = _safe_indexing(sample_weight, batch_indices)</span>
<span class="gi">+                    yield X_batch, y_batch, sw_batch</span>
<span class="gi">+                else:</span>
<span class="gi">+                    yield X_batch, y_batch</span>
<span class="gi">+    </span>
<span class="gi">+    return generator(), steps_per_epoch</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py b/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gh">index 8b6d169..61baf2b 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gu">@@ -117,4 +117,19 @@ class ClusterCentroids(BaseUnderSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the KMeans estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            self.estimator_ = KMeans(random_state=self.random_state)</span>
<span class="gi">+        elif isinstance(self.estimator, type):</span>
<span class="gi">+            self.estimator_ = self.estimator(random_state=self.random_state)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+</span>
<span class="gi">+        if self.voting == &quot;auto&quot;:</span>
<span class="gi">+            if sparse.issparse(X):</span>
<span class="gi">+                self.voting_ = &quot;hard&quot;</span>
<span class="gi">+            else:</span>
<span class="gi">+                self.voting_ = &quot;soft&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.voting_ = self.voting</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py b/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gh">index 5bd5434..947fba3 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gu">@@ -130,9 +130,116 @@ class CondensedNearestNeighbour(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.n_neighbors is None:</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=1, n_jobs=self.n_jobs)</span>
<span class="gi">+        elif isinstance(self.n_neighbors, int):</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs)</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator = clone(self.n_neighbors)</span>
<span class="gi">+</span>
<span class="gi">+        self.estimator_ = estimator</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Last fitted k-NN estimator.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`estimator_` attribute is deprecated in 0.12 and will be &quot;</span>
<span class="gi">+            &quot;removed in 0.14. Use `estimators_` instead.&quot;,</span>
<span class="gi">+            DeprecationWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.estimators_[0] if hasattr(self, &quot;estimators_&quot;) else None</span>
<span class="gi">+</span>
<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        &quot;&quot;&quot;Resample the dataset.</span>
<span class="gi">+</span>
<span class="gi">+        Parameters</span>
<span class="gi">+        ----------</span>
<span class="gi">+        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="gi">+            Matrix containing the data which have to be sampled.</span>
<span class="gi">+</span>
<span class="gi">+        y : array-like of shape (n_samples,)</span>
<span class="gi">+            Corresponding label for each sample in X.</span>
<span class="gi">+</span>
<span class="gi">+        Returns</span>
<span class="gi">+        -------</span>
<span class="gi">+        X_resampled : {array-like, sparse matrix} of shape (n_samples_new, n_features)</span>
<span class="gi">+            The array containing the resampled data.</span>
<span class="gi">+</span>
<span class="gi">+        y_resampled : array-like of shape (n_samples_new,)</span>
<span class="gi">+            The corresponding label of `X_resampled`.</span>
<span class="gi">+        &quot;&quot;&quot;</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X, y = self._check_X_y(X, y)</span>
<span class="gi">+        </span>
<span class="gi">+        self.sampling_strategy_ = self._check_sampling_strategy(y, self.sampling_strategy)</span>
<span class="gi">+        </span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        # Start with the minority class</span>
<span class="gi">+        X_min = _safe_indexing(X, y == self.sampling_strategy_)</span>
<span class="gi">+        y_min = _safe_indexing(y, y == self.sampling_strategy_)</span>
<span class="gi">+</span>
<span class="gi">+        # All the minority class samples will be preserved</span>
<span class="gi">+        X_resampled = X_min.copy()</span>
<span class="gi">+        y_resampled = y_min.copy()</span>
<span class="gi">+</span>
<span class="gi">+        # Loop over the other classes under-sampling them</span>
<span class="gi">+        for key in self.sampling_strategy_.keys():</span>
<span class="gi">+            if key == self.sampling_strategy_:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            # Select the majority class samples</span>
<span class="gi">+            X_maj = _safe_indexing(X, y == key)</span>
<span class="gi">+            y_maj = _safe_indexing(y, y == key)</span>
<span class="gi">+</span>
<span class="gi">+            # Initialize the set S with n_seeds_S samples</span>
<span class="gi">+            random_state.shuffle(X_maj)</span>
<span class="gi">+            S = X_maj[:self.n_seeds_S]</span>
<span class="gi">+            S_y = y_maj[:self.n_seeds_S]</span>
<span class="gi">+</span>
<span class="gi">+            # Create a k-NN classifier for the current majority class samples</span>
<span class="gi">+            self.estimator_.fit(S, S_y)</span>
<span class="gi">+</span>
<span class="gi">+            good_classif_label = idx_taken = idx_S = np.array([], dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+            # Loop as long as we have samples in S</span>
<span class="gi">+            while S.shape[0] &gt; 0:</span>
<span class="gi">+                # Do not select samples which are already well classified</span>
<span class="gi">+                idx_to_classify = np.setdiff1d(</span>
<span class="gi">+                    np.arange(X_maj.shape[0]), np.concatenate((idx_taken, good_classif_label))</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                # Stop when all samples are classified</span>
<span class="gi">+                if len(idx_to_classify) == 0:</span>
<span class="gi">+                    break</span>
<span class="gi">+</span>
<span class="gi">+                # Classify on S</span>
<span class="gi">+                pred = self.estimator_.predict(_safe_indexing(X_maj, idx_to_classify))</span>
<span class="gi">+</span>
<span class="gi">+                # If classified well, append it to S</span>
<span class="gi">+                idx_good_classif = idx_to_classify[pred == key]</span>
<span class="gi">+                idx_bad_classif = idx_to_classify[pred != key]</span>
<span class="gi">+</span>
<span class="gi">+                # Add the well classified samples to S</span>
<span class="gi">+                S = np.concatenate((S, _safe_indexing(X_maj, idx_good_classif)), axis=0)</span>
<span class="gi">+                S_y = np.concatenate((S_y, _safe_indexing(y_maj, idx_good_classif)), axis=0)</span>
<span class="gi">+</span>
<span class="gi">+                good_classif_label = np.concatenate((good_classif_label, idx_good_classif))</span>
<span class="gi">+                idx_S = np.concatenate((idx_S, idx_good_classif))</span>
<span class="gi">+</span>
<span class="gi">+                # Add the misclassified samples to the wrong set</span>
<span class="gi">+                idx_taken = np.concatenate((idx_taken, idx_bad_classif))</span>
<span class="gi">+</span>
<span class="gi">+                # Fit on the new S</span>
<span class="gi">+                self.estimator_.fit(S, S_y)</span>
<span class="gi">+</span>
<span class="gi">+            X_resampled = np.vstack((X_resampled, _safe_indexing(X_maj, idx_S)))</span>
<span class="gi">+            y_resampled = np.hstack((y_resampled, _safe_indexing(y_maj, idx_S)))</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = np.flatnonzero(np.isin(y, np.unique(y_resampled)))</span>
<span class="gi">+</span>
<span class="gi">+        if issparse(X):</span>
<span class="gi">+            X_resampled = X_resampled.tocsr()</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py b/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gh">index 067fe55..be5799c 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gu">@@ -126,7 +126,8 @@ class EditedNearestNeighbours(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Validate the estimator created in the ENN.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(&#39;n_neighbors&#39;, self.n_neighbors, additional_neighbor=1)</span>
<span class="gi">+        self.nn_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>


<span class="w"> </span>@Substitution(sampling_strategy=BaseCleaningSampler.
<span class="gu">@@ -258,7 +259,14 @@ class RepeatedEditedNearestNeighbours(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(&#39;n_neighbors&#39;, self.n_neighbors, additional_neighbor=1)</span>
<span class="gi">+        self.nn_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>
<span class="gi">+        self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            n_neighbors=self.nn_,</span>
<span class="gi">+            kind_sel=self.kind_sel,</span>
<span class="gi">+            n_jobs=self.n_jobs,</span>
<span class="gi">+        )</span>


<span class="w"> </span>@Substitution(sampling_strategy=BaseCleaningSampler.
<span class="gu">@@ -388,4 +396,11 @@ class AllKNN(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create objects required by AllKNN&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(&#39;n_neighbors&#39;, self.n_neighbors, additional_neighbor=1)</span>
<span class="gi">+        self.nn_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>
<span class="gi">+        self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            n_neighbors=self.nn_,</span>
<span class="gi">+            kind_sel=self.kind_sel,</span>
<span class="gi">+            n_jobs=self.n_jobs,</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py b/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gh">index c858b97..2451050 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gu">@@ -113,4 +113,22 @@ class InstanceHardnessThreshold(BaseUnderSampler):</span>

<span class="w"> </span>    def _validate_estimator(self, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the classifier&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            estimator = RandomForestClassifier(</span>
<span class="gi">+                n_estimators=100, random_state=random_state, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator = clone(self.estimator)</span>
<span class="gi">+</span>
<span class="gi">+        if not is_classifier(estimator):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;&#39;{estimator.__class__.__name__}&#39; is not a classifier.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if not hasattr(estimator, &quot;predict_proba&quot;):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;Estimator {estimator.__class__.__name__} does not have a predict_proba method.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        _set_random_states(estimator, random_state)</span>
<span class="gi">+        return estimator</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_nearmiss.py b/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gh">index f64b76a..bd22dd6 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gu">@@ -148,8 +148,83 @@ class NearMiss(BaseUnderSampler):</span>
<span class="w"> </span>            The list of the indices of the selected samples.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        target_class_indices = np.flatnonzero(y == key)</span>
<span class="gi">+        if sel_strategy == &#39;nearest&#39;:</span>
<span class="gi">+            sorted_indices = target_class_indices[np.argsort(dist_vec[target_class_indices])]</span>
<span class="gi">+        elif sel_strategy == &#39;farthest&#39;:</span>
<span class="gi">+            sorted_indices = target_class_indices[np.argsort(dist_vec[target_class_indices])[::-1]]</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(&quot;Unknown selection strategy: {}&quot;.format(sel_strategy))</span>
<span class="gi">+        </span>
<span class="gi">+        return sorted_indices[:num_samples]</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(&#39;n_neighbors&#39;, self.n_neighbors, additional_neighbor=1)</span>
<span class="gi">+        self.nn_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>
<span class="gi">+</span>
<span class="gi">+        if self.version == 3:</span>
<span class="gi">+            self.nn_ver3_ = check_neighbors_object(&#39;n_neighbors_ver3&#39;, self.n_neighbors_ver3)</span>
<span class="gi">+            self.nn_ver3_.set_params(**{&#39;n_jobs&#39;: self.n_jobs})</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                if self.version == 1:</span>
<span class="gi">+                    X_class = _safe_indexing(X, np.flatnonzero(y == target_class))</span>
<span class="gi">+                    self.nn_.fit(X_class)</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_.kneighbors(</span>
<span class="gi">+                        _safe_indexing(X, np.flatnonzero(y == class_minority)),</span>
<span class="gi">+                        n_neighbors=self.nn_.n_neighbors,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    idx_vec_farthest = idx_vec[:, -1]</span>
<span class="gi">+                    dist_vec = dist_vec[:, -1]</span>
<span class="gi">+                    idx_under = np.concatenate(</span>
<span class="gi">+                        (idx_under, self._selection_dist_based(</span>
<span class="gi">+                            X_class, y[y == target_class], dist_vec, n_samples,</span>
<span class="gi">+                            target_class, sel_strategy=&#39;nearest&#39;,</span>
<span class="gi">+                        )),</span>
<span class="gi">+                        axis=0,</span>
<span class="gi">+                    )</span>
<span class="gi">+                elif self.version == 2:</span>
<span class="gi">+                    X_class = _safe_indexing(X, np.flatnonzero(y == target_class))</span>
<span class="gi">+                    self.nn_.fit(X_class)</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_.kneighbors(</span>
<span class="gi">+                        _safe_indexing(X, np.flatnonzero(y == class_minority)),</span>
<span class="gi">+                        n_neighbors=self.nn_.n_neighbors,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    dist_vec = np.mean(dist_vec, axis=1)</span>
<span class="gi">+                    idx_under = np.concatenate(</span>
<span class="gi">+                        (idx_under, self._selection_dist_based(</span>
<span class="gi">+                            X_class, y[y == target_class], dist_vec, n_samples,</span>
<span class="gi">+                            target_class, sel_strategy=&#39;nearest&#39;,</span>
<span class="gi">+                        )),</span>
<span class="gi">+                        axis=0,</span>
<span class="gi">+                    )</span>
<span class="gi">+                elif self.version == 3:</span>
<span class="gi">+                    X_class = _safe_indexing(X, np.flatnonzero(y == target_class))</span>
<span class="gi">+                    self.nn_ver3_.fit(X)</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_ver3_.kneighbors(X_class)</span>
<span class="gi">+                    idx_vec_farthest = idx_vec[:, -self.nn_.n_neighbors :]</span>
<span class="gi">+                    dist_vec = np.mean(dist_vec[:, -self.nn_.n_neighbors :], axis=1)</span>
<span class="gi">+                    idx_under = np.concatenate(</span>
<span class="gi">+                        (idx_under, self._selection_dist_based(</span>
<span class="gi">+                            X_class, y[y == target_class], dist_vec, n_samples,</span>
<span class="gi">+                            target_class, sel_strategy=&#39;farthest&#39;,</span>
<span class="gi">+                        )),</span>
<span class="gi">+                        axis=0,</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = _safe_indexing(X, idx_under)</span>
<span class="gi">+        y_resampled = _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py b/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gh">index e0a2f31..a3808ca 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gu">@@ -150,4 +150,33 @@ class NeighbourhoodCleaningRule(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create the objects required by NCR.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.edited_nearest_neighbours is None:</span>
<span class="gi">+            self.edited_nearest_neighbours_ = EditedNearestNeighbours(</span>
<span class="gi">+                sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+                n_neighbors=self.n_neighbors,</span>
<span class="gi">+                kind_sel=&quot;all&quot;,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.edited_nearest_neighbours_ = clone(self.edited_nearest_neighbours)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.n_neighbors, numbers.Integral):</span>
<span class="gi">+            self.nn_ = KNeighborsClassifier(</span>
<span class="gi">+                n_neighbors=self.n_neighbors, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        elif isinstance(self.n_neighbors, KNeighborsClassifier):</span>
<span class="gi">+            self.nn_ = clone(self.n_neighbors)</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;`n_neighbors` has to be a int or an object&quot;</span>
<span class="gi">+                f&quot; inherited from KNeighborsClassifier.&quot;</span>
<span class="gi">+                f&quot; Got {type(self.n_neighbors)} instead.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if self.kind_sel != &quot;deprecated&quot;:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;&#39;kind_sel&#39; is deprecated in 0.12 and will be removed in 0.14. &quot;</span>
<span class="gi">+                &quot;The parameter currently has no effect and always corresponds &quot;</span>
<span class="gi">+                &quot;to the &#39;all&#39; strategy.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py b/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gh">index 6c0b322..cf5abb7 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gu">@@ -126,9 +126,69 @@ class OneSidedSelection(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.n_neighbors is None:</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=1, n_jobs=self.n_jobs)</span>
<span class="gi">+        elif isinstance(self.n_neighbors, int):</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs)</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator = clone(self.n_neighbors)</span>
<span class="gi">+</span>
<span class="gi">+        self.estimator_ = estimator</span>
<span class="gi">+        self.estimators_ = [clone(estimator) for _ in range(len(self.sampling_strategy_) - 1)]</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Last fitted k-NN estimator.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`estimator_` attribute is deprecated in 0.12 and will be &quot;</span>
<span class="gi">+            &quot;removed in 0.14. Use `estimators_` instead.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.estimators_[0] if hasattr(self, &quot;estimators_&quot;) else None</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                index_target_class = np.flatnonzero(y == target_class)</span>
<span class="gi">+                index_target_class = random_state.choice(</span>
<span class="gi">+                    index_target_class, size=n_samples, replace=False</span>
<span class="gi">+                )</span>
<span class="gi">+                idx_under = np.concatenate((idx_under, index_target_class))</span>
<span class="gi">+</span>
<span class="gi">+        # Create the set S with all the minority samples and n_seeds_S of the</span>
<span class="gi">+        # majority samples</span>
<span class="gi">+        idx_S = np.flatnonzero(y == class_minority)</span>
<span class="gi">+        idx_S = np.concatenate(</span>
<span class="gi">+            (idx_S, random_state.choice(idx_under, size=self.n_seeds_S, replace=False))</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Create the set C - One-sided selection</span>
<span class="gi">+        estimator = clone(self.estimator_)</span>
<span class="gi">+        estimator.fit(X[idx_S], y[idx_S])</span>
<span class="gi">+        pred_S = estimator.predict(X)</span>
<span class="gi">+</span>
<span class="gi">+        # Find the misclassified S_prime</span>
<span class="gi">+        idx_S_prime = np.flatnonzero(pred_S != y)</span>
<span class="gi">+        idx_C = np.concatenate((idx_S, idx_S_prime))</span>
<span class="gi">+</span>
<span class="gi">+        # Find the Tomek&#39;s links in C</span>
<span class="gi">+        tl = TomekLinks(sampling_strategy=&quot;all&quot;, n_jobs=self.n_jobs)</span>
<span class="gi">+        _, _ = tl.fit_resample(X[idx_C], y[idx_C])</span>
<span class="gi">+</span>
<span class="gi">+        # Remove the Tomek&#39;s links from S</span>
<span class="gi">+        idx_C = np.setdiff1d(idx_C, tl.sample_indices_)</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = _safe_indexing(X, idx_C)</span>
<span class="gi">+        y_resampled = _safe_indexing(y, idx_C)</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_C</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_tomek_links.py b/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gh">index a64dc2a..d13b67d 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gu">@@ -112,4 +112,11 @@ class TomekLinks(BaseCleaningSampler):</span>
<span class="w"> </span>            Boolean vector on len( # samples ), with True for majority samples
<span class="w"> </span>            that are Tomek links.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        is_tomek = np.zeros(len(y), dtype=bool)</span>
<span class="gi">+        </span>
<span class="gi">+        for idx, (label, nn_label) in enumerate(zip(y, y[nn_index])):</span>
<span class="gi">+            if label != class_type and nn_label == class_type:</span>
<span class="gi">+                if nn_index[nn_index[idx]] == idx:</span>
<span class="gi">+                    is_tomek[idx] = True</span>
<span class="gi">+        </span>
<span class="gi">+        return is_tomek</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py b/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gh">index b14a1ef..a822020 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gu">@@ -20,9 +20,23 @@ Y = np.array([1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1])</span>

<span class="w"> </span>def test_condensed_nearest_neighbour_multiclass():
<span class="w"> </span>    &quot;&quot;&quot;Check the validity of the fitted attributes `estimators_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+    cnn.fit_resample(X, Y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert hasattr(cnn, &#39;estimator_&#39;)</span>
<span class="gi">+    assert isinstance(cnn.estimator_, KNeighborsClassifier)</span>
<span class="gi">+    assert cnn.estimator_.n_neighbors == 1</span>
<span class="gi">+    </span>
<span class="gi">+    X_resampled, y_resampled = cnn.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape[0] &lt; X.shape[0]</span>
<span class="gi">+    assert y_resampled.shape[0] == X_resampled.shape[0]</span>
<span class="gi">+    assert len(np.unique(y_resampled)) == len(np.unique(Y))</span>


<span class="w"> </span>def test_condensed_nearest_neighbors_deprecation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing the parameter `estimator_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+    cnn.fit_resample(X, Y)</span>
<span class="gi">+    </span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The attribute `estimator_` is deprecated&quot;):</span>
<span class="gi">+        _ = cnn.estimator_</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py b/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gh">index 9333224..b9794d1 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gu">@@ -19,4 +19,19 @@ Y = np.array([1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1])</span>
<span class="w"> </span>def test_enn_check_kind_selection():
<span class="w"> </span>    &quot;&quot;&quot;Check that `check_sel=&quot;all&quot;` is more conservative than
<span class="w"> </span>    `check_sel=&quot;mode&quot;`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    enn_all = EditedNearestNeighbours(sampling_strategy=&quot;all&quot;, n_neighbors=3, kind_sel=&quot;all&quot;)</span>
<span class="gi">+    enn_mode = EditedNearestNeighbours(sampling_strategy=&quot;all&quot;, n_neighbors=3, kind_sel=&quot;mode&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X_resampled_all, y_resampled_all = enn_all.fit_resample(X, Y)</span>
<span class="gi">+    X_resampled_mode, y_resampled_mode = enn_mode.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    # Check that &quot;all&quot; selection is more conservative (removes fewer samples)</span>
<span class="gi">+    assert len(X_resampled_all) &gt;= len(X_resampled_mode)</span>
<span class="gi">+    assert len(y_resampled_all) &gt;= len(y_resampled_mode)</span>
<span class="gi">+</span>
<span class="gi">+    # Check that the samples in &quot;mode&quot; selection are a subset of &quot;all&quot; selection</span>
<span class="gi">+    assert set(map(tuple, X_resampled_mode)).issubset(set(map(tuple, X_resampled_all)))</span>
<span class="gi">+    assert set(y_resampled_mode).issubset(set(y_resampled_all))</span>
<span class="gi">+</span>
<span class="gi">+    # Check that at least one sample is different between the two methods</span>
<span class="gi">+    assert len(X_resampled_all) &gt; len(X_resampled_mode)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py b/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gh">index bdb3a01..cac46a7 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gu">@@ -26,4 +26,30 @@ def test_iht_estimator_pipeline():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/pull/1049
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Create a pipeline with a classifier</span>
<span class="gi">+    clf = make_pipeline(NB())</span>
<span class="gi">+    </span>
<span class="gi">+    # Create an InstanceHardnessThreshold object with the pipeline</span>
<span class="gi">+    iht = InstanceHardnessThreshold(estimator=clf, random_state=RND_SEED)</span>
<span class="gi">+    </span>
<span class="gi">+    # Fit and transform the data</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data has fewer samples than the original</span>
<span class="gi">+    assert len(X_resampled) &lt; len(X)</span>
<span class="gi">+    assert len(y_resampled) &lt; len(Y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data maintains the same number of features</span>
<span class="gi">+    assert X_resampled.shape[1] == X.shape[1]</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled labels are a subset of the original labels</span>
<span class="gi">+    assert set(y_resampled).issubset(set(Y))</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data and labels have the same length</span>
<span class="gi">+    assert len(X_resampled) == len(y_resampled)</span>
<span class="gi">+    </span>
<span class="gi">+    # Ensure that the random state produces consistent results</span>
<span class="gi">+    iht2 = InstanceHardnessThreshold(estimator=clf, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled2, y_resampled2 = iht2.fit_resample(X, Y)</span>
<span class="gi">+    assert_array_equal(X_resampled, X_resampled2)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_resampled2)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py b/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gh">index 97c8fd5..8f3e803 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gu">@@ -7,11 +7,50 @@ from sklearn.utils._testing import assert_array_equal</span>
<span class="w"> </span>from imblearn.under_sampling import EditedNearestNeighbours, NeighbourhoodCleaningRule


<span class="gd">-def test_ncr_threshold_cleaning(data):</span>
<span class="gi">+def test_ncr_threshold_cleaning():</span>
<span class="w"> </span>    &quot;&quot;&quot;Test the effect of the `threshold_cleaning` parameter.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000, n_classes=3, n_informative=4, weights=[0.2, 0.3, 0.5], random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with default threshold_cleaning (0.5)</span>
<span class="gi">+    ncr = NeighbourhoodCleaningRule(random_state=42)</span>
<span class="gi">+    X_resampled, y_resampled = ncr.fit_resample(X, y)</span>
<span class="gi">+    counter_default = Counter(y_resampled)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with higher threshold_cleaning (0.8)</span>
<span class="gi">+    ncr_high = NeighbourhoodCleaningRule(threshold_cleaning=0.8, random_state=42)</span>
<span class="gi">+    X_resampled_high, y_resampled_high = ncr_high.fit_resample(X, y)</span>
<span class="gi">+    counter_high = Counter(y_resampled_high)</span>
<span class="gi">+    </span>
<span class="gi">+    # The higher threshold should result in less cleaning, so more samples</span>
<span class="gi">+    assert sum(counter_high.values()) &gt; sum(counter_default.values())</span>


<span class="gd">-def test_ncr_n_neighbors(data):</span>
<span class="gi">+def test_ncr_n_neighbors():</span>
<span class="w"> </span>    &quot;&quot;&quot;Check the effect of the NN on the cleaning of the second phase.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000, n_classes=3, n_informative=4, weights=[0.2, 0.3, 0.5], random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with default n_neighbors (3)</span>
<span class="gi">+    ncr_default = NeighbourhoodCleaningRule(random_state=42)</span>
<span class="gi">+    X_resampled_default, y_resampled_default = ncr_default.fit_resample(X, y)</span>
<span class="gi">+    counter_default = Counter(y_resampled_default)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with higher n_neighbors (5)</span>
<span class="gi">+    ncr_high = NeighbourhoodCleaningRule(n_neighbors=5, random_state=42)</span>
<span class="gi">+    X_resampled_high, y_resampled_high = ncr_high.fit_resample(X, y)</span>
<span class="gi">+    counter_high = Counter(y_resampled_high)</span>
<span class="gi">+    </span>
<span class="gi">+    # The number of samples should be different due to different n_neighbors</span>
<span class="gi">+    assert sum(counter_default.values()) != sum(counter_high.values())</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the ENN step is affected by n_neighbors</span>
<span class="gi">+    enn_default = EditedNearestNeighbours(n_neighbors=3, kind_sel=&quot;all&quot;, random_state=42)</span>
<span class="gi">+    enn_high = EditedNearestNeighbours(n_neighbors=5, kind_sel=&quot;all&quot;, random_state=42)</span>
<span class="gi">+    </span>
<span class="gi">+    _, y_enn_default = enn_default.fit_resample(X, y)</span>
<span class="gi">+    _, y_enn_high = enn_high.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert Counter(y_enn_default) != Counter(y_enn_high)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py b/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gh">index e861896..a0dab1f 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gu">@@ -18,9 +18,22 @@ Y = np.array([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])</span>

<span class="w"> </span>def test_one_sided_selection_multiclass():
<span class="w"> </span>    &quot;&quot;&quot;Check the validity of the fitted attributes `estimators_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=300,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_informative=5,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+    )</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED)</span>
<span class="gi">+    oss.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    assert hasattr(oss, &#39;estimator_&#39;)</span>
<span class="gi">+    assert isinstance(oss.estimator_, KNeighborsClassifier)</span>
<span class="gi">+    assert oss.estimator_.n_neighbors == 1</span>


<span class="w"> </span>def test_one_sided_selection_deprecation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing the parameter `estimator_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    oss = OneSidedSelection()</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The attribute `estimator_` is deprecated&quot;):</span>
<span class="gi">+        _ = oss.estimator_</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py b/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gh">index 96745c6..75d2b85 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gu">@@ -19,12 +19,44 @@ Y = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1])</span>
<span class="w"> </span>def test_random_under_sampler_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseUnderSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000, n_classes=3, n_informative=4, weights=[0.2, 0.3, 0.5], random_state=RND_SEED</span>
<span class="gi">+    )</span>
<span class="gi">+    rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data is not empty</span>
<span class="gi">+    assert len(X_res) &gt; 0</span>
<span class="gi">+    assert len(y_res) &gt; 0</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples in each class is as expected</span>
<span class="gi">+    if sampling_strategy in [&#39;auto&#39;, &#39;not minority&#39;, &#39;all&#39;]:</span>
<span class="gi">+        assert len(np.unique(y_res)) == len(np.unique(y))</span>
<span class="gi">+    elif sampling_strategy == &#39;majority&#39;:</span>
<span class="gi">+        assert len(np.unique(y_res)) == 2  # majority and minority classes</span>
<span class="gi">+    elif sampling_strategy == &#39;not majority&#39;:</span>
<span class="gi">+        assert len(np.unique(y_res)) == 2  # minority classes</span>


<span class="w"> </span>def test_random_under_sampling_datetime():
<span class="w"> </span>    &quot;&quot;&quot;Check that we don&#39;t convert input data and only sample from it.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X = np.array([datetime(2022, 1, i) for i in range(1, 11)])</span>
<span class="gi">+    y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])</span>
<span class="gi">+    </span>
<span class="gi">+    rus = RandomUnderSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X.reshape(-1, 1), y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data is not empty</span>
<span class="gi">+    assert len(X_res) &gt; 0</span>
<span class="gi">+    assert len(y_res) &gt; 0</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data is still of datetime type</span>
<span class="gi">+    assert isinstance(X_res[0][0], datetime)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples in each class is balanced</span>
<span class="gi">+    unique, counts = np.unique(y_res, return_counts=True)</span>
<span class="gi">+    assert len(unique) == 2</span>
<span class="gi">+    assert counts[0] == counts[1]</span>


<span class="w"> </span>def test_random_under_sampler_full_nat():
<span class="gu">@@ -33,4 +65,20 @@ def test_random_under_sampler_full_nat():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X = np.array([np.timedelta64(&#39;NaT&#39;)] * 10).reshape(-1, 1)</span>
<span class="gi">+    y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])</span>
<span class="gi">+    </span>
<span class="gi">+    rus = RandomUnderSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X, y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data is not empty</span>
<span class="gi">+    assert len(X_res) &gt; 0</span>
<span class="gi">+    assert len(y_res) &gt; 0</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that all values in X_res are NaT</span>
<span class="gi">+    assert np.all(np.isnat(X_res))</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples in each class is balanced</span>
<span class="gi">+    unique, counts = np.unique(y_res, return_counts=True)</span>
<span class="gi">+    assert len(unique) == 2</span>
<span class="gi">+    assert counts[0] == counts[1]</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py b/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gh">index c1fd8e5..79b9776 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gu">@@ -21,4 +21,22 @@ Y = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])</span>
<span class="w"> </span>def test_tomek_links_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseCleaningSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    tomek = TomekLinks(sampling_strategy=sampling_strategy)</span>
<span class="gi">+    X_resampled, y_resampled = tomek.fit_resample(X, Y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the resampled data is not empty</span>
<span class="gi">+    assert len(X_resampled) &gt; 0</span>
<span class="gi">+    assert len(y_resampled) &gt; 0</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples in X and y are the same</span>
<span class="gi">+    assert len(X_resampled) == len(y_resampled)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the number of samples in the resampled data is less than or equal to the original data</span>
<span class="gi">+    assert len(X_resampled) &lt;= len(X)</span>
<span class="gi">+    assert len(y_resampled) &lt;= len(Y)</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that the shape of X_resampled is correct</span>
<span class="gi">+    assert X_resampled.shape[1] == X.shape[1]</span>
<span class="gi">+    </span>
<span class="gi">+    # Check that all classes are still present in the resampled data</span>
<span class="gi">+    assert set(np.unique(y_resampled)) == set(np.unique(Y))</span>
<span class="gh">diff --git a/imblearn/utils/_available_if.py b/imblearn/utils/_available_if.py</span>
<span class="gh">index 51d5fc6..bb7f712 100644</span>
<span class="gd">--- a/imblearn/utils/_available_if.py</span>
<span class="gi">+++ b/imblearn/utils/_available_if.py</span>
<span class="gu">@@ -82,6 +82,8 @@ if sklearn_version &lt; parse_version(&#39;1.1&#39;):</span>
<span class="w"> </span>        &gt;&gt;&gt; obj.say_hello()
<span class="w"> </span>        Hello
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def decorator(fn):</span>
<span class="gi">+            return _AvailableIfDescriptor(fn, check, attribute_name=fn.__name__)</span>
<span class="gi">+        return decorator</span>
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.utils.metaestimators import available_if
<span class="gh">diff --git a/imblearn/utils/_metadata_requests.py b/imblearn/utils/_metadata_requests.py</span>
<span class="gh">index aa6e024..8c7d280 100644</span>
<span class="gd">--- a/imblearn/utils/_metadata_requests.py</span>
<span class="gi">+++ b/imblearn/utils/_metadata_requests.py</span>
<span class="gu">@@ -104,7 +104,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            Whether metadata routing is enabled. If the config is not set, it
<span class="w"> </span>            defaults to False.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return get_config().get(&quot;enable_metadata_routing&quot;, False)</span>

<span class="w"> </span>    def _raise_for_params(params, owner, method):
<span class="w"> </span>        &quot;&quot;&quot;Raise an error if metadata routing is not enabled and params are passed.
<span class="gu">@@ -127,7 +127,13 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        ValueError
<span class="w"> </span>            If metadata routing is not enabled and params are passed.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not _routing_enabled() and params:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;Metadata routing is not enabled, but {owner.__class__.__name__}.&quot;</span>
<span class="gi">+                f&quot;{method} was called with metadata: {list(params.keys())}. &quot;</span>
<span class="gi">+                &quot;To enable metadata routing, set &quot;</span>
<span class="gi">+                &quot;enable_metadata_routing=True using sklearn.set_config().&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    def _raise_for_unsupported_routing(obj, method, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Raise when metadata routing is enabled and metadata is passed.
<span class="gu">@@ -149,7 +155,12 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        **kwargs : dict
<span class="w"> </span>            The metadata passed to the method.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if _routing_enabled() and kwargs:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;Metadata routing is enabled, but {obj.__class__.__name__} &quot;</span>
<span class="gi">+                f&quot;does not support metadata routing. {method} was called &quot;</span>
<span class="gi">+                f&quot;with unsupported metadata: {list(kwargs.keys())}.&quot;</span>
<span class="gi">+            )</span>


<span class="w"> </span>    class _RoutingNotSupportedMixin:
<span class="gu">@@ -188,7 +199,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        result : bool
<span class="w"> </span>            Whether the given item is a valid alias.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return isinstance(item, str) and item.isidentifier() and item not in VALID_REQUEST_VALUES</span>

<span class="w"> </span>    def request_is_valid(item):
<span class="w"> </span>        &quot;&quot;&quot;Check if an item is a valid request value (and not an alias).
<span class="gu">@@ -203,7 +214,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        result : bool
<span class="w"> </span>            Whether the given item is valid.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return item in VALID_REQUEST_VALUES</span>


<span class="w"> </span>    class MethodMetadataRequest:
<span class="gh">diff --git a/imblearn/utils/_param_validation.py b/imblearn/utils/_param_validation.py</span>
<span class="gh">index 47542c0..162567a 100644</span>
<span class="gd">--- a/imblearn/utils/_param_validation.py</span>
<span class="gi">+++ b/imblearn/utils/_param_validation.py</span>
<span class="gu">@@ -61,7 +61,34 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        caller_name : str
<span class="w"> </span>            The name of the estimator or function or method that called this function.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if parameter_constraints == &quot;no_validation&quot;:</span>
<span class="gi">+            return</span>
<span class="gi">+</span>
<span class="gi">+        for param_name, constraints in parameter_constraints.items():</span>
<span class="gi">+            if param_name not in params:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            param_value = params[param_name]</span>
<span class="gi">+            satisfied = False</span>
<span class="gi">+</span>
<span class="gi">+            for constraint in constraints:</span>
<span class="gi">+                if isinstance(constraint, _Constraint):</span>
<span class="gi">+                    if constraint.is_satisfied_by(param_value):</span>
<span class="gi">+                        satisfied = True</span>
<span class="gi">+                        break</span>
<span class="gi">+                elif constraint is None and param_value is None:</span>
<span class="gi">+                    satisfied = True</span>
<span class="gi">+                    break</span>
<span class="gi">+                elif isinstance(param_value, constraint):</span>
<span class="gi">+                    satisfied = True</span>
<span class="gi">+                    break</span>
<span class="gi">+</span>
<span class="gi">+            if not satisfied:</span>
<span class="gi">+                raise InvalidParameterError(</span>
<span class="gi">+                    f&quot;The {param_name!r} parameter of {caller_name} &quot;</span>
<span class="gi">+                    f&quot;must be {&#39; or &#39;.join(str(c) for c in constraints)}. &quot;</span>
<span class="gi">+                    f&quot;Got {param_value!r} instead.&quot;</span>
<span class="gi">+                )</span>

<span class="w"> </span>    def make_constraint(constraint):
<span class="w"> </span>        &quot;&quot;&quot;Convert the constraint into the appropriate Constraint object.
<span class="gu">@@ -76,7 +103,30 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        constraint : instance of _Constraint
<span class="w"> </span>            The converted constraint.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(constraint, _Constraint):</span>
<span class="gi">+            return constraint</span>
<span class="gi">+        elif constraint == &quot;array-like&quot;:</span>
<span class="gi">+            return _ArrayLikes()</span>
<span class="gi">+        elif constraint == &quot;sparse matrix&quot;:</span>
<span class="gi">+            return _SparseMatrices()</span>
<span class="gi">+        elif constraint == &quot;random_state&quot;:</span>
<span class="gi">+            return _RandomStates()</span>
<span class="gi">+        elif callable(constraint):</span>
<span class="gi">+            return _Callables()</span>
<span class="gi">+        elif constraint is None:</span>
<span class="gi">+            return _NoneConstraint()</span>
<span class="gi">+        elif isinstance(constraint, type):</span>
<span class="gi">+            return _InstancesOf(constraint)</span>
<span class="gi">+        elif constraint == &quot;boolean&quot;:</span>
<span class="gi">+            return _Booleans()</span>
<span class="gi">+        elif constraint == &quot;verbose&quot;:</span>
<span class="gi">+            return _VerboseHelper()</span>
<span class="gi">+        elif constraint == &quot;cv_object&quot;:</span>
<span class="gi">+            return _CVObjects()</span>
<span class="gi">+        elif constraint == &quot;nan&quot;:</span>
<span class="gi">+            return _NanConstraint()</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(f&quot;Unknown constraint: {constraint}&quot;)</span>

<span class="w"> </span>    def validate_params(parameter_constraints, *, prefer_skip_nested_validation
<span class="w"> </span>        ):
<span class="gu">@@ -110,7 +160,34 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        decorated_function : function or method
<span class="w"> </span>            The decorated function.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def decorator(func):</span>
<span class="gi">+            @functools.wraps(func)</span>
<span class="gi">+            def wrapper(*args, **kwargs):</span>
<span class="gi">+                sig = signature(func)</span>
<span class="gi">+                bound_arguments = sig.bind(*args, **kwargs)</span>
<span class="gi">+                bound_arguments.apply_defaults()</span>
<span class="gi">+</span>
<span class="gi">+                params_to_validate = {}</span>
<span class="gi">+                for param_name, param_value in bound_arguments.arguments.items():</span>
<span class="gi">+                    if param_name in parameter_constraints:</span>
<span class="gi">+                        params_to_validate[param_name] = param_value</span>
<span class="gi">+</span>
<span class="gi">+                validate_parameter_constraints(</span>
<span class="gi">+                    parameter_constraints,</span>
<span class="gi">+                    params_to_validate,</span>
<span class="gi">+                    func.__qualname__,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                with config_context(</span>
<span class="gi">+                    skip_parameter_validation=(</span>
<span class="gi">+                        prefer_skip_nested_validation or</span>
<span class="gi">+                        get_config()[&quot;skip_parameter_validation&quot;]</span>
<span class="gi">+                    )</span>
<span class="gi">+                ):</span>
<span class="gi">+                    return func(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+            return wrapper</span>
<span class="gi">+        return decorator</span>


<span class="w"> </span>    class RealNotInt(Real):
<span class="gh">diff --git a/imblearn/utils/_show_versions.py b/imblearn/utils/_show_versions.py</span>
<span class="gh">index e6bd42b..2fdc2a9 100644</span>
<span class="gd">--- a/imblearn/utils/_show_versions.py</span>
<span class="gi">+++ b/imblearn/utils/_show_versions.py</span>
<span class="gu">@@ -14,7 +14,30 @@ def _get_deps_info():</span>
<span class="w"> </span>    deps_info: dict
<span class="w"> </span>        version information on relevant Python libraries
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import importlib</span>
<span class="gi">+    import sys</span>
<span class="gi">+    deps = [</span>
<span class="gi">+        &quot;pip&quot;,</span>
<span class="gi">+        &quot;setuptools&quot;,</span>
<span class="gi">+        &quot;sklearn&quot;,</span>
<span class="gi">+        &quot;numpy&quot;,</span>
<span class="gi">+        &quot;scipy&quot;,</span>
<span class="gi">+        &quot;pandas&quot;,</span>
<span class="gi">+        &quot;joblib&quot;,</span>
<span class="gi">+        &quot;threadpoolctl&quot;,</span>
<span class="gi">+    ]</span>
<span class="gi">+    deps_info = {}</span>
<span class="gi">+    for modname in deps:</span>
<span class="gi">+        try:</span>
<span class="gi">+            if modname in sys.modules:</span>
<span class="gi">+                mod = sys.modules[modname]</span>
<span class="gi">+            else:</span>
<span class="gi">+                mod = importlib.import_module(modname)</span>
<span class="gi">+            ver = getattr(mod, &quot;__version__&quot;, &quot;unknown version&quot;)</span>
<span class="gi">+            deps_info[modname] = ver</span>
<span class="gi">+        except ImportError:</span>
<span class="gi">+            deps_info[modname] = &quot;not installed&quot;</span>
<span class="gi">+    return deps_info</span>


<span class="w"> </span>def show_versions(github=False):
<span class="gu">@@ -27,4 +50,24 @@ def show_versions(github=False):</span>
<span class="w"> </span>    github : bool,
<span class="w"> </span>        If true, wrap system info with GitHub markup.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import sys</span>
<span class="gi">+    import platform</span>
<span class="gi">+</span>
<span class="gi">+    sys_info = {</span>
<span class="gi">+        &quot;python&quot;: sys.version.replace(&quot;\n&quot;, &quot; &quot;),</span>
<span class="gi">+        &quot;executable&quot;: sys.executable,</span>
<span class="gi">+        &quot;machine&quot;: platform.platform(),</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    deps_info = _get_deps_info()</span>
<span class="gi">+</span>
<span class="gi">+    if github:</span>
<span class="gi">+        sys_info = &quot;**System Information**\n\n&quot; + &quot;\n&quot;.join(f&quot;* {k}: {v}&quot; for k, v in sys_info.items())</span>
<span class="gi">+        deps_info = &quot;**Python Dependencies**\n\n&quot; + &quot;\n&quot;.join(f&quot;* {k}: {v}&quot; for k, v in deps_info.items())</span>
<span class="gi">+    else:</span>
<span class="gi">+        sys_info = &quot;System Information\n&quot; + &quot;\n&quot;.join(f&quot;{k:&lt;10}: {v}&quot; for k, v in sys_info.items())</span>
<span class="gi">+        deps_info = &quot;Python Dependencies\n&quot; + &quot;\n&quot;.join(f&quot;{k:&lt;10}: {v}&quot; for k, v in deps_info.items())</span>
<span class="gi">+</span>
<span class="gi">+    print(sys_info)</span>
<span class="gi">+    print(&quot;\nimlearn version:&quot;, __version__)</span>
<span class="gi">+    print(&quot;\n&quot; + deps_info)</span>
<span class="gh">diff --git a/imblearn/utils/_validation.py b/imblearn/utils/_validation.py</span>
<span class="gh">index 38a7408..c54011f 100644</span>
<span class="gd">--- a/imblearn/utils/_validation.py</span>
<span class="gi">+++ b/imblearn/utils/_validation.py</span>
<span class="gu">@@ -41,7 +41,8 @@ def _is_neighbors_object(estimator):</span>
<span class="w"> </span>    is_neighbors_object : bool
<span class="w"> </span>        True if the estimator exposes a KNeighborsMixin-like API.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return (hasattr(estimator, &#39;kneighbors&#39;) and</span>
<span class="gi">+            hasattr(estimator, &#39;kneighbors_graph&#39;))</span>


<span class="w"> </span>def check_neighbors_object(nn_name, nn_object, additional_neighbor=0):
<span class="gu">@@ -68,7 +69,13 @@ def check_neighbors_object(nn_name, nn_object, additional_neighbor=0):</span>
<span class="w"> </span>    nn_object : KNeighborsMixin
<span class="w"> </span>        The k-NN object.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(nn_object, Integral):</span>
<span class="gi">+        return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)</span>
<span class="gi">+    elif _is_neighbors_object(nn_object):</span>
<span class="gi">+        return clone(nn_object)</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(f&quot;{nn_name} has to be a int or an object with a &quot;</span>
<span class="gi">+                         &quot;KNeighborsMixin-like API.&quot;)</span>


<span class="w"> </span>def check_target_type(y, indicate_one_vs_all=False):
<span class="gu">@@ -94,58 +101,142 @@ def check_target_type(y, indicate_one_vs_all=False):</span>
<span class="w"> </span>        Indicate if the target was originally encoded in a one-vs-all fashion.
<span class="w"> </span>        Only returned if ``indicate_multilabel=True``.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    y_type = type_of_target(y)</span>
<span class="gi">+    if y_type not in TARGET_KIND:</span>
<span class="gi">+        raise ValueError(f&quot;&#39;y&#39; should be one of {TARGET_KIND}. Got &#39;{y_type}&#39; instead.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    y_ = column_or_1d(y)</span>
<span class="gi">+    if y_type == &#39;binary&#39;:</span>
<span class="gi">+        classes = np.unique(y_)</span>
<span class="gi">+        if len(classes) != 2:</span>
<span class="gi">+            raise ValueError(&quot;&#39;y&#39; should encode binary targets. Got more than two unique values.&quot;)</span>
<span class="gi">+        y_ = (y_ == classes[1]).astype(np.float64)</span>
<span class="gi">+</span>
<span class="gi">+    if not indicate_one_vs_all:</span>
<span class="gi">+        return y_</span>
<span class="gi">+    else:</span>
<span class="gi">+        return y_, y_type == &#39;multilabel-indicator&#39;</span>


<span class="w"> </span>def _sampling_strategy_all(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    for class_sample in np.unique(y):</span>
<span class="gi">+        target_stats[class_sample] = _num_samples(y[y == class_sample])</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_majority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting the majority class only.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type == &#39;over-sampling&#39;:</span>
<span class="gi">+        raise ValueError(&quot;&#39;majority&#39; for over-sampling is not a valid option.&quot;)</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    class_majority = _num_samples(y) - np.bincount(y).min()</span>
<span class="gi">+    target_stats[np.argmax(np.bincount(y))] = class_majority</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_not_majority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes but not the
<span class="w"> </span>    majority.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    class_majority = np.argmax(np.bincount(y))</span>
<span class="gi">+    for class_sample, n_sample in enumerate(np.bincount(y)):</span>
<span class="gi">+        if class_sample == class_majority:</span>
<span class="gi">+            continue</span>
<span class="gi">+        if sampling_type == &#39;over-sampling&#39;:</span>
<span class="gi">+            n_sample = _num_samples(y[y == class_majority])</span>
<span class="gi">+        target_stats[class_sample] = n_sample</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_not_minority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes but not the
<span class="w"> </span>    minority.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    class_minority = np.argmin(np.bincount(y))</span>
<span class="gi">+    for class_sample, n_sample in enumerate(np.bincount(y)):</span>
<span class="gi">+        if class_sample == class_minority:</span>
<span class="gi">+            continue</span>
<span class="gi">+        if sampling_type == &#39;under-sampling&#39;:</span>
<span class="gi">+            n_sample = _num_samples(y[y == class_minority])</span>
<span class="gi">+        target_stats[class_sample] = n_sample</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_minority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting the minority class only.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type == &#39;under-sampling&#39;:</span>
<span class="gi">+        raise ValueError(&quot;&#39;minority&#39; for under-sampling is not a valid option.&quot;)</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    class_minority = np.argmin(np.bincount(y))</span>
<span class="gi">+    target_stats[class_minority] = _num_samples(y[y != class_minority])</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_auto(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target auto for over-sampling and not-minority for
<span class="w"> </span>    under-sampling.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type == &#39;over-sampling&#39;:</span>
<span class="gi">+        return _sampling_strategy_not_majority(y, sampling_type)</span>
<span class="gi">+    elif sampling_type == &#39;under-sampling&#39;:</span>
<span class="gi">+        return _sampling_strategy_not_minority(y, sampling_type)</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(&quot;&#39;auto&#39; is not a valid option for {}.&quot;.format(sampling_type))</span>


<span class="w"> </span>def _sampling_strategy_dict(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by converting the dictionary depending of the
<span class="w"> </span>    sampling.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    for class_sample, n_samples in sampling_strategy.items():</span>
<span class="gi">+        if n_samples &lt; 0:</span>
<span class="gi">+            raise ValueError(&quot;The number of samples in a class cannot be negative.&quot;</span>
<span class="gi">+                             &quot; Got {} in class {}.&quot;.format(n_samples, class_sample))</span>
<span class="gi">+        if n_samples != int(n_samples):</span>
<span class="gi">+            raise ValueError(&quot;The number of samples must be an integer.&quot;</span>
<span class="gi">+                             &quot; Got {} in class {}.&quot;.format(n_samples, class_sample))</span>
<span class="gi">+        if class_sample not in np.unique(y):</span>
<span class="gi">+            raise ValueError(&quot;The class {} is not present in the data.&quot;.format(class_sample))</span>
<span class="gi">+        target_stats[class_sample] = n_samples</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_list(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;With cleaning methods, sampling_strategy can be a list to target the
<span class="w"> </span>    class of interest.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type != &#39;clean-sampling&#39;:</span>
<span class="gi">+        raise ValueError(&quot;&#39;list&#39; is not a valid option for {}.&quot;.format(sampling_type))</span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    for class_sample in sampling_strategy:</span>
<span class="gi">+        if class_sample not in np.unique(y):</span>
<span class="gi">+            raise ValueError(&quot;The class {} is not present in the data.&quot;.format(class_sample))</span>
<span class="gi">+        target_stats[class_sample] = _num_samples(y[y == class_sample])</span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def _sampling_strategy_float(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Take a proportion of the majority (over-sampling) or minority
<span class="w"> </span>    (under-sampling) class in binary classification.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_strategy &lt;= 0 or sampling_strategy &gt; 1:</span>
<span class="gi">+        raise ValueError(&quot;When &#39;sampling_strategy&#39; is a float, it should be &quot;</span>
<span class="gi">+                         &quot;in the (0, 1] range. Got {} instead.&quot;.format(sampling_strategy))</span>
<span class="gi">+    </span>
<span class="gi">+    target_stats = OrderedDict()</span>
<span class="gi">+    class_minority, class_majority = np.unique(y)</span>
<span class="gi">+    n_minority = _num_samples(y[y == class_minority])</span>
<span class="gi">+    n_majority = _num_samples(y[y == class_majority])</span>
<span class="gi">+    </span>
<span class="gi">+    if sampling_type == &#39;over-sampling&#39;:</span>
<span class="gi">+        n_samples = int(n_majority * sampling_strategy - n_minority)</span>
<span class="gi">+        target_stats[class_minority] = n_minority + n_samples</span>
<span class="gi">+    elif sampling_type == &#39;under-sampling&#39;:</span>
<span class="gi">+        n_samples = int(n_minority / sampling_strategy - n_minority)</span>
<span class="gi">+        target_stats[class_majority] = n_samples</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(&quot;&#39;sampling_strategy&#39; as a float is not supported for {}.&quot;.format(sampling_type))</span>
<span class="gi">+    </span>
<span class="gi">+    return target_stats</span>


<span class="w"> </span>def check_sampling_strategy(sampling_strategy, y, sampling_type, **kwargs):
<span class="gh">diff --git a/imblearn/utils/deprecation.py b/imblearn/utils/deprecation.py</span>
<span class="gh">index a630c60..9724439 100644</span>
<span class="gd">--- a/imblearn/utils/deprecation.py</span>
<span class="gi">+++ b/imblearn/utils/deprecation.py</span>
<span class="gu">@@ -22,4 +22,20 @@ def deprecate_parameter(sampler, version_deprecation, param_deprecated,</span>
<span class="w"> </span>        The parameter used instead of the deprecated parameter. By default, no
<span class="w"> </span>        parameter is expected.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if hasattr(sampler, param_deprecated):</span>
<span class="gi">+        # Check if the deprecated parameter is set</span>
<span class="gi">+        if getattr(sampler, param_deprecated) is not None:</span>
<span class="gi">+            # Construct the deprecation message</span>
<span class="gi">+            msg = (</span>
<span class="gi">+                f&quot;The parameter &#39;{param_deprecated}&#39; is deprecated since version &quot;</span>
<span class="gi">+                f&quot;{version_deprecation} and will be removed in a future version.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            if new_param:</span>
<span class="gi">+                msg += f&quot; Use &#39;{new_param}&#39; instead.&quot;</span>
<span class="gi">+            </span>
<span class="gi">+            # Issue the deprecation warning</span>
<span class="gi">+            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)</span>
<span class="gi">+            </span>
<span class="gi">+            # If a new parameter is specified, set its value to the deprecated parameter&#39;s value</span>
<span class="gi">+            if new_param and hasattr(sampler, new_param):</span>
<span class="gi">+                setattr(sampler, new_param, getattr(sampler, param_deprecated))</span>
<span class="gh">diff --git a/imblearn/utils/estimator_checks.py b/imblearn/utils/estimator_checks.py</span>
<span class="gh">index d3aea67..a4bd429 100644</span>
<span class="gd">--- a/imblearn/utils/estimator_checks.py</span>
<span class="gi">+++ b/imblearn/utils/estimator_checks.py</span>
<span class="gu">@@ -59,4 +59,14 @@ def parametrize_with_checks(estimators):</span>
<span class="w"> </span>    ... def test_sklearn_compatible_estimator(estimator, check):
<span class="w"> </span>    ...     check(estimator)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import pytest</span>
<span class="gi">+    from sklearn.utils.estimator_checks import check_estimator</span>
<span class="gi">+</span>
<span class="gi">+    def checks_generator():</span>
<span class="gi">+        for estimator in estimators:</span>
<span class="gi">+            name = type(estimator).__name__</span>
<span class="gi">+            for check in check_estimator(estimator, generate_only=True):</span>
<span class="gi">+                check_name = check.func.__name__ if hasattr(check, &#39;func&#39;) else check.__name__</span>
<span class="gi">+                yield pytest.param(estimator, check, id=f&quot;{name}-{check_name}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return pytest.mark.parametrize(&quot;estimator,check&quot;, checks_generator())</span>
<span class="gh">diff --git a/imblearn/utils/fixes.py b/imblearn/utils/fixes.py</span>
<span class="gh">index 801067f..ec25066 100644</span>
<span class="gd">--- a/imblearn/utils/fixes.py</span>
<span class="gi">+++ b/imblearn/utils/fixes.py</span>
<span class="gu">@@ -21,7 +21,7 @@ else:</span>

<span class="w"> </span>    def _is_arraylike_not_scalar(array):
<span class="w"> </span>        &quot;&quot;&quot;Return True if array is array-like and not a scalar&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return _is_arraylike(array) and not np.isscalar(array)</span>
<span class="w"> </span>if sklearn_version &lt; parse_version(&#39;1.3&#39;):

<span class="w"> </span>    def _fit_context(*, prefer_skip_nested_validation):
<span class="gu">@@ -47,7 +47,13 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        decorated_fit : method
<span class="w"> </span>            The decorated fit method.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        def decorator(fit_method):</span>
<span class="gi">+            @functools.wraps(fit_method)</span>
<span class="gi">+            def wrapper(self, *args, **kwargs):</span>
<span class="gi">+                with config_context(assume_finite=get_config()[&quot;assume_finite&quot;]):</span>
<span class="gi">+                    return fit_method(self, *args, **kwargs)</span>
<span class="gi">+            return wrapper</span>
<span class="gi">+        return decorator</span>
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.base import _fit_context
<span class="w"> </span>if sklearn_version &lt; parse_version(&#39;1.3&#39;):
<span class="gu">@@ -76,7 +82,14 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        fitted : bool
<span class="w"> </span>            Whether the estimator is fitted.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if attributes is None:</span>
<span class="gi">+            attributes = [attr for attr in vars(estimator)</span>
<span class="gi">+                          if attr.endswith(&quot;_&quot;) and not attr.startswith(&quot;__&quot;)]</span>
<span class="gi">+</span>
<span class="gi">+        if not attributes:</span>
<span class="gi">+            raise ValueError(&quot;No valid attributes to check if estimator is fitted.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        return all_or_any([hasattr(estimator, attr) for attr in attributes])</span>
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.utils.validation import _is_fitted
<span class="w"> </span>try:
<span class="gu">@@ -85,4 +98,8 @@ except ImportError:</span>

<span class="w"> </span>    def _is_pandas_df(X):
<span class="w"> </span>        &quot;&quot;&quot;Return True if the X is a pandas dataframe.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        try:</span>
<span class="gi">+            import pandas as pd</span>
<span class="gi">+            return isinstance(X, pd.DataFrame)</span>
<span class="gi">+        except ImportError:</span>
<span class="gi">+            return False</span>
<span class="gh">diff --git a/imblearn/utils/testing.py b/imblearn/utils/testing.py</span>
<span class="gh">index aa344b1..9044f98 100644</span>
<span class="gd">--- a/imblearn/utils/testing.py</span>
<span class="gi">+++ b/imblearn/utils/testing.py</span>
<span class="gu">@@ -35,7 +35,43 @@ def all_estimators(type_filter=None):</span>
<span class="w"> </span>        List of (name, class), where ``name`` is the class name as string
<span class="w"> </span>        and ``class`` is the actual type of the class.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Get the imblearn package</span>
<span class="gi">+    import imblearn</span>
<span class="gi">+</span>
<span class="gi">+    def is_abstract(c):</span>
<span class="gi">+        if inspect.isabstract(c):</span>
<span class="gi">+            return True</span>
<span class="gi">+        if hasattr(c, &#39;__abstractmethods__&#39;):</span>
<span class="gi">+            return bool(getattr(c, &#39;__abstractmethods__&#39;))</span>
<span class="gi">+        return False</span>
<span class="gi">+</span>
<span class="gi">+    all_classes = []</span>
<span class="gi">+    modules_to_ignore = {&#39;tests&#39;, &#39;test&#39;, &#39;setup&#39;, &#39;conftest&#39;}</span>
<span class="gi">+</span>
<span class="gi">+    for importer, modname, ispkg in pkgutil.walk_packages(path=imblearn.__path__,</span>
<span class="gi">+                                                          prefix=&#39;imblearn.&#39;):</span>
<span class="gi">+        mod_parts = modname.split(&#39;.&#39;)</span>
<span class="gi">+        if any(part in modules_to_ignore for part in mod_parts):</span>
<span class="gi">+            continue</span>
<span class="gi">+</span>
<span class="gi">+        module = import_module(modname)</span>
<span class="gi">+        classes = inspect.getmembers(module, inspect.isclass)</span>
<span class="gi">+        classes = [(name, est_cls) for name, est_cls in classes</span>
<span class="gi">+                   if (issubclass(est_cls, BaseEstimator) and</span>
<span class="gi">+                       est_cls.__module__ == modname and</span>
<span class="gi">+                       not is_abstract(est_cls))]</span>
<span class="gi">+        all_classes.extend(classes)</span>
<span class="gi">+</span>
<span class="gi">+    all_classes = sorted(set(all_classes), key=itemgetter(0))</span>
<span class="gi">+</span>
<span class="gi">+    if type_filter is not None:</span>
<span class="gi">+        if not isinstance(type_filter, list):</span>
<span class="gi">+            type_filter = [type_filter]</span>
<span class="gi">+        all_classes = [est for est in all_classes</span>
<span class="gi">+                       if any(hasattr(est[1], attr)</span>
<span class="gi">+                              for attr in type_filter)]</span>
<span class="gi">+</span>
<span class="gi">+    return all_classes</span>


<span class="w"> </span>class _CustomNearestNeighbors(BaseEstimator):
<span class="gu">@@ -48,10 +84,38 @@ class _CustomNearestNeighbors(BaseEstimator):</span>
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.metric = metric

<span class="gd">-    def kneighbors_graph(X=None, n_neighbors=None, mode=&#39;connectivity&#39;):</span>
<span class="gi">+    def kneighbors_graph(self, X=None, n_neighbors=None, mode=&#39;connectivity&#39;):</span>
<span class="w"> </span>        &quot;&quot;&quot;This method is not used within imblearn but it is required for
<span class="w"> </span>        duck-typing.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if X is None:</span>
<span class="gi">+            raise ValueError(&quot;X must be provided&quot;)</span>
<span class="gi">+        if n_neighbors is None:</span>
<span class="gi">+            n_neighbors = self.n_neighbors</span>
<span class="gi">+        </span>
<span class="gi">+        n_samples = X.shape[0]</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X = X.toarray()</span>
<span class="gi">+        </span>
<span class="gi">+        kdtree = KDTree(X)</span>
<span class="gi">+        distances, indices = kdtree.query(X, k=n_neighbors + 1)</span>
<span class="gi">+        </span>
<span class="gi">+        # Remove the first column (self-connections)</span>
<span class="gi">+        indices = indices[:, 1:]</span>
<span class="gi">+        distances = distances[:, 1:]</span>
<span class="gi">+        </span>
<span class="gi">+        if mode == &#39;connectivity&#39;:</span>
<span class="gi">+            graph = sparse.lil_matrix((n_samples, n_samples), dtype=int)</span>
<span class="gi">+            graph[np.repeat(np.arange(n_samples), n_neighbors),</span>
<span class="gi">+                  indices.ravel()] = 1</span>
<span class="gi">+        elif mode == &#39;distance&#39;:</span>
<span class="gi">+            graph = sparse.lil_matrix((n_samples, n_samples), dtype=float)</span>
<span class="gi">+            graph[np.repeat(np.arange(n_samples), n_neighbors),</span>
<span class="gi">+                  indices.ravel()] = distances.ravel()</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(&quot;Unsupported mode, must be &#39;connectivity&#39; &quot;</span>
<span class="gi">+                             &quot;or &#39;distance&#39; but got %s instead&quot; % mode)</span>
<span class="gi">+        </span>
<span class="gi">+        return graph.tocsr()</span>


<span class="w"> </span>class _CustomClusterer(BaseEstimator):
<span class="gh">diff --git a/imblearn/utils/tests/test_docstring.py b/imblearn/utils/tests/test_docstring.py</span>
<span class="gh">index f377d75..aed8ed4 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_docstring.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_docstring.py</span>
<span class="gu">@@ -11,7 +11,7 @@ def _dedent_docstring(docstring):</span>

<span class="w"> </span>    xref: https://github.com/python/cpython/issues/81283
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return textwrap.dedent(docstring)</span>


<span class="w"> </span>func_docstring = &quot;&quot;&quot;A function.
<span class="gu">@@ -72,4 +72,16 @@ def test_docstring_with_python_OO():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/945
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Simulate Python -OO by setting __doc__ to None</span>
<span class="gi">+    original_doc = func.__doc__</span>
<span class="gi">+    func.__doc__ = None</span>
<span class="gi">+    </span>
<span class="gi">+    try:</span>
<span class="gi">+        # This should not raise a warning</span>
<span class="gi">+        with pytest.warns(None) as record:</span>
<span class="gi">+            Substitution(param_1=&quot;Parameter 1&quot;, param_2=&quot;Parameter 2&quot;)(func)</span>
<span class="gi">+        </span>
<span class="gi">+        assert len(record) == 0, &quot;Unexpected warning raised&quot;</span>
<span class="gi">+    finally:</span>
<span class="gi">+        # Restore the original docstring</span>
<span class="gi">+        func.__doc__ = original_doc</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_estimator_checks.py b/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gh">index e93b1c3..df71c2c 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gu">@@ -12,27 +12,69 @@ class BaseBadSampler(BaseEstimator):</span>
<span class="w"> </span>    &quot;&quot;&quot;Sampler without inputs checking.&quot;&quot;&quot;
<span class="w"> </span>    _sampling_type = &#39;bypass&#39;

<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>

<span class="w"> </span>class SamplerSingleClass(BaseSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler that would sample even with a single class.&quot;&quot;&quot;
<span class="w"> </span>    _sampling_type = &#39;bypass&#39;

<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>

<span class="w"> </span>class NotFittedSampler(BaseBadSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler without target checking.&quot;&quot;&quot;

<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        self.fitted_ = True</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>

<span class="w"> </span>class NoAcceptingSparseSampler(BaseBadSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler which does not accept sparse matrix.&quot;&quot;&quot;

<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        if not isinstance(X, np.ndarray):</span>
<span class="gi">+            raise TypeError(&quot;A numpy array is required. Got %s&quot; % type(X))</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>

<span class="w"> </span>class NotPreservingDtypeSampler(BaseSampler):
<span class="w"> </span>    _sampling_type = &#39;bypass&#39;
<span class="w"> </span>    _parameter_constraints: dict = {&#39;sampling_strategy&#39;: &#39;no_validation&#39;}

<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        return X.astype(float), y</span>
<span class="gi">+</span>

<span class="w"> </span>class IndicesSampler(BaseOverSampler):
<span class="gd">-    pass</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        check_classification_targets(y)</span>
<span class="gi">+        self.sampling_strategy_ = self.sampling_strategy</span>
<span class="gi">+        return self._sample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    def _sample(self, X, y):</span>
<span class="gi">+        n_samples, n_features = X.shape</span>
<span class="gi">+        classes = np.unique(y)</span>
<span class="gi">+        indices = np.arange(n_samples)</span>
<span class="gi">+        </span>
<span class="gi">+        for class_sample in classes:</span>
<span class="gi">+            if self.sampling_strategy_ == &#39;auto&#39;:</span>
<span class="gi">+                n_samples_class = max(np.bincount(y))</span>
<span class="gi">+            else:</span>
<span class="gi">+                n_samples_class = self.sampling_strategy_[class_sample]</span>
<span class="gi">+            </span>
<span class="gi">+            class_indices = indices[y == class_sample]</span>
<span class="gi">+            if len(class_indices) &lt; n_samples_class:</span>
<span class="gi">+                resampled_indices = np.random.choice(</span>
<span class="gi">+                    class_indices, size=n_samples_class, replace=True</span>
<span class="gi">+                )</span>
<span class="gi">+                indices = np.concatenate((indices, resampled_indices))</span>
<span class="gi">+                y = np.concatenate((y, [class_sample] * (n_samples_class - len(class_indices))))</span>
<span class="gi">+        </span>
<span class="gi">+        return X[indices], y</span>


<span class="w"> </span>mapping_estimator_error = {&#39;BaseBadSampler&#39;: (AssertionError,
<span class="gh">diff --git a/imblearn/utils/tests/test_param_validation.py b/imblearn/utils/tests/test_param_validation.py</span>
<span class="gh">index 8b0709d..70d370e 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_param_validation.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_param_validation.py</span>
<span class="gu">@@ -47,7 +47,36 @@ class _Estimator(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>@pytest.mark.parametrize(&#39;interval_type&#39;, [Integral, Real])
<span class="w"> </span>def test_interval_range(interval_type):
<span class="w"> </span>    &quot;&quot;&quot;Check the range of values depending on closed.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    left, right = 0, 10</span>
<span class="gi">+    </span>
<span class="gi">+    # Test closed=&#39;left&#39;</span>
<span class="gi">+    interval = Interval(interval_type, left, right, closed=&#39;left&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(left)</span>
<span class="gi">+    assert not interval.is_satisfied_by(right)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test closed=&#39;right&#39;</span>
<span class="gi">+    interval = Interval(interval_type, left, right, closed=&#39;right&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(left)</span>
<span class="gi">+    assert interval.is_satisfied_by(right)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test closed=&#39;both&#39;</span>
<span class="gi">+    interval = Interval(interval_type, left, right, closed=&#39;both&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(left)</span>
<span class="gi">+    assert interval.is_satisfied_by(right)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test closed=&#39;neither&#39;</span>
<span class="gi">+    interval = Interval(interval_type, left, right, closed=&#39;neither&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(left)</span>
<span class="gi">+    assert not interval.is_satisfied_by(right)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test values inside the interval</span>
<span class="gi">+    middle = (left + right) / 2 if interval_type is Real else (left + right) // 2</span>
<span class="gi">+    assert all(interval.is_satisfied_by(middle) for interval in [</span>
<span class="gi">+        Interval(interval_type, left, right, closed=&#39;left&#39;),</span>
<span class="gi">+        Interval(interval_type, left, right, closed=&#39;right&#39;),</span>
<span class="gi">+        Interval(interval_type, left, right, closed=&#39;both&#39;),</span>
<span class="gi">+        Interval(interval_type, left, right, closed=&#39;neither&#39;)</span>
<span class="gi">+    ])</span>


<span class="w"> </span>@pytest.mark.parametrize(&#39;interval_type&#39;, [Integral, Real])
<span class="gu">@@ -56,7 +85,20 @@ def test_interval_large_integers(interval_type):</span>

<span class="w"> </span>    non-regression test for #26648.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    large_int = 2**63 - 1  # Maximum value for 64-bit signed integer</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with large integers as bounds</span>
<span class="gi">+    interval = Interval(interval_type, -large_int, large_int, closed=&#39;both&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(0)</span>
<span class="gi">+    assert interval.is_satisfied_by(-large_int)</span>
<span class="gi">+    assert interval.is_satisfied_by(large_int)</span>
<span class="gi">+    assert not interval.is_satisfied_by(-large_int - 1)</span>
<span class="gi">+    assert not interval.is_satisfied_by(large_int + 1)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test with large integer as a value</span>
<span class="gi">+    interval = Interval(interval_type, -10, 10, closed=&#39;both&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(large_int)</span>
<span class="gi">+    assert not interval.is_satisfied_by(-large_int)</span>


<span class="w"> </span>def test_interval_inf_in_bounds():
<span class="gu">@@ -64,7 +106,30 @@ def test_interval_inf_in_bounds():</span>

<span class="w"> </span>    Only valid for real intervals.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    import numpy as np</span>
<span class="gi">+</span>
<span class="gi">+    # Test positive infinity</span>
<span class="gi">+    interval = Interval(Real, None, 0, closed=&#39;left&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(np.inf)</span>
<span class="gi">+    </span>
<span class="gi">+    interval = Interval(Real, None, 0, closed=&#39;neither&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(np.inf)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test negative infinity</span>
<span class="gi">+    interval = Interval(Real, 0, None, closed=&#39;right&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(-np.inf)</span>
<span class="gi">+    </span>
<span class="gi">+    interval = Interval(Real, 0, None, closed=&#39;neither&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(-np.inf)</span>
<span class="gi">+    </span>
<span class="gi">+    # Test both infinities</span>
<span class="gi">+    interval = Interval(Real, None, None, closed=&#39;both&#39;)</span>
<span class="gi">+    assert interval.is_satisfied_by(np.inf)</span>
<span class="gi">+    assert interval.is_satisfied_by(-np.inf)</span>
<span class="gi">+    </span>
<span class="gi">+    interval = Interval(Real, None, None, closed=&#39;neither&#39;)</span>
<span class="gi">+    assert not interval.is_satisfied_by(np.inf)</span>
<span class="gi">+    assert not interval.is_satisfied_by(-np.inf)</span>


<span class="w"> </span>@pytest.mark.parametrize(&#39;interval&#39;, [Interval(Real, 0, 1, closed=&#39;left&#39;),
<span class="gh">diff --git a/imblearn/utils/tests/test_show_versions.py b/imblearn/utils/tests/test_show_versions.py</span>
<span class="gh">index ca6a29e..979b929 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_show_versions.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_show_versions.py</span>
<span class="gu">@@ -1,2 +1,29 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the show_versions helper. Based on the sklearn tests.&quot;&quot;&quot;
<span class="gi">+import pytest</span>
<span class="w"> </span>from imblearn.utils._show_versions import _get_deps_info, show_versions
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_get_deps_info():</span>
<span class="gi">+    deps_info = _get_deps_info()</span>
<span class="gi">+    assert isinstance(deps_info, dict)</span>
<span class="gi">+    assert &quot;pip&quot; in deps_info</span>
<span class="gi">+    assert &quot;sklearn&quot; in deps_info</span>
<span class="gi">+    assert &quot;numpy&quot; in deps_info</span>
<span class="gi">+    assert &quot;scipy&quot; in deps_info</span>
<span class="gi">+    assert &quot;pandas&quot; in deps_info</span>
<span class="gi">+    assert &quot;joblib&quot; in deps_info</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;github&quot;, [True, False])</span>
<span class="gi">+def test_show_versions(capsys, github):</span>
<span class="gi">+    show_versions(github=github)</span>
<span class="gi">+    out, err = capsys.readouterr()</span>
<span class="gi">+    assert &quot;System Information&quot; in out</span>
<span class="gi">+    assert &quot;Python Dependencies&quot; in out</span>
<span class="gi">+    assert &quot;imlearn version&quot; in out</span>
<span class="gi">+    if github:</span>
<span class="gi">+        assert &quot;**System Information**&quot; in out</span>
<span class="gi">+        assert &quot;**Python Dependencies**&quot; in out</span>
<span class="gi">+    else:</span>
<span class="gi">+        assert &quot;System Information\n&quot; in out</span>
<span class="gi">+        assert &quot;Python Dependencies\n&quot; in out</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_testing.py b/imblearn/utils/tests/test_testing.py</span>
<span class="gh">index 421be2b..b917e42 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_testing.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_testing.py</span>
<span class="gu">@@ -9,4 +9,16 @@ from imblearn.utils.testing import _CustomNearestNeighbors, all_estimators</span>
<span class="w"> </span>def test_custom_nearest_neighbors():
<span class="w"> </span>    &quot;&quot;&quot;Check that our custom nearest neighbors can be used for our internal
<span class="w"> </span>    duck-typing.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # Create an instance of _CustomNearestNeighbors</span>
<span class="gi">+    custom_nn = _CustomNearestNeighbors()</span>
<span class="gi">+    </span>
<span class="gi">+    # Check if it&#39;s an instance of KNeighborsMixin</span>
<span class="gi">+    assert isinstance(custom_nn, KNeighborsMixin), &quot;_CustomNearestNeighbors should be an instance of KNeighborsMixin&quot;</span>
<span class="gi">+    </span>
<span class="gi">+    # Check if it has the required methods for duck-typing</span>
<span class="gi">+    required_methods = [&#39;kneighbors&#39;, &#39;fit&#39;]</span>
<span class="gi">+    for method in required_methods:</span>
<span class="gi">+        assert hasattr(custom_nn, method), f&quot;_CustomNearestNeighbors should have a &#39;{method}&#39; method&quot;</span>
<span class="gi">+    </span>
<span class="gi">+    # Check if it&#39;s not an instance of SamplerMixin</span>
<span class="gi">+    assert not isinstance(custom_nn, SamplerMixin), &quot;_CustomNearestNeighbors should not be an instance of SamplerMixin&quot;</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
        <script src="../javascripts/tablesort.number.js"></script>
      
    
  </body>
</html>