
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis commit0 all reference minitorch - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reference-gold-minitorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis commit0 all reference minitorch
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytest-summary-for-test-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Pytest Summary for test tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytests:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytests:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule1" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule2" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule3" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule4" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop1" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop2" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop3" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop4" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_batch" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv2" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_demo" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_demo
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_module_fail_forward" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_module_fail_forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_linear" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_linear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_nn_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_nn_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_avg" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_avg
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max_pool" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max_pool
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_drop" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_drop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_log_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_same_as_python" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_same_as_python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu_back" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu_back
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_id" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_lt" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_lt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_eq" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_eq
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sigmoid
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_transitive" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_transitive
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_symmetric" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_symmetric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_other" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_other
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_zip_with" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_zip_with
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_prod" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_prod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_neglist" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_negList
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_backs" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_backs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_central_diff" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_central_diff
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_create" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromlist" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromlist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_back_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_back_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_permute_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_permute_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_index" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromnumpy" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromnumpy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim_2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim_2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_all_dims" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_all_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_layout" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_layout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_layout_bad" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_layout_bad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_enumeration" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_enumeration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_index" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_shape_broadcast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_shape_broadcast
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_string" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_createfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_create[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_permutefast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_permute[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_mm2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_mm2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_bmmfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_bmm[fast]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_commit0_all_reference">back to Reference (Gold) summary</a></p>
<h1 id="reference-gold-minitorch"><strong>Reference (Gold)</strong>: minitorch</h1>
<h2 id="pytest-summary-for-test-tests">Pytest Summary for test <code>tests</code></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">status</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">211</td>
</tr>
<tr>
<td style="text-align: left;">xfailed</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">230</td>
</tr>
<tr>
<td style="text-align: left;">collected</td>
<td style="text-align: center;">230</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytests">Failed pytests:</h2>
<h3 id="test_autodiffpytest_chain_rule1">test_autodiff.py::test_chain_rule1</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule1</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule1() -> None:
        x = minitorch.Scalar(0.0)
        constant = minitorch.Scalar(
            0.0, ScalarHistory(Function1, ctx=Context(), inputs=[x, x])
        )
>       back = constant.chain_rule(d_output=5)

tests/test_autodiff.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule2">test_autodiff.py::test_chain_rule2</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule2</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule2() -> None:
        var = minitorch.Scalar(0.0, ScalarHistory())
        constant = minitorch.Scalar(
            0.0, ScalarHistory(Function1, ctx=Context(), inputs=[var, var])
        )
>       back = constant.chain_rule(d_output=5)

tests/test_autodiff.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule3">test_autodiff.py::test_chain_rule3</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule3</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule3() -> None:
        "Check that constrants are ignored and variables get derivatives."
        constant = 10
        var = minitorch.Scalar(5)

        y = Function2.apply(constant, var)

>       back = y.chain_rule(d_output=5)

tests/test_autodiff.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(60.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule4">test_autodiff.py::test_chain_rule4</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule4</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule4() -> None:
        var1 = minitorch.Scalar(5)
        var2 = minitorch.Scalar(10)

        y = Function2.apply(var1, var2)

>       back = y.chain_rule(d_output=5)

tests/test_autodiff.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(55.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop1">test_autodiff.py::test_backprop1</h3>
<details><summary> <pre>test_autodiff.py::test_backprop1</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop1() -> None:
        # Example 1: F1(0, v)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
>       var2.backward(d_output=5)

tests/test_autodiff.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(10.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop2">test_autodiff.py::test_backprop2</h3>
<details><summary> <pre>test_autodiff.py::test_backprop2</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop2() -> None:
        # Example 2: F1(0, 0)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
        var3 = Function1.apply(0, var2)
>       var3.backward(d_output=5)

tests/test_autodiff.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(20.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop3">test_autodiff.py::test_backprop3</h3>
<details><summary> <pre>test_autodiff.py::test_backprop3</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop3() -> None:
        # Example 3: F1(F1(0, v1), F1(0, v1))
        var1 = minitorch.Scalar(0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(30.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop4">test_autodiff.py::test_backprop4</h3>
<details><summary> <pre>test_autodiff.py::test_backprop4</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop4() -> None:
        # Example 4: F1(F1(0, v1), F1(0, v1))
        var0 = minitorch.Scalar(0)
        var1 = Function1.apply(0, var0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(50.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d_simple">test_conv.py::test_conv1d_simple</h3>
<details><summary> <pre>test_conv.py::test_conv1d_simple</pre></summary><pre>
@pytest.mark.task4_1
    def test_conv1d_simple() -> None:
>       t = minitorch.tensor([0, 1, 2, 3]).view(1, 1, 4)

tests/test_conv.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (4,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d">test_conv.py::test_conv1d</h3>
<details><summary> <pre>test_conv.py::test_conv1d</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(1, 1, 6)), tensors(shape=(1, 1, 4)))

tests/test_conv.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d_channel">test_conv.py::test_conv1d_channel</h3>
<details><summary> <pre>test_conv.py::test_conv1d_channel</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(2, 2, 6)), tensors(shape=(3, 2, 2)))

tests/test_conv.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv">test_conv.py::test_conv</h3>
<details><summary> <pre>test_conv.py::test_conv</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(1, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv_batch">test_conv.py::test_conv_batch</h3>
<details><summary> <pre>test_conv.py::test_conv_batch</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv_channel">test_conv.py::test_conv_channel</h3>
<details><summary> <pre>test_conv.py::test_conv_channel</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 2, 6, 6)), tensors(shape=(3, 2, 2, 4)))

tests/test_conv.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv2">test_conv.py::test_conv2</h3>
<details><summary> <pre>test_conv.py::test_conv2</pre></summary><pre>
@pytest.mark.task4_2
    def test_conv2() -> None:
>       t = minitorch.tensor([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]).view(
            1, 1, 4, 4
        )

tests/test_conv.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_stacked_demo">test_module.py::test_stacked_demo</h3>
<details><summary> <pre>test_module.py::test_stacked_demo</pre></summary><pre>
@pytest.mark.task0_4
    def test_stacked_demo() -> None:
        "Check that each of the properties match"
        mod = ModuleA1()
>       np = dict(mod.named_parameters())

tests/test_module.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ModuleA1(
  (a): ModuleA2()
  (b): ModuleA3(
    (c): ModuleA4()
  )
)

    def named_parameters(self) -> Sequence[Tuple[str, Parameter]]:
        """
        Collect all the parameters of this module and its descendents.


        Returns:
            The name and `Parameter` of each ancestor parameter.
        """
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4

minitorch/module.py:51: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_module">test_module.py::test_module</h3>
<details><summary> <pre>test_module.py::test_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints)

tests/test_module.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_module.py:100: in test_module
    module.eval()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module2(
  (module_c): Module3()
)

    def eval(self) -> None:
        "Set the mode of this module and all descendent modules to `eval`."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4
E       Falsifying example: test_module(
E           size_b=1, size_a=1,
E       )

minitorch/module.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_stacked_module">test_module.py::test_stacked_module</h3>
<details><summary> <pre>test_module.py::test_stacked_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints, small_floats)

tests/test_module.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_module.py:121: in test_stacked_module
    module.eval()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module1(
  (module_a): Module2(
    (module_c): Module3()
  )
  (module_b): Module2(
    (module_c): Module3()
  )
)

    def eval(self) -> None:
        "Set the mode of this module and all descendent modules to `eval`."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4
E       Falsifying example: test_stacked_module(
E           val=0.0, size_b=1, size_a=1,
E       )

minitorch/module.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_module_fail_forward">test_module.py::test_module_fail_forward</h3>
<details><summary> <pre>test_module.py::test_module_fail_forward</pre></summary><pre>
@pytest.mark.task0_4
    @pytest.mark.xfail
    def test_module_fail_forward() -> None:
        mod = minitorch.Module()
>       mod()

tests/test_module.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module(), args = (), kwargs = {}

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
>       return self.forward(*args, **kwargs)
E       TypeError: 'NoneType' object is not callable

minitorch/module.py:90: TypeError
</pre>
</details>
<h3 id="test_modulespytest_linear">test_modules.py::test_linear</h3>
<details><summary> <pre>test_modules.py::test_linear</pre></summary><pre>
@given(lists(scalars(), max_size=10), integers(min_value=5, max_value=20))
>   def test_linear(inputs: List[Scalar], out_size: int) -> None:

tests/test_modules.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_modules.py:65: in test_linear
    lin2.forward(mid)
tests/test_modules.py:56: in forward
    y[j] = y[j] + x * self.weights[i][j].value
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.6888437030500962
b = -0.19013172509917142

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_linear(
E           out_size=5, inputs=[],
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_modulespytest_nn_size">test_modules.py::test_nn_size</h3>
<details><summary> <pre>test_modules.py::test_nn_size</pre></summary><pre>
def test_nn_size() -> None:
        model = Network2()
>       assert len(model.parameters()) == (
            len(model.layer1.parameters()) + len(model.layer2.parameters())
        )

tests/test_modules.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Network2(
  (layer1): ScalarLinear()
  (layer2): ScalarLinear()
)

    def parameters(self) -> Sequence[Parameter]:
        "Enumerate over all the parameters of this module and its descendents."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4

minitorch/module.py:56: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_avg">test_nn.py::test_avg</h3>
<details><summary> <pre>test_nn.py::test_avg</pre></summary><pre>
@pytest.mark.task4_3
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_max">test_nn.py::test_max</h3>
<details><summary> <pre>test_nn.py::test_max</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(2, 3, 4)))

tests/test_nn.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_max_pool">test_nn.py::test_max_pool</h3>
<details><summary> <pre>test_nn.py::test_max_pool</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_drop">test_nn.py::test_drop</h3>
<details><summary> <pre>test_nn.py::test_drop</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors())

tests/test_nn.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_softmax">test_nn.py::test_softmax</h3>
<details><summary> <pre>test_nn.py::test_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_log_softmax">test_nn.py::test_log_softmax</h3>
<details><summary> <pre>test_nn.py::test_log_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_same_as_python">test_operators.py::test_same_as_python</h3>
<details><summary> <pre>test_operators.py::test_same_as_python</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats, small_floats)

tests/test_operators.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:37: in test_same_as_python
    assert_close(mul(x, y), x * y)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 0.0

    def mul(x: float, y: float) -> float:
        "$f(x, y) = x * y$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_same_as_python(
E           y=0.0, x=0.0,
E       )

minitorch/operators.py:16: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_relu">test_operators.py::test_relu</h3>
<details><summary> <pre>test_operators.py::test_relu</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:49: in test_relu
    assert relu(a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.0

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_relu(
E           a=1.0,
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_relu_back">test_operators.py::test_relu_back</h3>
<details><summary> <pre>test_operators.py::test_relu_back</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats, small_floats)

tests/test_operators.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:58: in test_relu_back
    assert relu_back(a, b) == b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.0, d = 0.0

    def relu_back(x: float, d: float) -> float:
        r"If $f = relu$ compute $d \times f'(x)$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_relu_back(
E           b=0.0, a=1.0,
E       )

minitorch/operators.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_id">test_operators.py::test_id</h3>
<details><summary> <pre>test_operators.py::test_id</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:66: in test_id
    assert id(a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0

    def id(x: float) -> float:
        "$f(x) = x$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_id(
E           a=0.0,
E       )

minitorch/operators.py:22: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_lt">test_operators.py::test_lt</h3>
<details><summary> <pre>test_operators.py::test_lt</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:73: in test_lt
    assert lt(a - 1.0, a) == 1.0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = -1.0, y = 0.0

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_lt(
E           a=0.0,
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_max">test_operators.py::test_max</h3>
<details><summary> <pre>test_operators.py::test_max</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:80: in test_max
    assert max(a - 1.0, a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = -1.0, y = 0.0

    def max(x: float, y: float) -> float:
        "$f(x) =$ x if x is greater than y else y"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_max(
E           a=0.0,
E       )

minitorch/operators.py:52: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_eq">test_operators.py::test_eq</h3>
<details><summary> <pre>test_operators.py::test_eq</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:89: in test_eq
    assert eq(a, a) == 1.0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 0.0

    def eq(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is equal to y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_eq(
E           a=0.0,
E       )

minitorch/operators.py:46: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sigmoid">test_operators.py::test_sigmoid</h3>
<details><summary> <pre>test_operators.py::test_sigmoid</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats)

tests/test_operators.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_2
    @given(small_floats)
    def test_sigmoid(a: float) -> None:
        """Check properties of the sigmoid function, specifically
        * It is always between 0.0 and 1.0.
        * one minus sigmoid is the same as sigmoid of the negative
        * It crosses 0 at 0.5
        * It is  strictly increasing.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_sigmoid(
E           a=0.0,
E       )

tests/test_operators.py:111: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_transitive">test_operators.py::test_transitive</h3>
<details><summary> <pre>test_operators.py::test_transitive</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0, b = 0.0, c = 0.0

    @pytest.mark.task0_2
    @given(small_floats, small_floats, small_floats)
    def test_transitive(a: float, b: float, c: float) -> None:
        "Test the transitive property of less-than (a < b and b < c implies a < c)"
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_transitive(
E           c=0.0, b=0.0, a=0.0,
E       )

tests/test_operators.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_symmetric">test_operators.py::test_symmetric</h3>
<details><summary> <pre>test_operators.py::test_symmetric</pre></summary><pre>
@pytest.mark.task0_2
    def test_symmetric() -> None:
        """
        Write a test that ensures that :func:`minitorch.operators.mul` is symmetric, i.e.
        gives the same value regardless of the order of its input.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:129: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_distribute">test_operators.py::test_distribute</h3>
<details><summary> <pre>test_operators.py::test_distribute</pre></summary><pre>
@pytest.mark.task0_2
    def test_distribute() -> None:
        r"""
        Write a test that ensures that your operators distribute, i.e.
        :math:`z \times (x + y) = z \times x + z \times y`
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:139: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_other">test_operators.py::test_other</h3>
<details><summary> <pre>test_operators.py::test_other</pre></summary><pre>
@pytest.mark.task0_2
    def test_other() -> None:
        """
        Write a test that ensures some other property holds for your functions.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:148: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_zip_with">test_operators.py::test_zip_with</h3>
<details><summary> <pre>test_operators.py::test_zip_with</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats, small_floats)

tests/test_operators.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:160: in test_zip_with
    x1, x2 = addLists([a, b], [c, d])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls1 = [0.0, 0.0], ls2 = [0.0, 0.0]

    def addLists(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:
        "Add the elements of `ls1` and `ls2` using `zipWith` and `add`"
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_zip_with(
E           d=0.0, c=0.0, b=0.0, a=0.0,
E       )

minitorch/operators.py:175: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sum_distribute">test_operators.py::test_sum_distribute</h3>
<details><summary> <pre>test_operators.py::test_sum_distribute</pre></summary><pre>
@pytest.mark.task0_3
>   @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )

tests/test_operators.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls1 = [0.0, 0.0, 0.0, 0.0, 0.0], ls2 = [0.0, 0.0, 0.0, 0.0, 0.0]

    @pytest.mark.task0_3
    @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )
    def test_sum_distribute(ls1: List[float], ls2: List[float]) -> None:
        """
        Write a test that ensures that the sum of `ls1` plus the sum of `ls2`
        is the same as the sum of each element of `ls1` plus each element of `ls2`.
        """
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_sum_distribute(
E           ls2=[0.0, 0.0, 0.0, 0.0, 0.0], ls1=[0.0, 0.0, 0.0, 0.0, 0.0],
E       )

tests/test_operators.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sum">test_operators.py::test_sum</h3>
<details><summary> <pre>test_operators.py::test_sum</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:183: in test_sum
    assert_close(sum(ls), sum(ls))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = []

    def sum(ls: Iterable[float]) -> float:
        "Sum up a list using `reduce` and `add`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_sum(
E           ls=[],
E       )

minitorch/operators.py:200: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_prod">test_operators.py::test_prod</h3>
<details><summary> <pre>test_operators.py::test_prod</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:189: in test_prod
    assert_close(prod([x, y, z]), x * y * z)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = [0.0, 0.0, 0.0]

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_prod(
E           z=0.0, y=0.0, x=0.0,
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_neglist">test_operators.py::test_negList</h3>
<details><summary> <pre>test_operators.py::test_negList</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:195: in test_negList
    check = negList(ls)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = []

    def negList(ls: Iterable[float]) -> Iterable[float]:
        "Use `map` and `neg` to negate each element in `ls`"
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_negList(
E           ls=[],
E       )

minitorch/operators.py:149: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn1">test_operators.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:119: in complex
    operators.relu(operators.relu(a * 10 + 7) * 6 + 5) * 10
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 7.0

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('complex', complex),
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn6">test_operators.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:49: in inv
    return operators.inv(a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 3.5

    def inv(x: float) -> float:
        "$f(x) = 1/x$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('inv', inv),
E       )

minitorch/operators.py:109: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn10">test_operators.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:64: in relu
    return operators.relu(a + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 5.5

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('relu', relu),
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn11">test_operators.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:54: in sig
    return operators.sigmoid(a)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0

    def sigmoid(x: float) -> float:
        r"""
        $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$

        (See https://en.wikipedia.org/wiki/Sigmoid_function )

        Calculate as

        $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$ if x >=0 else $\frac{e^x}{(1.0 + e^{x})}$

        for stability.
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('sig', sig),
E       )

minitorch/operators.py:74: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn2">test_operators.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:100: in eq2
    return operators.eq(a, (b + 5.5))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 5.5

    def eq(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is equal to y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('eq2', eq2),
E       )

minitorch/operators.py:46: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn3">test_operators.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:92: in gt2
    return operators.lt(b, a + 1.2)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 1.2

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('gt2', gt2),
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn4">test_operators.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:96: in lt2
    return operators.lt(a + 1.2, b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.2, y = 0.0

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('lt2', lt2),
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_backs">test_operators.py::test_backs</h3>
<details><summary> <pre>test_operators.py::test_backs</pre></summary><pre>
@given(small_floats, small_floats)
>   def test_backs(a: float, b: float) -> None:

tests/test_operators.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:226: in test_backs
    relu_back(a, b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, d = 0.0

    def relu_back(x: float, d: float) -> float:
        r"If $f = relu$ compute $d \times f'(x)$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_backs(
E           b=0.0, a=0.0,
E       )

minitorch/operators.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_central_diff">test_scalar.py::test_central_diff</h3>
<details><summary> <pre>test_scalar.py::test_central_diff</pre></summary><pre>
@pytest.mark.task1_1
    def test_central_diff() -> None:
>       d = central_difference(operators.id, 5, arg=0)

tests/test_scalar.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

f = <function id at 0x7f2cc4507490>, arg = 0, epsilon = 1e-06, vals = (5,)

    def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -> Any:
        r"""
        Computes an approximation to the derivative of `f` with respect to one arg.

        See :doc:`derivative` or https://en.wikipedia.org/wiki/Finite_difference for more details.

        Args:
            f : arbitrary function from n-scalar args to one value
            *vals : n-float values $x_0 \ldots x_{n-1}$
            arg : the number $i$ of the arg to compute the derivative
            epsilon : a small constant

        Returns:
            An approximation of $f'_i(x_0, \ldots, x_{n-1})$
        """
        # TODO: Implement for Task 1.1.
>       raise NotImplementedError('Need to implement for Task 1.1')
E       NotImplementedError: Need to implement for Task 1.1

minitorch/autodiff.py:26: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_simple">test_scalar.py::test_simple</h3>
<details><summary> <pre>test_scalar.py::test_simple</pre></summary><pre>
@given(small_floats, small_floats)
>   def test_simple(a: float, b: float) -> None:

tests/test_scalar.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:57: in test_simple
    c = Scalar(a) + Scalar(b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_simple(
E           b=0.0, a=0.0,
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn0">test_scalar.py::test_one_args[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:19: in addConstant
    return 5 + a
minitorch/scalar.py:122: in __radd__
    return self + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('addConstant', addConstant, addConstant),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn1">test_scalar.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:213: in complex
    return (((a * 10 + 7).relu() * 6 + 5).relu() * 10).sigmoid().log() / 50
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 10

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('complex', complex, complex),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn2">test_scalar.py::test_one_args[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:29: in cube
    return a * a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('cube', cube, cube),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn3">test_scalar.py::test_one_args[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:44: in div
    return a / 5
minitorch/scalar.py:89: in __truediv__
    return Mul.apply(self, Inv.apply(b))
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 5

    @staticmethod
    def forward(ctx: Context, a: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('div', div, div),
E       )

minitorch/scalar_functions.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn4">test_scalar.py::test_one_args[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:181: in exp
    return (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 200

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('exp', exp, exp),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn5">test_scalar.py::test_one_args[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:185: in explog
    return (a + 100000).log() + (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('explog', explog, explog),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn6">test_scalar.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:165: in inv
    return 1.0 / (a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 3.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('inv', inv, inv),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn7">test_scalar.py::test_one_args[fn7]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:173: in log
    return (x + 100000).log()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('log', log, log),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn8">test_scalar.py::test_one_args[fn8]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:39: in multConstant
    return 5 * a
minitorch/scalar.py:125: in __rmul__
    return self * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 5

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('multConstant', multConstant, multConstant),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn9">test_scalar.py::test_one_args[fn9]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:14: in neg
    return -a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def __neg__(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('neg', neg, neg),
E       )

minitorch/scalar.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn10">test_scalar.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:177: in relu
    return (x + 5.5).relu()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('relu', relu, relu),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn11">test_scalar.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:169: in sig
    return x.sigmoid()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def sigmoid(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('sig', sig, sig),
E       )

minitorch/scalar.py:137: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn12">test_scalar.py::test_one_args[fn12]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:24: in square
    return a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('square', square, square),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn13">test_scalar.py::test_one_args[fn13]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:34: in subConstant
    return a - 5
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('subConstant', subConstant, subConstant),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn0">test_scalar.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:78: in add2
    return a + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('add2', add2, add2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn1">test_scalar.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:88: in div2
    return a / (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('div2', div2, div2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn2">test_scalar.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:201: in eq2
    return a == (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq2', eq2, eq2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn3">test_scalar.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:205: in gt2
    return a + 1.2 > b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('gt2', gt2, gt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn4">test_scalar.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:209: in lt2
    return a + 1.2 < b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt2', lt2, lt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn5">test_scalar.py::test_two_args[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:83: in mul2
    return a * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('mul2', mul2, mul2),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn0">test_scalar.py::test_one_derivative[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:19: in addConstant
    return 5 + a
minitorch/scalar.py:122: in __radd__
    return self + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('addConstant', addConstant, addConstant),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn1">test_scalar.py::test_one_derivative[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:213: in complex
    return (((a * 10 + 7).relu() * 6 + 5).relu() * 10).sigmoid().log() / 50
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 10

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('complex', complex, complex),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn2">test_scalar.py::test_one_derivative[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:29: in cube
    return a * a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('cube', cube, cube),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn3">test_scalar.py::test_one_derivative[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:44: in div
    return a / 5
minitorch/scalar.py:89: in __truediv__
    return Mul.apply(self, Inv.apply(b))
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 5

    @staticmethod
    def forward(ctx: Context, a: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('div', div, div),
E       )

minitorch/scalar_functions.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn4">test_scalar.py::test_one_derivative[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:181: in exp
    return (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 200

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('exp', exp, exp),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn5">test_scalar.py::test_one_derivative[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:185: in explog
    return (a + 100000).log() + (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('explog', explog, explog),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn6">test_scalar.py::test_one_derivative[fn6]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:165: in inv
    return 1.0 / (a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 3.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('inv', inv, inv),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn7">test_scalar.py::test_one_derivative[fn7]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:173: in log
    return (x + 100000).log()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('log', log, log),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn8">test_scalar.py::test_one_derivative[fn8]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:39: in multConstant
    return 5 * a
minitorch/scalar.py:125: in __rmul__
    return self * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 5

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('multConstant', multConstant, multConstant),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn9">test_scalar.py::test_one_derivative[fn9]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:14: in neg
    return -a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def __neg__(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('neg', neg, neg),
E       )

minitorch/scalar.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn10">test_scalar.py::test_one_derivative[fn10]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:177: in relu
    return (x + 5.5).relu()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('relu', relu, relu),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn11">test_scalar.py::test_one_derivative[fn11]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:169: in sig
    return x.sigmoid()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def sigmoid(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('sig', sig, sig),
E       )

minitorch/scalar.py:137: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn12">test_scalar.py::test_one_derivative[fn12]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:24: in square
    return a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('square', square, square),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn13">test_scalar.py::test_one_derivative[fn13]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:34: in subConstant
    return a - 5
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('subConstant', subConstant, subConstant),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn0">test_scalar.py::test_two_derivative[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:78: in add2
    return a + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('add2', add2, add2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn1">test_scalar.py::test_two_derivative[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:88: in div2
    return a / (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('div2', div2, div2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn2">test_scalar.py::test_two_derivative[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:201: in eq2
    return a == (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq2', eq2, eq2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn3">test_scalar.py::test_two_derivative[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:205: in gt2
    return a + 1.2 > b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('gt2', gt2, gt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn4">test_scalar.py::test_two_derivative[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:209: in lt2
    return a + 1.2 < b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt2', lt2, lt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn5">test_scalar.py::test_two_derivative[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:83: in mul2
    return a * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('mul2', mul2, mul2),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_create">test_tensor.py::test_create</h3>
<details><summary> <pre>test_tensor.py::test_create</pre></summary><pre>
@given(lists(small_floats, min_size=1))
>   def test_create(t1: List[float]) -> None:

tests/test_tensor.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor.py:18: in test_create
    t2 = tensor(t1)
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_create(
E           t1=[0.0],
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn0">test_tensor.py::test_one_args[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn1">test_tensor.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn2">test_tensor.py::test_one_args[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn3">test_tensor.py::test_one_args[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn4">test_tensor.py::test_one_args[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn5">test_tensor.py::test_one_args[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn6">test_tensor.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn7">test_tensor.py::test_one_args[fn7]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn8">test_tensor.py::test_one_args[fn8]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn9">test_tensor.py::test_one_args[fn9]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn10">test_tensor.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn11">test_tensor.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn12">test_tensor.py::test_one_args[fn12]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn13">test_tensor.py::test_one_args[fn13]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn0">test_tensor.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn1">test_tensor.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn2">test_tensor.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn3">test_tensor.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn4">test_tensor.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn5">test_tensor.py::test_two_args[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn0">test_tensor.py::test_one_derivative[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn1">test_tensor.py::test_one_derivative[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn2">test_tensor.py::test_one_derivative[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn3">test_tensor.py::test_one_derivative[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn4">test_tensor.py::test_one_derivative[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn5">test_tensor.py::test_one_derivative[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn6">test_tensor.py::test_one_derivative[fn6]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn7">test_tensor.py::test_one_derivative[fn7]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn8">test_tensor.py::test_one_derivative[fn8]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn9">test_tensor.py::test_one_derivative[fn9]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn10">test_tensor.py::test_one_derivative[fn10]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn11">test_tensor.py::test_one_derivative[fn11]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn12">test_tensor.py::test_one_derivative[fn12]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn13">test_tensor.py::test_one_derivative[fn13]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_permute">test_tensor.py::test_permute</h3>
<details><summary> <pre>test_tensor.py::test_permute</pre></summary><pre>
@given(data(), tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_size">test_tensor.py::test_grad_size</h3>
<details><summary> <pre>test_tensor.py::test_grad_size</pre></summary><pre>
def test_grad_size() -> None:
        "Test the size of the gradient (from @WannaFy)"
>       a = tensor([1], requires_grad=True)

tests/test_tensor.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn0">test_tensor.py::test_grad_reduce[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn0]</pre></summary><pre>
fn = ('mean_full_red', <function MathTest.mean_full_red at 0x7f2cb9949750>, <function MathTestVariable.mean_full_red at 0x7f2cb9949e10>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn1">test_tensor.py::test_grad_reduce[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn1]</pre></summary><pre>
fn = ('mean_red', <function MathTest.mean_red at 0x7f2cb99496c0>, <function MathTestVariable.mean_red at 0x7f2cb9949d80>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn2">test_tensor.py::test_grad_reduce[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn2]</pre></summary><pre>
fn = ('sum_red', <function MathTest.sum_red at 0x7f2cb9949630>, <function MathTestVariable.sum_red at 0x7f2cb9949cf0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn0">test_tensor.py::test_two_grad[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn1">test_tensor.py::test_two_grad[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn2">test_tensor.py::test_two_grad[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn3">test_tensor.py::test_two_grad[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn4">test_tensor.py::test_two_grad[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn5">test_tensor.py::test_two_grad[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn0">test_tensor.py::test_two_grad_broadcast[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn1">test_tensor.py::test_two_grad_broadcast[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn2">test_tensor.py::test_two_grad_broadcast[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn3">test_tensor.py::test_two_grad_broadcast[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn4">test_tensor.py::test_two_grad_broadcast[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn5">test_tensor.py::test_two_grad_broadcast[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_fromlist">test_tensor.py::test_fromlist</h3>
<details><summary> <pre>test_tensor.py::test_fromlist</pre></summary><pre>
def test_fromlist() -> None:
        "Test longer from list conversion"
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_view">test_tensor.py::test_view</h3>
<details><summary> <pre>test_tensor.py::test_view</pre></summary><pre>
def test_view() -> None:
        "Test view"
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_back_view">test_tensor.py::test_back_view</h3>
<details><summary> <pre>test_tensor.py::test_back_view</pre></summary><pre>
@given(tensors())
>   def test_back_view(t1: Tensor) -> None:

tests/test_tensor.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_permute_view">test_tensor.py::test_permute_view</h3>
<details><summary> <pre>test_tensor.py::test_permute_view</pre></summary><pre>
@pytest.mark.xfail
    def test_permute_view() -> None:
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_index">test_tensor.py::test_index</h3>
<details><summary> <pre>test_tensor.py::test_index</pre></summary><pre>
@pytest.mark.xfail
    def test_index() -> None:
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_fromnumpy">test_tensor.py::test_fromnumpy</h3>
<details><summary> <pre>test_tensor.py::test_fromnumpy</pre></summary><pre>
def test_fromnumpy() -> None:
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim">test_tensor.py::test_reduce_forward_one_dim</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim_2">test_tensor.py::test_reduce_forward_one_dim_2</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim_2</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim_2() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_all_dims">test_tensor.py::test_reduce_forward_all_dims</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_all_dims</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_all_dims() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:219: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_layout">test_tensor_data.py::test_layout</h3>
<details><summary> <pre>test_tensor_data.py::test_layout</pre></summary><pre>
@pytest.mark.task2_1
    def test_layout() -> None:
        "Test basis properties of layout and strides"
        data = [0] * 3 * 5
>       tensor_data = minitorch.TensorData(data, (3, 5), (5, 1))

tests/test_tensor_data.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 5)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_layout_bad">test_tensor_data.py::test_layout_bad</h3>
<details><summary> <pre>test_tensor_data.py::test_layout_bad</pre></summary><pre>
@pytest.mark.xfail
    def test_layout_bad() -> None:
        "Test basis properties of layout and strides"
        data = [0] * 3 * 5
>       minitorch.TensorData(data, (3, 5), (6,))

tests/test_tensor_data.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <minitorch.tensor_data.TensorData object at 0x7f2cb905ff70>
storage = [0, 0, 0, 0, 0, 0, ...], shape = (3, 5), strides = (6,)

    def __init__(
        self,
        storage: Union[Sequence[float], Storage],
        shape: UserShape,
        strides: Optional[UserStrides] = None,
    ):
        if isinstance(storage, np.ndarray):
            self._storage = storage
        else:
            self._storage = array(storage, dtype=float64)

        if strides is None:
            strides = strides_from_shape(shape)

        assert isinstance(strides, tuple), "Strides must be tuple"
        assert isinstance(shape, tuple), "Shape must be tuple"
        if len(strides) != len(shape):
>           raise IndexingError(f"Len of strides {strides} must match {shape}.")
E           minitorch.tensor_data.IndexingError: Len of strides (6,) must match (3, 5).

minitorch/tensor_data.py:142: IndexingError
</pre>
</details>
<h3 id="test_tensor_datapytest_enumeration">test_tensor_data.py::test_enumeration</h3>
<details><summary> <pre>test_tensor_data.py::test_enumeration</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_index">test_tensor_data.py::test_index</h3>
<details><summary> <pre>test_tensor_data.py::test_index</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_permute">test_tensor_data.py::test_permute</h3>
<details><summary> <pre>test_tensor_data.py::test_permute</pre></summary><pre>
@pytest.mark.task2_1
>   @given(data())

tests/test_tensor_data.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_data.py:83: in test_permute
    td = data.draw(tensor_data())
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_permute(
E           data=data(...),
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_shape_broadcast">test_tensor_data.py::test_shape_broadcast</h3>
<details><summary> <pre>test_tensor_data.py::test_shape_broadcast</pre></summary><pre>
@pytest.mark.task2_2
    def test_shape_broadcast() -> None:
>       c = minitorch.shape_broadcast((1,), (5, 5))

tests/test_tensor_data.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shape1 = (1,), shape2 = (5, 5)

    def shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:
        """
        Broadcast two shapes to create a new union shape.

        Args:
            shape1 : first shape
            shape2 : second shape

        Returns:
            broadcasted shape

        Raises:
            IndexingError : if cannot broadcast
        """
        # TODO: Implement for Task 2.2.
>       raise NotImplementedError('Need to implement for Task 2.2')
E       NotImplementedError: Need to implement for Task 2.2

minitorch/tensor_data.py:105: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_string">test_tensor_data.py::test_string</h3>
<details><summary> <pre>test_tensor_data.py::test_string</pre></summary><pre>
@given(tensor_data())
>   def test_string(tensor_data: TensorData) -> None:

tests/test_tensor_data.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_createfast">test_tensor_general.py::test_create[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_create[fast]</pre></summary><pre>
backend = 'fast'

    @given(lists(small_floats, min_size=1))
>   @pytest.mark.parametrize("backend", backend_tests)

tests/test_tensor_general.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:48: in test_create
    t2 = minitorch.tensor(t1, backend=shared[backend])
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_create(
E           t1=[0.0], backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn0">test_tensor_general.py::test_one_args[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('addConstant', addConstant, addConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn1">test_tensor_general.py::test_one_args[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('complex', complex, complex), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn2">test_tensor_general.py::test_one_args[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('cube', cube, cube), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn3">test_tensor_general.py::test_one_args[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('div', div, div), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn4">test_tensor_general.py::test_one_args[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('exp', exp, exp), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn5">test_tensor_general.py::test_one_args[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('explog', explog, explog), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn6">test_tensor_general.py::test_one_args[fast-fn6]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('inv', inv, inv), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn7">test_tensor_general.py::test_one_args[fast-fn7]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('log', log, log), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn8">test_tensor_general.py::test_one_args[fast-fn8]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('multConstant', multConstant, multConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn9">test_tensor_general.py::test_one_args[fast-fn9]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('neg', neg, neg), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn10">test_tensor_general.py::test_one_args[fast-fn10]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('relu', relu, relu), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn11">test_tensor_general.py::test_one_args[fast-fn11]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('sig', sig, sig), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn12">test_tensor_general.py::test_one_args[fast-fn12]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('square', square, square), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn13">test_tensor_general.py::test_one_args[fast-fn13]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('subConstant', subConstant, subConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn0">test_tensor_general.py::test_two_args[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn1">test_tensor_general.py::test_two_args[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn2">test_tensor_general.py::test_two_args[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn3">test_tensor_general.py::test_two_args[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn4">test_tensor_general.py::test_two_args[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn5">test_tensor_general.py::test_two_args[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn0">test_tensor_general.py::test_one_derivative[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x7f2cb9948c10>, <function MathTest.addConstant at 0x7f2cb9948c10>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('addConstant', addConstant, addConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn1">test_tensor_general.py::test_one_derivative[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x7f2cb99497e0>, <function MathTestVariable.complex at 0x7f2cb994a050>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('complex', complex, complex), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn2">test_tensor_general.py::test_one_derivative[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x7f2cb9948d30>, <function MathTest.cube at 0x7f2cb9948d30>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('cube', cube, cube), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn3">test_tensor_general.py::test_one_derivative[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x7f2cb9948ee0>, <function MathTest.div at 0x7f2cb9948ee0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('div', div, div), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn4">test_tensor_general.py::test_one_derivative[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x7f2cb99491b0>, <function MathTestVariable.exp at 0x7f2cb9949bd0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('exp', exp, exp), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn5">test_tensor_general.py::test_one_derivative[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x7f2cb9949240>, <function MathTestVariable.explog at 0x7f2cb9949c60>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('explog', explog, explog), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn6">test_tensor_general.py::test_one_derivative[fast-fn6]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x7f2cb9948f70>, <function MathTestVariable.inv at 0x7f2cb9949990>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('inv', inv, inv), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn7">test_tensor_general.py::test_one_derivative[fast-fn7]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x7f2cb9949090>, <function MathTestVariable.log at 0x7f2cb9949ab0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('log', log, log), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn8">test_tensor_general.py::test_one_derivative[fast-fn8]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x7f2cb9948e50>, <function MathTest.multConstant at 0x7f2cb9948e50>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('multConstant', multConstant, multConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn9">test_tensor_general.py::test_one_derivative[fast-fn9]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x7f2cb9948b80>, <function MathTest.neg at 0x7f2cb9948b80>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('neg', neg, neg), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn10">test_tensor_general.py::test_one_derivative[fast-fn10]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x7f2cb9949120>, <function MathTestVariable.relu at 0x7f2cb9949b40>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('relu', relu, relu), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn11">test_tensor_general.py::test_one_derivative[fast-fn11]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x7f2cb9949000>, <function MathTestVariable.sig at 0x7f2cb9949a20>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('sig', sig, sig), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn12">test_tensor_general.py::test_one_derivative[fast-fn12]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x7f2cb9948ca0>, <function MathTest.square at 0x7f2cb9948ca0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('square', square, square), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn13">test_tensor_general.py::test_one_derivative[fast-fn13]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x7f2cb9948dc0>, <function MathTest.subConstant at 0x7f2cb9948dc0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('subConstant', subConstant, subConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn0">test_tensor_general.py::test_two_grad[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn1">test_tensor_general.py::test_two_grad[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn2">test_tensor_general.py::test_two_grad[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn3">test_tensor_general.py::test_two_grad[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn4">test_tensor_general.py::test_two_grad[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn5">test_tensor_general.py::test_two_grad[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn0">test_tensor_general.py::test_reduce[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn0]</pre></summary><pre>
fn = ('mean_full_red', <function MathTest.mean_full_red at 0x7f2cb9949750>, <function MathTestVariable.mean_full_red at 0x7f2cb9949e10>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...),
E           fn=('mean_full_red', mean_full_red, mean_full_red),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn1">test_tensor_general.py::test_reduce[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn1]</pre></summary><pre>
fn = ('mean_red', <function MathTest.mean_red at 0x7f2cb99496c0>, <function MathTestVariable.mean_red at 0x7f2cb9949d80>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...), fn=('mean_red', mean_red, mean_red), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn2">test_tensor_general.py::test_reduce[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn2]</pre></summary><pre>
fn = ('sum_red', <function MathTest.sum_red at 0x7f2cb9949630>, <function MathTestVariable.sum_red at 0x7f2cb9949cf0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...), fn=('sum_red', sum_red, sum_red), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn0">test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x7f2cb99492d0>, <function MathTest.add2 at 0x7f2cb99492d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn1">test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x7f2cb99493f0>, <function MathTest.div2 at 0x7f2cb99493f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn2">test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x7f2cb99495a0>, <function MathTestVariable.eq2 at 0x7f2cb9949ea0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn3">test_tensor_general.py::test_two_grad_broadcast[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x7f2cb9949480>, <function MathTestVariable.gt2 at 0x7f2cb9949f30>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn4">test_tensor_general.py::test_two_grad_broadcast[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x7f2cb9949510>, <function MathTestVariable.lt2 at 0x7f2cb9949fc0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn5">test_tensor_general.py::test_two_grad_broadcast[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x7f2cb9949360>, <function MathTest.mul2 at 0x7f2cb9949360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_permutefast">test_tensor_general.py::test_permute[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_permute[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:332: in test_permute
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_permute(
E           data=data(...), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_mm2">test_tensor_general.py::test_mm2</h3>
<details><summary> <pre>test_tensor_general.py::test_mm2</pre></summary><pre>
@pytest.mark.task3_2
    def test_mm2() -> None:
>       a = minitorch.rand((2, 3), backend=FastTensorBackend)

tests/test_tensor_general.py:343: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:308: in rand
    vals = [random.random() for _ in range(int(operators.prod(shape)))]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_bmmfast">test_tensor_general.py::test_bmm[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_bmm[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("backend", matmul_tests)

tests/test_tensor_general.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:370: in test_bmm
    a = data.draw(tensors(backend=shared[backend], shape=(D, A, B)))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 2, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_bmm(
E           data=data(...), backend='fast',
E       )
E       Draw 1: 2
E       Draw 2: 2
E       Draw 3: 2
E       Draw 4: 2

minitorch/operators.py:206: NotImplementedError
</pre>
</details>

<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/minitorch/autodiff.py b/minitorch/autodiff.py</span>
<span class="gh">index 39acbe8..06496e2 100644</span>
<span class="gd">--- a/minitorch/autodiff.py</span>
<span class="gi">+++ b/minitorch/autodiff.py</span>
<span class="gu">@@ -1,35 +1,57 @@</span>
<span class="w"> </span>from dataclasses import dataclass
<span class="w"> </span>from typing import Any, Iterable, List, Tuple
<span class="gi">+</span>
<span class="w"> </span>from typing_extensions import Protocol

<span class="gi">+# ## Task 1.1</span>
<span class="gi">+# Central Difference calculation</span>

<span class="gd">-def central_difference(f: Any, *vals: Any, arg: int=0, epsilon: float=1e-06</span>
<span class="gd">-    ) -&gt;Any:</span>
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -&gt; Any:</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="w"> </span>    Computes an approximation to the derivative of `f` with respect to one arg.

<span class="w"> </span>    See :doc:`derivative` or https://en.wikipedia.org/wiki/Finite_difference for more details.

<span class="w"> </span>    Args:
<span class="w"> </span>        f : arbitrary function from n-scalar args to one value
<span class="gd">-        *vals : n-float values $x_0 \\ldots x_{n-1}$</span>
<span class="gi">+        *vals : n-float values $x_0 \ldots x_{n-1}$</span>
<span class="w"> </span>        arg : the number $i$ of the arg to compute the derivative
<span class="w"> </span>        epsilon : a small constant

<span class="w"> </span>    Returns:
<span class="gd">-        An approximation of $f&#39;_i(x_0, \\ldots, x_{n-1})$</span>
<span class="gi">+        An approximation of $f&#39;_i(x_0, \ldots, x_{n-1})$</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 1.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 1.1&#39;)</span>


<span class="w"> </span>variable_count = 1


<span class="w"> </span>class Variable(Protocol):
<span class="gd">-    pass</span>
<span class="gi">+    def accumulate_derivative(self, x: Any) -&gt; None:</span>
<span class="gi">+        pass</span>

<span class="gi">+    @property</span>
<span class="gi">+    def unique_id(self) -&gt; int:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def is_leaf(self) -&gt; bool:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def is_constant(self) -&gt; bool:</span>
<span class="gi">+        pass</span>

<span class="gd">-def topological_sort(variable: Variable) -&gt;Iterable[Variable]:</span>
<span class="gi">+    @property</span>
<span class="gi">+    def parents(self) -&gt; Iterable[&quot;Variable&quot;]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    def chain_rule(self, d_output: Any) -&gt; Iterable[Tuple[&quot;Variable&quot;, Any]]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def topological_sort(variable: Variable) -&gt; Iterable[Variable]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Computes the topological order of the computation graph.

<span class="gu">@@ -39,10 +61,11 @@ def topological_sort(variable: Variable) -&gt;Iterable[Variable]:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Non-constant Variables in topological order starting from the right.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 1.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="gd">-def backpropagate(variable: Variable, deriv: Any) -&gt;None:</span>
<span class="gi">+def backpropagate(variable: Variable, deriv: Any) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Runs backpropagation on the computation graph in order to
<span class="w"> </span>    compute derivatives for the leave nodes.
<span class="gu">@@ -53,7 +76,8 @@ def backpropagate(variable: Variable, deriv: Any) -&gt;None:</span>

<span class="w"> </span>    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 1.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>@dataclass
<span class="gu">@@ -61,9 +85,16 @@ class Context:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Context class is used by `Function` to store information during the forward pass.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    no_grad: bool = False
<span class="w"> </span>    saved_values: Tuple[Any, ...] = ()

<span class="gd">-    def save_for_backward(self, *values: Any) -&gt;None:</span>
<span class="gd">-        &quot;&quot;&quot;Store the given `values` if they need to be used during backpropagation.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def save_for_backward(self, *values: Any) -&gt; None:</span>
<span class="gi">+        &quot;Store the given `values` if they need to be used during backpropagation.&quot;</span>
<span class="gi">+        if self.no_grad:</span>
<span class="gi">+            return</span>
<span class="gi">+        self.saved_values = values</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def saved_tensors(self) -&gt; Tuple[Any, ...]:</span>
<span class="gi">+        return self.saved_values</span>
<span class="gh">diff --git a/minitorch/cuda_ops.py b/minitorch/cuda_ops.py</span>
<span class="gh">index c1d7c5e..ac4cbae 100644</span>
<span class="gd">--- a/minitorch/cuda_ops.py</span>
<span class="gi">+++ b/minitorch/cuda_ops.py</span>
<span class="gu">@@ -1,12 +1,30 @@</span>
<span class="w"> </span>from typing import Callable, Optional
<span class="gi">+</span>
<span class="w"> </span>import numba
<span class="w"> </span>from numba import cuda
<span class="gi">+</span>
<span class="w"> </span>from .tensor import Tensor
<span class="gd">-from .tensor_data import MAX_DIMS, Shape, Storage, Strides, TensorData, broadcast_index, index_to_position, shape_broadcast, to_index</span>
<span class="gi">+from .tensor_data import (</span>
<span class="gi">+    MAX_DIMS,</span>
<span class="gi">+    Shape,</span>
<span class="gi">+    Storage,</span>
<span class="gi">+    Strides,</span>
<span class="gi">+    TensorData,</span>
<span class="gi">+    broadcast_index,</span>
<span class="gi">+    index_to_position,</span>
<span class="gi">+    shape_broadcast,</span>
<span class="gi">+    to_index,</span>
<span class="gi">+)</span>
<span class="w"> </span>from .tensor_ops import MapProto, TensorOps
<span class="gi">+</span>
<span class="gi">+# This code will CUDA compile fast versions your tensor_data functions.</span>
<span class="gi">+# If you get an error, read the docs for NUMBA as to what is allowed</span>
<span class="gi">+# in these functions.</span>
<span class="gi">+</span>
<span class="w"> </span>to_index = cuda.jit(device=True)(to_index)
<span class="w"> </span>index_to_position = cuda.jit(device=True)(index_to_position)
<span class="w"> </span>broadcast_index = cuda.jit(device=True)(broadcast_index)
<span class="gi">+</span>
<span class="w"> </span>THREADS_PER_BLOCK = 32


<span class="gu">@@ -14,13 +32,101 @@ class CudaOps(TensorOps):</span>
<span class="w"> </span>    cuda = True

<span class="w"> </span>    @staticmethod
<span class="gd">-    def map(fn: Callable[[float], float]) -&gt;MapProto:</span>
<span class="gd">-        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def map(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="gi">+        &quot;See `tensor_ops.py`&quot;</span>
<span class="gi">+        f = tensor_map(cuda.jit(device=True)(fn))</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+</span>
<span class="gi">+            # Instantiate and run the cuda kernel.</span>
<span class="gi">+            threadsperblock = THREADS_PER_BLOCK</span>
<span class="gi">+            blockspergrid = (out.size + THREADS_PER_BLOCK - 1) // THREADS_PER_BLOCK</span>
<span class="gi">+            f[blockspergrid, threadsperblock](*out.tuple(), out.size, *a.tuple())  # type: ignore</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def zip(fn: Callable[[float, float], float]) -&gt; Callable[[Tensor, Tensor], Tensor]:</span>
<span class="gi">+        f = tensor_zip(cuda.jit(device=True)(fn))</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+            c_shape = shape_broadcast(a.shape, b.shape)</span>
<span class="gi">+            out = a.zeros(c_shape)</span>
<span class="gi">+            threadsperblock = THREADS_PER_BLOCK</span>
<span class="gi">+            blockspergrid = (out.size + (threadsperblock - 1)) // threadsperblock</span>
<span class="gi">+            f[blockspergrid, threadsperblock](  # type: ignore</span>
<span class="gi">+                *out.tuple(), out.size, *a.tuple(), *b.tuple()</span>
<span class="gi">+            )</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def reduce(</span>
<span class="gi">+        fn: Callable[[float, float], float], start: float = 0.0</span>
<span class="gi">+    ) -&gt; Callable[[Tensor, int], Tensor]:</span>
<span class="gi">+        f = tensor_reduce(cuda.jit(device=True)(fn))</span>

<span class="gi">+        def ret(a: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+            out_shape = list(a.shape)</span>
<span class="gi">+            out_shape[dim] = (a.shape[dim] - 1) // 1024 + 1</span>
<span class="gi">+            out_a = a.zeros(tuple(out_shape))</span>

<span class="gd">-def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="gd">-    Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+            threadsperblock = 1024</span>
<span class="gi">+            blockspergrid = out_a.size</span>
<span class="gi">+            f[blockspergrid, threadsperblock](  # type: ignore</span>
<span class="gi">+                *out_a.tuple(), out_a.size, *a.tuple(), dim, start</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            return out_a</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def matrix_multiply(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # Make these always be a 3 dimensional multiply</span>
<span class="gi">+        both_2d = 0</span>
<span class="gi">+        if len(a.shape) == 2:</span>
<span class="gi">+            a = a.contiguous().view(1, a.shape[0], a.shape[1])</span>
<span class="gi">+            both_2d += 1</span>
<span class="gi">+        if len(b.shape) == 2:</span>
<span class="gi">+            b = b.contiguous().view(1, b.shape[0], b.shape[1])</span>
<span class="gi">+            both_2d += 1</span>
<span class="gi">+        both_2d = both_2d == 2</span>
<span class="gi">+</span>
<span class="gi">+        ls = list(shape_broadcast(a.shape[:-2], b.shape[:-2]))</span>
<span class="gi">+        ls.append(a.shape[-2])</span>
<span class="gi">+        ls.append(b.shape[-1])</span>
<span class="gi">+        assert a.shape[-1] == b.shape[-2]</span>
<span class="gi">+        out = a.zeros(tuple(ls))</span>
<span class="gi">+</span>
<span class="gi">+        # One block per batch, extra rows, extra col</span>
<span class="gi">+        blockspergrid = (</span>
<span class="gi">+            (out.shape[1] + (THREADS_PER_BLOCK - 1)) // THREADS_PER_BLOCK,</span>
<span class="gi">+            (out.shape[2] + (THREADS_PER_BLOCK - 1)) // THREADS_PER_BLOCK,</span>
<span class="gi">+            out.shape[0],</span>
<span class="gi">+        )</span>
<span class="gi">+        threadsperblock = (THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1)</span>
<span class="gi">+</span>
<span class="gi">+        tensor_matrix_multiply[blockspergrid, threadsperblock](</span>
<span class="gi">+            *out.tuple(), out.size, *a.tuple(), *b.tuple()</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Undo 3d if we added it.</span>
<span class="gi">+        if both_2d:</span>
<span class="gi">+            out = out.view(out.shape[1], out.shape[2])</span>
<span class="gi">+        return out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Implement</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_map(</span>
<span class="gi">+    fn: Callable[[float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    CUDA higher-order tensor map function. ::

<span class="gu">@@ -33,11 +139,31 @@ def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor map function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gd">-</span>
<span class="gd">-def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+    def _map(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        out_size: int,</span>
<span class="gi">+        in_storage: Storage,</span>
<span class="gi">+        in_shape: Shape,</span>
<span class="gi">+        in_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+</span>
<span class="gi">+        out_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        in_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x</span>
<span class="gi">+        # TODO: Implement for Task 3.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return cuda.jit()(_map)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_zip(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[</span>
<span class="gi">+    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None</span>
<span class="gi">+]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    CUDA higher-order tensor zipWith (or map2) function ::

<span class="gu">@@ -50,14 +176,36 @@ def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor zip function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    def _zip(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        out_size: int,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        b_storage: Storage,</span>
<span class="gi">+        b_shape: Shape,</span>
<span class="gi">+        b_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+</span>
<span class="gi">+        out_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        a_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        b_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x</span>
<span class="gi">+</span>
<span class="gi">+        # TODO: Implement for Task 3.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.3&#39;)</span>

<span class="gd">-def _sum_practice(out: Storage, a: Storage, size: int) -&gt;None:</span>
<span class="gi">+    return cuda.jit()(_zip)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _sum_practice(out: Storage, a: Storage, size: int) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    This is a practice sum kernel to prepare for reduce.

<span class="gd">-    Given an array of length $n$ and out of size $n //     ext{blockDIM}$</span>
<span class="gi">+    Given an array of length $n$ and out of size $n // \text{blockDIM}$</span>
<span class="w"> </span>    it should sum up each blockDim values into an out cell.

<span class="w"> </span>    $[a_1, a_2, ..., a_{100}]$
<span class="gu">@@ -74,14 +222,34 @@ def _sum_practice(out: Storage, a: Storage, size: int) -&gt;None:</span>
<span class="w"> </span>        size (int):  length of a.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    BLOCK_DIM = 32</span>
<span class="gi">+</span>
<span class="gi">+    cache = cuda.shared.array(BLOCK_DIM, numba.float64)</span>
<span class="gi">+    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x</span>
<span class="gi">+    pos = cuda.threadIdx.x</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: Implement for Task 3.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 3.3&#39;)</span>


<span class="w"> </span>jit_sum_practice = cuda.jit()(_sum_practice)


<span class="gd">-def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="gi">+def sum_practice(a: Tensor) -&gt; TensorData:</span>
<span class="gi">+    (size,) = a.shape</span>
<span class="gi">+    threadsperblock = THREADS_PER_BLOCK</span>
<span class="gi">+    blockspergrid = (size // THREADS_PER_BLOCK) + 1</span>
<span class="gi">+    out = TensorData([0.0 for i in range(2)], (2,))</span>
<span class="gi">+    out.to_cuda_()</span>
<span class="gi">+    jit_sum_practice[blockspergrid, threadsperblock](</span>
<span class="gi">+        out.tuple()[0], a._tensor._storage, size</span>
<span class="gi">+    )</span>
<span class="gi">+    return out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_reduce(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    CUDA higher-order tensor reduce function.

<span class="gu">@@ -92,10 +260,31 @@ def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>        Tensor reduce function.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-def _mm_practice(out: Storage, a: Storage, b: Storage, size: int) -&gt;None:</span>
<span class="gi">+    def _reduce(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        out_size: int,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        reduce_dim: int,</span>
<span class="gi">+        reduce_value: float,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        BLOCK_DIM = 1024</span>
<span class="gi">+        cache = cuda.shared.array(BLOCK_DIM, numba.float64)</span>
<span class="gi">+        out_index = cuda.local.array(MAX_DIMS, numba.int32)</span>
<span class="gi">+        out_pos = cuda.blockIdx.x</span>
<span class="gi">+        pos = cuda.threadIdx.x</span>
<span class="gi">+</span>
<span class="gi">+        # TODO: Implement for Task 3.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return cuda.jit()(_reduce)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _mm_practice(out: Storage, a: Storage, b: Storage, size: int) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    This is a practice square MM kernel to prepare for matmul.

<span class="gu">@@ -125,15 +314,38 @@ def _mm_practice(out: Storage, a: Storage, b: Storage, size: int) -&gt;None:</span>
<span class="w"> </span>        b (Storage): storage for `b` tensor.
<span class="w"> </span>        size (int): size of the square
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    BLOCK_DIM = 32</span>
<span class="gi">+    # TODO: Implement for Task 3.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 3.3&#39;)</span>


<span class="w"> </span>jit_mm_practice = cuda.jit()(_mm_practice)


<span class="gd">-def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides:</span>
<span class="gd">-    Strides, out_size: int, a_storage: Storage, a_shape: Shape, a_strides:</span>
<span class="gd">-    Strides, b_storage: Storage, b_shape: Shape, b_strides: Strides) -&gt;None:</span>
<span class="gi">+def mm_practice(a: Tensor, b: Tensor) -&gt; TensorData:</span>
<span class="gi">+    (size, _) = a.shape</span>
<span class="gi">+    threadsperblock = (THREADS_PER_BLOCK, THREADS_PER_BLOCK)</span>
<span class="gi">+    blockspergrid = 1</span>
<span class="gi">+    out = TensorData([0.0 for i in range(size * size)], (size, size))</span>
<span class="gi">+    out.to_cuda_()</span>
<span class="gi">+    jit_mm_practice[blockspergrid, threadsperblock](</span>
<span class="gi">+        out.tuple()[0], a._tensor._storage, b._tensor._storage, size</span>
<span class="gi">+    )</span>
<span class="gi">+    return out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _tensor_matrix_multiply(</span>
<span class="gi">+    out: Storage,</span>
<span class="gi">+    out_shape: Shape,</span>
<span class="gi">+    out_strides: Strides,</span>
<span class="gi">+    out_size: int,</span>
<span class="gi">+    a_storage: Storage,</span>
<span class="gi">+    a_shape: Shape,</span>
<span class="gi">+    a_strides: Strides,</span>
<span class="gi">+    b_storage: Storage,</span>
<span class="gi">+    b_shape: Shape,</span>
<span class="gi">+    b_strides: Strides,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    CUDA tensor matrix multiply function.

<span class="gu">@@ -151,7 +363,30 @@ def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        None : Fills in `out`
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    a_batch_stride = a_strides[0] if a_shape[0] &gt; 1 else 0</span>
<span class="gi">+    b_batch_stride = b_strides[0] if b_shape[0] &gt; 1 else 0</span>
<span class="gi">+    # Batch dimension - fixed</span>
<span class="gi">+    batch = cuda.blockIdx.z</span>
<span class="gi">+</span>
<span class="gi">+    BLOCK_DIM = 32</span>
<span class="gi">+    a_shared = cuda.shared.array((BLOCK_DIM, BLOCK_DIM), numba.float64)</span>
<span class="gi">+    b_shared = cuda.shared.array((BLOCK_DIM, BLOCK_DIM), numba.float64)</span>
<span class="gi">+</span>
<span class="gi">+    # The final position c[i, j]</span>
<span class="gi">+    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x</span>
<span class="gi">+    j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y</span>
<span class="gi">+</span>
<span class="gi">+    # The local position in the block.</span>
<span class="gi">+    pi = cuda.threadIdx.x</span>
<span class="gi">+    pj = cuda.threadIdx.y</span>
<span class="gi">+</span>
<span class="gi">+    # Code Plan:</span>
<span class="gi">+    # 1) Move across shared dimension by block dim.</span>
<span class="gi">+    #    a) Copy into shared memory for a matrix.</span>
<span class="gi">+    #    b) Copy into shared memory for b matrix</span>
<span class="gi">+    #    c) Compute the dot produce for position c[i, j]</span>
<span class="gi">+    # TODO: Implement for Task 3.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 3.4&#39;)</span>


<span class="w"> </span>tensor_matrix_multiply = cuda.jit(_tensor_matrix_multiply)
<span class="gh">diff --git a/minitorch/datasets.py b/minitorch/datasets.py</span>
<span class="gh">index 0547421..410dfa8 100644</span>
<span class="gd">--- a/minitorch/datasets.py</span>
<span class="gi">+++ b/minitorch/datasets.py</span>
<span class="gu">@@ -4,6 +4,15 @@ from dataclasses import dataclass</span>
<span class="w"> </span>from typing import List, Tuple


<span class="gi">+def make_pts(N: int) -&gt; List[Tuple[float, float]]:</span>
<span class="gi">+    X = []</span>
<span class="gi">+    for i in range(N):</span>
<span class="gi">+        x_1 = random.random()</span>
<span class="gi">+        x_2 = random.random()</span>
<span class="gi">+        X.append((x_1, x_2))</span>
<span class="gi">+    return X</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>@dataclass
<span class="w"> </span>class Graph:
<span class="w"> </span>    N: int
<span class="gu">@@ -11,5 +20,76 @@ class Graph:</span>
<span class="w"> </span>    y: List[int]


<span class="gd">-datasets = {&#39;Simple&#39;: simple, &#39;Diag&#39;: diag, &#39;Split&#39;: split, &#39;Xor&#39;: xor,</span>
<span class="gd">-    &#39;Circle&#39;: circle, &#39;Spiral&#39;: spiral}</span>
<span class="gi">+def simple(N: int) -&gt; Graph:</span>
<span class="gi">+    X = make_pts(N)</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for x_1, x_2 in X:</span>
<span class="gi">+        y1 = 1 if x_1 &lt; 0.5 else 0</span>
<span class="gi">+        y.append(y1)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def diag(N: int) -&gt; Graph:</span>
<span class="gi">+    X = make_pts(N)</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for x_1, x_2 in X:</span>
<span class="gi">+        y1 = 1 if x_1 + x_2 &lt; 0.5 else 0</span>
<span class="gi">+        y.append(y1)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def split(N: int) -&gt; Graph:</span>
<span class="gi">+    X = make_pts(N)</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for x_1, x_2 in X:</span>
<span class="gi">+        y1 = 1 if x_1 &lt; 0.2 or x_1 &gt; 0.8 else 0</span>
<span class="gi">+        y.append(y1)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def xor(N: int) -&gt; Graph:</span>
<span class="gi">+    X = make_pts(N)</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for x_1, x_2 in X:</span>
<span class="gi">+        y1 = 1 if ((x_1 &lt; 0.5 and x_2 &gt; 0.5) or (x_1 &gt; 0.5 and x_2 &lt; 0.5)) else 0</span>
<span class="gi">+        y.append(y1)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def circle(N: int) -&gt; Graph:</span>
<span class="gi">+    X = make_pts(N)</span>
<span class="gi">+    y = []</span>
<span class="gi">+    for x_1, x_2 in X:</span>
<span class="gi">+        x1, x2 = (x_1 - 0.5, x_2 - 0.5)</span>
<span class="gi">+        y1 = 1 if x1 * x1 + x2 * x2 &gt; 0.1 else 0</span>
<span class="gi">+        y.append(y1)</span>
<span class="gi">+    return Graph(N, X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def spiral(N: int) -&gt; Graph:</span>
<span class="gi">+    def x(t: float) -&gt; float:</span>
<span class="gi">+        return t * math.cos(t) / 20.0</span>
<span class="gi">+</span>
<span class="gi">+    def y(t: float) -&gt; float:</span>
<span class="gi">+        return t * math.sin(t) / 20.0</span>
<span class="gi">+</span>
<span class="gi">+    X = [</span>
<span class="gi">+        (x(10.0 * (float(i) / (N // 2))) + 0.5, y(10.0 * (float(i) / (N // 2))) + 0.5)</span>
<span class="gi">+        for i in range(5 + 0, 5 + N // 2)</span>
<span class="gi">+    ]</span>
<span class="gi">+    X = X + [</span>
<span class="gi">+        (y(-10.0 * (float(i) / (N // 2))) + 0.5, x(-10.0 * (float(i) / (N // 2))) + 0.5)</span>
<span class="gi">+        for i in range(5 + 0, 5 + N // 2)</span>
<span class="gi">+    ]</span>
<span class="gi">+    y2 = [0] * (N // 2) + [1] * (N // 2)</span>
<span class="gi">+    return Graph(N, X, y2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+datasets = {</span>
<span class="gi">+    &quot;Simple&quot;: simple,</span>
<span class="gi">+    &quot;Diag&quot;: diag,</span>
<span class="gi">+    &quot;Split&quot;: split,</span>
<span class="gi">+    &quot;Xor&quot;: xor,</span>
<span class="gi">+    &quot;Circle&quot;: circle,</span>
<span class="gi">+    &quot;Spiral&quot;: spiral,</span>
<span class="gi">+}</span>
<span class="gh">diff --git a/minitorch/fast_conv.py b/minitorch/fast_conv.py</span>
<span class="gh">index 14f13fc..ce4244c 100644</span>
<span class="gd">--- a/minitorch/fast_conv.py</span>
<span class="gi">+++ b/minitorch/fast_conv.py</span>
<span class="gu">@@ -1,19 +1,42 @@</span>
<span class="w"> </span>from typing import Tuple
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from numba import njit, prange
<span class="gi">+</span>
<span class="w"> </span>from .autodiff import Context
<span class="w"> </span>from .tensor import Tensor
<span class="gd">-from .tensor_data import MAX_DIMS, Index, Shape, Strides, broadcast_index, index_to_position, to_index</span>
<span class="gi">+from .tensor_data import (</span>
<span class="gi">+    MAX_DIMS,</span>
<span class="gi">+    Index,</span>
<span class="gi">+    Shape,</span>
<span class="gi">+    Strides,</span>
<span class="gi">+    broadcast_index,</span>
<span class="gi">+    index_to_position,</span>
<span class="gi">+    to_index,</span>
<span class="gi">+)</span>
<span class="w"> </span>from .tensor_functions import Function
<span class="gd">-to_index = njit(inline=&#39;always&#39;)(to_index)</span>
<span class="gd">-index_to_position = njit(inline=&#39;always&#39;)(index_to_position)</span>
<span class="gd">-broadcast_index = njit(inline=&#39;always&#39;)(broadcast_index)</span>

<span class="gd">-</span>
<span class="gd">-def _tensor_conv1d(out: Tensor, out_shape: Shape, out_strides: Strides,</span>
<span class="gd">-    out_size: int, input: Tensor, input_shape: Shape, input_strides:</span>
<span class="gd">-    Strides, weight: Tensor, weight_shape: Shape, weight_strides: Strides,</span>
<span class="gd">-    reverse: bool) -&gt;None:</span>
<span class="gi">+# This code will JIT compile fast versions your tensor_data functions.</span>
<span class="gi">+# If you get an error, read the docs for NUMBA as to what is allowed</span>
<span class="gi">+# in these functions.</span>
<span class="gi">+to_index = njit(inline=&quot;always&quot;)(to_index)</span>
<span class="gi">+index_to_position = njit(inline=&quot;always&quot;)(index_to_position)</span>
<span class="gi">+broadcast_index = njit(inline=&quot;always&quot;)(broadcast_index)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _tensor_conv1d(</span>
<span class="gi">+    out: Tensor,</span>
<span class="gi">+    out_shape: Shape,</span>
<span class="gi">+    out_strides: Strides,</span>
<span class="gi">+    out_size: int,</span>
<span class="gi">+    input: Tensor,</span>
<span class="gi">+    input_shape: Shape,</span>
<span class="gi">+    input_strides: Strides,</span>
<span class="gi">+    weight: Tensor,</span>
<span class="gi">+    weight_shape: Shape,</span>
<span class="gi">+    weight_strides: Strides,</span>
<span class="gi">+    reverse: bool,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    1D Convolution implementation.

<span class="gu">@@ -45,16 +68,28 @@ def _tensor_conv1d(out: Tensor, out_shape: Shape, out_strides: Strides,</span>
<span class="w"> </span>        weight_strides (Strides): strides for `input` tensor.
<span class="w"> </span>        reverse (bool): anchor weight at left or right
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch_, out_channels, out_width = out_shape</span>
<span class="gi">+    batch, in_channels, width = input_shape</span>
<span class="gi">+    out_channels_, in_channels_, kw = weight_shape</span>
<span class="gi">+</span>
<span class="gi">+    assert (</span>
<span class="gi">+        batch == batch_</span>
<span class="gi">+        and in_channels == in_channels_</span>
<span class="gi">+        and out_channels == out_channels_</span>
<span class="gi">+    )</span>
<span class="gi">+    s1 = input_strides</span>
<span class="gi">+    s2 = weight_strides</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: Implement for Task 4.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.1&#39;)</span>


<span class="w"> </span>tensor_conv1d = njit(parallel=True)(_tensor_conv1d)


<span class="w"> </span>class Conv1dFun(Function):
<span class="gd">-</span>
<span class="w"> </span>    @staticmethod
<span class="gd">-    def forward(ctx: Context, input: Tensor, weight: Tensor) -&gt;Tensor:</span>
<span class="gi">+    def forward(ctx: Context, input: Tensor, weight: Tensor) -&gt; Tensor:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Compute a 1D Convolution

<span class="gu">@@ -66,16 +101,63 @@ class Conv1dFun(Function):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            batch x out_channel x h x w
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ctx.save_for_backward(input, weight)</span>
<span class="gi">+        batch, in_channels, w = input.shape</span>
<span class="gi">+        out_channels, in_channels2, kw = weight.shape</span>
<span class="gi">+        assert in_channels == in_channels2</span>
<span class="gi">+</span>
<span class="gi">+        # Run convolution</span>
<span class="gi">+        output = input.zeros((batch, out_channels, w))</span>
<span class="gi">+        tensor_conv1d(</span>
<span class="gi">+            *output.tuple(), output.size, *input.tuple(), *weight.tuple(), False</span>
<span class="gi">+        )</span>
<span class="gi">+        return output</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        input, weight = ctx.saved_values</span>
<span class="gi">+        batch, in_channels, w = input.shape</span>
<span class="gi">+        out_channels, in_channels, kw = weight.shape</span>
<span class="gi">+        grad_weight = grad_output.zeros((in_channels, out_channels, kw))</span>
<span class="gi">+        new_input = input.permute(1, 0, 2)</span>
<span class="gi">+        new_grad_output = grad_output.permute(1, 0, 2)</span>
<span class="gi">+        tensor_conv1d(</span>
<span class="gi">+            *grad_weight.tuple(),</span>
<span class="gi">+            grad_weight.size,</span>
<span class="gi">+            *new_input.tuple(),</span>
<span class="gi">+            *new_grad_output.tuple(),</span>
<span class="gi">+            False,</span>
<span class="gi">+        )</span>
<span class="gi">+        grad_weight = grad_weight.permute(1, 0, 2)</span>
<span class="gi">+</span>
<span class="gi">+        grad_input = input.zeros((batch, in_channels, w))</span>
<span class="gi">+        new_weight = weight.permute(1, 0, 2)</span>
<span class="gi">+        tensor_conv1d(</span>
<span class="gi">+            *grad_input.tuple(),</span>
<span class="gi">+            grad_input.size,</span>
<span class="gi">+            *grad_output.tuple(),</span>
<span class="gi">+            *new_weight.tuple(),</span>
<span class="gi">+            True,</span>
<span class="gi">+        )</span>
<span class="gi">+        return grad_input, grad_weight</span>


<span class="w"> </span>conv1d = Conv1dFun.apply


<span class="gd">-def _tensor_conv2d(out: Tensor, out_shape: Shape, out_strides: Strides,</span>
<span class="gd">-    out_size: int, input: Tensor, input_shape: Shape, input_strides:</span>
<span class="gd">-    Strides, weight: Tensor, weight_shape: Shape, weight_strides: Strides,</span>
<span class="gd">-    reverse: bool) -&gt;None:</span>
<span class="gi">+def _tensor_conv2d(</span>
<span class="gi">+    out: Tensor,</span>
<span class="gi">+    out_shape: Shape,</span>
<span class="gi">+    out_strides: Strides,</span>
<span class="gi">+    out_size: int,</span>
<span class="gi">+    input: Tensor,</span>
<span class="gi">+    input_shape: Shape,</span>
<span class="gi">+    input_strides: Strides,</span>
<span class="gi">+    weight: Tensor,</span>
<span class="gi">+    weight_shape: Shape,</span>
<span class="gi">+    weight_strides: Strides,</span>
<span class="gi">+    reverse: bool,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    2D Convolution implementation.

<span class="gu">@@ -108,16 +190,32 @@ def _tensor_conv2d(out: Tensor, out_shape: Shape, out_strides: Strides,</span>
<span class="w"> </span>        weight_strides (Strides): strides for `input` tensor.
<span class="w"> </span>        reverse (bool): anchor weight at top-left or bottom-right
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch_, out_channels, _, _ = out_shape</span>
<span class="gi">+    batch, in_channels, height, width = input_shape</span>
<span class="gi">+    out_channels_, in_channels_, kh, kw = weight_shape</span>
<span class="gi">+</span>
<span class="gi">+    assert (</span>
<span class="gi">+        batch == batch_</span>
<span class="gi">+        and in_channels == in_channels_</span>
<span class="gi">+        and out_channels == out_channels_</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    s1 = input_strides</span>
<span class="gi">+    s2 = weight_strides</span>
<span class="gi">+    # inners</span>
<span class="gi">+    s10, s11, s12, s13 = s1[0], s1[1], s1[2], s1[3]</span>
<span class="gi">+    s20, s21, s22, s23 = s2[0], s2[1], s2[2], s2[3]</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: Implement for Task 4.2.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.2&#39;)</span>


<span class="w"> </span>tensor_conv2d = njit(parallel=True, fastmath=True)(_tensor_conv2d)


<span class="w"> </span>class Conv2dFun(Function):
<span class="gd">-</span>
<span class="w"> </span>    @staticmethod
<span class="gd">-    def forward(ctx: Context, input: Tensor, weight: Tensor) -&gt;Tensor:</span>
<span class="gi">+    def forward(ctx: Context, input: Tensor, weight: Tensor) -&gt; Tensor:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Compute a 2D Convolution

<span class="gu">@@ -129,7 +227,44 @@ class Conv2dFun(Function):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            (:class:`Tensor`) : batch x out_channel x h x w
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        ctx.save_for_backward(input, weight)</span>
<span class="gi">+        batch, in_channels, h, w = input.shape</span>
<span class="gi">+        out_channels, in_channels2, kh, kw = weight.shape</span>
<span class="gi">+        assert in_channels == in_channels2</span>
<span class="gi">+        output = input.zeros((batch, out_channels, h, w))</span>
<span class="gi">+        tensor_conv2d(</span>
<span class="gi">+            *output.tuple(), output.size, *input.tuple(), *weight.tuple(), False</span>
<span class="gi">+        )</span>
<span class="gi">+        return output</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        input, weight = ctx.saved_values</span>
<span class="gi">+        batch, in_channels, h, w = input.shape</span>
<span class="gi">+        out_channels, in_channels, kh, kw = weight.shape</span>
<span class="gi">+</span>
<span class="gi">+        grad_weight = grad_output.zeros((in_channels, out_channels, kh, kw))</span>
<span class="gi">+        new_input = input.permute(1, 0, 2, 3)</span>
<span class="gi">+        new_grad_output = grad_output.permute(1, 0, 2, 3)</span>
<span class="gi">+        tensor_conv2d(</span>
<span class="gi">+            *grad_weight.tuple(),</span>
<span class="gi">+            grad_weight.size,</span>
<span class="gi">+            *new_input.tuple(),</span>
<span class="gi">+            *new_grad_output.tuple(),</span>
<span class="gi">+            False,</span>
<span class="gi">+        )</span>
<span class="gi">+        grad_weight = grad_weight.permute(1, 0, 2, 3)</span>
<span class="gi">+</span>
<span class="gi">+        grad_input = input.zeros((batch, in_channels, h, w))</span>
<span class="gi">+        new_weight = weight.permute(1, 0, 2, 3)</span>
<span class="gi">+        tensor_conv2d(</span>
<span class="gi">+            *grad_input.tuple(),</span>
<span class="gi">+            grad_input.size,</span>
<span class="gi">+            *grad_output.tuple(),</span>
<span class="gi">+            *new_weight.tuple(),</span>
<span class="gi">+            True,</span>
<span class="gi">+        )</span>
<span class="gi">+        return grad_input, grad_weight</span>


<span class="w"> </span>conv2d = Conv2dFun.apply
<span class="gh">diff --git a/minitorch/fast_ops.py b/minitorch/fast_ops.py</span>
<span class="gh">index 71f96b7..dc73b86 100644</span>
<span class="gd">--- a/minitorch/fast_ops.py</span>
<span class="gi">+++ b/minitorch/fast_ops.py</span>
<span class="gu">@@ -1,39 +1,87 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from typing import TYPE_CHECKING
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from numba import njit, prange
<span class="gd">-from .tensor_data import MAX_DIMS, broadcast_index, index_to_position, shape_broadcast, to_index</span>
<span class="gi">+</span>
<span class="gi">+from .tensor_data import (</span>
<span class="gi">+    MAX_DIMS,</span>
<span class="gi">+    broadcast_index,</span>
<span class="gi">+    index_to_position,</span>
<span class="gi">+    shape_broadcast,</span>
<span class="gi">+    to_index,</span>
<span class="gi">+)</span>
<span class="w"> </span>from .tensor_ops import MapProto, TensorOps
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from typing import Callable, Optional
<span class="gi">+</span>
<span class="w"> </span>    from .tensor import Tensor
<span class="w"> </span>    from .tensor_data import Index, Shape, Storage, Strides
<span class="gd">-to_index = njit(inline=&#39;always&#39;)(to_index)</span>
<span class="gd">-index_to_position = njit(inline=&#39;always&#39;)(index_to_position)</span>
<span class="gd">-broadcast_index = njit(inline=&#39;always&#39;)(broadcast_index)</span>

<span class="gi">+# TIP: Use `NUMBA_DISABLE_JIT=1 pytest tests/ -m task3_1` to run these tests without JIT.</span>
<span class="gi">+</span>
<span class="gi">+# This code will JIT compile fast versions your tensor_data functions.</span>
<span class="gi">+# If you get an error, read the docs for NUMBA as to what is allowed</span>
<span class="gi">+# in these functions.</span>
<span class="gi">+to_index = njit(inline=&quot;always&quot;)(to_index)</span>
<span class="gi">+index_to_position = njit(inline=&quot;always&quot;)(index_to_position)</span>
<span class="gi">+broadcast_index = njit(inline=&quot;always&quot;)(broadcast_index)</span>

<span class="gd">-class FastOps(TensorOps):</span>

<span class="gi">+class FastOps(TensorOps):</span>
<span class="w"> </span>    @staticmethod
<span class="gd">-    def map(fn: Callable[[float], float]) -&gt;MapProto:</span>
<span class="gd">-        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def map(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="gi">+        &quot;See `tensor_ops.py`&quot;</span>
<span class="gi">+</span>
<span class="gi">+        # This line JIT compiles your tensor_map</span>
<span class="gi">+        f = tensor_map(njit()(fn))</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            f(*out.tuple(), *a.tuple())</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def zip(fn: Callable[[float, float], float]) -&gt;Callable[[Tensor, Tensor</span>
<span class="gd">-        ], Tensor]:</span>
<span class="gd">-        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def zip(fn: Callable[[float, float], float]) -&gt; Callable[[Tensor, Tensor], Tensor]:</span>
<span class="gi">+        &quot;See `tensor_ops.py`&quot;</span>
<span class="gi">+</span>
<span class="gi">+        f = tensor_zip(njit()(fn))</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+            c_shape = shape_broadcast(a.shape, b.shape)</span>
<span class="gi">+            out = a.zeros(c_shape)</span>
<span class="gi">+            f(*out.tuple(), *a.tuple(), *b.tuple())</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def reduce(fn: Callable[[float, float], float], start: float=0.0</span>
<span class="gd">-        ) -&gt;Callable[[Tensor, int], Tensor]:</span>
<span class="gd">-        &quot;&quot;&quot;See `tensor_ops.py`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def reduce(</span>
<span class="gi">+        fn: Callable[[float, float], float], start: float = 0.0</span>
<span class="gi">+    ) -&gt; Callable[[Tensor, int], Tensor]:</span>
<span class="gi">+        &quot;See `tensor_ops.py`&quot;</span>
<span class="gi">+        f = tensor_reduce(njit()(fn))</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+            out_shape = list(a.shape)</span>
<span class="gi">+            out_shape[dim] = 1</span>
<span class="gi">+</span>
<span class="gi">+            # Other values when not sum.</span>
<span class="gi">+            out = a.zeros(tuple(out_shape))</span>
<span class="gi">+            out._tensor._storage[:] = start</span>
<span class="gi">+</span>
<span class="gi">+            f(*out.tuple(), *a.tuple(), dim)</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def matrix_multiply(a: Tensor, b: Tensor) -&gt;Tensor:</span>
<span class="gi">+    def matrix_multiply(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Batched tensor matrix multiply ::

<span class="gu">@@ -56,11 +104,37 @@ class FastOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            New tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="gi">+        # Make these always be a 3 dimensional multiply</span>
<span class="gi">+        both_2d = 0</span>
<span class="gi">+        if len(a.shape) == 2:</span>
<span class="gi">+            a = a.contiguous().view(1, a.shape[0], a.shape[1])</span>
<span class="gi">+            both_2d += 1</span>
<span class="gi">+        if len(b.shape) == 2:</span>
<span class="gi">+            b = b.contiguous().view(1, b.shape[0], b.shape[1])</span>
<span class="gi">+            both_2d += 1</span>
<span class="gi">+        both_2d = both_2d == 2</span>
<span class="gi">+</span>
<span class="gi">+        ls = list(shape_broadcast(a.shape[:-2], b.shape[:-2]))</span>
<span class="gi">+        ls.append(a.shape[-2])</span>
<span class="gi">+        ls.append(b.shape[-1])</span>
<span class="gi">+        assert a.shape[-1] == b.shape[-2]</span>
<span class="gi">+        out = a.zeros(tuple(ls))</span>
<span class="gi">+</span>
<span class="gi">+        tensor_matrix_multiply(*out.tuple(), *a.tuple(), *b.tuple())</span>
<span class="gi">+</span>
<span class="gi">+        # Undo 3d if we added it.</span>
<span class="gi">+        if both_2d:</span>
<span class="gi">+            out = out.view(out.shape[1], out.shape[2])</span>
<span class="gi">+        return out</span>

<span class="gd">-def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="gd">-    Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+</span>
<span class="gi">+# Implementations</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_map(</span>
<span class="gi">+    fn: Callable[[float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    NUMBA low_level tensor_map function. See `tensor_ops.py` for description.

<span class="gu">@@ -76,11 +150,26 @@ def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor map function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+    def _map(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        in_storage: Storage,</span>
<span class="gi">+        in_shape: Shape,</span>
<span class="gi">+        in_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 3.1.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.1&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return njit(parallel=True)(_map)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_zip(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[</span>
<span class="gi">+    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None</span>
<span class="gi">+]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    NUMBA higher-order tensor zip function. See `tensor_ops.py` for description.

<span class="gu">@@ -97,11 +186,27 @@ def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor zip function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gd">-</span>
<span class="gd">-def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="gi">+    def _zip(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        b_storage: Storage,</span>
<span class="gi">+        b_shape: Shape,</span>
<span class="gi">+        b_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 3.1.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.1&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return njit(parallel=True)(_zip)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_reduce(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    NUMBA higher-order tensor reduce function. See `tensor_ops.py` for description.

<span class="gu">@@ -117,12 +222,33 @@ def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor reduce function
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides:</span>
<span class="gd">-    Strides, a_storage: Storage, a_shape: Shape, a_strides: Strides,</span>
<span class="gd">-    b_storage: Storage, b_shape: Shape, b_strides: Strides) -&gt;None:</span>
<span class="gi">+    def _reduce(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        reduce_dim: int,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 3.1.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 3.1&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return njit(parallel=True)(_reduce)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _tensor_matrix_multiply(</span>
<span class="gi">+    out: Storage,</span>
<span class="gi">+    out_shape: Shape,</span>
<span class="gi">+    out_strides: Strides,</span>
<span class="gi">+    a_storage: Storage,</span>
<span class="gi">+    a_shape: Shape,</span>
<span class="gi">+    a_strides: Strides,</span>
<span class="gi">+    b_storage: Storage,</span>
<span class="gi">+    b_shape: Shape,</span>
<span class="gi">+    b_strides: Strides,</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    NUMBA tensor matrix multiply function.

<span class="gu">@@ -153,8 +279,11 @@ def _tensor_matrix_multiply(out: Storage, out_shape: Shape, out_strides:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        None : Fills in `out`
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    a_batch_stride = a_strides[0] if a_shape[0] &gt; 1 else 0</span>
<span class="gi">+    b_batch_stride = b_strides[0] if b_shape[0] &gt; 1 else 0</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: Implement for Task 3.2.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 3.2&#39;)</span>


<span class="gd">-tensor_matrix_multiply = njit(parallel=True, fastmath=True)(</span>
<span class="gd">-    _tensor_matrix_multiply)</span>
<span class="gi">+tensor_matrix_multiply = njit(parallel=True, fastmath=True)(_tensor_matrix_multiply)</span>
<span class="gh">diff --git a/minitorch/module.py b/minitorch/module.py</span>
<span class="gh">index 91f0dc2..d32c609 100644</span>
<span class="gd">--- a/minitorch/module.py</span>
<span class="gi">+++ b/minitorch/module.py</span>
<span class="gu">@@ -1,4 +1,5 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from typing import Any, Dict, Optional, Sequence, Tuple


<span class="gu">@@ -13,28 +14,32 @@ class Module:</span>
<span class="w"> </span>        training : Whether the module is in training mode or evaluation mode

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    _modules: Dict[str, Module]
<span class="w"> </span>    _parameters: Dict[str, Parameter]
<span class="w"> </span>    training: bool

<span class="gd">-    def __init__(self) -&gt;None:</span>
<span class="gi">+    def __init__(self) -&gt; None:</span>
<span class="w"> </span>        self._modules = {}
<span class="w"> </span>        self._parameters = {}
<span class="w"> </span>        self.training = True

<span class="gd">-    def modules(self) -&gt;Sequence[Module]:</span>
<span class="gd">-        &quot;&quot;&quot;Return the direct child modules of this module.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def modules(self) -&gt; Sequence[Module]:</span>
<span class="gi">+        &quot;Return the direct child modules of this module.&quot;</span>
<span class="gi">+        m: Dict[str, Module] = self.__dict__[&quot;_modules&quot;]</span>
<span class="gi">+        return list(m.values())</span>

<span class="gd">-    def train(self) -&gt;None:</span>
<span class="gd">-        &quot;&quot;&quot;Set the mode of this module and all descendent modules to `train`.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def train(self) -&gt; None:</span>
<span class="gi">+        &quot;Set the mode of this module and all descendent modules to `train`.&quot;</span>
<span class="gi">+        # TODO: Implement for Task 0.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 0.4&#39;)</span>

<span class="gd">-    def eval(self) -&gt;None:</span>
<span class="gd">-        &quot;&quot;&quot;Set the mode of this module and all descendent modules to `eval`.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def eval(self) -&gt; None:</span>
<span class="gi">+        &quot;Set the mode of this module and all descendent modules to `eval`.&quot;</span>
<span class="gi">+        # TODO: Implement for Task 0.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 0.4&#39;)</span>

<span class="gd">-    def named_parameters(self) -&gt;Sequence[Tuple[str, Parameter]]:</span>
<span class="gi">+    def named_parameters(self) -&gt; Sequence[Tuple[str, Parameter]]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Collect all the parameters of this module and its descendents.

<span class="gu">@@ -42,13 +47,15 @@ class Module:</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            The name and `Parameter` of each ancestor parameter.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # TODO: Implement for Task 0.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 0.4&#39;)</span>

<span class="gd">-    def parameters(self) -&gt;Sequence[Parameter]:</span>
<span class="gd">-        &quot;&quot;&quot;Enumerate over all the parameters of this module and its descendents.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def parameters(self) -&gt; Sequence[Parameter]:</span>
<span class="gi">+        &quot;Enumerate over all the parameters of this module and its descendents.&quot;</span>
<span class="gi">+        # TODO: Implement for Task 0.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 0.4&#39;)</span>

<span class="gd">-    def add_parameter(self, k: str, v: Any) -&gt;Parameter:</span>
<span class="gi">+    def add_parameter(self, k: str, v: Any) -&gt; Parameter:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Manually add a parameter. Useful helper for scalar parameters.

<span class="gu">@@ -59,47 +66,54 @@ class Module:</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            Newly created parameter.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        val = Parameter(v, k)</span>
<span class="gi">+        self.__dict__[&quot;_parameters&quot;][k] = val</span>
<span class="gi">+        return val</span>

<span class="gd">-    def __setattr__(self, key: str, val: Parameter) -&gt;None:</span>
<span class="gi">+    def __setattr__(self, key: str, val: Parameter) -&gt; None:</span>
<span class="w"> </span>        if isinstance(val, Parameter):
<span class="gd">-            self.__dict__[&#39;_parameters&#39;][key] = val</span>
<span class="gi">+            self.__dict__[&quot;_parameters&quot;][key] = val</span>
<span class="w"> </span>        elif isinstance(val, Module):
<span class="gd">-            self.__dict__[&#39;_modules&#39;][key] = val</span>
<span class="gi">+            self.__dict__[&quot;_modules&quot;][key] = val</span>
<span class="w"> </span>        else:
<span class="w"> </span>            super().__setattr__(key, val)

<span class="gd">-    def __getattr__(self, key: str) -&gt;Any:</span>
<span class="gd">-        if key in self.__dict__[&#39;_parameters&#39;]:</span>
<span class="gd">-            return self.__dict__[&#39;_parameters&#39;][key]</span>
<span class="gd">-        if key in self.__dict__[&#39;_modules&#39;]:</span>
<span class="gd">-            return self.__dict__[&#39;_modules&#39;][key]</span>
<span class="gi">+    def __getattr__(self, key: str) -&gt; Any:</span>
<span class="gi">+        if key in self.__dict__[&quot;_parameters&quot;]:</span>
<span class="gi">+            return self.__dict__[&quot;_parameters&quot;][key]</span>
<span class="gi">+</span>
<span class="gi">+        if key in self.__dict__[&quot;_modules&quot;]:</span>
<span class="gi">+            return self.__dict__[&quot;_modules&quot;][key]</span>
<span class="w"> </span>        return None

<span class="gd">-    def __call__(self, *args: Any, **kwargs: Any) -&gt;Any:</span>
<span class="gi">+    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:</span>
<span class="w"> </span>        return self.forward(*args, **kwargs)

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gd">-</span>
<span class="gd">-        def _addindent(s_: str, numSpaces: int) -&gt;str:</span>
<span class="gd">-            s2 = s_.split(&#39;\n&#39;)</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="gi">+        def _addindent(s_: str, numSpaces: int) -&gt; str:</span>
<span class="gi">+            s2 = s_.split(&quot;\n&quot;)</span>
<span class="w"> </span>            if len(s2) == 1:
<span class="w"> </span>                return s_
<span class="w"> </span>            first = s2.pop(0)
<span class="gd">-            s2 = [(numSpaces * &#39; &#39; + line) for line in s2]</span>
<span class="gd">-            s = &#39;\n&#39;.join(s2)</span>
<span class="gd">-            s = first + &#39;\n&#39; + s</span>
<span class="gi">+            s2 = [(numSpaces * &quot; &quot;) + line for line in s2]</span>
<span class="gi">+            s = &quot;\n&quot;.join(s2)</span>
<span class="gi">+            s = first + &quot;\n&quot; + s</span>
<span class="w"> </span>            return s
<span class="gi">+</span>
<span class="w"> </span>        child_lines = []
<span class="gi">+</span>
<span class="w"> </span>        for key, module in self._modules.items():
<span class="w"> </span>            mod_str = repr(module)
<span class="w"> </span>            mod_str = _addindent(mod_str, 2)
<span class="gd">-            child_lines.append(&#39;(&#39; + key + &#39;): &#39; + mod_str)</span>
<span class="gi">+            child_lines.append(&quot;(&quot; + key + &quot;): &quot; + mod_str)</span>
<span class="w"> </span>        lines = child_lines
<span class="gd">-        main_str = self.__class__.__name__ + &#39;(&#39;</span>
<span class="gi">+</span>
<span class="gi">+        main_str = self.__class__.__name__ + &quot;(&quot;</span>
<span class="w"> </span>        if lines:
<span class="gd">-            main_str += &#39;\n  &#39; + &#39;\n  &#39;.join(lines) + &#39;\n&#39;</span>
<span class="gd">-        main_str += &#39;)&#39;</span>
<span class="gi">+            # simple one-liner info, which most builtin Modules will use</span>
<span class="gi">+            main_str += &quot;\n  &quot; + &quot;\n  &quot;.join(lines) + &quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+        main_str += &quot;)&quot;</span>
<span class="w"> </span>        return main_str


<span class="gu">@@ -111,20 +125,24 @@ class Parameter:</span>
<span class="w"> </span>    any value for testing.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, x: Any, name: Optional[str]=None) -&gt;None:</span>
<span class="gi">+    def __init__(self, x: Any, name: Optional[str] = None) -&gt; None:</span>
<span class="w"> </span>        self.value = x
<span class="w"> </span>        self.name = name
<span class="gd">-        if hasattr(x, &#39;requires_grad_&#39;):</span>
<span class="gi">+        if hasattr(x, &quot;requires_grad_&quot;):</span>
<span class="w"> </span>            self.value.requires_grad_(True)
<span class="w"> </span>            if self.name:
<span class="w"> </span>                self.value.name = self.name

<span class="gd">-    def update(self, x: Any) -&gt;None:</span>
<span class="gd">-        &quot;&quot;&quot;Update the parameter value.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def update(self, x: Any) -&gt; None:</span>
<span class="gi">+        &quot;Update the parameter value.&quot;</span>
<span class="gi">+        self.value = x</span>
<span class="gi">+        if hasattr(x, &quot;requires_grad_&quot;):</span>
<span class="gi">+            self.value.requires_grad_(True)</span>
<span class="gi">+            if self.name:</span>
<span class="gi">+                self.value.name = self.name</span>

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="w"> </span>        return repr(self.value)

<span class="gd">-    def __str__(self) -&gt;str:</span>
<span class="gi">+    def __str__(self) -&gt; str:</span>
<span class="w"> </span>        return str(self.value)
<span class="gh">diff --git a/minitorch/modules.py b/minitorch/modules.py</span>
<span class="gh">index e69de29..de7519e 100644</span>
<span class="gd">--- a/minitorch/modules.py</span>
<span class="gi">+++ b/minitorch/modules.py</span>
<span class="gu">@@ -0,0 +1,60 @@</span>
<span class="gi">+# from .tensor import rand</span>
<span class="gi">+# from .functions import matmul, conv2d</span>
<span class="gi">+# from .module import Module, Parameter</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# class tLinear(Module):</span>
<span class="gi">+#     def __init__(self, in_size, out_size):</span>
<span class="gi">+#         super().__init__()</span>
<span class="gi">+#         self.weights = Parameter(rand((in_size, out_size)))</span>
<span class="gi">+#         self.bias = Parameter(rand((out_size,)))</span>
<span class="gi">+#         self.out_size = out_size</span>
<span class="gi">+</span>
<span class="gi">+#     def forward(self, x):</span>
<span class="gi">+#         batch, in_size = x.shape</span>
<span class="gi">+#         return (</span>
<span class="gi">+#             self.weights.value.view(1, in_size, self.out_size)</span>
<span class="gi">+#             * x.view(batch, in_size, 1)</span>
<span class="gi">+#         ).sum(1).view(batch, self.out_size) + self.bias.value.view(1, self.out_size)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# class tLinear2(Module):</span>
<span class="gi">+#     def __init__(self, in_size, out_size):</span>
<span class="gi">+#         super().__init__()</span>
<span class="gi">+#         self.weights = Parameter(rand((in_size, out_size)))</span>
<span class="gi">+#         self.bias = Parameter(rand((out_size,)))</span>
<span class="gi">+#         self.out_size = out_size</span>
<span class="gi">+</span>
<span class="gi">+#     def forward(self, x):</span>
<span class="gi">+#         batch, in_size = x.shape</span>
<span class="gi">+#         return matmul(x, self.weights.value) + self.bias.value.view(1, self.out_size)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# class Dropout(Module):</span>
<span class="gi">+#     def __init__(self, rate):</span>
<span class="gi">+#         super().__init__()</span>
<span class="gi">+#         self.rate = rate</span>
<span class="gi">+</span>
<span class="gi">+#     def forward(self, x):</span>
<span class="gi">+#         return (rand(x.shape) / 2 + 0.5 &lt; self.rate) * x</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# class Conv2d(Module):</span>
<span class="gi">+#     def __init__(self, in_features, out_features, size):</span>
<span class="gi">+#         super().__init__()</span>
<span class="gi">+#         size1 = [size[0], size[1], in_features, out_features]</span>
<span class="gi">+#         size2 = [size[0], size[1], out_features]</span>
<span class="gi">+#         self.weights = Parameter(rand(size1))</span>
<span class="gi">+#         self.bias = Parameter(rand(size2))</span>
<span class="gi">+</span>
<span class="gi">+#     def forward(self, x):</span>
<span class="gi">+#         return conv2d(x, self.weights.value, self.bias.value)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# # class MaxPool2d(Module):</span>
<span class="gi">+# #     def __init__(self, in_features, out_features, size):</span>
<span class="gi">+# #         super().__init__()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# #     def forward(self, x):</span>
<span class="gi">+# #         return conv2d(x, self.weights.value, self.bias.value)</span>
<span class="gh">diff --git a/minitorch/nn.py b/minitorch/nn.py</span>
<span class="gh">index 577a3ff..92c0c8f 100644</span>
<span class="gd">--- a/minitorch/nn.py</span>
<span class="gi">+++ b/minitorch/nn.py</span>
<span class="gu">@@ -1,4 +1,5 @@</span>
<span class="w"> </span>from typing import Tuple
<span class="gi">+</span>
<span class="w"> </span>from . import operators
<span class="w"> </span>from .autodiff import Context
<span class="w"> </span>from .fast_ops import FastOps
<span class="gu">@@ -6,7 +7,7 @@ from .tensor import Tensor</span>
<span class="w"> </span>from .tensor_functions import Function, rand, tensor


<span class="gd">-def tile(input: Tensor, kernel: Tuple[int, int]) -&gt;Tuple[Tensor, int, int]:</span>
<span class="gi">+def tile(input: Tensor, kernel: Tuple[int, int]) -&gt; Tuple[Tensor, int, int]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Reshape an image tensor for 2D pooling

<span class="gu">@@ -17,10 +18,16 @@ def tile(input: Tensor, kernel: Tuple[int, int]) -&gt;Tuple[Tensor, int, int]:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor of size batch x channel x new_height x new_width x (kernel_height * kernel_width) as well as the new_height and new_width value.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    batch, channel, height, width = input.shape</span>
<span class="gi">+    kh, kw = kernel</span>
<span class="gi">+    assert height % kh == 0</span>
<span class="gi">+    assert width % kw == 0</span>
<span class="gi">+    # TODO: Implement for Task 4.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.3&#39;)</span>


<span class="gd">-def avgpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt;Tensor:</span>
<span class="gi">+def avgpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Tiled average pooling 2D

<span class="gu">@@ -31,13 +38,15 @@ def avgpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Pooled tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch, channel, height, width = input.shape</span>
<span class="gi">+    # TODO: Implement for Task 4.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.3&#39;)</span>


<span class="gd">-max_reduce = FastOps.reduce(operators.max, -1000000000.0)</span>
<span class="gi">+max_reduce = FastOps.reduce(operators.max, -1e9)</span>


<span class="gd">-def argmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="gi">+def argmax(input: Tensor, dim: int) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Compute the argmax as a 1-hot tensor.

<span class="gu">@@ -50,29 +59,35 @@ def argmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="w"> </span>        :class:`Tensor` : tensor with 1 on highest cell in dim, 0 otherwise

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    out = max_reduce(input, dim)</span>
<span class="gi">+    return out == input</span>


<span class="w"> </span>class Max(Function):
<span class="gd">-</span>
<span class="w"> </span>    @staticmethod
<span class="gd">-    def forward(ctx: Context, input: Tensor, dim: Tensor) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Forward of max should be max reduction&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def forward(ctx: Context, input: Tensor, dim: Tensor) -&gt; Tensor:</span>
<span class="gi">+        &quot;Forward of max should be max reduction&quot;</span>
<span class="gi">+        # TODO: Implement for Task 4.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def backward(ctx: Context, grad_output: Tensor) -&gt;Tuple[Tensor, float]:</span>
<span class="gd">-        &quot;&quot;&quot;Backward of max should be argmax (see above)&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, float]:</span>
<span class="gi">+        &quot;Backward of max should be argmax (see above)&quot;</span>
<span class="gi">+        # TODO: Implement for Task 4.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>


<span class="gd">-def softmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gi">+def max(input: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+    return Max.apply(input, input._ensure_tensor(dim))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def softmax(input: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="w"> </span>    Compute the softmax as a tensor.



<span class="gd">-    $z_i = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$</span>
<span class="gi">+    $z_i = \frac{e^{x_i}}{\sum_i e^{x_i}}$</span>

<span class="w"> </span>    Args:
<span class="w"> </span>        input : input tensor
<span class="gu">@@ -81,14 +96,15 @@ def softmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        softmax tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 4.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>


<span class="gd">-def logsoftmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gi">+def logsoftmax(input: Tensor, dim: int) -&gt; Tensor:</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="w"> </span>    Compute the log of the softmax as a tensor.

<span class="gd">-    $z_i = x_i - \\log \\sum_i e^{x_i}$</span>
<span class="gi">+    $z_i = x_i - \log \sum_i e^{x_i}$</span>

<span class="w"> </span>    See https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations

<span class="gu">@@ -99,10 +115,11 @@ def logsoftmax(input: Tensor, dim: int) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>         log of softmax tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 4.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>


<span class="gd">-def maxpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt;Tensor:</span>
<span class="gi">+def maxpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Tiled max pooling 2D

<span class="gu">@@ -113,10 +130,12 @@ def maxpool2d(input: Tensor, kernel: Tuple[int, int]) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor : pooled tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    batch, channel, height, width = input.shape</span>
<span class="gi">+    # TODO: Implement for Task 4.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>


<span class="gd">-def dropout(input: Tensor, rate: float, ignore: bool=False) -&gt;Tensor:</span>
<span class="gi">+def dropout(input: Tensor, rate: float, ignore: bool = False) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Dropout positions based on random noise.

<span class="gu">@@ -128,4 +147,5 @@ def dropout(input: Tensor, rate: float, ignore: bool=False) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        tensor with random positions dropped out
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 4.4.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 4.4&#39;)</span>
<span class="gh">diff --git a/minitorch/operators.py b/minitorch/operators.py</span>
<span class="gh">index 8740347..3334e64 100644</span>
<span class="gd">--- a/minitorch/operators.py</span>
<span class="gi">+++ b/minitorch/operators.py</span>
<span class="gu">@@ -1,109 +1,132 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Collection of the core mathematical operators used throughout the code base.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>import math
<span class="w"> </span>from typing import Callable, Iterable

<span class="gi">+# ## Task 0.1</span>
<span class="gi">+#</span>
<span class="gi">+# Implementation of a prelude of elementary functions.</span>

<span class="gd">-def mul(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x, y) = x * y$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def mul(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x, y) = x * y$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def id(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = x$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def id(x: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = x$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def add(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x, y) = x + y$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def add(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x, y) = x + y$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def neg(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = -x$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def neg(x: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = -x$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def lt(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) =$ 1.0 if x is less than y else 0.0&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def lt(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) =$ 1.0 if x is less than y else 0.0&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def eq(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) =$ 1.0 if x is equal to y else 0.0&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def eq(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) =$ 1.0 if x is equal to y else 0.0&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def max(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) =$ x if x is greater than y else y&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def max(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) =$ x if x is greater than y else y&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def is_close(x: float, y: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = |x - y| &lt; 1e-2$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def is_close(x: float, y: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = |x - y| &lt; 1e-2$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def sigmoid(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gd">-    $f(x) =  \\frac{1.0}{(1.0 + e^{-x})}$</span>
<span class="gi">+</span>
<span class="gi">+def sigmoid(x: float) -&gt; float:</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="gi">+    $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$</span>

<span class="w"> </span>    (See https://en.wikipedia.org/wiki/Sigmoid_function )

<span class="w"> </span>    Calculate as

<span class="gd">-    $f(x) =  \\frac{1.0}{(1.0 + e^{-x})}$ if x &gt;=0 else $\\frac{e^x}{(1.0 + e^{x})}$</span>
<span class="gi">+    $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$ if x &gt;=0 else $\frac{e^x}{(1.0 + e^{x})}$</span>

<span class="w"> </span>    for stability.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>


<span class="gd">-def relu(x: float) -&gt;float:</span>
<span class="gi">+def relu(x: float) -&gt; float:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    $f(x) =$ x if x is greater than 0, else 0

<span class="w"> </span>    (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>
<span class="gi">+</span>

<span class="gi">+EPS = 1e-6</span>

<span class="gd">-EPS = 1e-06</span>

<span class="gi">+def log(x: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = log(x)$&quot;</span>
<span class="gi">+    return math.log(x + EPS)</span>

<span class="gd">-def log(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = log(x)$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def exp(x: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = e^{x}$&quot;</span>
<span class="gi">+    return math.exp(x)</span>

<span class="gd">-def exp(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = e^{x}$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def log_back(x: float, d: float) -&gt; float:</span>
<span class="gi">+    r&quot;If $f = log$ as above, compute $d \times f&#39;(x)$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def log_back(x: float, d: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;If $f = log$ as above, compute $d \\times f&#39;(x)$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def inv(x: float) -&gt; float:</span>
<span class="gi">+    &quot;$f(x) = 1/x$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def inv(x: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;$f(x) = 1/x$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def inv_back(x: float, d: float) -&gt; float:</span>
<span class="gi">+    r&quot;If $f(x) = 1/x$ compute $d \times f&#39;(x)$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def inv_back(x: float, d: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;If $f(x) = 1/x$ compute $d \\times f&#39;(x)$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+def relu_back(x: float, d: float) -&gt; float:</span>
<span class="gi">+    r&quot;If $f = relu$ compute $d \times f&#39;(x)$&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.1&#39;)</span>

<span class="gd">-def relu_back(x: float, d: float) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;If $f = relu$ compute $d \\times f&#39;(x)$&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>

<span class="gi">+# ## Task 0.3</span>

<span class="gd">-def map(fn: Callable[[float], float]) -&gt;Callable[[Iterable[float]],</span>
<span class="gd">-    Iterable[float]]:</span>
<span class="gi">+# Small practice library of elementary higher-order functions.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def map(fn: Callable[[float], float]) -&gt; Callable[[Iterable[float]], Iterable[float]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Higher-order map.

<span class="gu">@@ -116,16 +139,19 @@ def map(fn: Callable[[float], float]) -&gt;Callable[[Iterable[float]],</span>
<span class="w"> </span>         A function that takes a list, applies `fn` to each element, and returns a
<span class="w"> </span>         new list
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def negList(ls: Iterable[float]) -&gt;Iterable[float]:</span>
<span class="gd">-    &quot;&quot;&quot;Use `map` and `neg` to negate each element in `ls`&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def negList(ls: Iterable[float]) -&gt; Iterable[float]:</span>
<span class="gi">+    &quot;Use `map` and `neg` to negate each element in `ls`&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def zipWith(fn: Callable[[float, float], float]) -&gt;Callable[[Iterable[float</span>
<span class="gd">-    ], Iterable[float]], Iterable[float]]:</span>
<span class="gi">+def zipWith(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[[Iterable[float], Iterable[float]], Iterable[float]]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Higher-order zipwith (or map2).

<span class="gu">@@ -139,17 +165,20 @@ def zipWith(fn: Callable[[float, float], float]) -&gt;Callable[[Iterable[float</span>
<span class="w"> </span>         applying fn(x, y) on each pair of elements.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def addLists(ls1: Iterable[float], ls2: Iterable[float]) -&gt;Iterable[float]:</span>
<span class="gd">-    &quot;&quot;&quot;Add the elements of `ls1` and `ls2` using `zipWith` and `add`&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def addLists(ls1: Iterable[float], ls2: Iterable[float]) -&gt; Iterable[float]:</span>
<span class="gi">+    &quot;Add the elements of `ls1` and `ls2` using `zipWith` and `add`&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def reduce(fn: Callable[[float, float], float], start: float) -&gt;Callable[[</span>
<span class="gd">-    Iterable[float]], float]:</span>
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gi">+def reduce(</span>
<span class="gi">+    fn: Callable[[float, float], float], start: float</span>
<span class="gi">+) -&gt; Callable[[Iterable[float]], float]:</span>
<span class="gi">+    r&quot;&quot;&quot;</span>
<span class="w"> </span>    Higher-order reduce.

<span class="w"> </span>    Args:
<span class="gu">@@ -158,17 +187,20 @@ def reduce(fn: Callable[[float, float], float], start: float) -&gt;Callable[[</span>

<span class="w"> </span>    Returns:
<span class="w"> </span>         Function that takes a list `ls` of elements
<span class="gd">-         $x_1 \\ldots x_n$ and computes the reduction :math:`fn(x_3, fn(x_2,</span>
<span class="gi">+         $x_1 \ldots x_n$ and computes the reduction :math:`fn(x_3, fn(x_2,</span>
<span class="w"> </span>         fn(x_1, x_0)))`
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def sum(ls: Iterable[float]) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;Sum up a list using `reduce` and `add`.&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def sum(ls: Iterable[float]) -&gt; float:</span>
<span class="gi">+    &quot;Sum up a list using `reduce` and `add`.&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>


<span class="gd">-def prod(ls: Iterable[float]) -&gt;float:</span>
<span class="gd">-    &quot;&quot;&quot;Product of a list using `reduce` and `mul`.&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def prod(ls: Iterable[float]) -&gt; float:</span>
<span class="gi">+    &quot;Product of a list using `reduce` and `mul`.&quot;</span>
<span class="gi">+    # TODO: Implement for Task 0.3.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 0.3&#39;)</span>
<span class="gh">diff --git a/minitorch/optim.py b/minitorch/optim.py</span>
<span class="gh">index b6358e6..21c9dde 100644</span>
<span class="gd">--- a/minitorch/optim.py</span>
<span class="gi">+++ b/minitorch/optim.py</span>
<span class="gu">@@ -1,16 +1,37 @@</span>
<span class="w"> </span>from typing import Sequence
<span class="gi">+</span>
<span class="w"> </span>from .module import Parameter
<span class="w"> </span>from .scalar import Scalar


<span class="w"> </span>class Optimizer:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, parameters: Sequence[Parameter]):
<span class="w"> </span>        self.parameters = parameters


<span class="w"> </span>class SGD(Optimizer):
<span class="gd">-</span>
<span class="gd">-    def __init__(self, parameters: Sequence[Parameter], lr: float=1.0):</span>
<span class="gi">+    def __init__(self, parameters: Sequence[Parameter], lr: float = 1.0):</span>
<span class="w"> </span>        super().__init__(parameters)
<span class="w"> </span>        self.lr = lr
<span class="gi">+</span>
<span class="gi">+    def zero_grad(self) -&gt; None:</span>
<span class="gi">+        for p in self.parameters:</span>
<span class="gi">+            if p.value is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            if hasattr(p.value, &quot;derivative&quot;):</span>
<span class="gi">+                if p.value.derivative is not None:</span>
<span class="gi">+                    p.value.derivative = None</span>
<span class="gi">+            if hasattr(p.value, &quot;grad&quot;):</span>
<span class="gi">+                if p.value.grad is not None:</span>
<span class="gi">+                    p.value.grad = None</span>
<span class="gi">+</span>
<span class="gi">+    def step(self) -&gt; None:</span>
<span class="gi">+        for p in self.parameters:</span>
<span class="gi">+            if p.value is None:</span>
<span class="gi">+                continue</span>
<span class="gi">+            if hasattr(p.value, &quot;derivative&quot;):</span>
<span class="gi">+                if p.value.derivative is not None:</span>
<span class="gi">+                    p.update(Scalar(p.value.data - self.lr * p.value.derivative))</span>
<span class="gi">+            elif hasattr(p.value, &quot;grad&quot;):</span>
<span class="gi">+                if p.value.grad is not None:</span>
<span class="gi">+                    p.update(p.value - self.lr * p.value.grad)</span>
<span class="gh">diff --git a/minitorch/scalar.py b/minitorch/scalar.py</span>
<span class="gh">index a8a0420..942079d 100644</span>
<span class="gd">--- a/minitorch/scalar.py</span>
<span class="gi">+++ b/minitorch/scalar.py</span>
<span class="gu">@@ -1,10 +1,26 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from dataclasses import dataclass
<span class="w"> </span>from typing import Any, Iterable, Optional, Sequence, Tuple, Type, Union
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="gi">+</span>
<span class="w"> </span>from .autodiff import Context, Variable, backpropagate, central_difference
<span class="gd">-from .scalar_functions import EQ, LT, Add, Exp, Inv, Log, Mul, Neg, ReLU, ScalarFunction, Sigmoid</span>
<span class="gd">-ScalarLike = Union[float, int, &#39;Scalar&#39;]</span>
<span class="gi">+from .scalar_functions import (</span>
<span class="gi">+    EQ,</span>
<span class="gi">+    LT,</span>
<span class="gi">+    Add,</span>
<span class="gi">+    Exp,</span>
<span class="gi">+    Inv,</span>
<span class="gi">+    Log,</span>
<span class="gi">+    Mul,</span>
<span class="gi">+    Neg,</span>
<span class="gi">+    ReLU,</span>
<span class="gi">+    ScalarFunction,</span>
<span class="gi">+    Sigmoid,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+ScalarLike = Union[float, int, &quot;Scalar&quot;]</span>


<span class="w"> </span>@dataclass
<span class="gu">@@ -19,11 +35,15 @@ class ScalarHistory:</span>
<span class="w"> </span>        inputs : The inputs that were given when `last_fn.forward` was called.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    last_fn: Optional[Type[ScalarFunction]] = None
<span class="w"> </span>    ctx: Optional[Context] = None
<span class="w"> </span>    inputs: Sequence[Scalar] = ()


<span class="gi">+# ## Task 1.2 and 1.4</span>
<span class="gi">+# Scalar Forward and Backward</span>
<span class="gi">+</span>
<span class="w"> </span>_var_count = 0


<span class="gu">@@ -35,14 +55,19 @@ class Scalar:</span>
<span class="w"> </span>    number&#39;s creation. They can only be manipulated by
<span class="w"> </span>    `ScalarFunction`.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    history: Optional[ScalarHistory]
<span class="w"> </span>    derivative: Optional[float]
<span class="w"> </span>    data: float
<span class="w"> </span>    unique_id: int
<span class="w"> </span>    name: str

<span class="gd">-    def __init__(self, v: float, back: ScalarHistory=ScalarHistory(), name:</span>
<span class="gd">-        Optional[str]=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        v: float,</span>
<span class="gi">+        back: ScalarHistory = ScalarHistory(),</span>
<span class="gi">+        name: Optional[str] = None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        global _var_count
<span class="w"> </span>        _var_count += 1
<span class="w"> </span>        self.unique_id = _var_count
<span class="gu">@@ -54,46 +79,70 @@ class Scalar:</span>
<span class="w"> </span>        else:
<span class="w"> </span>            self.name = str(self.unique_id)

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gd">-        return &#39;Scalar(%f)&#39; % self.data</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="gi">+        return &quot;Scalar(%f)&quot; % self.data</span>

<span class="gd">-    def __mul__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __mul__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="w"> </span>        return Mul.apply(self, b)

<span class="gd">-    def __truediv__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __truediv__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="w"> </span>        return Mul.apply(self, Inv.apply(b))

<span class="gd">-    def __rtruediv__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __rtruediv__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="w"> </span>        return Mul.apply(b, Inv.apply(self))

<span class="gd">-    def __add__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __add__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __bool__(self) -&gt;bool:</span>
<span class="gi">+    def __bool__(self) -&gt; bool:</span>
<span class="w"> </span>        return bool(self.data)

<span class="gd">-    def __lt__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __lt__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __gt__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __gt__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __eq__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __eq__(self, b: ScalarLike) -&gt; Scalar:  # type: ignore[override]</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __sub__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __sub__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __neg__(self) -&gt;Scalar:</span>
<span class="gi">+    def __neg__(self) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="w"> </span>        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)

<span class="gd">-    def __radd__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __radd__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="w"> </span>        return self + b

<span class="gd">-    def __rmul__(self, b: ScalarLike) -&gt;Scalar:</span>
<span class="gi">+    def __rmul__(self, b: ScalarLike) -&gt; Scalar:</span>
<span class="w"> </span>        return self * b

<span class="gd">-    def accumulate_derivative(self, x: Any) -&gt;None:</span>
<span class="gi">+    def log(self) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    def exp(self) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    def sigmoid(self) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    def relu(self) -&gt; Scalar:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    # Variable elements for backprop</span>
<span class="gi">+</span>
<span class="gi">+    def accumulate_derivative(self, x: Any) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Add `val` to the the derivative accumulated on this variable.
<span class="w"> </span>        Should only be called during autodifferentiation on leaf variables.
<span class="gu">@@ -101,13 +150,33 @@ class Scalar:</span>
<span class="w"> </span>        Args:
<span class="w"> </span>            x: value to be accumulated
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.is_leaf(), &quot;Only leaf variables can have derivatives.&quot;</span>
<span class="gi">+        if self.derivative is None:</span>
<span class="gi">+            self.derivative = 0.0</span>
<span class="gi">+        self.derivative += x</span>
<span class="gi">+</span>
<span class="gi">+    def is_leaf(self) -&gt; bool:</span>
<span class="gi">+        &quot;True if this variable created by the user (no `last_fn`)&quot;</span>
<span class="gi">+        return self.history is not None and self.history.last_fn is None</span>
<span class="gi">+</span>
<span class="gi">+    def is_constant(self) -&gt; bool:</span>
<span class="gi">+        return self.history is None</span>
<span class="gi">+</span>
<span class="gi">+    @property</span>
<span class="gi">+    def parents(self) -&gt; Iterable[Variable]:</span>
<span class="gi">+        assert self.history is not None</span>
<span class="gi">+        return self.history.inputs</span>
<span class="gi">+</span>
<span class="gi">+    def chain_rule(self, d_output: Any) -&gt; Iterable[Tuple[Variable, Any]]:</span>
<span class="gi">+        h = self.history</span>
<span class="gi">+        assert h is not None</span>
<span class="gi">+        assert h.last_fn is not None</span>
<span class="gi">+        assert h.ctx is not None</span>

<span class="gd">-    def is_leaf(self) -&gt;bool:</span>
<span class="gd">-        &quot;&quot;&quot;True if this variable created by the user (no `last_fn`)&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+        # TODO: Implement for Task 1.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.3&#39;)</span>

<span class="gd">-    def backward(self, d_output: Optional[float]=None) -&gt;None:</span>
<span class="gi">+    def backward(self, d_output: Optional[float] = None) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Calls autodiff to fill in the derivatives for the history of this object.

<span class="gu">@@ -115,10 +184,12 @@ class Scalar:</span>
<span class="w"> </span>            d_output (number, opt): starting derivative to backpropagate through the model
<span class="w"> </span>                                   (typically left out, and assumed to be 1.0).
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if d_output is None:</span>
<span class="gi">+            d_output = 1.0</span>
<span class="gi">+        backpropagate(self, d_output)</span>


<span class="gd">-def derivative_check(f: Any, *scalars: Scalar) -&gt;None:</span>
<span class="gi">+def derivative_check(f: Any, *scalars: Scalar) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Checks that autodiff works on a python function.
<span class="w"> </span>    Asserts False if derivative is incorrect.
<span class="gu">@@ -127,4 +198,21 @@ def derivative_check(f: Any, *scalars: Scalar) -&gt;None:</span>
<span class="w"> </span>        f : function from n-scalars to 1-scalar.
<span class="w"> </span>        *scalars  : n input scalar values.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    out = f(*scalars)</span>
<span class="gi">+    out.backward()</span>
<span class="gi">+</span>
<span class="gi">+    err_msg = &quot;&quot;&quot;</span>
<span class="gi">+Derivative check at arguments f(%s) and received derivative f&#39;=%f for argument %d,</span>
<span class="gi">+but was expecting derivative f&#39;=%f from central difference.&quot;&quot;&quot;</span>
<span class="gi">+    for i, x in enumerate(scalars):</span>
<span class="gi">+        check = central_difference(f, *scalars, arg=i)</span>
<span class="gi">+        print(str([x.data for x in scalars]), x.derivative, i, check)</span>
<span class="gi">+        assert x.derivative is not None</span>
<span class="gi">+        np.testing.assert_allclose(</span>
<span class="gi">+            x.derivative,</span>
<span class="gi">+            check.data,</span>
<span class="gi">+            1e-2,</span>
<span class="gi">+            1e-2,</span>
<span class="gi">+            err_msg=err_msg</span>
<span class="gi">+            % (str([x.data for x in scalars]), x.derivative, i, check.data),</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/minitorch/scalar_functions.py b/minitorch/scalar_functions.py</span>
<span class="gh">index c55ef86..b5deab0 100644</span>
<span class="gd">--- a/minitorch/scalar_functions.py</span>
<span class="gi">+++ b/minitorch/scalar_functions.py</span>
<span class="gu">@@ -1,21 +1,30 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from typing import TYPE_CHECKING
<span class="gi">+</span>
<span class="w"> </span>import minitorch
<span class="gi">+</span>
<span class="w"> </span>from . import operators
<span class="w"> </span>from .autodiff import Context
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from typing import Tuple
<span class="gi">+</span>
<span class="w"> </span>    from .scalar import Scalar, ScalarLike


<span class="gd">-def wrap_tuple(x):</span>
<span class="gd">-    &quot;&quot;&quot;Turn a possible value into a tuple&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def wrap_tuple(x):  # type: ignore</span>
<span class="gi">+    &quot;Turn a possible value into a tuple&quot;</span>
<span class="gi">+    if isinstance(x, tuple):</span>
<span class="gi">+        return x</span>
<span class="gi">+    return (x,)</span>


<span class="gd">-def unwrap_tuple(x):</span>
<span class="gd">-    &quot;&quot;&quot;Turn a singleton tuple into a value&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def unwrap_tuple(x):  # type: ignore</span>
<span class="gi">+    &quot;Turn a singleton tuple into a value&quot;</span>
<span class="gi">+    if len(x) == 1:</span>
<span class="gi">+        return x[0]</span>
<span class="gi">+    return x</span>


<span class="w"> </span>class ScalarFunction:
<span class="gu">@@ -27,42 +36,175 @@ class ScalarFunction:</span>
<span class="w"> </span>    here to group together the `forward` and `backward` code.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gi">+    @classmethod</span>
<span class="gi">+    def _backward(cls, ctx: Context, d_out: float) -&gt; Tuple[float, ...]:</span>
<span class="gi">+        return wrap_tuple(cls.backward(ctx, d_out))  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _forward(cls, ctx: Context, *inps: float) -&gt; float:</span>
<span class="gi">+        return cls.forward(ctx, *inps)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def apply(cls, *vals: &quot;ScalarLike&quot;) -&gt; Scalar:</span>
<span class="gi">+        raw_vals = []</span>
<span class="gi">+        scalars = []</span>
<span class="gi">+        for v in vals:</span>
<span class="gi">+            if isinstance(v, minitorch.scalar.Scalar):</span>
<span class="gi">+                scalars.append(v)</span>
<span class="gi">+                raw_vals.append(v.data)</span>
<span class="gi">+            else:</span>
<span class="gi">+                scalars.append(minitorch.scalar.Scalar(v))</span>
<span class="gi">+                raw_vals.append(v)</span>
<span class="gi">+</span>
<span class="gi">+        # Create the context.</span>
<span class="gi">+        ctx = Context(False)</span>
<span class="gi">+</span>
<span class="gi">+        # Call forward with the variables.</span>
<span class="gi">+        c = cls._forward(ctx, *raw_vals)</span>
<span class="gi">+        assert isinstance(c, float), &quot;Expected return type float got %s&quot; % (type(c))</span>

<span class="gi">+        # Create a new variable from the result with a new history.</span>
<span class="gi">+        back = minitorch.scalar.ScalarHistory(cls, ctx, scalars)</span>
<span class="gi">+        return minitorch.scalar.Scalar(c, back)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Examples</span>
<span class="w"> </span>class Add(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Addition function $f(x, y) = x + y$&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Addition function $f(x, y) = x + y$&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        return a + b</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, ...]:</span>
<span class="gi">+        return d_output, d_output</span>


<span class="w"> </span>class Log(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Log function $f(x) = log(x)$&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Log function $f(x) = log(x)$&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        ctx.save_for_backward(a)</span>
<span class="gi">+        return operators.log(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        (a,) = ctx.saved_values</span>
<span class="gi">+        return operators.log_back(a, d_output)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# To implement.</span>


<span class="w"> </span>class Mul(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Multiplication function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Multiplication function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class Inv(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Inverse function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Inverse function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class Neg(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Negation function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Negation function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class Sigmoid(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Sigmoid function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Sigmoid function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class ReLU(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;ReLU function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;ReLU function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class Exp(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Exp function&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Exp function&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class LT(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Less-than function $f(x) =$ 1.0 if x is less than y else 0.0&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Less-than function $f(x) =$ 1.0 if x is less than y else 0.0&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>


<span class="w"> </span>class EQ(ScalarFunction):
<span class="gd">-    &quot;&quot;&quot;Equal function $f(x) =$ 1.0 if x is equal to y else 0.0&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Equal function $f(x) =$ 1.0 if x is equal to y else 0.0&quot;</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: float, b: float) -&gt; float:</span>
<span class="gi">+        # TODO: Implement for Task 1.2.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, d_output: float) -&gt; Tuple[float, float]:</span>
<span class="gi">+        # TODO: Implement for Task 1.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 1.4&#39;)</span>
<span class="gh">diff --git a/minitorch/tensor.py b/minitorch/tensor.py</span>
<span class="gh">index dc62ddc..0ff577b 100644</span>
<span class="gd">--- a/minitorch/tensor.py</span>
<span class="gi">+++ b/minitorch/tensor.py</span>
<span class="gu">@@ -1,21 +1,48 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Implementation of the core Tensor object for autodifferentiation.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from dataclasses import dataclass
<span class="w"> </span>from typing import TYPE_CHECKING
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="gi">+</span>
<span class="w"> </span>from . import operators
<span class="w"> </span>from .autodiff import Context, Variable, backpropagate
<span class="w"> </span>from .tensor_data import TensorData
<span class="gd">-from .tensor_functions import EQ, LT, Add, All, Copy, Exp, Inv, IsClose, Log, MatMul, Mul, Neg, Permute, ReLU, Sigmoid, Sum, View, tensor</span>
<span class="gi">+from .tensor_functions import (</span>
<span class="gi">+    EQ,</span>
<span class="gi">+    LT,</span>
<span class="gi">+    Add,</span>
<span class="gi">+    All,</span>
<span class="gi">+    Copy,</span>
<span class="gi">+    Exp,</span>
<span class="gi">+    Inv,</span>
<span class="gi">+    IsClose,</span>
<span class="gi">+    Log,</span>
<span class="gi">+    MatMul,</span>
<span class="gi">+    Mul,</span>
<span class="gi">+    Neg,</span>
<span class="gi">+    Permute,</span>
<span class="gi">+    ReLU,</span>
<span class="gi">+    Sigmoid,</span>
<span class="gi">+    Sum,</span>
<span class="gi">+    View,</span>
<span class="gi">+    tensor,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from typing import Any, Iterable, List, Optional, Sequence, Tuple, Type, Union
<span class="gi">+</span>
<span class="w"> </span>    import numpy.typing as npt
<span class="gi">+</span>
<span class="w"> </span>    from .tensor_data import Shape, Storage, Strides, UserIndex, UserShape, UserStrides
<span class="w"> </span>    from .tensor_functions import Function
<span class="w"> </span>    from .tensor_ops import TensorBackend
<span class="gd">-    TensorLike = Union[float, int, &#39;Tensor&#39;]</span>
<span class="gi">+</span>
<span class="gi">+    TensorLike = Union[float, int, &quot;Tensor&quot;]</span>


<span class="w"> </span>@dataclass
<span class="gu">@@ -24,6 +51,7 @@ class History:</span>
<span class="w"> </span>    `History` stores the history of `Function` operations that was
<span class="w"> </span>    used to construct the current Variable.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    last_fn: Optional[Type[Function]] = None
<span class="w"> </span>    ctx: Optional[Context] = None
<span class="w"> </span>    inputs: Sequence[Tensor] = ()
<span class="gu">@@ -37,6 +65,7 @@ class Tensor:</span>
<span class="w"> </span>    Tensor is a generalization of Scalar in that it is a Variable that
<span class="w"> </span>    handles multidimensional arrays.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    backend: TensorBackend
<span class="w"> </span>    history: Optional[History]
<span class="w"> </span>    grad: Optional[Tensor]
<span class="gu">@@ -44,8 +73,13 @@ class Tensor:</span>
<span class="w"> </span>    unique_id: int
<span class="w"> </span>    name: str

<span class="gd">-    def __init__(self, v: TensorData, back: Optional[History]=None, name:</span>
<span class="gd">-        Optional[str]=None, backend: Optional[TensorBackend]=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        v: TensorData,</span>
<span class="gi">+        back: Optional[History] = None,</span>
<span class="gi">+        name: Optional[str] = None,</span>
<span class="gi">+        backend: Optional[TensorBackend] = None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        global _tensor_count
<span class="w"> </span>        _tensor_count += 1
<span class="w"> </span>        self.unique_id = _tensor_count
<span class="gu">@@ -59,119 +93,177 @@ class Tensor:</span>
<span class="w"> </span>            self.name = name
<span class="w"> </span>        else:
<span class="w"> </span>            self.name = str(self.unique_id)
<span class="gi">+</span>
<span class="w"> </span>        self.f = backend

<span class="gd">-    def to_numpy(self) -&gt;npt.NDArray[np.float64]:</span>
<span class="gi">+    def requires_grad_(self, x: bool) -&gt; None:</span>
<span class="gi">+        self.history = History()</span>
<span class="gi">+</span>
<span class="gi">+    def requires_grad(self) -&gt; bool:</span>
<span class="gi">+        return self.history is not None</span>
<span class="gi">+</span>
<span class="gi">+    def to_numpy(self) -&gt; npt.NDArray[np.float64]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns:
<span class="w"> </span>             Converted to numpy array
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self.contiguous()._tensor._storage.reshape(self.shape)</span>

<span class="gi">+    # Properties</span>
<span class="w"> </span>    @property
<span class="gd">-    def shape(self) -&gt;UserShape:</span>
<span class="gi">+    def shape(self) -&gt; UserShape:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns:
<span class="w"> </span>             shape of the tensor
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._tensor.shape</span>

<span class="w"> </span>    @property
<span class="gd">-    def size(self) -&gt;int:</span>
<span class="gi">+    def size(self) -&gt; int:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns:
<span class="w"> </span>             int : size of the tensor
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._tensor.size</span>

<span class="w"> </span>    @property
<span class="gd">-    def dims(self) -&gt;int:</span>
<span class="gi">+    def dims(self) -&gt; int:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns:
<span class="w"> </span>             int : dimensionality of the tensor
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return self._tensor.dims</span>

<span class="gd">-    def _ensure_tensor(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Turns a python number into a tensor with the same backend.&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def _ensure_tensor(self, b: TensorLike) -&gt; Tensor:</span>
<span class="gi">+        &quot;Turns a python number into a tensor with the same backend.&quot;</span>
<span class="gi">+        if isinstance(b, (int, float)):</span>
<span class="gi">+            c = Tensor.make([b], (1,), backend=self.backend)</span>
<span class="gi">+        else:</span>
<span class="gi">+            b._type_(self.backend)</span>
<span class="gi">+            c = b</span>
<span class="gi">+        return c</span>

<span class="gd">-    def __add__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    # Functions</span>
<span class="gi">+    def __add__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return Add.apply(self, self._ensure_tensor(b))

<span class="gd">-    def __sub__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __sub__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return Add.apply(self, -self._ensure_tensor(b))

<span class="gd">-    def __mul__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __mul__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return Mul.apply(self, self._ensure_tensor(b))

<span class="gd">-    def __truediv__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __truediv__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return Mul.apply(self, Inv.apply(self._ensure_tensor(b)))

<span class="gd">-    def __rtruediv__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __rtruediv__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return Mul.apply(self._ensure_tensor(b), Inv.apply(self))

<span class="gd">-    def __matmul__(self, b: Tensor) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Not used until Module 3&quot;&quot;&quot;</span>
<span class="gi">+    def __matmul__(self, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        &quot;Not used until Module 3&quot;</span>
<span class="w"> </span>        return MatMul.apply(self, b)

<span class="gd">-    def __lt__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __lt__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return LT.apply(self, self._ensure_tensor(b))

<span class="gd">-    def __eq__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __eq__(self, b: TensorLike) -&gt; Tensor:  # type: ignore[override]</span>
<span class="w"> </span>        return EQ.apply(self, self._ensure_tensor(b))

<span class="gd">-    def __gt__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __gt__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return LT.apply(self._ensure_tensor(b), self)

<span class="gd">-    def __neg__(self) -&gt;Tensor:</span>
<span class="gi">+    def __neg__(self) -&gt; Tensor:</span>
<span class="w"> </span>        return Neg.apply(self)

<span class="gd">-    def __radd__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __radd__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return self + b

<span class="gd">-    def __rmul__(self, b: TensorLike) -&gt;Tensor:</span>
<span class="gi">+    def __rmul__(self, b: TensorLike) -&gt; Tensor:</span>
<span class="w"> </span>        return self * b

<span class="gd">-    def sum(self, dim: Optional[int]=None) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Compute the sum over dimension `dim`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def all(self, dim: Optional[int] = None) -&gt; Tensor:</span>
<span class="gi">+        if dim is None:</span>
<span class="gi">+            return All.apply(self.view(self.size), self._ensure_tensor(0))</span>
<span class="gi">+        else:</span>
<span class="gi">+            return All.apply(self, self._ensure_tensor(dim))</span>
<span class="gi">+</span>
<span class="gi">+    def is_close(self, y: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return IsClose.apply(self, y)</span>
<span class="gi">+</span>
<span class="gi">+    def sigmoid(self) -&gt; Tensor:</span>
<span class="gi">+        return Sigmoid.apply(self)</span>
<span class="gi">+</span>
<span class="gi">+    def relu(self) -&gt; Tensor:</span>
<span class="gi">+        return ReLU.apply(self)</span>

<span class="gd">-    def mean(self, dim: Optional[int]=None) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Compute the mean over dimension `dim`&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def log(self) -&gt; Tensor:</span>
<span class="gi">+        return Log.apply(self)</span>

<span class="gd">-    def permute(self, *order: int) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Permute tensor dimensions to *order&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def exp(self) -&gt; Tensor:</span>
<span class="gi">+        return Exp.apply(self)</span>

<span class="gd">-    def view(self, *shape: int) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Change the shape of the tensor to a new shape with the same size&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def item(self) -&gt; float:</span>
<span class="gi">+        assert self.size == 1</span>
<span class="gi">+        x: float = self._tensor._storage[0]</span>
<span class="gi">+        return x</span>

<span class="gd">-    def contiguous(self) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Return a contiguous tensor with the same data&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def sum(self, dim: Optional[int] = None) -&gt; Tensor:</span>
<span class="gi">+        &quot;Compute the sum over dimension `dim`&quot;</span>
<span class="gi">+        if dim is None:</span>
<span class="gi">+            return Sum.apply(self.contiguous().view(self.size), self._ensure_tensor(0))</span>
<span class="gi">+        else:</span>
<span class="gi">+            return Sum.apply(self, self._ensure_tensor(dim))</span>
<span class="gi">+</span>
<span class="gi">+    def mean(self, dim: Optional[int] = None) -&gt; Tensor:</span>
<span class="gi">+        &quot;Compute the mean over dimension `dim`&quot;</span>
<span class="gi">+        if dim is not None:</span>
<span class="gi">+            return self.sum(dim) / self.shape[dim]</span>
<span class="gi">+        else:</span>
<span class="gi">+            return self.sum() / self.size</span>
<span class="gi">+</span>
<span class="gi">+    def permute(self, *order: int) -&gt; Tensor:</span>
<span class="gi">+        &quot;Permute tensor dimensions to *order&quot;</span>
<span class="gi">+        return Permute.apply(self, tensor(list(order)))</span>
<span class="gi">+</span>
<span class="gi">+    def view(self, *shape: int) -&gt; Tensor:</span>
<span class="gi">+        &quot;Change the shape of the tensor to a new shape with the same size&quot;</span>
<span class="gi">+        return View.apply(self, tensor(list(shape)))</span>

<span class="gd">-    def __repr__(self) -&gt;str:</span>
<span class="gi">+    def contiguous(self) -&gt; Tensor:</span>
<span class="gi">+        &quot;Return a contiguous tensor with the same data&quot;</span>
<span class="gi">+        return Copy.apply(self)</span>
<span class="gi">+</span>
<span class="gi">+    def __repr__(self) -&gt; str:</span>
<span class="w"> </span>        return self._tensor.to_string()

<span class="gd">-    def __getitem__(self, key: Union[int, UserIndex]) -&gt;float:</span>
<span class="gi">+    def __getitem__(self, key: Union[int, UserIndex]) -&gt; float:</span>
<span class="w"> </span>        key2 = (key,) if isinstance(key, int) else key
<span class="w"> </span>        return self._tensor.get(key2)

<span class="gd">-    def __setitem__(self, key: Union[int, UserIndex], val: float) -&gt;None:</span>
<span class="gi">+    def __setitem__(self, key: Union[int, UserIndex], val: float) -&gt; None:</span>
<span class="w"> </span>        key2 = (key,) if isinstance(key, int) else key
<span class="w"> </span>        self._tensor.set(key2, val)

<span class="gd">-    @staticmethod</span>
<span class="gd">-    def make(storage: Union[Storage, List[float]], shape: UserShape,</span>
<span class="gd">-        strides: Optional[UserStrides]=None, backend: Optional[</span>
<span class="gd">-        TensorBackend]=None) -&gt;Tensor:</span>
<span class="gd">-        &quot;&quot;&quot;Create a new tensor from data&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    # Internal methods used for autodiff.</span>
<span class="gi">+    def _type_(self, backend: TensorBackend) -&gt; None:</span>
<span class="gi">+        self.backend = backend</span>
<span class="gi">+        if backend.cuda:  # pragma: no cover</span>
<span class="gi">+            self._tensor.to_cuda_()</span>
<span class="gi">+</span>
<span class="gi">+    def _new(self, tensor_data: TensorData) -&gt; Tensor:</span>
<span class="gi">+        return Tensor(tensor_data, backend=self.backend)</span>

<span class="gd">-    def expand(self, other: Tensor) -&gt;Tensor:</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def make(</span>
<span class="gi">+        storage: Union[Storage, List[float]],</span>
<span class="gi">+        shape: UserShape,</span>
<span class="gi">+        strides: Optional[UserStrides] = None,</span>
<span class="gi">+        backend: Optional[TensorBackend] = None,</span>
<span class="gi">+    ) -&gt; Tensor:</span>
<span class="gi">+        &quot;Create a new tensor from data&quot;</span>
<span class="gi">+        return Tensor(TensorData(storage, shape, strides), backend=backend)</span>
<span class="gi">+</span>
<span class="gi">+    def expand(self, other: Tensor) -&gt; Tensor:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Method used to allow for backprop over broadcasting.
<span class="w"> </span>        This method is called when the output of `backward`
<span class="gu">@@ -185,9 +277,51 @@ class Tensor:</span>
<span class="w"> </span>            Expanded version of `other` with the right derivatives

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="gd">-    def accumulate_derivative(self, x: Any) -&gt;None:</span>
<span class="gi">+        # Case 1: Both the same shape.</span>
<span class="gi">+        if self.shape == other.shape:</span>
<span class="gi">+            return other</span>
<span class="gi">+</span>
<span class="gi">+        # Case 2: Backward is a smaller than self. Broadcast up.</span>
<span class="gi">+        true_shape = TensorData.shape_broadcast(self.shape, other.shape)</span>
<span class="gi">+        buf = self.zeros(true_shape)</span>
<span class="gi">+        self.backend.id_map(other, buf)</span>
<span class="gi">+        if self.shape == true_shape:</span>
<span class="gi">+            return buf</span>
<span class="gi">+</span>
<span class="gi">+        # Case 3: Still different, reduce extra dims.</span>
<span class="gi">+        out = buf</span>
<span class="gi">+        orig_shape = [1] * (len(out.shape) - len(self.shape)) + list(self.shape)</span>
<span class="gi">+        for dim, shape in enumerate(out.shape):</span>
<span class="gi">+            if orig_shape[dim] == 1 and shape != 1:</span>
<span class="gi">+                out = self.backend.add_reduce(out, dim)</span>
<span class="gi">+        assert out.size == self.size, f&quot;{out.shape} {self.shape}&quot;</span>
<span class="gi">+        # START CODE CHANGE (2021)</span>
<span class="gi">+        return Tensor.make(out._tensor._storage, self.shape, backend=self.backend)</span>
<span class="gi">+        # END CODE CHANGE (2021)</span>
<span class="gi">+</span>
<span class="gi">+    def zeros(self, shape: Optional[UserShape] = None) -&gt; Tensor:</span>
<span class="gi">+        def zero(shape: UserShape) -&gt; Tensor:</span>
<span class="gi">+            return Tensor.make(</span>
<span class="gi">+                [0.0] * int(operators.prod(shape)), shape, backend=self.backend</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if shape is None:</span>
<span class="gi">+            out = zero(self.shape)</span>
<span class="gi">+        else:</span>
<span class="gi">+            out = zero(shape)</span>
<span class="gi">+        out._type_(self.backend)</span>
<span class="gi">+        return out</span>
<span class="gi">+</span>
<span class="gi">+    def tuple(self) -&gt; Tuple[Storage, Shape, Strides]:</span>
<span class="gi">+        return self._tensor.tuple()</span>
<span class="gi">+</span>
<span class="gi">+    def detach(self) -&gt; Tensor:</span>
<span class="gi">+        return Tensor(self._tensor, backend=self.backend)</span>
<span class="gi">+</span>
<span class="gi">+    # Variable elements for backprop</span>
<span class="gi">+</span>
<span class="gi">+    def accumulate_derivative(self, x: Any) -&gt; None:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Add `val` to the the derivative accumulated on this variable.
<span class="w"> </span>        Should only be called during autodifferentiation on leaf variables.
<span class="gu">@@ -195,14 +329,46 @@ class Tensor:</span>
<span class="w"> </span>        Args:
<span class="w"> </span>            x : value to be accumulated
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert self.is_leaf(), &quot;Only leaf variables can have derivatives.&quot;</span>
<span class="gi">+        if self.grad is None:</span>
<span class="gi">+            self.grad = Tensor.make(</span>
<span class="gi">+                [0] * int(operators.prod(self.shape)), self.shape, backend=self.backend</span>
<span class="gi">+            )</span>
<span class="gi">+        self.grad += x</span>
<span class="gi">+</span>
<span class="gi">+    def is_leaf(self) -&gt; bool:</span>
<span class="gi">+        &quot;True if this variable created by the user (no `last_fn`)&quot;</span>
<span class="gi">+        return self.history is not None and self.history.last_fn is None</span>

<span class="gd">-    def is_leaf(self) -&gt;bool:</span>
<span class="gd">-        &quot;&quot;&quot;True if this variable created by the user (no `last_fn`)&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def is_constant(self) -&gt; bool:</span>
<span class="gi">+        return self.history is None</span>

<span class="gd">-    def zero_grad_(self) -&gt;None:</span>
<span class="gi">+    @property</span>
<span class="gi">+    def parents(self) -&gt; Iterable[Variable]:</span>
<span class="gi">+        assert self.history is not None</span>
<span class="gi">+        return self.history.inputs</span>
<span class="gi">+</span>
<span class="gi">+    def chain_rule(self, d_output: Any) -&gt; Iterable[Tuple[Variable, Any]]:</span>
<span class="gi">+        h = self.history</span>
<span class="gi">+        assert h is not None</span>
<span class="gi">+        assert h.last_fn is not None</span>
<span class="gi">+        assert h.ctx is not None</span>
<span class="gi">+</span>
<span class="gi">+        x = h.last_fn._backward(h.ctx, d_output)</span>
<span class="gi">+        assert len(x) == len(h.inputs), f&quot;Bug in function {h.last_fn}&quot;</span>
<span class="gi">+        return [</span>
<span class="gi">+            (inp, inp.expand(self._ensure_tensor(d_in)))</span>
<span class="gi">+            for inp, d_in in zip(h.inputs, x)</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+    def backward(self, grad_output: Optional[Tensor] = None) -&gt; None:</span>
<span class="gi">+        if grad_output is None:</span>
<span class="gi">+            assert self.shape == (1,), &quot;Must provide grad_output if non-scalar&quot;</span>
<span class="gi">+            grad_output = Tensor.make([1.0], (1,), backend=self.backend)</span>
<span class="gi">+        backpropagate(self, grad_output)</span>
<span class="gi">+</span>
<span class="gi">+    def zero_grad_(self) -&gt; None:  # pragma: no cover</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Reset the derivative on this variable.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.grad = None</span>
<span class="gh">diff --git a/minitorch/tensor_data.py b/minitorch/tensor_data.py</span>
<span class="gh">index a28b7f8..1d4a0c9 100644</span>
<span class="gd">--- a/minitorch/tensor_data.py</span>
<span class="gi">+++ b/minitorch/tensor_data.py</span>
<span class="gu">@@ -1,17 +1,21 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import random
<span class="w"> </span>from typing import Iterable, Optional, Sequence, Tuple, Union
<span class="gi">+</span>
<span class="w"> </span>import numba
<span class="w"> </span>import numpy as np
<span class="w"> </span>import numpy.typing as npt
<span class="w"> </span>from numpy import array, float64
<span class="w"> </span>from typing_extensions import TypeAlias
<span class="gi">+</span>
<span class="w"> </span>from .operators import prod
<span class="gi">+</span>
<span class="w"> </span>MAX_DIMS = 32


<span class="w"> </span>class IndexingError(RuntimeError):
<span class="gd">-    &quot;&quot;&quot;Exception raised for indexing errors.&quot;&quot;&quot;</span>
<span class="gi">+    &quot;Exception raised for indexing errors.&quot;</span>
<span class="w"> </span>    pass


<span class="gu">@@ -20,12 +24,13 @@ OutIndex: TypeAlias = npt.NDArray[np.int32]</span>
<span class="w"> </span>Index: TypeAlias = npt.NDArray[np.int32]
<span class="w"> </span>Shape: TypeAlias = npt.NDArray[np.int32]
<span class="w"> </span>Strides: TypeAlias = npt.NDArray[np.int32]
<span class="gi">+</span>
<span class="w"> </span>UserIndex: TypeAlias = Sequence[int]
<span class="w"> </span>UserShape: TypeAlias = Sequence[int]
<span class="w"> </span>UserStrides: TypeAlias = Sequence[int]


<span class="gd">-def index_to_position(index: Index, strides: Strides) -&gt;int:</span>
<span class="gi">+def index_to_position(index: Index, strides: Strides) -&gt; int:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Converts a multidimensional tensor `index` into a single-dimensional position in
<span class="w"> </span>    storage based on strides.
<span class="gu">@@ -37,10 +42,12 @@ def index_to_position(index: Index, strides: Strides) -&gt;int:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Position in storage
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    # TODO: Implement for Task 2.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 2.1&#39;)</span>

<span class="gd">-def to_index(ordinal: int, shape: Shape, out_index: OutIndex) -&gt;None:</span>
<span class="gi">+</span>
<span class="gi">+def to_index(ordinal: int, shape: Shape, out_index: OutIndex) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Convert an `ordinal` to an index in the `shape`.
<span class="w"> </span>    Should ensure that enumerating position 0 ... size of a
<span class="gu">@@ -53,11 +60,13 @@ def to_index(ordinal: int, shape: Shape, out_index: OutIndex) -&gt;None:</span>
<span class="w"> </span>        out_index : return index corresponding to position.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 2.1.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 2.1&#39;)</span>


<span class="gd">-def broadcast_index(big_index: Index, big_shape: Shape, shape: Shape,</span>
<span class="gd">-    out_index: OutIndex) -&gt;None:</span>
<span class="gi">+def broadcast_index(</span>
<span class="gi">+    big_index: Index, big_shape: Shape, shape: Shape, out_index: OutIndex</span>
<span class="gi">+) -&gt; None:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Convert a `big_index` into `big_shape` to a smaller `out_index`
<span class="w"> </span>    into `shape` following broadcasting rules. In this case
<span class="gu">@@ -74,10 +83,11 @@ def broadcast_index(big_index: Index, big_shape: Shape, shape: Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        None
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 2.2.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 2.2&#39;)</span>


<span class="gd">-def shape_broadcast(shape1: UserShape, shape2: UserShape) -&gt;UserShape:</span>
<span class="gi">+def shape_broadcast(shape1: UserShape, shape2: UserShape) -&gt; UserShape:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Broadcast two shapes to create a new union shape.

<span class="gu">@@ -91,7 +101,17 @@ def shape_broadcast(shape1: UserShape, shape2: UserShape) -&gt;UserShape:</span>
<span class="w"> </span>    Raises:
<span class="w"> </span>        IndexingError : if cannot broadcast
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # TODO: Implement for Task 2.2.</span>
<span class="gi">+    raise NotImplementedError(&#39;Need to implement for Task 2.2&#39;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def strides_from_shape(shape: UserShape) -&gt; UserStrides:</span>
<span class="gi">+    layout = [1]</span>
<span class="gi">+    offset = 1</span>
<span class="gi">+    for s in reversed(shape):</span>
<span class="gi">+        layout.append(s * offset)</span>
<span class="gi">+        offset = s * offset</span>
<span class="gi">+    return tuple(reversed(layout[:-1]))</span>


<span class="w"> </span>class TensorData:
<span class="gu">@@ -102,19 +122,24 @@ class TensorData:</span>
<span class="w"> </span>    shape: UserShape
<span class="w"> </span>    dims: int

<span class="gd">-    def __init__(self, storage: Union[Sequence[float], Storage], shape:</span>
<span class="gd">-        UserShape, strides: Optional[UserStrides]=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        storage: Union[Sequence[float], Storage],</span>
<span class="gi">+        shape: UserShape,</span>
<span class="gi">+        strides: Optional[UserStrides] = None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if isinstance(storage, np.ndarray):
<span class="w"> </span>            self._storage = storage
<span class="w"> </span>        else:
<span class="w"> </span>            self._storage = array(storage, dtype=float64)
<span class="gi">+</span>
<span class="w"> </span>        if strides is None:
<span class="w"> </span>            strides = strides_from_shape(shape)
<span class="gd">-        assert isinstance(strides, tuple), &#39;Strides must be tuple&#39;</span>
<span class="gd">-        assert isinstance(shape, tuple), &#39;Shape must be tuple&#39;</span>
<span class="gi">+</span>
<span class="gi">+        assert isinstance(strides, tuple), &quot;Strides must be tuple&quot;</span>
<span class="gi">+        assert isinstance(shape, tuple), &quot;Shape must be tuple&quot;</span>
<span class="w"> </span>        if len(strides) != len(shape):
<span class="gd">-            raise IndexingError(f&#39;Len of strides {strides} must match {shape}.&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+            raise IndexingError(f&quot;Len of strides {strides} must match {shape}.&quot;)</span>
<span class="w"> </span>        self._strides = array(strides)
<span class="w"> </span>        self._shape = array(shape)
<span class="w"> </span>        self.strides = strides
<span class="gu">@@ -123,16 +148,72 @@ class TensorData:</span>
<span class="w"> </span>        self.shape = shape
<span class="w"> </span>        assert len(self._storage) == self.size

<span class="gd">-    def is_contiguous(self) -&gt;bool:</span>
<span class="gi">+    def to_cuda_(self) -&gt; None:  # pragma: no cover</span>
<span class="gi">+        if not numba.cuda.is_cuda_array(self._storage):</span>
<span class="gi">+            self._storage = numba.cuda.to_device(self._storage)</span>
<span class="gi">+</span>
<span class="gi">+    def is_contiguous(self) -&gt; bool:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Check that the layout is contiguous, i.e. outer dimensions have bigger strides than inner dimensions.

<span class="w"> </span>        Returns:
<span class="w"> </span>            bool : True if contiguous
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def permute(self, *order: int) -&gt;TensorData:</span>
<span class="gi">+        last = 1e9</span>
<span class="gi">+        for stride in self._strides:</span>
<span class="gi">+            if stride &gt; last:</span>
<span class="gi">+                return False</span>
<span class="gi">+            last = stride</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def shape_broadcast(shape_a: UserShape, shape_b: UserShape) -&gt; UserShape:</span>
<span class="gi">+        return shape_broadcast(shape_a, shape_b)</span>
<span class="gi">+</span>
<span class="gi">+    def index(self, index: Union[int, UserIndex]) -&gt; int:</span>
<span class="gi">+        if isinstance(index, int):</span>
<span class="gi">+            aindex: Index = array([index])</span>
<span class="gi">+        if isinstance(index, tuple):</span>
<span class="gi">+            aindex = array(index)</span>
<span class="gi">+</span>
<span class="gi">+        # Pretend 0-dim shape is 1-dim shape of singleton</span>
<span class="gi">+        shape = self.shape</span>
<span class="gi">+        if len(shape) == 0 and len(aindex) != 0:</span>
<span class="gi">+            shape = (1,)</span>
<span class="gi">+</span>
<span class="gi">+        # Check for errors</span>
<span class="gi">+        if aindex.shape[0] != len(self.shape):</span>
<span class="gi">+            raise IndexingError(f&quot;Index {aindex} must be size of {self.shape}.&quot;)</span>
<span class="gi">+        for i, ind in enumerate(aindex):</span>
<span class="gi">+            if ind &gt;= self.shape[i]:</span>
<span class="gi">+                raise IndexingError(f&quot;Index {aindex} out of range {self.shape}.&quot;)</span>
<span class="gi">+            if ind &lt; 0:</span>
<span class="gi">+                raise IndexingError(f&quot;Negative indexing for {aindex} not supported.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # Call fast indexing.</span>
<span class="gi">+        return index_to_position(array(index), self._strides)</span>
<span class="gi">+</span>
<span class="gi">+    def indices(self) -&gt; Iterable[UserIndex]:</span>
<span class="gi">+        lshape: Shape = array(self.shape)</span>
<span class="gi">+        out_index: Index = array(self.shape)</span>
<span class="gi">+        for i in range(self.size):</span>
<span class="gi">+            to_index(i, lshape, out_index)</span>
<span class="gi">+            yield tuple(out_index)</span>
<span class="gi">+</span>
<span class="gi">+    def sample(self) -&gt; UserIndex:</span>
<span class="gi">+        return tuple((random.randint(0, s - 1) for s in self.shape))</span>
<span class="gi">+</span>
<span class="gi">+    def get(self, key: UserIndex) -&gt; float:</span>
<span class="gi">+        x: float = self._storage[self.index(key)]</span>
<span class="gi">+        return x</span>
<span class="gi">+</span>
<span class="gi">+    def set(self, key: UserIndex, val: float) -&gt; None:</span>
<span class="gi">+        self._storage[self.index(key)] = val</span>
<span class="gi">+</span>
<span class="gi">+    def tuple(self) -&gt; Tuple[Storage, Shape, Strides]:</span>
<span class="gi">+        return (self._storage, self._shape, self._strides)</span>
<span class="gi">+</span>
<span class="gi">+    def permute(self, *order: int) -&gt; TensorData:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Permute the dimensions of the tensor.

<span class="gu">@@ -142,4 +223,33 @@ class TensorData:</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            New `TensorData` with the same storage and a new dimension order.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        assert list(sorted(order)) == list(</span>
<span class="gi">+            range(len(self.shape))</span>
<span class="gi">+        ), f&quot;Must give a position to each dimension. Shape: {self.shape} Order: {order}&quot;</span>
<span class="gi">+</span>
<span class="gi">+        # TODO: Implement for Task 2.1.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.1&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    def to_string(self) -&gt; str:</span>
<span class="gi">+        s = &quot;&quot;</span>
<span class="gi">+        for index in self.indices():</span>
<span class="gi">+            l = &quot;&quot;</span>
<span class="gi">+            for i in range(len(index) - 1, -1, -1):</span>
<span class="gi">+                if index[i] == 0:</span>
<span class="gi">+                    l = &quot;\n%s[&quot; % (&quot;\t&quot; * i) + l</span>
<span class="gi">+                else:</span>
<span class="gi">+                    break</span>
<span class="gi">+            s += l</span>
<span class="gi">+            v = self.get(index)</span>
<span class="gi">+            s += f&quot;{v:3.2f}&quot;</span>
<span class="gi">+            l = &quot;&quot;</span>
<span class="gi">+            for i in range(len(index) - 1, -1, -1):</span>
<span class="gi">+                if index[i] == self.shape[i] - 1:</span>
<span class="gi">+                    l += &quot;]&quot;</span>
<span class="gi">+                else:</span>
<span class="gi">+                    break</span>
<span class="gi">+            if l:</span>
<span class="gi">+                s += l</span>
<span class="gi">+            else:</span>
<span class="gi">+                s += &quot; &quot;</span>
<span class="gi">+        return s</span>
<span class="gh">diff --git a/minitorch/tensor_functions.py b/minitorch/tensor_functions.py</span>
<span class="gh">index 7602588..f1c0547 100644</span>
<span class="gd">--- a/minitorch/tensor_functions.py</span>
<span class="gi">+++ b/minitorch/tensor_functions.py</span>
<span class="gu">@@ -1,98 +1,279 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Implementation of the autodifferentiation Functions for Tensor.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>import random
<span class="w"> </span>from typing import TYPE_CHECKING
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="gi">+</span>
<span class="w"> </span>import minitorch
<span class="gi">+</span>
<span class="w"> </span>from . import operators
<span class="w"> </span>from .autodiff import Context
<span class="w"> </span>from .tensor_ops import SimpleBackend, TensorBackend
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from typing import Any, List, Tuple
<span class="gi">+</span>
<span class="w"> </span>    from .tensor import Tensor
<span class="w"> </span>    from .tensor_data import UserIndex, UserShape


<span class="gd">-def wrap_tuple(x):</span>
<span class="gd">-    &quot;&quot;&quot;Turn a possible value into a tuple&quot;&quot;&quot;</span>
<span class="gd">-    pass</span>
<span class="gi">+def wrap_tuple(x):  # type: ignore</span>
<span class="gi">+    &quot;Turn a possible value into a tuple&quot;</span>
<span class="gi">+    if isinstance(x, tuple):</span>
<span class="gi">+        return x</span>
<span class="gi">+    return (x,)</span>


<span class="gi">+# Constructors</span>
<span class="w"> </span>class Function:
<span class="gd">-    pass</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _backward(cls, ctx: Context, grad_out: Tensor) -&gt; Tuple[Tensor, ...]:</span>
<span class="gi">+        return wrap_tuple(cls.backward(ctx, grad_out))  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _forward(cls, ctx: Context, *inps: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return cls.forward(ctx, *inps)  # type: ignore</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def apply(cls, *vals: Tensor) -&gt; Tensor:</span>
<span class="gi">+        raw_vals = []</span>
<span class="gi">+        need_grad = False</span>
<span class="gi">+        for v in vals:</span>
<span class="gi">+            if v.requires_grad():</span>
<span class="gi">+                need_grad = True</span>
<span class="gi">+            raw_vals.append(v.detach())</span>
<span class="gi">+</span>
<span class="gi">+        # Create the context.</span>
<span class="gi">+        ctx = Context(not need_grad)</span>
<span class="gi">+</span>
<span class="gi">+        # Call forward with the variables.</span>
<span class="gi">+        c = cls._forward(ctx, *raw_vals)</span>
<span class="gi">+        # assert isinstance(c, Tensor), &quot;Expected return type Tensor got %s&quot; % (</span>
<span class="gi">+        #     type(c)</span>
<span class="gi">+        # )</span>
<span class="gi">+</span>
<span class="gi">+        # Create a new variable from the result with a new history.</span>
<span class="gi">+        back = None</span>
<span class="gi">+        if need_grad:</span>
<span class="gi">+            back = minitorch.History(cls, ctx, vals)</span>
<span class="gi">+        return minitorch.Tensor(c._tensor, back, backend=c.backend)</span>


<span class="w"> </span>class Neg(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return t1.f.neg_map(t1)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return grad_output.f.neg_map(grad_output)</span>


<span class="w"> </span>class Inv(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        ctx.save_for_backward(t1)</span>
<span class="gi">+        return t1.f.inv_map(t1)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        (t1,) = ctx.saved_values</span>
<span class="gi">+        return grad_output.f.inv_back_zip(t1, grad_output)</span>


<span class="w"> </span>class Add(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor, t2: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return t1.f.add_zip(t1, t2)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        return grad_output, grad_output</span>


<span class="w"> </span>class Mul(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class Sigmoid(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class ReLU(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class Log(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class Exp(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class Sum(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, dim: Tensor) -&gt; Tensor:</span>
<span class="gi">+        ctx.save_for_backward(a.shape, dim)</span>
<span class="gi">+        return a.f.add_reduce(a, int(dim.item()))</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, float]:</span>
<span class="gi">+        a_shape, dim = ctx.saved_values</span>
<span class="gi">+        return grad_output, 0.0</span>


<span class="w"> </span>class All(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, dim: Tensor) -&gt; Tensor:</span>
<span class="gi">+        if dim is not None:</span>
<span class="gi">+            return a.f.mul_reduce(a, int(dim.item()))</span>
<span class="gi">+        else:</span>
<span class="gi">+            return a.f.mul_reduce(a.contiguous().view(int(operators.prod(a.shape))), 0)</span>


<span class="w"> </span>class LT(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class EQ(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class IsClose(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>


<span class="w"> </span>class Permute(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, order: Tensor) -&gt; Tensor:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, float]:</span>
<span class="gi">+        # TODO: Implement for Task 2.4.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.4&#39;)</span>


<span class="w"> </span>class View(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor, shape: Tensor) -&gt; Tensor:</span>
<span class="gi">+        ctx.save_for_backward(a.shape)</span>
<span class="gi">+        assert a._tensor.is_contiguous(), &quot;Must be contiguous to view&quot;</span>
<span class="gi">+        shape2 = [int(shape[i]) for i in range(shape.size)]</span>
<span class="gi">+        return minitorch.Tensor.make(</span>
<span class="gi">+            a._tensor._storage, tuple(shape2), backend=a.backend</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, float]:</span>
<span class="gi">+        (original,) = ctx.saved_values</span>
<span class="gi">+        return (</span>
<span class="gi">+            minitorch.Tensor.make(</span>
<span class="gi">+                grad_output._tensor._storage, original, backend=grad_output.backend</span>
<span class="gi">+            ),</span>
<span class="gi">+            0.0,</span>
<span class="gi">+        )</span>


<span class="w"> </span>class Copy(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, a: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return a.f.id_map(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tensor:</span>
<span class="gi">+        return grad_output</span>


<span class="w"> </span>class MatMul(Function):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def forward(ctx: Context, t1: Tensor, t2: Tensor) -&gt; Tensor:</span>
<span class="gi">+        ctx.save_for_backward(t1, t2)</span>
<span class="gi">+        return t1.f.matrix_multiply(t1, t2)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="gi">+        t1, t2 = ctx.saved_values</span>

<span class="gi">+        def transpose(a: Tensor) -&gt; Tensor:</span>
<span class="gi">+            order = list(range(a.dims))</span>
<span class="gi">+            order[-2], order[-1] = order[-1], order[-2]</span>
<span class="gi">+            return a._new(a._tensor.permute(*order))</span>

<span class="gd">-def zeros(shape: UserShape, backend: TensorBackend=SimpleBackend) -&gt;Tensor:</span>
<span class="gi">+        return (</span>
<span class="gi">+            grad_output.f.matrix_multiply(grad_output, transpose(t2)),</span>
<span class="gi">+            grad_output.f.matrix_multiply(transpose(t1), grad_output),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Helpers for Constructing tensors</span>
<span class="gi">+def zeros(shape: UserShape, backend: TensorBackend = SimpleBackend) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Produce a zero tensor of size `shape`.

<span class="gu">@@ -103,11 +284,16 @@ def zeros(shape: UserShape, backend: TensorBackend=SimpleBackend) -&gt;Tensor:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        new tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return minitorch.Tensor.make(</span>
<span class="gi">+        [0] * int(operators.prod(shape)), shape, backend=backend</span>
<span class="gi">+    )</span>


<span class="gd">-def rand(shape: UserShape, backend: TensorBackend=SimpleBackend,</span>
<span class="gd">-    requires_grad: bool=False) -&gt;Tensor:</span>
<span class="gi">+def rand(</span>
<span class="gi">+    shape: UserShape,</span>
<span class="gi">+    backend: TensorBackend = SimpleBackend,</span>
<span class="gi">+    requires_grad: bool = False,</span>
<span class="gi">+) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Produce a random tensor of size `shape`.

<span class="gu">@@ -119,11 +305,18 @@ def rand(shape: UserShape, backend: TensorBackend=SimpleBackend,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        :class:`Tensor` : new tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-def _tensor(ls: Any, shape: UserShape, backend: TensorBackend=SimpleBackend,</span>
<span class="gd">-    requires_grad: bool=False) -&gt;Tensor:</span>
<span class="gi">+    vals = [random.random() for _ in range(int(operators.prod(shape)))]</span>
<span class="gi">+    tensor = minitorch.Tensor.make(vals, shape, backend=backend)</span>
<span class="gi">+    tensor.requires_grad_(requires_grad)</span>
<span class="gi">+    return tensor</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _tensor(</span>
<span class="gi">+    ls: Any,</span>
<span class="gi">+    shape: UserShape,</span>
<span class="gi">+    backend: TensorBackend = SimpleBackend,</span>
<span class="gi">+    requires_grad: bool = False,</span>
<span class="gi">+) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Produce a tensor with data ls and shape `shape`.

<span class="gu">@@ -136,11 +329,14 @@ def _tensor(ls: Any, shape: UserShape, backend: TensorBackend=SimpleBackend,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        new tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    tensor = minitorch.Tensor.make(ls, shape, backend=backend)</span>
<span class="gi">+    tensor.requires_grad_(requires_grad)</span>
<span class="gi">+    return tensor</span>


<span class="gd">-def tensor(ls: Any, backend: TensorBackend=SimpleBackend, requires_grad:</span>
<span class="gd">-    bool=False) -&gt;Tensor:</span>
<span class="gi">+def tensor(</span>
<span class="gi">+    ls: Any, backend: TensorBackend = SimpleBackend, requires_grad: bool = False</span>
<span class="gi">+) -&gt; Tensor:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Produce a tensor with data and shape from ls

<span class="gu">@@ -152,4 +348,66 @@ def tensor(ls: Any, backend: TensorBackend=SimpleBackend, requires_grad:</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        :class:`Tensor` : new tensor
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def shape(ls: Any) -&gt; List[int]:</span>
<span class="gi">+        if isinstance(ls, (list, tuple)):</span>
<span class="gi">+            return [len(ls)] + shape(ls[0])</span>
<span class="gi">+        else:</span>
<span class="gi">+            return []</span>
<span class="gi">+</span>
<span class="gi">+    def flatten(ls: Any) -&gt; List[float]:</span>
<span class="gi">+        if isinstance(ls, (list, tuple)):</span>
<span class="gi">+            return [y for x in ls for y in flatten(x)]</span>
<span class="gi">+        else:</span>
<span class="gi">+            return [ls]</span>
<span class="gi">+</span>
<span class="gi">+    cur = flatten(ls)</span>
<span class="gi">+    shape2 = shape(ls)</span>
<span class="gi">+    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Gradient check for tensors</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def grad_central_difference(</span>
<span class="gi">+    f: Any, *vals: Tensor, arg: int = 0, epsilon: float = 1e-6, ind: UserIndex</span>
<span class="gi">+) -&gt; float:</span>
<span class="gi">+    x = vals[arg]</span>
<span class="gi">+    up = zeros(x.shape)</span>
<span class="gi">+    up[ind] = epsilon</span>
<span class="gi">+    vals1 = [x if j != arg else x + up for j, x in enumerate(vals)]</span>
<span class="gi">+    vals2 = [x if j != arg else x - up for j, x in enumerate(vals)]</span>
<span class="gi">+    delta: Tensor = f(*vals1).sum() - f(*vals2).sum()</span>
<span class="gi">+</span>
<span class="gi">+    return delta[0] / (2.0 * epsilon)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def grad_check(f: Any, *vals: Tensor) -&gt; None:</span>
<span class="gi">+    for x in vals:</span>
<span class="gi">+        x.requires_grad_(True)</span>
<span class="gi">+        x.zero_grad_()</span>
<span class="gi">+    random.seed(10)</span>
<span class="gi">+    out = f(*vals)</span>
<span class="gi">+    out.sum().backward()</span>
<span class="gi">+    err_msg = &quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+Gradient check error for function %s.</span>
<span class="gi">+</span>
<span class="gi">+Input %s</span>
<span class="gi">+</span>
<span class="gi">+Received derivative %f for argument %d and index %s,</span>
<span class="gi">+but was expecting derivative %f from central difference.</span>
<span class="gi">+</span>
<span class="gi">+&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+    for i, x in enumerate(vals):</span>
<span class="gi">+        ind = x._tensor.sample()</span>
<span class="gi">+        check = grad_central_difference(f, *vals, arg=i, ind=ind)</span>
<span class="gi">+        assert x.grad is not None</span>
<span class="gi">+        np.testing.assert_allclose(</span>
<span class="gi">+            x.grad[ind],</span>
<span class="gi">+            check,</span>
<span class="gi">+            1e-2,</span>
<span class="gi">+            1e-2,</span>
<span class="gi">+            err_msg=err_msg % (f, vals, x.grad[ind], i, ind, check),</span>
<span class="gi">+        )</span>
<span class="gh">diff --git a/minitorch/tensor_ops.py b/minitorch/tensor_ops.py</span>
<span class="gh">index e10ff6c..db82d54 100644</span>
<span class="gd">--- a/minitorch/tensor_ops.py</span>
<span class="gi">+++ b/minitorch/tensor_ops.py</span>
<span class="gu">@@ -1,26 +1,56 @@</span>
<span class="w"> </span>from __future__ import annotations
<span class="gi">+</span>
<span class="w"> </span>from typing import TYPE_CHECKING, Callable, Optional, Type
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from typing_extensions import Protocol
<span class="gi">+</span>
<span class="w"> </span>from . import operators
<span class="gd">-from .tensor_data import MAX_DIMS, broadcast_index, index_to_position, shape_broadcast, to_index</span>
<span class="gi">+from .tensor_data import (</span>
<span class="gi">+    MAX_DIMS,</span>
<span class="gi">+    broadcast_index,</span>
<span class="gi">+    index_to_position,</span>
<span class="gi">+    shape_broadcast,</span>
<span class="gi">+    to_index,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>if TYPE_CHECKING:
<span class="w"> </span>    from .tensor import Tensor
<span class="w"> </span>    from .tensor_data import Index, Shape, Storage, Strides


<span class="w"> </span>class MapProto(Protocol):
<span class="gd">-</span>
<span class="gd">-    def __call__(self, x: Tensor, out: Optional[Tensor]=..., /) -&gt;Tensor:</span>
<span class="gi">+    def __call__(self, x: Tensor, out: Optional[Tensor] = ..., /) -&gt; Tensor:</span>
<span class="w"> </span>        ...


<span class="w"> </span>class TensorOps:
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def map(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def cmap(fn: Callable[[float], float]) -&gt; Callable[[Tensor, Tensor], Tensor]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def zip(fn: Callable[[float, float], float]) -&gt; Callable[[Tensor, Tensor], Tensor]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def reduce(</span>
<span class="gi">+        fn: Callable[[float, float], float], start: float = 0.0</span>
<span class="gi">+    ) -&gt; Callable[[Tensor, int], Tensor]:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def matrix_multiply(a: Tensor, b: Tensor) -&gt; Tensor:</span>
<span class="gi">+        raise NotImplementedError(&quot;Not implemented in this assignment&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>    cuda = False


<span class="w"> </span>class TensorBackend:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self, ops: Type[TensorOps]):
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Dynamically construct a tensor backend based on a `tensor_ops` object
<span class="gu">@@ -34,6 +64,8 @@ class TensorBackend:</span>
<span class="w"> </span>            A collection of tensor functions

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+        # Maps</span>
<span class="w"> </span>        self.neg_map = ops.map(operators.neg)
<span class="w"> </span>        self.sigmoid_map = ops.map(operators.sigmoid)
<span class="w"> </span>        self.relu_map = ops.map(operators.relu)
<span class="gu">@@ -42,6 +74,8 @@ class TensorBackend:</span>
<span class="w"> </span>        self.id_map = ops.map(operators.id)
<span class="w"> </span>        self.id_cmap = ops.cmap(operators.id)
<span class="w"> </span>        self.inv_map = ops.map(operators.inv)
<span class="gi">+</span>
<span class="gi">+        # Zips</span>
<span class="w"> </span>        self.add_zip = ops.zip(operators.add)
<span class="w"> </span>        self.mul_zip = ops.zip(operators.mul)
<span class="w"> </span>        self.lt_zip = ops.zip(operators.lt)
<span class="gu">@@ -50,6 +84,8 @@ class TensorBackend:</span>
<span class="w"> </span>        self.relu_back_zip = ops.zip(operators.relu_back)
<span class="w"> </span>        self.log_back_zip = ops.zip(operators.log_back)
<span class="w"> </span>        self.inv_back_zip = ops.zip(operators.inv_back)
<span class="gi">+</span>
<span class="gi">+        # Reduce</span>
<span class="w"> </span>        self.add_reduce = ops.reduce(operators.add, 0.0)
<span class="w"> </span>        self.mul_reduce = ops.reduce(operators.mul, 1.0)
<span class="w"> </span>        self.matrix_multiply = ops.matrix_multiply
<span class="gu">@@ -57,9 +93,8 @@ class TensorBackend:</span>


<span class="w"> </span>class SimpleOps(TensorOps):
<span class="gd">-</span>
<span class="w"> </span>    @staticmethod
<span class="gd">-    def map(fn: Callable[[float], float]) -&gt;MapProto:</span>
<span class="gi">+    def map(fn: Callable[[float], float]) -&gt; MapProto:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Higher-order tensor map function ::

<span class="gu">@@ -88,11 +123,21 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            new tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        f = tensor_map(fn)</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: Tensor, out: Optional[Tensor] = None) -&gt; Tensor:</span>
<span class="gi">+            if out is None:</span>
<span class="gi">+                out = a.zeros(a.shape)</span>
<span class="gi">+            f(*out.tuple(), *a.tuple())</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def zip(fn: Callable[[float, float], float]) -&gt;Callable[[&#39;Tensor&#39;,</span>
<span class="gd">-        &#39;Tensor&#39;], &#39;Tensor&#39;]:</span>
<span class="gi">+    def zip(</span>
<span class="gi">+        fn: Callable[[float, float], float]</span>
<span class="gi">+    ) -&gt; Callable[[&quot;Tensor&quot;, &quot;Tensor&quot;], &quot;Tensor&quot;]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Higher-order tensor zip function ::

<span class="gu">@@ -120,11 +165,24 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            :class:`TensorData` : new tensor data
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        f = tensor_zip(fn)</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: &quot;Tensor&quot;, b: &quot;Tensor&quot;) -&gt; &quot;Tensor&quot;:</span>
<span class="gi">+            if a.shape != b.shape:</span>
<span class="gi">+                c_shape = shape_broadcast(a.shape, b.shape)</span>
<span class="gi">+            else:</span>
<span class="gi">+                c_shape = a.shape</span>
<span class="gi">+            out = a.zeros(c_shape)</span>
<span class="gi">+            f(*out.tuple(), *a.tuple(), *b.tuple())</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def reduce(fn: Callable[[float, float], float], start: float=0.0</span>
<span class="gd">-        ) -&gt;Callable[[&#39;Tensor&#39;, int], &#39;Tensor&#39;]:</span>
<span class="gi">+    def reduce(</span>
<span class="gi">+        fn: Callable[[float, float], float], start: float = 0.0</span>
<span class="gi">+    ) -&gt; Callable[[&quot;Tensor&quot;, int], &quot;Tensor&quot;]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Higher-order tensor reduce function. ::

<span class="gu">@@ -147,12 +205,34 @@ class SimpleOps(TensorOps):</span>
<span class="w"> </span>        Returns:
<span class="w"> </span>            :class:`TensorData` : new tensor
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        f = tensor_reduce(fn)</span>
<span class="gi">+</span>
<span class="gi">+        def ret(a: &quot;Tensor&quot;, dim: int) -&gt; &quot;Tensor&quot;:</span>
<span class="gi">+            out_shape = list(a.shape)</span>
<span class="gi">+            out_shape[dim] = 1</span>
<span class="gi">+</span>
<span class="gi">+            # Other values when not sum.</span>
<span class="gi">+            out = a.zeros(tuple(out_shape))</span>
<span class="gi">+            out._tensor._storage[:] = start</span>
<span class="gi">+</span>
<span class="gi">+            f(*out.tuple(), *a.tuple(), dim)</span>
<span class="gi">+            return out</span>
<span class="gi">+</span>
<span class="gi">+        return ret</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def matrix_multiply(a: &quot;Tensor&quot;, b: &quot;Tensor&quot;) -&gt; &quot;Tensor&quot;:</span>
<span class="gi">+        raise NotImplementedError(&quot;Not implemented in this assignment&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>    is_cuda = False


<span class="gd">-def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="gd">-    Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+# Implementations.</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_map(</span>
<span class="gi">+    fn: Callable[[float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Low-level implementation of tensor map between
<span class="w"> </span>    tensors with *possibly different strides*.
<span class="gu">@@ -175,11 +255,26 @@ def tensor_map(fn: Callable[[float], float]) -&gt;Callable[[Storage, Shape,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor map function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None]:</span>
<span class="gi">+    def _map(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        in_storage: Storage,</span>
<span class="gi">+        in_shape: Shape,</span>
<span class="gi">+        in_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return _map</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_zip(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[</span>
<span class="gi">+    [Storage, Shape, Strides, Storage, Shape, Strides, Storage, Shape, Strides], None</span>
<span class="gi">+]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Low-level implementation of tensor zip between
<span class="w"> </span>    tensors with *possibly different strides*.
<span class="gu">@@ -202,11 +297,27 @@ def tensor_zip(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor zip function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gd">-</span>
<span class="gd">-def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="gd">-    Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="gi">+    def _zip(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        b_storage: Storage,</span>
<span class="gi">+        b_shape: Shape,</span>
<span class="gi">+        b_strides: Strides,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return _zip</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def tensor_reduce(</span>
<span class="gi">+    fn: Callable[[float, float], float]</span>
<span class="gi">+) -&gt; Callable[[Storage, Shape, Strides, Storage, Shape, Strides, int], None]:</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Low-level implementation of tensor reduce.

<span class="gu">@@ -219,7 +330,20 @@ def tensor_reduce(fn: Callable[[float, float], float]) -&gt;Callable[[Storage,</span>
<span class="w"> </span>    Returns:
<span class="w"> </span>        Tensor reduce function.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def _reduce(</span>
<span class="gi">+        out: Storage,</span>
<span class="gi">+        out_shape: Shape,</span>
<span class="gi">+        out_strides: Strides,</span>
<span class="gi">+        a_storage: Storage,</span>
<span class="gi">+        a_shape: Shape,</span>
<span class="gi">+        a_strides: Strides,</span>
<span class="gi">+        reduce_dim: int,</span>
<span class="gi">+    ) -&gt; None:</span>
<span class="gi">+        # TODO: Implement for Task 2.3.</span>
<span class="gi">+        raise NotImplementedError(&#39;Need to implement for Task 2.3&#39;)</span>
<span class="gi">+</span>
<span class="gi">+    return _reduce</span>


<span class="w"> </span>SimpleBackend = TensorBackend(SimpleOps)
<span class="gh">diff --git a/minitorch/testing.py b/minitorch/testing.py</span>
<span class="gh">index bc2dc74..add0003 100644</span>
<span class="gd">--- a/minitorch/testing.py</span>
<span class="gi">+++ b/minitorch/testing.py</span>
<span class="gu">@@ -1,93 +1,213 @@</span>
<span class="gi">+# type: ignore</span>
<span class="gi">+</span>
<span class="w"> </span>from typing import Callable, Generic, Iterable, Tuple, TypeVar
<span class="gi">+</span>
<span class="w"> </span>import minitorch.operators as operators
<span class="gd">-A = TypeVar(&#39;A&#39;)</span>
<span class="gi">+</span>
<span class="gi">+A = TypeVar(&quot;A&quot;)</span>


<span class="w"> </span>class MathTest(Generic[A]):
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def neg(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Negate the argument&quot;</span>
<span class="gi">+        return -a</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def addConstant(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Add contant to the argument&quot;</span>
<span class="gi">+        return 5 + a</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def square(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Manual square&quot;</span>
<span class="gi">+        return a * a</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def cube(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Manual cube&quot;</span>
<span class="gi">+        return a * a * a</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def subConstant(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Subtract a constant from the argument&quot;</span>
<span class="gi">+        return a - 5</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def neg(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Negate the argument&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def multConstant(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Multiply a constant to the argument&quot;</span>
<span class="gi">+        return 5 * a</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def addConstant(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Add contant to the argument&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def div(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Divide by a constant&quot;</span>
<span class="gi">+        return a / 5</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def square(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Manual square&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def inv(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Invert after adding&quot;</span>
<span class="gi">+        return operators.inv(a + 3.5)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def cube(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Manual cube&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def sig(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Apply sigmoid&quot;</span>
<span class="gi">+        return operators.sigmoid(a)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def subConstant(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Subtract a constant from the argument&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def log(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Apply log to a large value&quot;</span>
<span class="gi">+        return operators.log(a + 100000)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def multConstant(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Multiply a constant to the argument&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def relu(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Apply relu&quot;</span>
<span class="gi">+        return operators.relu(a + 5.5)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def div(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Divide by a constant&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def exp(a: A) -&gt; A:</span>
<span class="gi">+        &quot;Apply exp to a smaller value&quot;</span>
<span class="gi">+        return operators.exp(a - 200)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def inv(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Invert after adding&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def explog(a: A) -&gt; A:</span>
<span class="gi">+        return operators.log(a + 100000) + operators.exp(a - 200)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def sig(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Apply sigmoid&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def add2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        &quot;Add two arguments&quot;</span>
<span class="gi">+        return a + b</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def log(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Apply log to a large value&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def mul2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        &quot;Mul two arguments&quot;</span>
<span class="gi">+        return a * b</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def relu(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Apply relu&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def div2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        &quot;Divide two arguments&quot;</span>
<span class="gi">+        return a / (b + 5.5)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def exp(a: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Apply exp to a smaller value&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def gt2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        return operators.lt(b, a + 1.2)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def add2(a: A, b: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Add two arguments&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def lt2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        return operators.lt(a + 1.2, b)</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def mul2(a: A, b: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Mul two arguments&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def eq2(a: A, b: A) -&gt; A:</span>
<span class="gi">+        return operators.eq(a, (b + 5.5))</span>

<span class="w"> </span>    @staticmethod
<span class="gd">-    def div2(a: A, b: A) -&gt;A:</span>
<span class="gd">-        &quot;&quot;&quot;Divide two arguments&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+    def sum_red(a: Iterable[A]) -&gt; A:</span>
<span class="gi">+        return operators.sum(a)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def mean_red(a: Iterable[A]) -&gt; A:</span>
<span class="gi">+        return operators.sum(a) / float(len(a))</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def mean_full_red(a: Iterable[A]) -&gt; A:</span>
<span class="gi">+        return operators.sum(a) / float(len(a))</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def complex(a: A) -&gt; A:</span>
<span class="gi">+        return (</span>
<span class="gi">+            operators.log(</span>
<span class="gi">+                operators.sigmoid(</span>
<span class="gi">+                    operators.relu(operators.relu(a * 10 + 7) * 6 + 5) * 10</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+            / 50</span>
<span class="gi">+        )</span>

<span class="w"> </span>    @classmethod
<span class="gd">-    def _tests(cls) -&gt;Tuple[Tuple[str, Callable[[A], A]], Tuple[str,</span>
<span class="gd">-        Callable[[A, A], A]], Tuple[str, Callable[[Iterable[A]], A]]]:</span>
<span class="gi">+    def _tests(</span>
<span class="gi">+        cls,</span>
<span class="gi">+    ) -&gt; Tuple[</span>
<span class="gi">+        Tuple[str, Callable[[A], A]],</span>
<span class="gi">+        Tuple[str, Callable[[A, A], A]],</span>
<span class="gi">+        Tuple[str, Callable[[Iterable[A]], A]],</span>
<span class="gi">+    ]:</span>
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        Returns a list of all the math tests.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        one_arg = []</span>
<span class="gi">+        two_arg = []</span>
<span class="gi">+        red_arg = []</span>
<span class="gi">+        for k in dir(MathTest):</span>
<span class="gi">+            if callable(getattr(MathTest, k)) and not k.startswith(&quot;_&quot;):</span>
<span class="gi">+                base_fn = getattr(cls, k)</span>
<span class="gi">+                # scalar_fn = getattr(cls, k)</span>
<span class="gi">+                tup = (k, base_fn)</span>
<span class="gi">+                if k.endswith(&quot;2&quot;):</span>
<span class="gi">+                    two_arg.append(tup)</span>
<span class="gi">+                elif k.endswith(&quot;red&quot;):</span>
<span class="gi">+                    red_arg.append(tup)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    one_arg.append(tup)</span>
<span class="gi">+        return one_arg, two_arg, red_arg</span>
<span class="gi">+</span>
<span class="gi">+    @classmethod</span>
<span class="gi">+    def _comp_testing(cls):</span>
<span class="gi">+        one_arg, two_arg, red_arg = cls._tests()</span>
<span class="gi">+        one_argv, two_argv, red_argv = MathTest._tests()</span>
<span class="gi">+        one_arg = [(n1, f2, f1) for (n1, f1), (n2, f2) in zip(one_arg, one_argv)]</span>
<span class="gi">+        two_arg = [(n1, f2, f1) for (n1, f1), (n2, f2) in zip(two_arg, two_argv)]</span>
<span class="gi">+        red_arg = [(n1, f2, f1) for (n1, f1), (n2, f2) in zip(red_arg, red_argv)]</span>
<span class="gi">+        return one_arg, two_arg, red_arg</span>


<span class="w"> </span>class MathTestVariable(MathTest):
<span class="gd">-    pass</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def inv(a):</span>
<span class="gi">+        return 1.0 / (a + 3.5)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def sig(x):</span>
<span class="gi">+        return x.sigmoid()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def log(x):</span>
<span class="gi">+        return (x + 100000).log()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def relu(x):</span>
<span class="gi">+        return (x + 5.5).relu()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def exp(a):</span>
<span class="gi">+        return (a - 200).exp()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def explog(a):</span>
<span class="gi">+        return (a + 100000).log() + (a - 200).exp()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def sum_red(a):</span>
<span class="gi">+        return a.sum(0)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def mean_red(a):</span>
<span class="gi">+        return a.mean(0)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def mean_full_red(a):</span>
<span class="gi">+        return a.mean()</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def eq2(a, b):</span>
<span class="gi">+        return a == (b + 5.5)</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def gt2(a, b):</span>
<span class="gi">+        return a + 1.2 &gt; b</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def lt2(a, b):</span>
<span class="gi">+        return a + 1.2 &lt; b</span>
<span class="gi">+</span>
<span class="gi">+    @staticmethod</span>
<span class="gi">+    def complex(a):</span>
<span class="gi">+        return (((a * 10 + 7).relu() * 6 + 5).relu() * 10).sigmoid().log() / 50</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
    
  </body>
</html>