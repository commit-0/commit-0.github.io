
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.37">
    
    
      
        <title>Analysis commit0 all reference imbalanced learn - Commit-0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reference-gold-imbalanced-learn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Commit-0" class="md-header__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Commit-0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Analysis commit0 all reference imbalanced learn
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Commit-0" class="md-nav__button md-logo" aria-label="Commit-0" data-md-component="logo">
      
  <img src="../logo2.webp" alt="logo">

    </a>
    Commit-0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setupdist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Commit0
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytest-summary-for-test-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Pytest Summary for test tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytests" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytests:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytests:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_compatibility_sklearnrandomoversamplerrandom_state0-check_complex_data" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_compatibility_sklearn[RandomOverSampler(random_state=0)-check_complex_data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_compatibility_sklearnrandomundersamplerrandom_state0-check_complex_data" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_compatibility_sklearn[RandomUnderSampler(random_state=0)-check_complex_data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnallknn-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[AllKNN()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnborderlinesmoterandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[BorderlineSMOTE(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnclustercentroidsrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[ClusterCentroids(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearncondensednearestneighbourrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[CondensedNearestNeighbour(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearneditednearestneighbours-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[EditedNearestNeighbours()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnfunctionsampler-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[FunctionSampler()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearninstancehardnessthresholdrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[InstanceHardnessThreshold(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnkmeanssmoterandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[KMeansSMOTE(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmiss-check_samplers_fit_resample" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_fit_resample]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmiss-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmissversion2-check_samplers_fit_resample" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_fit_resample]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmissversion2-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmissversion3-check_samplers_fit_resample" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_fit_resample]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnnearmissversion3-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnneighbourhoodcleaningrule-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[NeighbourhoodCleaningRule()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnonesidedselectionrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[OneSidedSelection(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnrandomoversamplerrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[RandomOverSampler(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnrandomundersamplerrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[RandomUnderSampler(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnrepeatededitednearestneighbours-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[RepeatedEditedNearestNeighbours()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnsmoterandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[SMOTE(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnsmoteennrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[SMOTEENN(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnsmotetomekrandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[SMOTETomek(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearnsvmsmoterandom_state0-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[SVMSMOTE(random_state=0)-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_commonpytest_estimators_imblearntomeklinks-check_samplers_sparse" class="md-nav__link">
    <span class="md-ellipsis">
      test_common.py::test_estimators_imblearn[TomekLinks()-check_samplers_sparse]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#patch-diff" class="md-nav__link">
    <span class="md-ellipsis">
      Patch diff
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="/analysis_commit0_all_reference">back to Reference (Gold) summary</a></p>
<h1 id="reference-gold-imbalanced-learn"><strong>Reference (Gold)</strong>: imbalanced-learn</h1>
<h2 id="pytest-summary-for-test-tests">Pytest Summary for test <code>tests</code></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">status</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">1441</td>
</tr>
<tr>
<td style="text-align: left;">xpassed</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: left;">xfailed</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">skipped</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">1468</td>
</tr>
<tr>
<td style="text-align: left;">collected</td>
<td style="text-align: center;">1468</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytests">Failed pytests:</h2>
<h3 id="test_commonpytest_estimators_compatibility_sklearnrandomoversamplerrandom_state0-check_complex_data">test_common.py::test_estimators_compatibility_sklearn[RandomOverSampler(random_state=0)-check_complex_data]</h3>
<details><summary> <pre>test_common.py::test_estimators_compatibility_sklearn[RandomOverSampler(random_state=0)-check_complex_data]</pre></summary><pre>

</pre>
</details>
<h3 id="test_commonpytest_estimators_compatibility_sklearnrandomundersamplerrandom_state0-check_complex_data">test_common.py::test_estimators_compatibility_sklearn[RandomUnderSampler(random_state=0)-check_complex_data]</h3>
<details><summary> <pre>test_common.py::test_estimators_compatibility_sklearn[RandomUnderSampler(random_state=0)-check_complex_data]</pre></summary><pre>

</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnallknn-check_samplers_sparse">test_common.py::test_estimators_imblearn[AllKNN()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[AllKNN()-check_samplers_sparse]</pre></summary><pre>
estimator = AllKNN()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'AllKNN')
request = <FixtureRequest for <Function test_estimators_imblearn[AllKNN()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'AllKNN', sampler_orig = AllKNN()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnborderlinesmoterandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[BorderlineSMOTE(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[BorderlineSMOTE(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = BorderlineSMOTE(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'BorderlineSMOTE')
request = <FixtureRequest for <Function test_estimators_imblearn[BorderlineSMOTE(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'BorderlineSMOTE', sampler_orig = BorderlineSMOTE(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnclustercentroidsrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[ClusterCentroids(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[ClusterCentroids(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = ClusterCentroids(estimator=KMeans(n_init=1, random_state=0), random_state=0,
                 voting='soft')
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'ClusterCentroids')
request = <FixtureRequest for <Function test_estimators_imblearn[ClusterCentroids(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'ClusterCentroids'
sampler_orig = ClusterCentroids(estimator=KMeans(n_init=1, random_state=0), random_state=0,
                 voting='soft')

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearncondensednearestneighbourrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[CondensedNearestNeighbour(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[CondensedNearestNeighbour(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = CondensedNearestNeighbour(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'CondensedNearestNeighbour')
request = <FixtureRequest for <Function test_estimators_imblearn[CondensedNearestNeighbour(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'CondensedNearestNeighbour'
sampler_orig = CondensedNearestNeighbour(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearneditednearestneighbours-check_samplers_sparse">test_common.py::test_estimators_imblearn[EditedNearestNeighbours()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[EditedNearestNeighbours()-check_samplers_sparse]</pre></summary><pre>
estimator = EditedNearestNeighbours()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'EditedNearestNeighbours')
request = <FixtureRequest for <Function test_estimators_imblearn[EditedNearestNeighbours()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'EditedNearestNeighbours', sampler_orig = EditedNearestNeighbours()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnfunctionsampler-check_samplers_sparse">test_common.py::test_estimators_imblearn[FunctionSampler()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[FunctionSampler()-check_samplers_sparse]</pre></summary><pre>
estimator = FunctionSampler()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'FunctionSampler')
request = <FixtureRequest for <Function test_estimators_imblearn[FunctionSampler()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'FunctionSampler', sampler_orig = FunctionSampler()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearninstancehardnessthresholdrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[InstanceHardnessThreshold(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[InstanceHardnessThreshold(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = InstanceHardnessThreshold(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'InstanceHardnessThreshold')
request = <FixtureRequest for <Function test_estimators_imblearn[InstanceHardnessThreshold(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'InstanceHardnessThreshold'
sampler_orig = InstanceHardnessThreshold(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnkmeanssmoterandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[KMeansSMOTE(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[KMeansSMOTE(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = KMeansSMOTE(kmeans_estimator=12, random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'KMeansSMOTE')
request = <FixtureRequest for <Function test_estimators_imblearn[KMeansSMOTE(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'KMeansSMOTE'
sampler_orig = KMeansSMOTE(kmeans_estimator=12, random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmiss-check_samplers_fit_resample">test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_fit_resample]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_fit_resample]</pre></summary><pre>

</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmiss-check_samplers_sparse">test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss()-check_samplers_sparse]</pre></summary><pre>
estimator = NearMiss()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'NearMiss')
request = <FixtureRequest for <Function test_estimators_imblearn[NearMiss()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'NearMiss', sampler_orig = NearMiss()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmissversion2-check_samplers_fit_resample">test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_fit_resample]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_fit_resample]</pre></summary><pre>

</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmissversion2-check_samplers_sparse">test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss(version=2)-check_samplers_sparse]</pre></summary><pre>
estimator = NearMiss(version=2)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'NearMiss')
request = <FixtureRequest for <Function test_estimators_imblearn[NearMiss(version=2)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'NearMiss', sampler_orig = NearMiss(version=2)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmissversion3-check_samplers_fit_resample">test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_fit_resample]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_fit_resample]</pre></summary><pre>
estimator = NearMiss(version=3)
check = functools.partial(<function check_samplers_fit_resample at 0x7fb0143c3c70>, 'NearMiss')
request = <FixtureRequest for <Function test_estimators_imblearn[NearMiss(version=3)-check_samplers_fit_resample]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'NearMiss', sampler_orig = NearMiss(version=3)

    def check_samplers_fit_resample(name, sampler_orig):
        sampler = clone(sampler_orig)
        X, y = sample_dataset_generator()
        target_stats = Counter(y)
        X_res, y_res = sampler.fit_resample(X, y)
        if isinstance(sampler, BaseOverSampler):
            target_stats_res = Counter(y_res)
            n_samples = max(target_stats.values())
            assert all(value >= n_samples for value in Counter(y_res).values())
        elif isinstance(sampler, BaseUnderSampler):
            n_samples = min(target_stats.values())
            if name == "InstanceHardnessThreshold":
                # IHT does not enforce the number of samples but provide a number
                # of samples the closest to the desired target.
                assert all(
                    Counter(y_res)[k] <= target_stats[k] for k in target_stats.keys()
                )
            else:
>               assert all(value == n_samples for value in Counter(y_res).values())
E               AssertionError

imblearn/utils/estimator_checks.py:269: AssertionError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnnearmissversion3-check_samplers_sparse">test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NearMiss(version=3)-check_samplers_sparse]</pre></summary><pre>
estimator = NearMiss(version=3)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'NearMiss')
request = <FixtureRequest for <Function test_estimators_imblearn[NearMiss(version=3)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'NearMiss', sampler_orig = NearMiss(version=3)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnneighbourhoodcleaningrule-check_samplers_sparse">test_common.py::test_estimators_imblearn[NeighbourhoodCleaningRule()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[NeighbourhoodCleaningRule()-check_samplers_sparse]</pre></summary><pre>
estimator = NeighbourhoodCleaningRule()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'NeighbourhoodCleaningRule')
request = <FixtureRequest for <Function test_estimators_imblearn[NeighbourhoodCleaningRule()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'NeighbourhoodCleaningRule', sampler_orig = NeighbourhoodCleaningRule()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnonesidedselectionrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[OneSidedSelection(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[OneSidedSelection(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = OneSidedSelection(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'OneSidedSelection')
request = <FixtureRequest for <Function test_estimators_imblearn[OneSidedSelection(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'OneSidedSelection', sampler_orig = OneSidedSelection(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnrandomoversamplerrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[RandomOverSampler(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[RandomOverSampler(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = RandomOverSampler(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'RandomOverSampler')
request = <FixtureRequest for <Function test_estimators_imblearn[RandomOverSampler(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'RandomOverSampler', sampler_orig = RandomOverSampler(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnrandomundersamplerrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[RandomUnderSampler(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[RandomUnderSampler(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = RandomUnderSampler(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'RandomUnderSampler')
request = <FixtureRequest for <Function test_estimators_imblearn[RandomUnderSampler(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'RandomUnderSampler', sampler_orig = RandomUnderSampler(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnrepeatededitednearestneighbours-check_samplers_sparse">test_common.py::test_estimators_imblearn[RepeatedEditedNearestNeighbours()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[RepeatedEditedNearestNeighbours()-check_samplers_sparse]</pre></summary><pre>
estimator = RepeatedEditedNearestNeighbours()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'RepeatedEditedNearestNeighbours')
request = <FixtureRequest for <Function test_estimators_imblearn[RepeatedEditedNearestNeighbours()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'RepeatedEditedNearestNeighbours'
sampler_orig = RepeatedEditedNearestNeighbours()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnsmoterandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[SMOTE(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[SMOTE(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = SMOTE(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'SMOTE')
request = <FixtureRequest for <Function test_estimators_imblearn[SMOTE(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'SMOTE', sampler_orig = SMOTE(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnsmoteennrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[SMOTEENN(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[SMOTEENN(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = SMOTEENN(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'SMOTEENN')
request = <FixtureRequest for <Function test_estimators_imblearn[SMOTEENN(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'SMOTEENN', sampler_orig = SMOTEENN(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnsmotetomekrandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[SMOTETomek(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[SMOTETomek(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = SMOTETomek(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'SMOTETomek')
request = <FixtureRequest for <Function test_estimators_imblearn[SMOTETomek(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'SMOTETomek', sampler_orig = SMOTETomek(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearnsvmsmoterandom_state0-check_samplers_sparse">test_common.py::test_estimators_imblearn[SVMSMOTE(random_state=0)-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[SVMSMOTE(random_state=0)-check_samplers_sparse]</pre></summary><pre>
estimator = SVMSMOTE(random_state=0)
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'SVMSMOTE')
request = <FixtureRequest for <Function test_estimators_imblearn[SVMSMOTE(random_state=0)-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'SVMSMOTE', sampler_orig = SVMSMOTE(random_state=0)

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>
<h3 id="test_commonpytest_estimators_imblearntomeklinks-check_samplers_sparse">test_common.py::test_estimators_imblearn[TomekLinks()-check_samplers_sparse]</h3>
<details><summary> <pre>test_common.py::test_estimators_imblearn[TomekLinks()-check_samplers_sparse]</pre></summary><pre>
estimator = TomekLinks()
check = functools.partial(<function check_samplers_sparse at 0x7fb0143c3d90>, 'TomekLinks')
request = <FixtureRequest for <Function test_estimators_imblearn[TomekLinks()-check_samplers_sparse]>>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_estimators_imblearn(estimator, check, request):
        # Common tests for estimator instances
        with ignore_warnings(
            category=(
                FutureWarning,
                ConvergenceWarning,
                UserWarning,
                FutureWarning,
            )
        ):
            _set_checking_parameters(estimator)
>           check(estimator)

imblearn/tests/test_common.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'TomekLinks', sampler_orig = TomekLinks()

    def check_samplers_sparse(name, sampler_orig):
        sampler = clone(sampler_orig)
        # check that sparse matrices can be passed through the sampler leading to
        # the same results than dense
        X, y = sample_dataset_generator()
        X_sparse = sparse.csr_matrix(X)
        X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)
        sampler = clone(sampler)
        X_res, y_res = sampler.fit_resample(X, y)
        assert sparse.issparse(X_res_sparse)
>       assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)
E       AttributeError: 'csr_matrix' object has no attribute 'A'

imblearn/utils/estimator_checks.py:312: AttributeError
</pre>
</details>

<h2 id="patch-diff">Patch diff</h2>
<div class="highlight"><pre><span></span><code><span class="gh">diff --git a/imblearn/_config.py b/imblearn/_config.py</span>
<span class="gh">index 88884fe..ef98e73 100644</span>
<span class="gd">--- a/imblearn/_config.py</span>
<span class="gi">+++ b/imblearn/_config.py</span>
<span class="gu">@@ -5,23 +5,34 @@ We remove the array_api_dispatch for the moment.</span>
<span class="w"> </span>import os
<span class="w"> </span>import threading
<span class="w"> </span>from contextlib import contextmanager as contextmanager
<span class="gi">+</span>
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gd">-if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="gd">-    _global_config = {&#39;assume_finite&#39;: bool(os.environ.get(</span>
<span class="gd">-        &#39;SKLEARN_ASSUME_FINITE&#39;, False)), &#39;working_memory&#39;: int(os.environ.</span>
<span class="gd">-        get(&#39;SKLEARN_WORKING_MEMORY&#39;, 1024)), &#39;print_changed_only&#39;: True,</span>
<span class="gd">-        &#39;display&#39;: &#39;diagram&#39;, &#39;pairwise_dist_chunk_size&#39;: int(os.environ.</span>
<span class="gd">-        get(&#39;SKLEARN_PAIRWISE_DIST_CHUNK_SIZE&#39;, 256)),</span>
<span class="gd">-        &#39;enable_cython_pairwise_dist&#39;: True, &#39;transform_output&#39;: &#39;default&#39;,</span>
<span class="gd">-        &#39;enable_metadata_routing&#39;: False, &#39;skip_parameter_validation&#39;: False}</span>
<span class="gi">+</span>
<span class="gi">+if sklearn_version &lt; parse_version(&quot;1.3&quot;):</span>
<span class="gi">+    _global_config = {</span>
<span class="gi">+        &quot;assume_finite&quot;: bool(os.environ.get(&quot;SKLEARN_ASSUME_FINITE&quot;, False)),</span>
<span class="gi">+        &quot;working_memory&quot;: int(os.environ.get(&quot;SKLEARN_WORKING_MEMORY&quot;, 1024)),</span>
<span class="gi">+        &quot;print_changed_only&quot;: True,</span>
<span class="gi">+        &quot;display&quot;: &quot;diagram&quot;,</span>
<span class="gi">+        &quot;pairwise_dist_chunk_size&quot;: int(</span>
<span class="gi">+            os.environ.get(&quot;SKLEARN_PAIRWISE_DIST_CHUNK_SIZE&quot;, 256)</span>
<span class="gi">+        ),</span>
<span class="gi">+        &quot;enable_cython_pairwise_dist&quot;: True,</span>
<span class="gi">+        &quot;transform_output&quot;: &quot;default&quot;,</span>
<span class="gi">+        &quot;enable_metadata_routing&quot;: False,</span>
<span class="gi">+        &quot;skip_parameter_validation&quot;: False,</span>
<span class="gi">+    }</span>
<span class="w"> </span>    _threadlocal = threading.local()

<span class="w"> </span>    def _get_threadlocal_config():
<span class="w"> </span>        &quot;&quot;&quot;Get a threadlocal **mutable** configuration. If the configuration
<span class="w"> </span>        does not exist, copy the default global configuration.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not hasattr(_threadlocal, &quot;global_config&quot;):</span>
<span class="gi">+            _threadlocal.global_config = _global_config.copy()</span>
<span class="gi">+        return _threadlocal.global_config</span>

<span class="w"> </span>    def get_config():
<span class="w"> </span>        &quot;&quot;&quot;Retrieve current values for configuration set by :func:`set_config`.
<span class="gu">@@ -36,12 +47,21 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        config_context : Context manager for global scikit-learn configuration.
<span class="w"> </span>        set_config : Set global scikit-learn configuration.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def set_config(assume_finite=None, working_memory=None,</span>
<span class="gd">-        print_changed_only=None, display=None, pairwise_dist_chunk_size=</span>
<span class="gd">-        None, enable_cython_pairwise_dist=None, transform_output=None,</span>
<span class="gd">-        enable_metadata_routing=None, skip_parameter_validation=None):</span>
<span class="gi">+        # Return a copy of the threadlocal configuration so that users will</span>
<span class="gi">+        # not be able to modify the configuration with the returned dict.</span>
<span class="gi">+        return _get_threadlocal_config().copy()</span>
<span class="gi">+</span>
<span class="gi">+    def set_config(</span>
<span class="gi">+        assume_finite=None,</span>
<span class="gi">+        working_memory=None,</span>
<span class="gi">+        print_changed_only=None,</span>
<span class="gi">+        display=None,</span>
<span class="gi">+        pairwise_dist_chunk_size=None,</span>
<span class="gi">+        enable_cython_pairwise_dist=None,</span>
<span class="gi">+        transform_output=None,</span>
<span class="gi">+        enable_metadata_routing=None,</span>
<span class="gi">+        skip_parameter_validation=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;Set global scikit-learn configuration

<span class="w"> </span>        .. versionadded:: 0.19
<span class="gu">@@ -142,13 +162,40 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        config_context : Context manager for global scikit-learn configuration.
<span class="w"> </span>        get_config : Retrieve current values of the global configuration.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        local_config = _get_threadlocal_config()</span>
<span class="gi">+</span>
<span class="gi">+        if assume_finite is not None:</span>
<span class="gi">+            local_config[&quot;assume_finite&quot;] = assume_finite</span>
<span class="gi">+        if working_memory is not None:</span>
<span class="gi">+            local_config[&quot;working_memory&quot;] = working_memory</span>
<span class="gi">+        if print_changed_only is not None:</span>
<span class="gi">+            local_config[&quot;print_changed_only&quot;] = print_changed_only</span>
<span class="gi">+        if display is not None:</span>
<span class="gi">+            local_config[&quot;display&quot;] = display</span>
<span class="gi">+        if pairwise_dist_chunk_size is not None:</span>
<span class="gi">+            local_config[&quot;pairwise_dist_chunk_size&quot;] = pairwise_dist_chunk_size</span>
<span class="gi">+        if enable_cython_pairwise_dist is not None:</span>
<span class="gi">+            local_config[&quot;enable_cython_pairwise_dist&quot;] = enable_cython_pairwise_dist</span>
<span class="gi">+        if transform_output is not None:</span>
<span class="gi">+            local_config[&quot;transform_output&quot;] = transform_output</span>
<span class="gi">+        if enable_metadata_routing is not None:</span>
<span class="gi">+            local_config[&quot;enable_metadata_routing&quot;] = enable_metadata_routing</span>
<span class="gi">+        if skip_parameter_validation is not None:</span>
<span class="gi">+            local_config[&quot;skip_parameter_validation&quot;] = skip_parameter_validation</span>

<span class="w"> </span>    @contextmanager
<span class="gd">-    def config_context(*, assume_finite=None, working_memory=None,</span>
<span class="gd">-        print_changed_only=None, display=None, pairwise_dist_chunk_size=</span>
<span class="gd">-        None, enable_cython_pairwise_dist=None, transform_output=None,</span>
<span class="gd">-        enable_metadata_routing=None, skip_parameter_validation=None):</span>
<span class="gi">+    def config_context(</span>
<span class="gi">+        *,</span>
<span class="gi">+        assume_finite=None,</span>
<span class="gi">+        working_memory=None,</span>
<span class="gi">+        print_changed_only=None,</span>
<span class="gi">+        display=None,</span>
<span class="gi">+        pairwise_dist_chunk_size=None,</span>
<span class="gi">+        enable_cython_pairwise_dist=None,</span>
<span class="gi">+        transform_output=None,</span>
<span class="gi">+        enable_metadata_routing=None,</span>
<span class="gi">+        skip_parameter_validation=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;Context manager for global scikit-learn configuration.

<span class="w"> </span>        Parameters
<span class="gu">@@ -270,6 +317,28 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        ...
<span class="w"> </span>        ValueError: Input contains NaN...
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        old_config = get_config()</span>
<span class="gi">+        set_config(</span>
<span class="gi">+            assume_finite=assume_finite,</span>
<span class="gi">+            working_memory=working_memory,</span>
<span class="gi">+            print_changed_only=print_changed_only,</span>
<span class="gi">+            display=display,</span>
<span class="gi">+            pairwise_dist_chunk_size=pairwise_dist_chunk_size,</span>
<span class="gi">+            enable_cython_pairwise_dist=enable_cython_pairwise_dist,</span>
<span class="gi">+            transform_output=transform_output,</span>
<span class="gi">+            enable_metadata_routing=enable_metadata_routing,</span>
<span class="gi">+            skip_parameter_validation=skip_parameter_validation,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        try:</span>
<span class="gi">+            yield</span>
<span class="gi">+        finally:</span>
<span class="gi">+            set_config(**old_config)</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="gd">-    from sklearn._config import _get_threadlocal_config, _global_config, config_context, get_config</span>
<span class="gi">+    from sklearn._config import (  # type: ignore[no-redef]</span>
<span class="gi">+        _get_threadlocal_config,</span>
<span class="gi">+        _global_config,</span>
<span class="gi">+        config_context,  # noqa</span>
<span class="gi">+        get_config,</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/_min_dependencies.py b/imblearn/_min_dependencies.py</span>
<span class="gh">index d713aa5..ec1f5de 100644</span>
<span class="gd">--- a/imblearn/_min_dependencies.py</span>
<span class="gi">+++ b/imblearn/_min_dependencies.py</span>
<span class="gu">@@ -1,38 +1,60 @@</span>
<span class="w"> </span>&quot;&quot;&quot;All minimum dependencies for imbalanced-learn.&quot;&quot;&quot;
<span class="w"> </span>import argparse
<span class="gd">-NUMPY_MIN_VERSION = &#39;1.17.3&#39;</span>
<span class="gd">-SCIPY_MIN_VERSION = &#39;1.5.0&#39;</span>
<span class="gd">-PANDAS_MIN_VERSION = &#39;1.0.5&#39;</span>
<span class="gd">-SKLEARN_MIN_VERSION = &#39;1.0.2&#39;</span>
<span class="gd">-TENSORFLOW_MIN_VERSION = &#39;2.4.3&#39;</span>
<span class="gd">-KERAS_MIN_VERSION = &#39;2.4.3&#39;</span>
<span class="gd">-JOBLIB_MIN_VERSION = &#39;1.1.1&#39;</span>
<span class="gd">-THREADPOOLCTL_MIN_VERSION = &#39;2.0.0&#39;</span>
<span class="gd">-PYTEST_MIN_VERSION = &#39;5.0.1&#39;</span>
<span class="gd">-dependent_packages = {&#39;numpy&#39;: (NUMPY_MIN_VERSION, &#39;install&#39;), &#39;scipy&#39;: (</span>
<span class="gd">-    SCIPY_MIN_VERSION, &#39;install&#39;), &#39;scikit-learn&#39;: (SKLEARN_MIN_VERSION,</span>
<span class="gd">-    &#39;install&#39;), &#39;joblib&#39;: (JOBLIB_MIN_VERSION, &#39;install&#39;), &#39;threadpoolctl&#39;:</span>
<span class="gd">-    (THREADPOOLCTL_MIN_VERSION, &#39;install&#39;), &#39;pandas&#39;: (PANDAS_MIN_VERSION,</span>
<span class="gd">-    &#39;optional, docs, examples, tests&#39;), &#39;tensorflow&#39;: (</span>
<span class="gd">-    TENSORFLOW_MIN_VERSION, &#39;optional, docs, examples, tests&#39;), &#39;keras&#39;: (</span>
<span class="gd">-    KERAS_MIN_VERSION, &#39;optional, docs, examples, tests&#39;), &#39;matplotlib&#39;: (</span>
<span class="gd">-    &#39;3.1.2&#39;, &#39;docs, examples&#39;), &#39;seaborn&#39;: (&#39;0.9.0&#39;, &#39;docs, examples&#39;),</span>
<span class="gd">-    &#39;memory_profiler&#39;: (&#39;0.57.0&#39;, &#39;docs&#39;), &#39;pytest&#39;: (PYTEST_MIN_VERSION,</span>
<span class="gd">-    &#39;tests&#39;), &#39;pytest-cov&#39;: (&#39;2.9.0&#39;, &#39;tests&#39;), &#39;flake8&#39;: (&#39;3.8.2&#39;, &#39;tests&#39;</span>
<span class="gd">-    ), &#39;black&#39;: (&#39;23.3.0&#39;, &#39;tests&#39;), &#39;mypy&#39;: (&#39;1.3.0&#39;, &#39;tests&#39;), &#39;sphinx&#39;:</span>
<span class="gd">-    (&#39;6.0.0&#39;, &#39;docs&#39;), &#39;sphinx-gallery&#39;: (&#39;0.13.0&#39;, &#39;docs&#39;),</span>
<span class="gd">-    &#39;sphinx-copybutton&#39;: (&#39;0.5.2&#39;, &#39;docs&#39;), &#39;numpydoc&#39;: (&#39;1.5.0&#39;, &#39;docs&#39;),</span>
<span class="gd">-    &#39;sphinxcontrib-bibtex&#39;: (&#39;2.4.1&#39;, &#39;docs&#39;), &#39;pydata-sphinx-theme&#39;: (</span>
<span class="gd">-    &#39;0.13.3&#39;, &#39;docs&#39;), &#39;sphinx-design&#39;: (&#39;0.5.0&#39;, &#39;docs&#39;)}</span>
<span class="gd">-tag_to_packages: dict = {extra: [] for extra in [&#39;install&#39;, &#39;optional&#39;,</span>
<span class="gd">-    &#39;docs&#39;, &#39;examples&#39;, &#39;tests&#39;]}</span>
<span class="gi">+</span>
<span class="gi">+NUMPY_MIN_VERSION = &quot;1.17.3&quot;</span>
<span class="gi">+SCIPY_MIN_VERSION = &quot;1.5.0&quot;</span>
<span class="gi">+PANDAS_MIN_VERSION = &quot;1.0.5&quot;</span>
<span class="gi">+SKLEARN_MIN_VERSION = &quot;1.0.2&quot;</span>
<span class="gi">+TENSORFLOW_MIN_VERSION = &quot;2.4.3&quot;</span>
<span class="gi">+KERAS_MIN_VERSION = &quot;2.4.3&quot;</span>
<span class="gi">+JOBLIB_MIN_VERSION = &quot;1.1.1&quot;</span>
<span class="gi">+THREADPOOLCTL_MIN_VERSION = &quot;2.0.0&quot;</span>
<span class="gi">+PYTEST_MIN_VERSION = &quot;5.0.1&quot;</span>
<span class="gi">+</span>
<span class="gi">+# &#39;build&#39; and &#39;install&#39; is included to have structured metadata for CI.</span>
<span class="gi">+# It will NOT be included in setup&#39;s extras_require</span>
<span class="gi">+# The values are (version_spec, comma separated tags)</span>
<span class="gi">+dependent_packages = {</span>
<span class="gi">+    &quot;numpy&quot;: (NUMPY_MIN_VERSION, &quot;install&quot;),</span>
<span class="gi">+    &quot;scipy&quot;: (SCIPY_MIN_VERSION, &quot;install&quot;),</span>
<span class="gi">+    &quot;scikit-learn&quot;: (SKLEARN_MIN_VERSION, &quot;install&quot;),</span>
<span class="gi">+    &quot;joblib&quot;: (JOBLIB_MIN_VERSION, &quot;install&quot;),</span>
<span class="gi">+    &quot;threadpoolctl&quot;: (THREADPOOLCTL_MIN_VERSION, &quot;install&quot;),</span>
<span class="gi">+    &quot;pandas&quot;: (PANDAS_MIN_VERSION, &quot;optional, docs, examples, tests&quot;),</span>
<span class="gi">+    &quot;tensorflow&quot;: (TENSORFLOW_MIN_VERSION, &quot;optional, docs, examples, tests&quot;),</span>
<span class="gi">+    &quot;keras&quot;: (KERAS_MIN_VERSION, &quot;optional, docs, examples, tests&quot;),</span>
<span class="gi">+    &quot;matplotlib&quot;: (&quot;3.1.2&quot;, &quot;docs, examples&quot;),</span>
<span class="gi">+    &quot;seaborn&quot;: (&quot;0.9.0&quot;, &quot;docs, examples&quot;),</span>
<span class="gi">+    &quot;memory_profiler&quot;: (&quot;0.57.0&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;pytest&quot;: (PYTEST_MIN_VERSION, &quot;tests&quot;),</span>
<span class="gi">+    &quot;pytest-cov&quot;: (&quot;2.9.0&quot;, &quot;tests&quot;),</span>
<span class="gi">+    &quot;flake8&quot;: (&quot;3.8.2&quot;, &quot;tests&quot;),</span>
<span class="gi">+    &quot;black&quot;: (&quot;23.3.0&quot;, &quot;tests&quot;),</span>
<span class="gi">+    &quot;mypy&quot;: (&quot;1.3.0&quot;, &quot;tests&quot;),</span>
<span class="gi">+    &quot;sphinx&quot;: (&quot;6.0.0&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;sphinx-gallery&quot;: (&quot;0.13.0&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;sphinx-copybutton&quot;: (&quot;0.5.2&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;numpydoc&quot;: (&quot;1.5.0&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;sphinxcontrib-bibtex&quot;: (&quot;2.4.1&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;pydata-sphinx-theme&quot;: (&quot;0.13.3&quot;, &quot;docs&quot;),</span>
<span class="gi">+    &quot;sphinx-design&quot;: (&quot;0.5.0&quot;, &quot;docs&quot;),</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# create inverse mapping for setuptools</span>
<span class="gi">+tag_to_packages: dict = {</span>
<span class="gi">+    extra: [] for extra in [&quot;install&quot;, &quot;optional&quot;, &quot;docs&quot;, &quot;examples&quot;, &quot;tests&quot;]</span>
<span class="gi">+}</span>
<span class="w"> </span>for package, (min_version, extras) in dependent_packages.items():
<span class="gd">-    for extra in extras.split(&#39;, &#39;):</span>
<span class="gd">-        tag_to_packages[extra].append(&#39;{}&gt;={}&#39;.format(package, min_version))</span>
<span class="gd">-if __name__ == &#39;__main__&#39;:</span>
<span class="gd">-    parser = argparse.ArgumentParser(description=</span>
<span class="gd">-        &#39;Get min dependencies for a package&#39;)</span>
<span class="gd">-    parser.add_argument(&#39;package&#39;, choices=dependent_packages)</span>
<span class="gi">+    for extra in extras.split(&quot;, &quot;):</span>
<span class="gi">+        tag_to_packages[extra].append(&quot;{}&gt;={}&quot;.format(package, min_version))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# Used by CI to get the min dependencies</span>
<span class="gi">+if __name__ == &quot;__main__&quot;:</span>
<span class="gi">+    parser = argparse.ArgumentParser(description=&quot;Get min dependencies for a package&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    parser.add_argument(&quot;package&quot;, choices=dependent_packages)</span>
<span class="w"> </span>    args = parser.parse_args()
<span class="w"> </span>    min_version = dependent_packages[args.package][0]
<span class="w"> </span>    print(min_version)
<span class="gh">diff --git a/imblearn/_version.py b/imblearn/_version.py</span>
<span class="gh">index ed49005..19c405e 100644</span>
<span class="gd">--- a/imblearn/_version.py</span>
<span class="gi">+++ b/imblearn/_version.py</span>
<span class="gu">@@ -2,4 +2,24 @@</span>
<span class="w"> </span>``imbalanced-learn`` is a set of python methods to deal with imbalanced
<span class="w"> </span>datset in machine learning and pattern recognition.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gd">-__version__ = &#39;0.12.3&#39;</span>
<span class="gi">+# Based on NiLearn package</span>
<span class="gi">+# License: simplified BSD</span>
<span class="gi">+</span>
<span class="gi">+# PEP0440 compatible formatted version, see:</span>
<span class="gi">+# https://www.python.org/dev/peps/pep-0440/</span>
<span class="gi">+#</span>
<span class="gi">+# Generic release markers:</span>
<span class="gi">+# X.Y</span>
<span class="gi">+# X.Y.Z # For bugfix releases</span>
<span class="gi">+#</span>
<span class="gi">+# Admissible pre-release markers:</span>
<span class="gi">+# X.YaN # Alpha release</span>
<span class="gi">+# X.YbN # Beta release</span>
<span class="gi">+# X.YrcN # Release Candidate</span>
<span class="gi">+# X.Y # Final release</span>
<span class="gi">+#</span>
<span class="gi">+# Dev branch marker is: &#39;X.Y.dev&#39; or &#39;X.Y.devN&#39; where N is an integer.</span>
<span class="gi">+# &#39;X.Y.dev0&#39; is the canonical version of &#39;X.Y.dev&#39;</span>
<span class="gi">+#</span>
<span class="gi">+</span>
<span class="gi">+__version__ = &quot;0.12.3&quot;</span>
<span class="gh">diff --git a/imblearn/base.py b/imblearn/base.py</span>
<span class="gh">index b4c50a8..0b2d94e 100644</span>
<span class="gd">--- a/imblearn/base.py</span>
<span class="gi">+++ b/imblearn/base.py</span>
<span class="gu">@@ -1,18 +1,29 @@</span>
<span class="gd">-&quot;&quot;&quot;Base class for sampling&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;Base class for sampling&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from abc import ABCMeta, abstractmethod
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.base import BaseEstimator
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # scikit-learn &gt;= 1.2</span>
<span class="w"> </span>    from sklearn.base import OneToOneFeatureMixin
<span class="w"> </span>except ImportError:
<span class="w"> </span>    from sklearn.base import _OneToOneFeatureMixin as OneToOneFeatureMixin
<span class="gi">+</span>
<span class="w"> </span>from sklearn.preprocessing import label_binarize
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.multiclass import check_classification_targets
<span class="gi">+</span>
<span class="w"> </span>from .utils import check_sampling_strategy, check_target_type
<span class="w"> </span>from .utils._param_validation import validate_parameter_constraints
<span class="w"> </span>from .utils._validation import ArraysTransformer
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gu">@@ -27,7 +38,12 @@ class _ParamsValidationMixin:</span>
<span class="w"> </span>        the docstring of `validate_parameter_constraints` for a description of the
<span class="w"> </span>        accepted constraints.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(self, &quot;_parameter_constraints&quot;):</span>
<span class="gi">+            validate_parameter_constraints(</span>
<span class="gi">+                self._parameter_constraints,</span>
<span class="gi">+                self.get_params(deep=False),</span>
<span class="gi">+                caller_name=self.__class__.__name__,</span>
<span class="gi">+            )</span>


<span class="w"> </span>class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):
<span class="gu">@@ -36,7 +52,8 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>
<span class="w"> </span>    Warning: This class should not be used directly. Use the derive classes
<span class="w"> </span>    instead.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _estimator_type = &#39;sampler&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _estimator_type = &quot;sampler&quot;</span>

<span class="w"> </span>    def fit(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Check inputs and statistics of the sampler.
<span class="gu">@@ -45,7 +62,8 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gd">-        X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)</span>
<span class="gi">+        X : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples, n_features)</span>
<span class="w"> </span>            Data array.

<span class="w"> </span>        y : array-like of shape (n_samples,)
<span class="gu">@@ -56,14 +74,19 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        X, y, _ = self._check_X_y(X, y)</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gd">-        X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)</span>
<span class="gi">+        X : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples, n_features)</span>
<span class="w"> </span>            Matrix containing the data which have to be sampled.

<span class="w"> </span>        y : array-like of shape (n_samples,)
<span class="gu">@@ -71,13 +94,29 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        X_resampled : {array-like, dataframe, sparse matrix} of shape                 (n_samples_new, n_features)</span>
<span class="gi">+        X_resampled : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples_new, n_features)</span>
<span class="w"> </span>            The array containing the resampled data.

<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_classification_targets(y)</span>
<span class="gi">+        arrays_transformer = ArraysTransformer(X, y)</span>
<span class="gi">+        X, y, binarize_y = self._check_X_y(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        output = self._fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        y_ = (</span>
<span class="gi">+            label_binarize(output[1], classes=np.unique(y)) if binarize_y else output[1]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        X_, y_ = arrays_transformer.transform(output[0], y_)</span>
<span class="gi">+        return (X_, y_) if len(output) == 2 else (X_, y_, output[2])</span>

<span class="w"> </span>    @abstractmethod
<span class="w"> </span>    def _fit_resample(self, X, y):
<span class="gu">@@ -94,7 +133,8 @@ class SamplerMixin(_ParamsValidationMixin, BaseEstimator, metaclass=ABCMeta):</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        X_resampled : {ndarray, sparse matrix} of shape                 (n_samples_new, n_features)</span>
<span class="gi">+        X_resampled : {ndarray, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples_new, n_features)</span>
<span class="w"> </span>            The array containing the resampled data.

<span class="w"> </span>        y_resampled : ndarray of shape (n_samples_new,)
<span class="gu">@@ -111,9 +151,16 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>
<span class="w"> </span>    instead.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, sampling_strategy=&#39;auto&#39;):</span>
<span class="gi">+    def __init__(self, sampling_strategy=&quot;auto&quot;):</span>
<span class="w"> </span>        self.sampling_strategy = sampling_strategy

<span class="gi">+    def _check_X_y(self, X, y, accept_sparse=None):</span>
<span class="gi">+        if accept_sparse is None:</span>
<span class="gi">+            accept_sparse = [&quot;csr&quot;, &quot;csc&quot;]</span>
<span class="gi">+        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)</span>
<span class="gi">+        return X, y, binarize_y</span>
<span class="gi">+</span>
<span class="w"> </span>    def fit(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Check inputs and statistics of the sampler.

<span class="gu">@@ -121,7 +168,8 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gd">-        X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)</span>
<span class="gi">+        X : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples, n_features)</span>
<span class="w"> </span>            Data array.

<span class="w"> </span>        y : array-like of shape (n_samples,)
<span class="gu">@@ -132,14 +180,16 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        return super().fit(X, y)</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gd">-        X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)</span>
<span class="gi">+        X : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples, n_features)</span>
<span class="w"> </span>            Matrix containing the data which have to be sampled.

<span class="w"> </span>        y : array-like of shape (n_samples,)
<span class="gu">@@ -147,13 +197,22 @@ class BaseSampler(SamplerMixin, OneToOneFeatureMixin):</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        X_resampled : {array-like, dataframe, sparse matrix} of shape                 (n_samples_new, n_features)</span>
<span class="gi">+        X_resampled : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples_new, n_features)</span>
<span class="w"> </span>            The array containing the resampled data.

<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        return super().fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;X_types&quot;: [&quot;2darray&quot;, &quot;sparse&quot;, &quot;dataframe&quot;]}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _identity(X, y):</span>
<span class="gi">+    return X, y</span>


<span class="w"> </span>def is_sampler(estimator):
<span class="gu">@@ -169,7 +228,9 @@ def is_sampler(estimator):</span>
<span class="w"> </span>    is_sampler : bool
<span class="w"> </span>        True if estimator is a sampler, otherwise False.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if estimator._estimator_type == &quot;sampler&quot;:</span>
<span class="gi">+        return True</span>
<span class="gi">+    return False</span>


<span class="w"> </span>class FunctionSampler(BaseSampler):
<span class="gu">@@ -260,13 +321,17 @@ class FunctionSampler(BaseSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(f&#39;Resampled dataset shape {sorted(Counter(y_res).items())}&#39;)
<span class="w"> </span>    Resampled dataset shape [(0, 100), (1, 100)]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;bypass&#39;</span>
<span class="gd">-    _parameter_constraints: dict = {&#39;func&#39;: [callable, None],</span>
<span class="gd">-        &#39;accept_sparse&#39;: [&#39;boolean&#39;], &#39;kw_args&#39;: [dict, None], &#39;validate&#39;:</span>
<span class="gd">-        [&#39;boolean&#39;]}</span>

<span class="gd">-    def __init__(self, *, func=None, accept_sparse=True, kw_args=None,</span>
<span class="gd">-        validate=True):</span>
<span class="gi">+    _sampling_type = &quot;bypass&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;func&quot;: [callable, None],</span>
<span class="gi">+        &quot;accept_sparse&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;kw_args&quot;: [dict, None],</span>
<span class="gi">+        &quot;validate&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, *, func=None, accept_sparse=True, kw_args=None, validate=True):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.func = func
<span class="w"> </span>        self.accept_sparse = accept_sparse
<span class="gu">@@ -280,7 +345,8 @@ class FunctionSampler(BaseSampler):</span>

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gd">-        X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)</span>
<span class="gi">+        X : {array-like, dataframe, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples, n_features)</span>
<span class="w"> </span>            Data array.

<span class="w"> </span>        y : array-like of shape (n_samples,)
<span class="gu">@@ -291,7 +357,17 @@ class FunctionSampler(BaseSampler):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        # we need to overwrite SamplerMixin.fit to bypass the validation</span>
<span class="gi">+        if self.validate:</span>
<span class="gi">+            check_classification_targets(y)</span>
<span class="gi">+            X, y, _ = self._check_X_y(X, y, accept_sparse=self.accept_sparse)</span>
<span class="gi">+</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def fit_resample(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Resample the dataset.
<span class="gu">@@ -306,10 +382,38 @@ class FunctionSampler(BaseSampler):</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        X_resampled : {array-like, sparse matrix} of shape                 (n_samples_new, n_features)</span>
<span class="gi">+        X_resampled : {array-like, sparse matrix} of shape \</span>
<span class="gi">+                (n_samples_new, n_features)</span>
<span class="w"> </span>            The array containing the resampled data.

<span class="w"> </span>        y_resampled : array-like of shape (n_samples_new,)
<span class="w"> </span>            The corresponding label of `X_resampled`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        arrays_transformer = ArraysTransformer(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        if self.validate:</span>
<span class="gi">+            check_classification_targets(y)</span>
<span class="gi">+            X, y, binarize_y = self._check_X_y(X, y, accept_sparse=self.accept_sparse)</span>
<span class="gi">+</span>
<span class="gi">+        self.sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            self.sampling_strategy, y, self._sampling_type</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        output = self._fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        if self.validate:</span>
<span class="gi">+            y_ = (</span>
<span class="gi">+                label_binarize(output[1], classes=np.unique(y))</span>
<span class="gi">+                if binarize_y</span>
<span class="gi">+                else output[1]</span>
<span class="gi">+            )</span>
<span class="gi">+            X_, y_ = arrays_transformer.transform(output[0], y_)</span>
<span class="gi">+            return (X_, y_) if len(output) == 2 else (X_, y_, output[2])</span>
<span class="gi">+</span>
<span class="gi">+        return output</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        func = _identity if self.func is None else self.func</span>
<span class="gi">+        output = func(X, y, **(self.kw_args if self.kw_args else {}))</span>
<span class="gi">+        return output</span>
<span class="gh">diff --git a/imblearn/combine/_smote_enn.py b/imblearn/combine/_smote_enn.py</span>
<span class="gh">index 451604e..1b0ffe0 100644</span>
<span class="gd">--- a/imblearn/combine/_smote_enn.py</span>
<span class="gi">+++ b/imblearn/combine/_smote_enn.py</span>
<span class="gu">@@ -1,7 +1,14 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform over-sampling using SMOTE and cleaning using ENN.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="gi">+</span>
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.utils import check_X_y
<span class="gi">+</span>
<span class="w"> </span>from ..base import BaseSampler
<span class="w"> </span>from ..over_sampling import SMOTE
<span class="w"> </span>from ..over_sampling.base import BaseOverSampler
<span class="gu">@@ -10,9 +17,11 @@ from ..utils import Substitution, check_target_type</span>
<span class="w"> </span>from ..utils._docstring import _n_jobs_docstring, _random_state_docstring


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SMOTEENN(BaseSampler):
<span class="w"> </span>    &quot;&quot;&quot;Over-sampling using SMOTE and cleaning using ENN.

<span class="gu">@@ -98,13 +107,25 @@ class SMOTEENN(BaseSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 881}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;over-sampling&#39;</span>
<span class="gd">-    _parameter_constraints: dict = {**BaseOverSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;smote&#39;: [SMOTE, None], &#39;enn&#39;: [</span>
<span class="gd">-        EditedNearestNeighbours, None], &#39;n_jobs&#39;: [numbers.Integral, None]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        smote=None, enn=None, n_jobs=None):</span>
<span class="gi">+    _sampling_type = &quot;over-sampling&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseOverSampler._parameter_constraints,</span>
<span class="gi">+        &quot;smote&quot;: [SMOTE, None],</span>
<span class="gi">+        &quot;enn&quot;: [EditedNearestNeighbours, None],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        smote=None,</span>
<span class="gi">+        enn=None,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.random_state = random_state
<span class="gu">@@ -113,5 +134,28 @@ class SMOTEENN(BaseSampler):</span>
<span class="w"> </span>        self.n_jobs = n_jobs

<span class="w"> </span>    def _validate_estimator(self):
<span class="gd">-        &quot;&quot;&quot;Private function to validate SMOTE and ENN objects&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+        &quot;Private function to validate SMOTE and ENN objects&quot;</span>
<span class="gi">+        if self.smote is not None:</span>
<span class="gi">+            self.smote_ = clone(self.smote)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.smote_ = SMOTE(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if self.enn is not None:</span>
<span class="gi">+            self.enn_ = clone(self.enn)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+                sampling_strategy=&quot;all&quot;, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        y = check_target_type(y)</span>
<span class="gi">+        X, y = check_X_y(X, y, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;])</span>
<span class="gi">+        self.sampling_strategy_ = self.sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+        X_res, y_res = self.smote_.fit_resample(X, y)</span>
<span class="gi">+        return self.enn_.fit_resample(X_res, y_res)</span>
<span class="gh">diff --git a/imblearn/combine/_smote_tomek.py b/imblearn/combine/_smote_tomek.py</span>
<span class="gh">index 2bbf9bf..94d7c4d 100644</span>
<span class="gd">--- a/imblearn/combine/_smote_tomek.py</span>
<span class="gi">+++ b/imblearn/combine/_smote_tomek.py</span>
<span class="gu">@@ -1,8 +1,15 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform over-sampling using SMOTE and cleaning using Tomek
<span class="w"> </span>links.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="gi">+</span>
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.utils import check_X_y
<span class="gi">+</span>
<span class="w"> </span>from ..base import BaseSampler
<span class="w"> </span>from ..over_sampling import SMOTE
<span class="w"> </span>from ..over_sampling.base import BaseOverSampler
<span class="gu">@@ -11,9 +18,11 @@ from ..utils import Substitution, check_target_type</span>
<span class="w"> </span>from ..utils._docstring import _n_jobs_docstring, _random_state_docstring


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SMOTETomek(BaseSampler):
<span class="w"> </span>    &quot;&quot;&quot;Over-sampling using SMOTE and cleaning using Tomek links.

<span class="gu">@@ -96,13 +105,25 @@ class SMOTETomek(BaseSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;over-sampling&#39;</span>
<span class="gd">-    _parameter_constraints: dict = {**BaseOverSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;smote&#39;: [SMOTE, None], &#39;tomek&#39;: [</span>
<span class="gd">-        TomekLinks, None], &#39;n_jobs&#39;: [numbers.Integral, None]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        smote=None, tomek=None, n_jobs=None):</span>
<span class="gi">+    _sampling_type = &quot;over-sampling&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseOverSampler._parameter_constraints,</span>
<span class="gi">+        &quot;smote&quot;: [SMOTE, None],</span>
<span class="gi">+        &quot;tomek&quot;: [TomekLinks, None],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        smote=None,</span>
<span class="gi">+        tomek=None,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__()
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.random_state = random_state
<span class="gu">@@ -111,5 +132,27 @@ class SMOTETomek(BaseSampler):</span>
<span class="w"> </span>        self.n_jobs = n_jobs

<span class="w"> </span>    def _validate_estimator(self):
<span class="gd">-        &quot;&quot;&quot;Private function to validate SMOTE and ENN objects&quot;&quot;&quot;</span>
<span class="gd">-        pass</span>
<span class="gi">+        &quot;Private function to validate SMOTE and ENN objects&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if self.smote is not None:</span>
<span class="gi">+            self.smote_ = clone(self.smote)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.smote_ = SMOTE(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if self.tomek is not None:</span>
<span class="gi">+            self.tomek_ = clone(self.tomek)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.tomek_ = TomekLinks(sampling_strategy=&quot;all&quot;, n_jobs=self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        y = check_target_type(y)</span>
<span class="gi">+        X, y = check_X_y(X, y, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;])</span>
<span class="gi">+        self.sampling_strategy_ = self.sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+        X_res, y_res = self.smote_.fit_resample(X, y)</span>
<span class="gi">+        return self.tomek_.fit_resample(X_res, y_res)</span>
<span class="gh">diff --git a/imblearn/combine/tests/test_smote_enn.py b/imblearn/combine/tests/test_smote_enn.py</span>
<span class="gh">index f6dabe0..df72cc7 100644</span>
<span class="gd">--- a/imblearn/combine/tests/test_smote_enn.py</span>
<span class="gi">+++ b/imblearn/combine/tests/test_smote_enn.py</span>
<span class="gu">@@ -1,18 +1,157 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module SMOTE ENN.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.combine import SMOTEENN
<span class="w"> </span>from imblearn.over_sampling import SMOTE
<span class="w"> </span>from imblearn.under_sampling import EditedNearestNeighbours
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.11622591, -0.0317206], [0.77481731, 0.60935141], [</span>
<span class="gd">-    1.25192108, -0.22367336], [0.53366841, -0.30312976], [1.52091956, -</span>
<span class="gd">-    0.49283504], [-0.28162401, -2.10400981], [0.83680821, 1.72827342], [</span>
<span class="gd">-    0.3084254, 0.33299982], [0.70472253, -0.73309052], [0.28893132, -</span>
<span class="gd">-    0.38761769], [1.15514042, 0.0129463], [0.88407872, 0.35454207], [</span>
<span class="gd">-    1.31301027, -0.92648734], [-1.11515198, -0.93689695], [-0.18410027, -</span>
<span class="gd">-    0.45194484], [0.9281014, 0.53085498], [-0.14374509, 0.27370049], [-</span>
<span class="gd">-    0.41635887, -0.38299653], [0.08711622, 0.93259929], [1.70580611, -</span>
<span class="gd">-    0.11219234]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.11622591, -0.0317206],</span>
<span class="gi">+        [0.77481731, 0.60935141],</span>
<span class="gi">+        [1.25192108, -0.22367336],</span>
<span class="gi">+        [0.53366841, -0.30312976],</span>
<span class="gi">+        [1.52091956, -0.49283504],</span>
<span class="gi">+        [-0.28162401, -2.10400981],</span>
<span class="gi">+        [0.83680821, 1.72827342],</span>
<span class="gi">+        [0.3084254, 0.33299982],</span>
<span class="gi">+        [0.70472253, -0.73309052],</span>
<span class="gi">+        [0.28893132, -0.38761769],</span>
<span class="gi">+        [1.15514042, 0.0129463],</span>
<span class="gi">+        [0.88407872, 0.35454207],</span>
<span class="gi">+        [1.31301027, -0.92648734],</span>
<span class="gi">+        [-1.11515198, -0.93689695],</span>
<span class="gi">+        [-0.18410027, -0.45194484],</span>
<span class="gi">+        [0.9281014, 0.53085498],</span>
<span class="gi">+        [-0.14374509, 0.27370049],</span>
<span class="gi">+        [-0.41635887, -0.38299653],</span>
<span class="gi">+        [0.08711622, 0.93259929],</span>
<span class="gi">+        [1.70580611, -0.11219234],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular():</span>
<span class="gi">+    smote = SMOTEENN(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571667],</span>
<span class="gi">+            [0.66052536, -0.28246518],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 1, 1, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular_pass_smote_enn():</span>
<span class="gi">+    smote = SMOTEENN(</span>
<span class="gi">+        smote=SMOTE(sampling_strategy=&quot;auto&quot;, random_state=RND_SEED),</span>
<span class="gi">+        enn=EditedNearestNeighbours(sampling_strategy=&quot;all&quot;),</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571667],</span>
<span class="gi">+            [0.66052536, -0.28246518],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 1, 1, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular_half():</span>
<span class="gi">+    sampling_strategy = {0: 10, 1: 12}</span>
<span class="gi">+    smote = SMOTEENN(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 1, 1, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_validate_estimator_init():</span>
<span class="gi">+    smote = SMOTE(random_state=RND_SEED)</span>
<span class="gi">+    enn = EditedNearestNeighbours(sampling_strategy=&quot;all&quot;)</span>
<span class="gi">+    smt = SMOTEENN(smote=smote, enn=enn, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smt.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571667],</span>
<span class="gi">+            [0.66052536, -0.28246518],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 1, 1, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_validate_estimator_default():</span>
<span class="gi">+    smt = SMOTEENN(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smt.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571667],</span>
<span class="gi">+            [0.66052536, -0.28246518],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 1, 1, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_parallelisation():</span>
<span class="gi">+    # Check if default job count is none</span>
<span class="gi">+    smt = SMOTEENN(random_state=RND_SEED)</span>
<span class="gi">+    smt._validate_estimator()</span>
<span class="gi">+    assert smt.n_jobs is None</span>
<span class="gi">+    assert smt.smote_.n_jobs is None</span>
<span class="gi">+    assert smt.enn_.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+    # Check if job count is set</span>
<span class="gi">+    smt = SMOTEENN(random_state=RND_SEED, n_jobs=8)</span>
<span class="gi">+    smt._validate_estimator()</span>
<span class="gi">+    assert smt.n_jobs == 8</span>
<span class="gi">+    assert smt.smote_.n_jobs == 8</span>
<span class="gi">+    assert smt.enn_.n_jobs == 8</span>
<span class="gh">diff --git a/imblearn/combine/tests/test_smote_tomek.py b/imblearn/combine/tests/test_smote_tomek.py</span>
<span class="gh">index 5685726..2ca3e38 100644</span>
<span class="gd">--- a/imblearn/combine/tests/test_smote_tomek.py</span>
<span class="gi">+++ b/imblearn/combine/tests/test_smote_tomek.py</span>
<span class="gu">@@ -1,18 +1,167 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module SMOTE ENN.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.combine import SMOTETomek
<span class="w"> </span>from imblearn.over_sampling import SMOTE
<span class="w"> </span>from imblearn.under_sampling import TomekLinks
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.20622591, 0.0582794], [0.68481731, 0.51935141], [</span>
<span class="gd">-    1.34192108, -0.13367336], [0.62366841, -0.21312976], [1.61091956, -</span>
<span class="gd">-    0.40283504], [-0.37162401, -2.19400981], [0.74680821, 1.63827342], [</span>
<span class="gd">-    0.2184254, 0.24299982], [0.61472253, -0.82309052], [0.19893132, -</span>
<span class="gd">-    0.47761769], [1.06514042, -0.0770537], [0.97407872, 0.44454207], [</span>
<span class="gd">-    1.40301027, -0.83648734], [-1.20515198, -1.02689695], [-0.27410027, -</span>
<span class="gd">-    0.54194484], [0.8381014, 0.44085498], [-0.23374509, 0.18370049], [-</span>
<span class="gd">-    0.32635887, -0.29299653], [-0.00288378, 0.84259929], [1.79580611, -</span>
<span class="gd">-    0.02219234]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.20622591, 0.0582794],</span>
<span class="gi">+        [0.68481731, 0.51935141],</span>
<span class="gi">+        [1.34192108, -0.13367336],</span>
<span class="gi">+        [0.62366841, -0.21312976],</span>
<span class="gi">+        [1.61091956, -0.40283504],</span>
<span class="gi">+        [-0.37162401, -2.19400981],</span>
<span class="gi">+        [0.74680821, 1.63827342],</span>
<span class="gi">+        [0.2184254, 0.24299982],</span>
<span class="gi">+        [0.61472253, -0.82309052],</span>
<span class="gi">+        [0.19893132, -0.47761769],</span>
<span class="gi">+        [1.06514042, -0.0770537],</span>
<span class="gi">+        [0.97407872, 0.44454207],</span>
<span class="gi">+        [1.40301027, -0.83648734],</span>
<span class="gi">+        [-1.20515198, -1.02689695],</span>
<span class="gi">+        [-0.27410027, -0.54194484],</span>
<span class="gi">+        [0.8381014, 0.44085498],</span>
<span class="gi">+        [-0.23374509, 0.18370049],</span>
<span class="gi">+        [-0.32635887, -0.29299653],</span>
<span class="gi">+        [-0.00288378, 0.84259929],</span>
<span class="gi">+        [1.79580611, -0.02219234],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular():</span>
<span class="gi">+    smote = SMOTETomek(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.68481731, 0.51935141],</span>
<span class="gi">+            [1.34192108, -0.13367336],</span>
<span class="gi">+            [0.62366841, -0.21312976],</span>
<span class="gi">+            [1.61091956, -0.40283504],</span>
<span class="gi">+            [-0.37162401, -2.19400981],</span>
<span class="gi">+            [0.74680821, 1.63827342],</span>
<span class="gi">+            [0.61472253, -0.82309052],</span>
<span class="gi">+            [0.19893132, -0.47761769],</span>
<span class="gi">+            [1.40301027, -0.83648734],</span>
<span class="gi">+            [-1.20515198, -1.02689695],</span>
<span class="gi">+            [-0.23374509, 0.18370049],</span>
<span class="gi">+            [-0.00288378, 0.84259929],</span>
<span class="gi">+            [1.79580611, -0.02219234],</span>
<span class="gi">+            [0.38307743, -0.05670439],</span>
<span class="gi">+            [0.70319159, -0.02571667],</span>
<span class="gi">+            [0.75052536, -0.19246518],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular_half():</span>
<span class="gi">+    sampling_strategy = {0: 9, 1: 12}</span>
<span class="gi">+    smote = SMOTETomek(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.68481731, 0.51935141],</span>
<span class="gi">+            [0.62366841, -0.21312976],</span>
<span class="gi">+            [1.61091956, -0.40283504],</span>
<span class="gi">+            [-0.37162401, -2.19400981],</span>
<span class="gi">+            [0.74680821, 1.63827342],</span>
<span class="gi">+            [0.61472253, -0.82309052],</span>
<span class="gi">+            [0.19893132, -0.47761769],</span>
<span class="gi">+            [1.40301027, -0.83648734],</span>
<span class="gi">+            [-1.20515198, -1.02689695],</span>
<span class="gi">+            [-0.23374509, 0.18370049],</span>
<span class="gi">+            [-0.00288378, 0.84259929],</span>
<span class="gi">+            [1.79580611, -0.02219234],</span>
<span class="gi">+            [0.45784496, -0.1053161],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_validate_estimator_init():</span>
<span class="gi">+    smote = SMOTE(random_state=RND_SEED)</span>
<span class="gi">+    tomek = TomekLinks(sampling_strategy=&quot;all&quot;)</span>
<span class="gi">+    smt = SMOTETomek(smote=smote, tomek=tomek, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smt.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.68481731, 0.51935141],</span>
<span class="gi">+            [1.34192108, -0.13367336],</span>
<span class="gi">+            [0.62366841, -0.21312976],</span>
<span class="gi">+            [1.61091956, -0.40283504],</span>
<span class="gi">+            [-0.37162401, -2.19400981],</span>
<span class="gi">+            [0.74680821, 1.63827342],</span>
<span class="gi">+            [0.61472253, -0.82309052],</span>
<span class="gi">+            [0.19893132, -0.47761769],</span>
<span class="gi">+            [1.40301027, -0.83648734],</span>
<span class="gi">+            [-1.20515198, -1.02689695],</span>
<span class="gi">+            [-0.23374509, 0.18370049],</span>
<span class="gi">+            [-0.00288378, 0.84259929],</span>
<span class="gi">+            [1.79580611, -0.02219234],</span>
<span class="gi">+            [0.38307743, -0.05670439],</span>
<span class="gi">+            [0.70319159, -0.02571667],</span>
<span class="gi">+            [0.75052536, -0.19246518],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_validate_estimator_default():</span>
<span class="gi">+    smt = SMOTETomek(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smt.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.68481731, 0.51935141],</span>
<span class="gi">+            [1.34192108, -0.13367336],</span>
<span class="gi">+            [0.62366841, -0.21312976],</span>
<span class="gi">+            [1.61091956, -0.40283504],</span>
<span class="gi">+            [-0.37162401, -2.19400981],</span>
<span class="gi">+            [0.74680821, 1.63827342],</span>
<span class="gi">+            [0.61472253, -0.82309052],</span>
<span class="gi">+            [0.19893132, -0.47761769],</span>
<span class="gi">+            [1.40301027, -0.83648734],</span>
<span class="gi">+            [-1.20515198, -1.02689695],</span>
<span class="gi">+            [-0.23374509, 0.18370049],</span>
<span class="gi">+            [-0.00288378, 0.84259929],</span>
<span class="gi">+            [1.79580611, -0.02219234],</span>
<span class="gi">+            [0.38307743, -0.05670439],</span>
<span class="gi">+            [0.70319159, -0.02571667],</span>
<span class="gi">+            [0.75052536, -0.19246518],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_parallelisation():</span>
<span class="gi">+    # Check if default job count is None</span>
<span class="gi">+    smt = SMOTETomek(random_state=RND_SEED)</span>
<span class="gi">+    smt._validate_estimator()</span>
<span class="gi">+    assert smt.n_jobs is None</span>
<span class="gi">+    assert smt.smote_.n_jobs is None</span>
<span class="gi">+    assert smt.tomek_.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+    # Check if job count is set</span>
<span class="gi">+    smt = SMOTETomek(random_state=RND_SEED, n_jobs=8)</span>
<span class="gi">+    smt._validate_estimator()</span>
<span class="gi">+    assert smt.n_jobs == 8</span>
<span class="gi">+    assert smt.smote_.n_jobs == 8</span>
<span class="gi">+    assert smt.tomek_.n_jobs == 8</span>
<span class="gh">diff --git a/imblearn/datasets/_imbalance.py b/imblearn/datasets/_imbalance.py</span>
<span class="gh">index 53e40de..9e6e512 100644</span>
<span class="gd">--- a/imblearn/datasets/_imbalance.py</span>
<span class="gi">+++ b/imblearn/datasets/_imbalance.py</span>
<span class="gu">@@ -1,17 +1,31 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Transform a dataset into an imbalanced dataset.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Dayvid Oliveira</span>
<span class="gi">+#          Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="w"> </span>from collections.abc import Mapping
<span class="gi">+</span>
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="w"> </span>from ..utils import check_sampling_strategy
<span class="w"> </span>from ..utils._param_validation import validate_params


<span class="gd">-@validate_params({&#39;X&#39;: [&#39;array-like&#39;], &#39;y&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;sampling_strategy&#39;: [Mapping, callable, None], &#39;random_state&#39;: [</span>
<span class="gd">-    &#39;random_state&#39;], &#39;verbose&#39;: [&#39;boolean&#39;]}, prefer_skip_nested_validation</span>
<span class="gd">-    =True)</span>
<span class="gd">-def make_imbalance(X, y, *, sampling_strategy=None, random_state=None,</span>
<span class="gd">-    verbose=False, **kwargs):</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;X&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;sampling_strategy&quot;: [Mapping, callable, None],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+        &quot;verbose&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def make_imbalance(</span>
<span class="gi">+    X, y, *, sampling_strategy=None, random_state=None, verbose=False, **kwargs</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Turn a dataset into an imbalanced dataset with a specific sampling strategy.

<span class="w"> </span>    A simple toy dataset to visualize clustering and classification
<span class="gu">@@ -82,4 +96,22 @@ def make_imbalance(X, y, *, sampling_strategy=None, random_state=None,</span>
<span class="w"> </span>    &gt;&gt;&gt; print(f&#39;Distribution after imbalancing: {Counter(y_res)}&#39;)
<span class="w"> </span>    Distribution after imbalancing: Counter({2: 30, 1: 20, 0: 10})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = Counter(y)</span>
<span class="gi">+    # restrict ratio to be a dict or a callable</span>
<span class="gi">+    if isinstance(sampling_strategy, Mapping) or callable(sampling_strategy):</span>
<span class="gi">+        sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+            sampling_strategy, y, &quot;under-sampling&quot;, **kwargs</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        print(f&quot;The original target distribution in the dataset is: {target_stats}&quot;)</span>
<span class="gi">+    rus = RandomUnderSampler(</span>
<span class="gi">+        sampling_strategy=sampling_strategy_,</span>
<span class="gi">+        replacement=False,</span>
<span class="gi">+        random_state=random_state,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = rus.fit_resample(X, y)</span>
<span class="gi">+    if verbose:</span>
<span class="gi">+        print(f&quot;Make the dataset imbalanced: {Counter(y_resampled)}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/datasets/_zenodo.py b/imblearn/datasets/_zenodo.py</span>
<span class="gh">index f9062b9..a73ef37 100644</span>
<span class="gd">--- a/imblearn/datasets/_zenodo.py</span>
<span class="gi">+++ b/imblearn/datasets/_zenodo.py</span>
<span class="gu">@@ -39,25 +39,57 @@ References</span>
<span class="w"> </span>   Imbalanced Data Learning and their Application in Bioinformatics.&quot;
<span class="w"> </span>   Dissertation, Georgia State University, (2011).
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Guillaume Lemaitre</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>import tarfile
<span class="w"> </span>from collections import OrderedDict
<span class="w"> </span>from io import BytesIO
<span class="w"> </span>from os import makedirs
<span class="w"> </span>from os.path import isfile, join
<span class="w"> </span>from urllib.request import urlopen
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.datasets import get_data_home
<span class="w"> </span>from sklearn.utils import Bunch, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ..utils._param_validation import validate_params
<span class="gd">-URL = &#39;https://zenodo.org/record/61452/files/benchmark-imbalanced-learn.tar.gz&#39;</span>
<span class="gd">-PRE_FILENAME = &#39;x&#39;</span>
<span class="gd">-POST_FILENAME = &#39;data.npz&#39;</span>
<span class="gd">-MAP_NAME_ID_KEYS = [&#39;ecoli&#39;, &#39;optical_digits&#39;, &#39;satimage&#39;, &#39;pen_digits&#39;,</span>
<span class="gd">-    &#39;abalone&#39;, &#39;sick_euthyroid&#39;, &#39;spectrometer&#39;, &#39;car_eval_34&#39;, &#39;isolet&#39;,</span>
<span class="gd">-    &#39;us_crime&#39;, &#39;yeast_ml8&#39;, &#39;scene&#39;, &#39;libras_move&#39;, &#39;thyroid_sick&#39;,</span>
<span class="gd">-    &#39;coil_2000&#39;, &#39;arrhythmia&#39;, &#39;solar_flare_m0&#39;, &#39;oil&#39;, &#39;car_eval_4&#39;,</span>
<span class="gd">-    &#39;wine_quality&#39;, &#39;letter_img&#39;, &#39;yeast_me2&#39;, &#39;webpage&#39;, &#39;ozone_level&#39;,</span>
<span class="gd">-    &#39;mammography&#39;, &#39;protein_homo&#39;, &#39;abalone_19&#39;]</span>
<span class="gi">+</span>
<span class="gi">+URL = &quot;https://zenodo.org/record/61452/files/benchmark-imbalanced-learn.tar.gz&quot;</span>
<span class="gi">+PRE_FILENAME = &quot;x&quot;</span>
<span class="gi">+POST_FILENAME = &quot;data.npz&quot;</span>
<span class="gi">+</span>
<span class="gi">+MAP_NAME_ID_KEYS = [</span>
<span class="gi">+    &quot;ecoli&quot;,</span>
<span class="gi">+    &quot;optical_digits&quot;,</span>
<span class="gi">+    &quot;satimage&quot;,</span>
<span class="gi">+    &quot;pen_digits&quot;,</span>
<span class="gi">+    &quot;abalone&quot;,</span>
<span class="gi">+    &quot;sick_euthyroid&quot;,</span>
<span class="gi">+    &quot;spectrometer&quot;,</span>
<span class="gi">+    &quot;car_eval_34&quot;,</span>
<span class="gi">+    &quot;isolet&quot;,</span>
<span class="gi">+    &quot;us_crime&quot;,</span>
<span class="gi">+    &quot;yeast_ml8&quot;,</span>
<span class="gi">+    &quot;scene&quot;,</span>
<span class="gi">+    &quot;libras_move&quot;,</span>
<span class="gi">+    &quot;thyroid_sick&quot;,</span>
<span class="gi">+    &quot;coil_2000&quot;,</span>
<span class="gi">+    &quot;arrhythmia&quot;,</span>
<span class="gi">+    &quot;solar_flare_m0&quot;,</span>
<span class="gi">+    &quot;oil&quot;,</span>
<span class="gi">+    &quot;car_eval_4&quot;,</span>
<span class="gi">+    &quot;wine_quality&quot;,</span>
<span class="gi">+    &quot;letter_img&quot;,</span>
<span class="gi">+    &quot;yeast_me2&quot;,</span>
<span class="gi">+    &quot;webpage&quot;,</span>
<span class="gi">+    &quot;ozone_level&quot;,</span>
<span class="gi">+    &quot;mammography&quot;,</span>
<span class="gi">+    &quot;protein_homo&quot;,</span>
<span class="gi">+    &quot;abalone_19&quot;,</span>
<span class="gi">+]</span>
<span class="gi">+</span>
<span class="w"> </span>MAP_NAME_ID = OrderedDict()
<span class="w"> </span>MAP_ID_NAME = OrderedDict()
<span class="w"> </span>for v, k in enumerate(MAP_NAME_ID_KEYS):
<span class="gu">@@ -65,12 +97,26 @@ for v, k in enumerate(MAP_NAME_ID_KEYS):</span>
<span class="w"> </span>    MAP_ID_NAME[v + 1] = k


<span class="gd">-@validate_params({&#39;data_home&#39;: [None, str], &#39;filter_data&#39;: [None, tuple],</span>
<span class="gd">-    &#39;download_if_missing&#39;: [&#39;boolean&#39;], &#39;random_state&#39;: [&#39;random_state&#39;],</span>
<span class="gd">-    &#39;shuffle&#39;: [&#39;boolean&#39;], &#39;verbose&#39;: [&#39;boolean&#39;]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gd">-def fetch_datasets(*, data_home=None, filter_data=None, download_if_missing</span>
<span class="gd">-    =True, random_state=None, shuffle=False, verbose=False):</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;data_home&quot;: [None, str],</span>
<span class="gi">+        &quot;filter_data&quot;: [None, tuple],</span>
<span class="gi">+        &quot;download_if_missing&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+        &quot;shuffle&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;verbose&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def fetch_datasets(</span>
<span class="gi">+    *,</span>
<span class="gi">+    data_home=None,</span>
<span class="gi">+    filter_data=None,</span>
<span class="gi">+    download_if_missing=True,</span>
<span class="gi">+    random_state=None,</span>
<span class="gi">+    shuffle=False,</span>
<span class="gi">+    verbose=False,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Load the benchmark datasets from Zenodo, downloading it if necessary.

<span class="w"> </span>    .. versionadded:: 0.3
<span class="gu">@@ -185,4 +231,68 @@ def fetch_datasets(*, data_home=None, filter_data=None, download_if_missing</span>
<span class="w"> </span>       Imbalanced Data Learning and their Application in Bioinformatics.&quot;
<span class="w"> </span>       Dissertation, Georgia State University, (2011).
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    data_home = get_data_home(data_home=data_home)</span>
<span class="gi">+    zenodo_dir = join(data_home, &quot;zenodo&quot;)</span>
<span class="gi">+    datasets = OrderedDict()</span>
<span class="gi">+</span>
<span class="gi">+    if filter_data is None:</span>
<span class="gi">+        filter_data_ = MAP_NAME_ID.keys()</span>
<span class="gi">+    else:</span>
<span class="gi">+        list_data = MAP_NAME_ID.keys()</span>
<span class="gi">+        filter_data_ = []</span>
<span class="gi">+        for it in filter_data:</span>
<span class="gi">+            if isinstance(it, str):</span>
<span class="gi">+                if it not in list_data:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;{it} is not a dataset available. &quot;</span>
<span class="gi">+                        f&quot;The available datasets are {list_data}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    filter_data_.append(it)</span>
<span class="gi">+            elif isinstance(it, int):</span>
<span class="gi">+                if it &lt; 1 or it &gt; 27:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;The dataset with the ID={it} is not an &quot;</span>
<span class="gi">+                        f&quot;available dataset. The IDs are &quot;</span>
<span class="gi">+                        f&quot;{range(1, 28)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                else:</span>
<span class="gi">+                    # The index start at one, then we need to remove one</span>
<span class="gi">+                    # to not have issue with the indexing.</span>
<span class="gi">+                    filter_data_.append(MAP_ID_NAME[it])</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;The value in the tuple should be str or int.&quot;</span>
<span class="gi">+                    f&quot; Got {type(it)} instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    # go through the list and check if the data are available</span>
<span class="gi">+    for it in filter_data_:</span>
<span class="gi">+        filename = PRE_FILENAME + str(MAP_NAME_ID[it]) + POST_FILENAME</span>
<span class="gi">+        filename = join(zenodo_dir, filename)</span>
<span class="gi">+        available = isfile(filename)</span>
<span class="gi">+</span>
<span class="gi">+        if download_if_missing and not available:</span>
<span class="gi">+            makedirs(zenodo_dir, exist_ok=True)</span>
<span class="gi">+            if verbose:</span>
<span class="gi">+                print(&quot;Downloading %s&quot; % URL)</span>
<span class="gi">+            f = BytesIO(urlopen(URL).read())</span>
<span class="gi">+            tar = tarfile.open(fileobj=f)</span>
<span class="gi">+            tar.extractall(path=zenodo_dir)</span>
<span class="gi">+        elif not download_if_missing and not available:</span>
<span class="gi">+            raise IOError(&quot;Data not found and `download_if_missing` is False&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        data = np.load(filename)</span>
<span class="gi">+        X, y = data[&quot;data&quot;], data[&quot;label&quot;]</span>
<span class="gi">+</span>
<span class="gi">+        if shuffle:</span>
<span class="gi">+            ind = np.arange(X.shape[0])</span>
<span class="gi">+            rng = check_random_state(random_state)</span>
<span class="gi">+            rng.shuffle(ind)</span>
<span class="gi">+            X = X[ind]</span>
<span class="gi">+            y = y[ind]</span>
<span class="gi">+</span>
<span class="gi">+        datasets[it] = Bunch(data=X, target=y, DESCR=it)</span>
<span class="gi">+</span>
<span class="gi">+    return datasets</span>
<span class="gh">diff --git a/imblearn/datasets/tests/test_imbalance.py b/imblearn/datasets/tests/test_imbalance.py</span>
<span class="gh">index 1067628..ac3b417 100644</span>
<span class="gd">--- a/imblearn/datasets/tests/test_imbalance.py</span>
<span class="gi">+++ b/imblearn/datasets/tests/test_imbalance.py</span>
<span class="gu">@@ -1,6 +1,80 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module easy ensemble.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import load_iris
<span class="gi">+</span>
<span class="w"> </span>from imblearn.datasets import make_imbalance
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def iris():</span>
<span class="gi">+    return load_iris(return_X_y=True)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, err_msg&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ({0: -100, 1: 50, 2: 50}, &quot;in a class cannot be negative&quot;),</span>
<span class="gi">+        ({0: 10, 1: 70}, &quot;should be less or equal to the original&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_make_imbalance_error(iris, sampling_strategy, err_msg):</span>
<span class="gi">+    # we are reusing part of utils.check_sampling_strategy, however this is not</span>
<span class="gi">+    # cover in the common tests so we will repeat it here</span>
<span class="gi">+    X, y = iris</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        make_imbalance(X, y, sampling_strategy=sampling_strategy)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_make_imbalance_error_single_class(iris):</span>
<span class="gi">+    X, y = iris</span>
<span class="gi">+    y = np.zeros_like(y)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;needs to have more than 1 class.&quot;):</span>
<span class="gi">+        make_imbalance(X, y, sampling_strategy={0: 10})</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, expected_counts&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ({0: 10, 1: 20, 2: 30}, {0: 10, 1: 20, 2: 30}),</span>
<span class="gi">+        ({0: 10, 1: 20}, {0: 10, 1: 20, 2: 50}),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_make_imbalance_dict(iris, sampling_strategy, expected_counts):</span>
<span class="gi">+    X, y = iris</span>
<span class="gi">+    _, y_ = make_imbalance(X, y, sampling_strategy=sampling_strategy)</span>
<span class="gi">+    assert Counter(y_) == expected_counts</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;as_frame&quot;, [True, False], ids=[&quot;dataframe&quot;, &quot;array&quot;])</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, expected_counts&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;setosa&quot;: 10, &quot;versicolor&quot;: 20, &quot;virginica&quot;: 30},</span>
<span class="gi">+            {&quot;setosa&quot;: 10, &quot;versicolor&quot;: 20, &quot;virginica&quot;: 30},</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;setosa&quot;: 10, &quot;versicolor&quot;: 20},</span>
<span class="gi">+            {&quot;setosa&quot;: 10, &quot;versicolor&quot;: 20, &quot;virginica&quot;: 50},</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_make_imbalanced_iris(as_frame, sampling_strategy, expected_counts):</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    iris = load_iris(as_frame=as_frame)</span>
<span class="gi">+    X, y = iris.data, iris.target</span>
<span class="gi">+    y = iris.target_names[iris.target]</span>
<span class="gi">+    if as_frame:</span>
<span class="gi">+        y = pd.Series(iris.target_names[iris.target], name=&quot;target&quot;)</span>
<span class="gi">+    X_res, y_res = make_imbalance(X, y, sampling_strategy=sampling_strategy)</span>
<span class="gi">+    if as_frame:</span>
<span class="gi">+        assert hasattr(X_res, &quot;loc&quot;)</span>
<span class="gi">+        pd.testing.assert_index_equal(X_res.index, y_res.index)</span>
<span class="gi">+    assert Counter(y_res) == expected_counts</span>
<span class="gh">diff --git a/imblearn/datasets/tests/test_zenodo.py b/imblearn/datasets/tests/test_zenodo.py</span>
<span class="gh">index b9c2288..3854fd2 100644</span>
<span class="gd">--- a/imblearn/datasets/tests/test_zenodo.py</span>
<span class="gi">+++ b/imblearn/datasets/tests/test_zenodo.py</span>
<span class="gu">@@ -2,17 +2,97 @@</span>

<span class="w"> </span>Skipped if datasets is not already downloaded to data_home.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.utils._testing import SkipTest
<span class="gi">+</span>
<span class="w"> </span>from imblearn.datasets import fetch_datasets
<span class="gd">-DATASET_SHAPE = {&#39;ecoli&#39;: (336, 7), &#39;optical_digits&#39;: (5620, 64),</span>
<span class="gd">-    &#39;satimage&#39;: (6435, 36), &#39;pen_digits&#39;: (10992, 16), &#39;abalone&#39;: (4177, 10</span>
<span class="gd">-    ), &#39;sick_euthyroid&#39;: (3163, 42), &#39;spectrometer&#39;: (531, 93),</span>
<span class="gd">-    &#39;car_eval_34&#39;: (1728, 21), &#39;isolet&#39;: (7797, 617), &#39;us_crime&#39;: (1994, </span>
<span class="gd">-    100), &#39;yeast_ml8&#39;: (2417, 103), &#39;scene&#39;: (2407, 294), &#39;libras_move&#39;: (</span>
<span class="gd">-    360, 90), &#39;thyroid_sick&#39;: (3772, 52), &#39;coil_2000&#39;: (9822, 85),</span>
<span class="gd">-    &#39;arrhythmia&#39;: (452, 278), &#39;solar_flare_m0&#39;: (1389, 32), &#39;oil&#39;: (937, 49</span>
<span class="gd">-    ), &#39;car_eval_4&#39;: (1728, 21), &#39;wine_quality&#39;: (4898, 11), &#39;letter_img&#39;:</span>
<span class="gd">-    (20000, 16), &#39;yeast_me2&#39;: (1484, 8), &#39;webpage&#39;: (34780, 300),</span>
<span class="gd">-    &#39;ozone_level&#39;: (2536, 72), &#39;mammography&#39;: (11183, 6), &#39;protein_homo&#39;: (</span>
<span class="gd">-    145751, 74), &#39;abalone_19&#39;: (4177, 10)}</span>
<span class="gi">+</span>
<span class="gi">+DATASET_SHAPE = {</span>
<span class="gi">+    &quot;ecoli&quot;: (336, 7),</span>
<span class="gi">+    &quot;optical_digits&quot;: (5620, 64),</span>
<span class="gi">+    &quot;satimage&quot;: (6435, 36),</span>
<span class="gi">+    &quot;pen_digits&quot;: (10992, 16),</span>
<span class="gi">+    &quot;abalone&quot;: (4177, 10),</span>
<span class="gi">+    &quot;sick_euthyroid&quot;: (3163, 42),</span>
<span class="gi">+    &quot;spectrometer&quot;: (531, 93),</span>
<span class="gi">+    &quot;car_eval_34&quot;: (1728, 21),</span>
<span class="gi">+    &quot;isolet&quot;: (7797, 617),</span>
<span class="gi">+    &quot;us_crime&quot;: (1994, 100),</span>
<span class="gi">+    &quot;yeast_ml8&quot;: (2417, 103),</span>
<span class="gi">+    &quot;scene&quot;: (2407, 294),</span>
<span class="gi">+    &quot;libras_move&quot;: (360, 90),</span>
<span class="gi">+    &quot;thyroid_sick&quot;: (3772, 52),</span>
<span class="gi">+    &quot;coil_2000&quot;: (9822, 85),</span>
<span class="gi">+    &quot;arrhythmia&quot;: (452, 278),</span>
<span class="gi">+    &quot;solar_flare_m0&quot;: (1389, 32),</span>
<span class="gi">+    &quot;oil&quot;: (937, 49),</span>
<span class="gi">+    &quot;car_eval_4&quot;: (1728, 21),</span>
<span class="gi">+    &quot;wine_quality&quot;: (4898, 11),</span>
<span class="gi">+    &quot;letter_img&quot;: (20000, 16),</span>
<span class="gi">+    &quot;yeast_me2&quot;: (1484, 8),</span>
<span class="gi">+    &quot;webpage&quot;: (34780, 300),</span>
<span class="gi">+    &quot;ozone_level&quot;: (2536, 72),</span>
<span class="gi">+    &quot;mammography&quot;: (11183, 6),</span>
<span class="gi">+    &quot;protein_homo&quot;: (145751, 74),</span>
<span class="gi">+    &quot;abalone_19&quot;: (4177, 10),</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def fetch(*args, **kwargs):</span>
<span class="gi">+    return fetch_datasets(*args, download_if_missing=True, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.xfail</span>
<span class="gi">+def test_fetch():</span>
<span class="gi">+    try:</span>
<span class="gi">+        datasets1 = fetch(shuffle=True, random_state=42)</span>
<span class="gi">+    except IOError:</span>
<span class="gi">+        raise SkipTest(&quot;Zenodo dataset can not be loaded.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    datasets2 = fetch(shuffle=True, random_state=37)</span>
<span class="gi">+</span>
<span class="gi">+    for k in DATASET_SHAPE.keys():</span>
<span class="gi">+        X1, X2 = datasets1[k].data, datasets2[k].data</span>
<span class="gi">+        assert DATASET_SHAPE[k] == X1.shape</span>
<span class="gi">+        assert X1.shape == X2.shape</span>
<span class="gi">+</span>
<span class="gi">+        y1, y2 = datasets1[k].target, datasets2[k].target</span>
<span class="gi">+        assert (X1.shape[0],) == y1.shape</span>
<span class="gi">+        assert (X1.shape[0],) == y2.shape</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_fetch_filter():</span>
<span class="gi">+    try:</span>
<span class="gi">+        datasets1 = fetch(filter_data=tuple([1]), shuffle=True, random_state=42)</span>
<span class="gi">+    except IOError:</span>
<span class="gi">+        raise SkipTest(&quot;Zenodo dataset can not be loaded.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    datasets2 = fetch(filter_data=tuple([&quot;ecoli&quot;]), shuffle=True, random_state=37)</span>
<span class="gi">+</span>
<span class="gi">+    X1, X2 = datasets1[&quot;ecoli&quot;].data, datasets2[&quot;ecoli&quot;].data</span>
<span class="gi">+    assert DATASET_SHAPE[&quot;ecoli&quot;] == X1.shape</span>
<span class="gi">+    assert X1.shape == X2.shape</span>
<span class="gi">+</span>
<span class="gi">+    assert X1.sum() == pytest.approx(X2.sum())</span>
<span class="gi">+</span>
<span class="gi">+    y1, y2 = datasets1[&quot;ecoli&quot;].target, datasets2[&quot;ecoli&quot;].target</span>
<span class="gi">+    assert (X1.shape[0],) == y1.shape</span>
<span class="gi">+    assert (X1.shape[0],) == y2.shape</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;filter_data, err_msg&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ((&quot;rnf&quot;,), &quot;is not a dataset available&quot;),</span>
<span class="gi">+        ((-1,), &quot;dataset with the ID=&quot;),</span>
<span class="gi">+        ((100,), &quot;dataset with the ID=&quot;),</span>
<span class="gi">+        ((1.00,), &quot;value in the tuple&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_fetch_error(filter_data, err_msg):</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        fetch_datasets(filter_data=filter_data)</span>
<span class="gh">diff --git a/imblearn/ensemble/_bagging.py b/imblearn/ensemble/_bagging.py</span>
<span class="gh">index b1905ed..acb0c70 100644</span>
<span class="gd">--- a/imblearn/ensemble/_bagging.py</span>
<span class="gi">+++ b/imblearn/ensemble/_bagging.py</span>
<span class="gu">@@ -1,7 +1,13 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Bagging classifier trained on balanced bootstrap samples.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import copy
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.base import clone
<span class="gu">@@ -12,11 +18,14 @@ from sklearn.exceptions import NotFittedError</span>
<span class="w"> </span>from sklearn.tree import DecisionTreeClassifier
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.validation import check_is_fitted
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # scikit-learn &gt;= 1.2</span>
<span class="w"> </span>    from sklearn.utils.parallel import Parallel, delayed
<span class="w"> </span>except (ImportError, ModuleNotFoundError):
<span class="w"> </span>    from joblib import Parallel
<span class="w"> </span>    from sklearn.utils.fixes import delayed
<span class="gi">+</span>
<span class="w"> </span>from ..base import _ParamsValidationMixin
<span class="w"> </span>from ..pipeline import Pipeline
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="gu">@@ -27,12 +36,15 @@ from ..utils._docstring import _n_jobs_docstring, _random_state_docstring</span>
<span class="w"> </span>from ..utils._param_validation import HasMethods, Interval, StrOptions
<span class="w"> </span>from ..utils.fixes import _fit_context
<span class="w"> </span>from ._common import _bagging_parameter_constraints, _estimator_has
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):
<span class="w"> </span>    &quot;&quot;&quot;A Bagging classifier with additional balancing.

<span class="gu">@@ -235,43 +247,108 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>    [[ 23   0]
<span class="w"> </span>     [  2 225]]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    if sklearn_version &gt;= parse_version(&#39;1.4&#39;):</span>
<span class="gd">-        _parameter_constraints = copy.deepcopy(BaggingClassifier.</span>
<span class="gd">-            _parameter_constraints)</span>
<span class="gi">+</span>
<span class="gi">+    # make a deepcopy to not modify the original dictionary</span>
<span class="gi">+    if sklearn_version &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+        _parameter_constraints = copy.deepcopy(BaggingClassifier._parameter_constraints)</span>
<span class="w"> </span>    else:
<span class="w"> </span>        _parameter_constraints = copy.deepcopy(_bagging_parameter_constraints)
<span class="gd">-    _parameter_constraints.update({&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), dict, callable],</span>
<span class="gd">-        &#39;replacement&#39;: [&#39;boolean&#39;], &#39;sampler&#39;: [HasMethods([&#39;fit_resample&#39;]</span>
<span class="gd">-        ), None]})</span>
<span class="gd">-    if &#39;base_estimator&#39; in _parameter_constraints:</span>
<span class="gd">-        del _parameter_constraints[&#39;base_estimator&#39;]</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, estimator=None, n_estimators=10, *, max_samples=1.0,</span>
<span class="gd">-        max_features=1.0, bootstrap=True, bootstrap_features=False,</span>
<span class="gd">-        oob_score=False, warm_start=False, sampling_strategy=&#39;auto&#39;,</span>
<span class="gd">-        replacement=False, n_jobs=None, random_state=None, verbose=0,</span>
<span class="gd">-        sampler=None):</span>
<span class="gd">-        super().__init__(n_estimators=n_estimators, max_samples=max_samples,</span>
<span class="gd">-            max_features=max_features, bootstrap=bootstrap,</span>
<span class="gd">-            bootstrap_features=bootstrap_features, oob_score=oob_score,</span>
<span class="gd">-            warm_start=warm_start, n_jobs=n_jobs, random_state=random_state,</span>
<span class="gd">-            verbose=verbose)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints.update(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;sampling_strategy&quot;: [</span>
<span class="gi">+                Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+                StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+                dict,</span>
<span class="gi">+                callable,</span>
<span class="gi">+            ],</span>
<span class="gi">+            &quot;replacement&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+            &quot;sampler&quot;: [HasMethods([&quot;fit_resample&quot;]), None],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.4</span>
<span class="gi">+    if &quot;base_estimator&quot; in _parameter_constraints:</span>
<span class="gi">+        del _parameter_constraints[&quot;base_estimator&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        estimator=None,</span>
<span class="gi">+        n_estimators=10,</span>
<span class="gi">+        *,</span>
<span class="gi">+        max_samples=1.0,</span>
<span class="gi">+        max_features=1.0,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+        bootstrap_features=False,</span>
<span class="gi">+        oob_score=False,</span>
<span class="gi">+        warm_start=False,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        replacement=False,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        verbose=0,</span>
<span class="gi">+        sampler=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            n_estimators=n_estimators,</span>
<span class="gi">+            max_samples=max_samples,</span>
<span class="gi">+            max_features=max_features,</span>
<span class="gi">+            bootstrap=bootstrap,</span>
<span class="gi">+            bootstrap_features=bootstrap_features,</span>
<span class="gi">+            oob_score=oob_score,</span>
<span class="gi">+            warm_start=warm_start,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            verbose=verbose,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.estimator = estimator
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.replacement = replacement
<span class="w"> </span>        self.sampler = sampler

<span class="gi">+    def _validate_y(self, y):</span>
<span class="gi">+        y_encoded = super()._validate_y(y)</span>
<span class="gi">+        if (</span>
<span class="gi">+            isinstance(self.sampling_strategy, dict)</span>
<span class="gi">+            and self.sampler_._sampling_type != &quot;bypass&quot;</span>
<span class="gi">+        ):</span>
<span class="gi">+            self._sampling_strategy = {</span>
<span class="gi">+                np.where(self.classes_ == key)[0][0]: value</span>
<span class="gi">+                for key, value in check_sampling_strategy(</span>
<span class="gi">+                    self.sampling_strategy,</span>
<span class="gi">+                    y,</span>
<span class="gi">+                    self.sampler_._sampling_type,</span>
<span class="gi">+                ).items()</span>
<span class="gi">+            }</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._sampling_strategy = self.sampling_strategy</span>
<span class="gi">+        return y_encoded</span>
<span class="gi">+</span>
<span class="w"> </span>    def _validate_estimator(self, default=DecisionTreeClassifier()):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is not None:</span>
<span class="gi">+            estimator = clone(self.estimator)</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator = clone(default)</span>

<span class="gi">+        if self.sampler_._sampling_type != &quot;bypass&quot;:</span>
<span class="gi">+            self.sampler_.set_params(sampling_strategy=self._sampling_strategy)</span>
<span class="gi">+</span>
<span class="gi">+        self.estimator_ = Pipeline(</span>
<span class="gi">+            [(&quot;sampler&quot;, self.sampler_), (&quot;classifier&quot;, estimator)]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when supporting scikit-learn&gt;=1.2</span>
<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`n_features_` was deprecated in scikit-learn 1.0. This attribute will &quot;</span>
<span class="gi">+            &quot;not be accessible when the minimum supported version of scikit-learn &quot;</span>
<span class="gi">+            &quot;is 1.2.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.n_features_in_</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=False)
<span class="w"> </span>    def fit(self, X, y):
<span class="gu">@@ -292,9 +369,27 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Fitted estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(_estimator_has(&#39;decision_function&#39;))</span>
<span class="gi">+        # overwrite the base class method by disallowing `sample_weight`</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        return super().fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):</span>
<span class="gi">+        check_target_type(y)</span>
<span class="gi">+        # the sampler needs to be validated before to call _fit because</span>
<span class="gi">+        # _validate_y is called before _validate_estimator and would require</span>
<span class="gi">+        # to know which type of sampler we are using.</span>
<span class="gi">+        if self.sampler is None:</span>
<span class="gi">+            self.sampler_ = RandomUnderSampler(</span>
<span class="gi">+                replacement=self.replacement,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.sampler_ = clone(self.sampler)</span>
<span class="gi">+        # RandomUnderSampler is not supporting sample_weight. We need to pass</span>
<span class="gi">+        # None.</span>
<span class="gi">+        return super()._fit(X, y, self.max_samples)</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.1</span>
<span class="gi">+    @available_if(_estimator_has(&quot;decision_function&quot;))</span>
<span class="w"> </span>    def decision_function(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Average of the decision functions of the base classifiers.

<span class="gu">@@ -312,9 +407,57 @@ class BalancedBaggingClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>            ``classes_``. Regression and binary classification are special
<span class="w"> </span>            cases with ``k == 1``, otherwise ``k==n_classes``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+</span>
<span class="gi">+        # Check data</span>
<span class="gi">+        X = self._validate_data(</span>
<span class="gi">+            X,</span>
<span class="gi">+            accept_sparse=[&quot;csr&quot;, &quot;csc&quot;],</span>
<span class="gi">+            dtype=None,</span>
<span class="gi">+            force_all_finite=False,</span>
<span class="gi">+            reset=False,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(</span>
<span class="gi">+            delayed(_parallel_decision_function)(</span>
<span class="gi">+                self.estimators_[starts[i] : starts[i + 1]],</span>
<span class="gi">+                self.estimators_features_[starts[i] : starts[i + 1]],</span>
<span class="gi">+                X,</span>
<span class="gi">+            )</span>
<span class="gi">+            for i in range(n_jobs)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Reduce</span>
<span class="gi">+        decisions = sum(all_decisions) / self.n_estimators</span>
<span class="gi">+</span>
<span class="gi">+        return decisions</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def base_estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Attribute for older sklearn version compatibility.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        error = AttributeError(</span>
<span class="gi">+            f&quot;{self.__class__.__name__} object has no attribute &#39;base_estimator_&#39;.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        if sklearn_version &lt; parse_version(&quot;1.2&quot;):</span>
<span class="gi">+            # The base class require to have the attribute defined. For scikit-learn</span>
<span class="gi">+            # &gt; 1.2, we are going to raise an error.</span>
<span class="gi">+            try:</span>
<span class="gi">+                check_is_fitted(self)</span>
<span class="gi">+                return self.estimator_</span>
<span class="gi">+            except NotFittedError:</span>
<span class="gi">+                raise error</span>
<span class="gi">+        raise error</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        tags = super()._more_tags()</span>
<span class="gi">+        tags_key = &quot;_xfail_checks&quot;</span>
<span class="gi">+        failing_test = &quot;check_estimators_nan_inf&quot;</span>
<span class="gi">+        reason = &quot;Fails because the sampler removed infinity and NaN values&quot;</span>
<span class="gi">+        if tags_key in tags:</span>
<span class="gi">+            tags[tags_key][failing_test] = reason</span>
<span class="gi">+        else:</span>
<span class="gi">+            tags[tags_key] = {failing_test: reason}</span>
<span class="gi">+        return tags</span>
<span class="gh">diff --git a/imblearn/ensemble/_common.py b/imblearn/ensemble/_common.py</span>
<span class="gh">index f7dcb6e..588fa5e 100644</span>
<span class="gd">--- a/imblearn/ensemble/_common.py</span>
<span class="gi">+++ b/imblearn/ensemble/_common.py</span>
<span class="gu">@@ -1,6 +1,14 @@</span>
<span class="w"> </span>from numbers import Integral, Real
<span class="gi">+</span>
<span class="w"> </span>from sklearn.tree._criterion import Criterion
<span class="gd">-from ..utils._param_validation import HasMethods, Hidden, Interval, RealNotInt, StrOptions</span>
<span class="gi">+</span>
<span class="gi">+from ..utils._param_validation import (</span>
<span class="gi">+    HasMethods,</span>
<span class="gi">+    Hidden,</span>
<span class="gi">+    Interval,</span>
<span class="gi">+    RealNotInt,</span>
<span class="gi">+    StrOptions,</span>
<span class="gi">+)</span>


<span class="w"> </span>def _estimator_has(attr):
<span class="gu">@@ -8,41 +16,90 @@ def _estimator_has(attr):</span>
<span class="w"> </span>    First, we check the first fitted estimator if available, otherwise we
<span class="w"> </span>    check the estimator attribute.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-_bagging_parameter_constraints = {&#39;estimator&#39;: [HasMethods([&#39;fit&#39;,</span>
<span class="gd">-    &#39;predict&#39;]), None], &#39;n_estimators&#39;: [Interval(Integral, 1, None, closed</span>
<span class="gd">-    =&#39;left&#39;)], &#39;max_samples&#39;: [Interval(Integral, 1, None, closed=&#39;left&#39;),</span>
<span class="gd">-    Interval(RealNotInt, 0, 1, closed=&#39;right&#39;)], &#39;max_features&#39;: [Interval(</span>
<span class="gd">-    Integral, 1, None, closed=&#39;left&#39;), Interval(RealNotInt, 0, 1, closed=</span>
<span class="gd">-    &#39;right&#39;)], &#39;bootstrap&#39;: [&#39;boolean&#39;], &#39;bootstrap_features&#39;: [&#39;boolean&#39;],</span>
<span class="gd">-    &#39;oob_score&#39;: [&#39;boolean&#39;], &#39;warm_start&#39;: [&#39;boolean&#39;], &#39;n_jobs&#39;: [None,</span>
<span class="gd">-    Integral], &#39;random_state&#39;: [&#39;random_state&#39;], &#39;verbose&#39;: [&#39;verbose&#39;],</span>
<span class="gd">-    &#39;base_estimator&#39;: [HasMethods([&#39;fit&#39;, &#39;predict&#39;]), StrOptions({</span>
<span class="gd">-    &#39;deprecated&#39;}), None]}</span>
<span class="gd">-_adaboost_classifier_parameter_constraints = {&#39;estimator&#39;: [HasMethods([</span>
<span class="gd">-    &#39;fit&#39;, &#39;predict&#39;]), None], &#39;n_estimators&#39;: [Interval(Integral, 1, None,</span>
<span class="gd">-    closed=&#39;left&#39;)], &#39;learning_rate&#39;: [Interval(Real, 0, None, closed=</span>
<span class="gd">-    &#39;neither&#39;)], &#39;random_state&#39;: [&#39;random_state&#39;], &#39;base_estimator&#39;: [</span>
<span class="gd">-    HasMethods([&#39;fit&#39;, &#39;predict&#39;]), StrOptions({&#39;deprecated&#39;})],</span>
<span class="gd">-    &#39;algorithm&#39;: [StrOptions({&#39;SAMME&#39;, &#39;SAMME.R&#39;})]}</span>
<span class="gd">-_random_forest_classifier_parameter_constraints = {&#39;n_estimators&#39;: [</span>
<span class="gd">-    Interval(Integral, 1, None, closed=&#39;left&#39;)], &#39;bootstrap&#39;: [&#39;boolean&#39;],</span>
<span class="gd">-    &#39;oob_score&#39;: [&#39;boolean&#39;], &#39;n_jobs&#39;: [Integral, None], &#39;random_state&#39;: [</span>
<span class="gd">-    &#39;random_state&#39;], &#39;verbose&#39;: [&#39;verbose&#39;], &#39;warm_start&#39;: [&#39;boolean&#39;],</span>
<span class="gd">-    &#39;criterion&#39;: [StrOptions({&#39;gini&#39;, &#39;entropy&#39;, &#39;log_loss&#39;}), Hidden(</span>
<span class="gd">-    Criterion)], &#39;max_samples&#39;: [None, Interval(Real, 0.0, 1.0, closed=</span>
<span class="gd">-    &#39;right&#39;), Interval(Integral, 1, None, closed=&#39;left&#39;)], &#39;max_depth&#39;: [</span>
<span class="gd">-    Interval(Integral, 1, None, closed=&#39;left&#39;), None], &#39;min_samples_split&#39;:</span>
<span class="gd">-    [Interval(Integral, 2, None, closed=&#39;left&#39;), Interval(RealNotInt, 0.0, </span>
<span class="gd">-    1.0, closed=&#39;right&#39;)], &#39;min_samples_leaf&#39;: [Interval(Integral, 1, None,</span>
<span class="gd">-    closed=&#39;left&#39;), Interval(RealNotInt, 0.0, 1.0, closed=&#39;neither&#39;)],</span>
<span class="gd">-    &#39;min_weight_fraction_leaf&#39;: [Interval(Real, 0.0, 0.5, closed=&#39;both&#39;)],</span>
<span class="gd">-    &#39;max_features&#39;: [Interval(Integral, 1, None, closed=&#39;left&#39;), Interval(</span>
<span class="gd">-    RealNotInt, 0.0, 1.0, closed=&#39;right&#39;), StrOptions({&#39;sqrt&#39;, &#39;log2&#39;}),</span>
<span class="gd">-    None], &#39;max_leaf_nodes&#39;: [Interval(Integral, 2, None, closed=&#39;left&#39;),</span>
<span class="gd">-    None], &#39;min_impurity_decrease&#39;: [Interval(Real, 0.0, None, closed=</span>
<span class="gd">-    &#39;left&#39;)], &#39;ccp_alpha&#39;: [Interval(Real, 0.0, None, closed=&#39;left&#39;)],</span>
<span class="gd">-    &#39;class_weight&#39;: [StrOptions({&#39;balanced_subsample&#39;, &#39;balanced&#39;}), dict,</span>
<span class="gd">-    list, None], &#39;monotonic_cst&#39;: [&#39;array-like&#39;, None]}</span>
<span class="gi">+</span>
<span class="gi">+    def check(self):</span>
<span class="gi">+        if hasattr(self, &quot;estimators_&quot;):</span>
<span class="gi">+            return hasattr(self.estimators_[0], attr)</span>
<span class="gi">+        elif self.estimator is not None:</span>
<span class="gi">+            return hasattr(self.estimator, attr)</span>
<span class="gi">+        else:  # TODO(1.4): Remove when the base_estimator deprecation cycle ends</span>
<span class="gi">+            return hasattr(self.base_estimator, attr)</span>
<span class="gi">+</span>
<span class="gi">+    return check</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+_bagging_parameter_constraints = {</span>
<span class="gi">+    &quot;estimator&quot;: [HasMethods([&quot;fit&quot;, &quot;predict&quot;]), None],</span>
<span class="gi">+    &quot;n_estimators&quot;: [Interval(Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    &quot;max_samples&quot;: [</span>
<span class="gi">+        Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(RealNotInt, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;max_features&quot;: [</span>
<span class="gi">+        Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(RealNotInt, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;bootstrap&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;bootstrap_features&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;oob_score&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;warm_start&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;n_jobs&quot;: [None, Integral],</span>
<span class="gi">+    &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    &quot;verbose&quot;: [&quot;verbose&quot;],</span>
<span class="gi">+    &quot;base_estimator&quot;: [</span>
<span class="gi">+        HasMethods([&quot;fit&quot;, &quot;predict&quot;]),</span>
<span class="gi">+        StrOptions({&quot;deprecated&quot;}),</span>
<span class="gi">+        None,</span>
<span class="gi">+    ],</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+_adaboost_classifier_parameter_constraints = {</span>
<span class="gi">+    &quot;estimator&quot;: [HasMethods([&quot;fit&quot;, &quot;predict&quot;]), None],</span>
<span class="gi">+    &quot;n_estimators&quot;: [Interval(Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    &quot;learning_rate&quot;: [Interval(Real, 0, None, closed=&quot;neither&quot;)],</span>
<span class="gi">+    &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    &quot;base_estimator&quot;: [HasMethods([&quot;fit&quot;, &quot;predict&quot;]), StrOptions({&quot;deprecated&quot;})],</span>
<span class="gi">+    &quot;algorithm&quot;: [StrOptions({&quot;SAMME&quot;, &quot;SAMME.R&quot;})],</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+_random_forest_classifier_parameter_constraints = {</span>
<span class="gi">+    &quot;n_estimators&quot;: [Interval(Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    &quot;bootstrap&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;oob_score&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;n_jobs&quot;: [Integral, None],</span>
<span class="gi">+    &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    &quot;verbose&quot;: [&quot;verbose&quot;],</span>
<span class="gi">+    &quot;warm_start&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    &quot;criterion&quot;: [StrOptions({&quot;gini&quot;, &quot;entropy&quot;, &quot;log_loss&quot;}), Hidden(Criterion)],</span>
<span class="gi">+    &quot;max_samples&quot;: [</span>
<span class="gi">+        None,</span>
<span class="gi">+        Interval(Real, 0.0, 1.0, closed=&quot;right&quot;),</span>
<span class="gi">+        Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;max_depth&quot;: [Interval(Integral, 1, None, closed=&quot;left&quot;), None],</span>
<span class="gi">+    &quot;min_samples_split&quot;: [</span>
<span class="gi">+        Interval(Integral, 2, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(RealNotInt, 0.0, 1.0, closed=&quot;right&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;min_samples_leaf&quot;: [</span>
<span class="gi">+        Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(RealNotInt, 0.0, 1.0, closed=&quot;neither&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;min_weight_fraction_leaf&quot;: [Interval(Real, 0.0, 0.5, closed=&quot;both&quot;)],</span>
<span class="gi">+    &quot;max_features&quot;: [</span>
<span class="gi">+        Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(RealNotInt, 0.0, 1.0, closed=&quot;right&quot;),</span>
<span class="gi">+        StrOptions({&quot;sqrt&quot;, &quot;log2&quot;}),</span>
<span class="gi">+        None,</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;max_leaf_nodes&quot;: [Interval(Integral, 2, None, closed=&quot;left&quot;), None],</span>
<span class="gi">+    &quot;min_impurity_decrease&quot;: [Interval(Real, 0.0, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    &quot;ccp_alpha&quot;: [Interval(Real, 0.0, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    &quot;class_weight&quot;: [</span>
<span class="gi">+        StrOptions({&quot;balanced_subsample&quot;, &quot;balanced&quot;}),</span>
<span class="gi">+        dict,</span>
<span class="gi">+        list,</span>
<span class="gi">+        None,</span>
<span class="gi">+    ],</span>
<span class="gi">+    &quot;monotonic_cst&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+}</span>
<span class="gh">diff --git a/imblearn/ensemble/_easy_ensemble.py b/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gh">index 6d79bd6..e3c8574 100644</span>
<span class="gd">--- a/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gi">+++ b/imblearn/ensemble/_easy_ensemble.py</span>
<span class="gu">@@ -1,7 +1,13 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling using easy ensemble.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import copy
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.base import clone
<span class="gu">@@ -12,11 +18,14 @@ from sklearn.exceptions import NotFittedError</span>
<span class="w"> </span>from sklearn.utils._tags import _safe_tags
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.validation import check_is_fitted
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # scikit-learn &gt;= 1.2</span>
<span class="w"> </span>    from sklearn.utils.parallel import Parallel, delayed
<span class="w"> </span>except (ImportError, ModuleNotFoundError):
<span class="w"> </span>    from joblib import Parallel
<span class="w"> </span>    from sklearn.utils.fixes import delayed
<span class="gi">+</span>
<span class="w"> </span>from ..base import _ParamsValidationMixin
<span class="w"> </span>from ..pipeline import Pipeline
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="gu">@@ -27,13 +36,16 @@ from ..utils._docstring import _n_jobs_docstring, _random_state_docstring</span>
<span class="w"> </span>from ..utils._param_validation import Interval, StrOptions
<span class="w"> </span>from ..utils.fixes import _fit_context
<span class="w"> </span>from ._common import _bagging_parameter_constraints, _estimator_has
<span class="gi">+</span>
<span class="w"> </span>MAX_INT = np.iinfo(np.int32).max
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):
<span class="w"> </span>    &quot;&quot;&quot;Bag of balanced boosted learners also known as EasyEnsemble.

<span class="gu">@@ -160,43 +172,106 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>    [[ 23   0]
<span class="w"> </span>     [  2 225]]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    if sklearn_version &gt;= parse_version(&#39;1.4&#39;):</span>
<span class="gd">-        _parameter_constraints = copy.deepcopy(BaggingClassifier.</span>
<span class="gd">-            _parameter_constraints)</span>
<span class="gi">+</span>
<span class="gi">+    # make a deepcopy to not modify the original dictionary</span>
<span class="gi">+    if sklearn_version &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+        _parameter_constraints = copy.deepcopy(BaggingClassifier._parameter_constraints)</span>
<span class="w"> </span>    else:
<span class="w"> </span>        _parameter_constraints = copy.deepcopy(_bagging_parameter_constraints)
<span class="gd">-    excluded_params = {&#39;bootstrap&#39;, &#39;bootstrap_features&#39;, &#39;max_features&#39;,</span>
<span class="gd">-        &#39;oob_score&#39;, &#39;max_samples&#39;}</span>
<span class="gi">+</span>
<span class="gi">+    excluded_params = {</span>
<span class="gi">+        &quot;bootstrap&quot;,</span>
<span class="gi">+        &quot;bootstrap_features&quot;,</span>
<span class="gi">+        &quot;max_features&quot;,</span>
<span class="gi">+        &quot;oob_score&quot;,</span>
<span class="gi">+        &quot;max_samples&quot;,</span>
<span class="gi">+    }</span>
<span class="w"> </span>    for param in excluded_params:
<span class="w"> </span>        _parameter_constraints.pop(param, None)
<span class="gd">-    _parameter_constraints.update({&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), dict, callable],</span>
<span class="gd">-        &#39;replacement&#39;: [&#39;boolean&#39;]})</span>
<span class="gd">-    if &#39;base_estimator&#39; in _parameter_constraints:</span>
<span class="gd">-        del _parameter_constraints[&#39;base_estimator&#39;]</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, n_estimators=10, estimator=None, *, warm_start=False,</span>
<span class="gd">-        sampling_strategy=&#39;auto&#39;, replacement=False, n_jobs=None,</span>
<span class="gd">-        random_state=None, verbose=0):</span>
<span class="gd">-        super().__init__(n_estimators=n_estimators, max_samples=1.0,</span>
<span class="gd">-            max_features=1.0, bootstrap=False, bootstrap_features=False,</span>
<span class="gd">-            oob_score=False, warm_start=warm_start, n_jobs=n_jobs,</span>
<span class="gd">-            random_state=random_state, verbose=verbose)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints.update(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;sampling_strategy&quot;: [</span>
<span class="gi">+                Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+                StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+                dict,</span>
<span class="gi">+                callable,</span>
<span class="gi">+            ],</span>
<span class="gi">+            &quot;replacement&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.4</span>
<span class="gi">+    if &quot;base_estimator&quot; in _parameter_constraints:</span>
<span class="gi">+        del _parameter_constraints[&quot;base_estimator&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        n_estimators=10,</span>
<span class="gi">+        estimator=None,</span>
<span class="gi">+        *,</span>
<span class="gi">+        warm_start=False,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        replacement=False,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        verbose=0,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            n_estimators=n_estimators,</span>
<span class="gi">+            max_samples=1.0,</span>
<span class="gi">+            max_features=1.0,</span>
<span class="gi">+            bootstrap=False,</span>
<span class="gi">+            bootstrap_features=False,</span>
<span class="gi">+            oob_score=False,</span>
<span class="gi">+            warm_start=warm_start,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            verbose=verbose,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.estimator = estimator
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.replacement = replacement

<span class="gd">-    def _validate_estimator(self, default=AdaBoostClassifier(algorithm=&#39;SAMME&#39;)</span>
<span class="gd">-        ):</span>
<span class="gi">+    def _validate_y(self, y):</span>
<span class="gi">+        y_encoded = super()._validate_y(y)</span>
<span class="gi">+        if isinstance(self.sampling_strategy, dict):</span>
<span class="gi">+            self._sampling_strategy = {</span>
<span class="gi">+                np.where(self.classes_ == key)[0][0]: value</span>
<span class="gi">+                for key, value in check_sampling_strategy(</span>
<span class="gi">+                    self.sampling_strategy,</span>
<span class="gi">+                    y,</span>
<span class="gi">+                    &quot;under-sampling&quot;,</span>
<span class="gi">+                ).items()</span>
<span class="gi">+            }</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._sampling_strategy = self.sampling_strategy</span>
<span class="gi">+        return y_encoded</span>
<span class="gi">+</span>
<span class="gi">+    def _validate_estimator(self, default=AdaBoostClassifier(algorithm=&quot;SAMME&quot;)):</span>
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        if self.estimator is not None:</span>
<span class="gi">+            estimator = clone(self.estimator)</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator = clone(default)</span>
<span class="gi">+</span>
<span class="gi">+        sampler = RandomUnderSampler(</span>
<span class="gi">+            sampling_strategy=self._sampling_strategy,</span>
<span class="gi">+            replacement=self.replacement,</span>
<span class="gi">+        )</span>
<span class="gi">+        self.estimator_ = Pipeline([(&quot;sampler&quot;, sampler), (&quot;classifier&quot;, estimator)])</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when supporting scikit-learn&gt;=1.2</span>
<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`n_features_` was deprecated in scikit-learn 1.0. This attribute will &quot;</span>
<span class="gi">+            &quot;not be accessible when the minimum supported version of scikit-learn &quot;</span>
<span class="gi">+            &quot;is 1.2.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.n_features_in_</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=False)
<span class="w"> </span>    def fit(self, X, y):
<span class="gu">@@ -217,9 +292,18 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Fitted estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(_estimator_has(&#39;decision_function&#39;))</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        # overwrite the base class method by disallowing `sample_weight`</span>
<span class="gi">+        return super().fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):</span>
<span class="gi">+        check_target_type(y)</span>
<span class="gi">+        # RandomUnderSampler is not supporting sample_weight. We need to pass</span>
<span class="gi">+        # None.</span>
<span class="gi">+        return super()._fit(X, y, self.max_samples)</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.1</span>
<span class="gi">+    @available_if(_estimator_has(&quot;decision_function&quot;))</span>
<span class="w"> </span>    def decision_function(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Average of the decision functions of the base classifiers.

<span class="gu">@@ -237,9 +321,55 @@ class EasyEnsembleClassifier(_ParamsValidationMixin, BaggingClassifier):</span>
<span class="w"> </span>            ``classes_``. Regression and binary classification are special
<span class="w"> </span>            cases with ``k == 1``, otherwise ``k==n_classes``.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+</span>
<span class="gi">+        # Check data</span>
<span class="gi">+        X = self._validate_data(</span>
<span class="gi">+            X,</span>
<span class="gi">+            accept_sparse=[&quot;csr&quot;, &quot;csc&quot;],</span>
<span class="gi">+            dtype=None,</span>
<span class="gi">+            force_all_finite=False,</span>
<span class="gi">+            reset=False,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Parallel loop</span>
<span class="gi">+        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(</span>
<span class="gi">+            delayed(_parallel_decision_function)(</span>
<span class="gi">+                self.estimators_[starts[i] : starts[i + 1]],</span>
<span class="gi">+                self.estimators_features_[starts[i] : starts[i + 1]],</span>
<span class="gi">+                X,</span>
<span class="gi">+            )</span>
<span class="gi">+            for i in range(n_jobs)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Reduce</span>
<span class="gi">+        decisions = sum(all_decisions) / self.n_estimators</span>
<span class="gi">+</span>
<span class="gi">+        return decisions</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def base_estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Attribute for older sklearn version compatibility.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        error = AttributeError(</span>
<span class="gi">+            f&quot;{self.__class__.__name__} object has no attribute &#39;base_estimator_&#39;.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        if sklearn_version &lt; parse_version(&quot;1.2&quot;):</span>
<span class="gi">+            # The base class require to have the attribute defined. For scikit-learn</span>
<span class="gi">+            # &gt; 1.2, we are going to raise an error.</span>
<span class="gi">+            try:</span>
<span class="gi">+                check_is_fitted(self)</span>
<span class="gi">+                return self.estimator_</span>
<span class="gi">+            except NotFittedError:</span>
<span class="gi">+                raise error</span>
<span class="gi">+        raise error</span>
<span class="gi">+</span>
<span class="gi">+    def _get_estimator(self):</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            return AdaBoostClassifier(algorithm=&quot;SAMME&quot;)</span>
<span class="gi">+        return self.estimator</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.5</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;allow_nan&quot;: _safe_tags(self._get_estimator(), &quot;allow_nan&quot;)}</span>
<span class="gh">diff --git a/imblearn/ensemble/_forest.py b/imblearn/ensemble/_forest.py</span>
<span class="gh">index 6a8de5d..5f8d08e 100644</span>
<span class="gd">--- a/imblearn/ensemble/_forest.py</span>
<span class="gi">+++ b/imblearn/ensemble/_forest.py</span>
<span class="gu">@@ -1,7 +1,12 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Forest classifiers trained on balanced boostrasp samples.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>from copy import deepcopy
<span class="w"> </span>from warnings import warn
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from numpy import float32 as DTYPE
<span class="gu">@@ -10,18 +15,25 @@ from scipy.sparse import issparse</span>
<span class="w"> </span>from sklearn.base import clone, is_classifier
<span class="w"> </span>from sklearn.ensemble import RandomForestClassifier
<span class="w"> </span>from sklearn.ensemble._base import _set_random_states
<span class="gd">-from sklearn.ensemble._forest import _generate_unsampled_indices, _get_n_samples_bootstrap, _parallel_build_trees</span>
<span class="gi">+from sklearn.ensemble._forest import (</span>
<span class="gi">+    _generate_unsampled_indices,</span>
<span class="gi">+    _get_n_samples_bootstrap,</span>
<span class="gi">+    _parallel_build_trees,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.exceptions import DataConversionWarning
<span class="w"> </span>from sklearn.tree import DecisionTreeClassifier
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.multiclass import type_of_target
<span class="w"> </span>from sklearn.utils.validation import _check_sample_weight
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="gi">+    # scikit-learn &gt;= 1.2</span>
<span class="w"> </span>    from sklearn.utils.parallel import Parallel, delayed
<span class="w"> </span>except (ImportError, ModuleNotFoundError):
<span class="w"> </span>    from joblib import Parallel
<span class="w"> </span>    from sklearn.utils.fixes import delayed
<span class="gi">+</span>
<span class="w"> </span>from ..base import _ParamsValidationMixin
<span class="w"> </span>from ..pipeline import make_pipeline
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="gu">@@ -31,13 +43,69 @@ from ..utils._param_validation import Hidden, Interval, StrOptions</span>
<span class="w"> </span>from ..utils._validation import check_sampling_strategy
<span class="w"> </span>from ..utils.fixes import _fit_context
<span class="w"> </span>from ._common import _random_forest_classifier_parameter_constraints
<span class="gi">+</span>
<span class="w"> </span>MAX_INT = np.iinfo(np.int32).max
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gd">-@Substitution(n_jobs=_n_jobs_docstring, random_state=_random_state_docstring)</span>
<span class="gd">-class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="gd">-    RandomForestClassifier):</span>
<span class="gi">+def _local_parallel_build_trees(</span>
<span class="gi">+    sampler,</span>
<span class="gi">+    tree,</span>
<span class="gi">+    bootstrap,</span>
<span class="gi">+    X,</span>
<span class="gi">+    y,</span>
<span class="gi">+    sample_weight,</span>
<span class="gi">+    tree_idx,</span>
<span class="gi">+    n_trees,</span>
<span class="gi">+    verbose=0,</span>
<span class="gi">+    class_weight=None,</span>
<span class="gi">+    n_samples_bootstrap=None,</span>
<span class="gi">+    forest=None,</span>
<span class="gi">+    missing_values_in_feature_mask=None,</span>
<span class="gi">+):</span>
<span class="gi">+    # resample before to fit the tree</span>
<span class="gi">+    X_resampled, y_resampled = sampler.fit_resample(X, y)</span>
<span class="gi">+    if sample_weight is not None:</span>
<span class="gi">+        sample_weight = _safe_indexing(sample_weight, sampler.sample_indices_)</span>
<span class="gi">+    if _get_n_samples_bootstrap is not None:</span>
<span class="gi">+        n_samples_bootstrap = min(n_samples_bootstrap, X_resampled.shape[0])</span>
<span class="gi">+</span>
<span class="gi">+    params_parallel_build_trees = {</span>
<span class="gi">+        &quot;tree&quot;: tree,</span>
<span class="gi">+        &quot;X&quot;: X_resampled,</span>
<span class="gi">+        &quot;y&quot;: y_resampled,</span>
<span class="gi">+        &quot;sample_weight&quot;: sample_weight,</span>
<span class="gi">+        &quot;tree_idx&quot;: tree_idx,</span>
<span class="gi">+        &quot;n_trees&quot;: n_trees,</span>
<span class="gi">+        &quot;verbose&quot;: verbose,</span>
<span class="gi">+        &quot;class_weight&quot;: class_weight,</span>
<span class="gi">+        &quot;n_samples_bootstrap&quot;: n_samples_bootstrap,</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    if parse_version(sklearn_version.base_version) &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+        # TODO: remove when the minimum supported version of scikit-learn will be 1.4</span>
<span class="gi">+        # support for missing values</span>
<span class="gi">+        params_parallel_build_trees[</span>
<span class="gi">+            &quot;missing_values_in_feature_mask&quot;</span>
<span class="gi">+        ] = missing_values_in_feature_mask</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when the minimum supported version of scikit-learn will be 1.1</span>
<span class="gi">+    # change of signature in scikit-learn 1.1</span>
<span class="gi">+    if parse_version(sklearn_version.base_version) &gt;= parse_version(&quot;1.1&quot;):</span>
<span class="gi">+        params_parallel_build_trees[&quot;bootstrap&quot;] = bootstrap</span>
<span class="gi">+    else:</span>
<span class="gi">+        params_parallel_build_trees[&quot;forest&quot;] = forest</span>
<span class="gi">+</span>
<span class="gi">+    tree = _parallel_build_trees(**params_parallel_build_trees)</span>
<span class="gi">+</span>
<span class="gi">+    return sampler, tree</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="gi">+class BalancedRandomForestClassifier(_ParamsValidationMixin, RandomForestClassifier):</span>
<span class="w"> </span>    &quot;&quot;&quot;A balanced random forest classifier.

<span class="w"> </span>    A balanced random forest differs from a classical random forest by the
<span class="gu">@@ -85,7 +153,8 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        the input samples) required to be at a leaf node. Samples have
<span class="w"> </span>        equal weight when sample_weight is not provided.

<span class="gd">-    max_features : {{&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;}}, int, float, or None,             default=&quot;sqrt&quot;</span>
<span class="gi">+    max_features : {{&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;}}, int, float, or None, \</span>
<span class="gi">+            default=&quot;sqrt&quot;</span>
<span class="w"> </span>        The number of features to consider when looking for the best split:

<span class="w"> </span>        - If int, then consider `max_features` features at each split.
<span class="gu">@@ -195,7 +264,8 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        and add more estimators to the ensemble, otherwise, just fit a whole
<span class="w"> </span>        new forest.

<span class="gd">-    class_weight : dict, list of dicts, {{&quot;balanced&quot;, &quot;balanced_subsample&quot;}},             default=None</span>
<span class="gi">+    class_weight : dict, list of dicts, {{&quot;balanced&quot;, &quot;balanced_subsample&quot;}}, \</span>
<span class="gi">+            default=None</span>
<span class="w"> </span>        Weights associated with classes in the form dictionary with the key
<span class="w"> </span>        being the class_label and the value the weight.
<span class="w"> </span>        If not given, all classes are supposed to have weight one. For
<span class="gu">@@ -355,59 +425,124 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>    ...                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))
<span class="w"> </span>    [1]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    if sklearn_version &gt;= parse_version(&#39;1.4&#39;):</span>
<span class="gd">-        _parameter_constraints = deepcopy(RandomForestClassifier.</span>
<span class="gd">-            _parameter_constraints)</span>
<span class="gi">+</span>
<span class="gi">+    # make a deepcopy to not modify the original dictionary</span>
<span class="gi">+    if sklearn_version &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+        _parameter_constraints = deepcopy(RandomForestClassifier._parameter_constraints)</span>
<span class="w"> </span>    else:
<span class="w"> </span>        _parameter_constraints = deepcopy(
<span class="gd">-            _random_forest_classifier_parameter_constraints)</span>
<span class="gd">-    _parameter_constraints.update({&#39;bootstrap&#39;: [&#39;boolean&#39;, Hidden(</span>
<span class="gd">-        StrOptions({&#39;warn&#39;}))], &#39;sampling_strategy&#39;: [Interval(numbers.Real,</span>
<span class="gd">-        0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), dict, callable, Hidden(</span>
<span class="gd">-        StrOptions({&#39;warn&#39;}))], &#39;replacement&#39;: [&#39;boolean&#39;, Hidden(</span>
<span class="gd">-        StrOptions({&#39;warn&#39;}))]})</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, n_estimators=100, *, criterion=&#39;gini&#39;, max_depth=</span>
<span class="gd">-        None, min_samples_split=2, min_samples_leaf=1,</span>
<span class="gd">-        min_weight_fraction_leaf=0.0, max_features=&#39;sqrt&#39;, max_leaf_nodes=</span>
<span class="gd">-        None, min_impurity_decrease=0.0, bootstrap=&#39;warn&#39;, oob_score=False,</span>
<span class="gd">-        sampling_strategy=&#39;warn&#39;, replacement=&#39;warn&#39;, n_jobs=None,</span>
<span class="gd">-        random_state=None, verbose=0, warm_start=False, class_weight=None,</span>
<span class="gd">-        ccp_alpha=0.0, max_samples=None, monotonic_cst=None):</span>
<span class="gd">-        params_random_forest = {&#39;criterion&#39;: criterion, &#39;max_depth&#39;:</span>
<span class="gd">-            max_depth, &#39;n_estimators&#39;: n_estimators, &#39;bootstrap&#39;: bootstrap,</span>
<span class="gd">-            &#39;oob_score&#39;: oob_score, &#39;n_jobs&#39;: n_jobs, &#39;random_state&#39;:</span>
<span class="gd">-            random_state, &#39;verbose&#39;: verbose, &#39;warm_start&#39;: warm_start,</span>
<span class="gd">-            &#39;class_weight&#39;: class_weight, &#39;min_samples_split&#39;:</span>
<span class="gd">-            min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf,</span>
<span class="gd">-            &#39;min_weight_fraction_leaf&#39;: min_weight_fraction_leaf,</span>
<span class="gd">-            &#39;max_features&#39;: max_features, &#39;max_leaf_nodes&#39;: max_leaf_nodes,</span>
<span class="gd">-            &#39;min_impurity_decrease&#39;: min_impurity_decrease, &#39;ccp_alpha&#39;:</span>
<span class="gd">-            ccp_alpha, &#39;max_samples&#39;: max_samples}</span>
<span class="gd">-        if parse_version(sklearn_version.base_version) &gt;= parse_version(&#39;1.4&#39;):</span>
<span class="gd">-            params_random_forest[&#39;monotonic_cst&#39;] = monotonic_cst</span>
<span class="gi">+            _random_forest_classifier_parameter_constraints</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints.update(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;bootstrap&quot;: [&quot;boolean&quot;, Hidden(StrOptions({&quot;warn&quot;}))],</span>
<span class="gi">+            &quot;sampling_strategy&quot;: [</span>
<span class="gi">+                Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+                StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+                dict,</span>
<span class="gi">+                callable,</span>
<span class="gi">+                Hidden(StrOptions({&quot;warn&quot;})),</span>
<span class="gi">+            ],</span>
<span class="gi">+            &quot;replacement&quot;: [&quot;boolean&quot;, Hidden(StrOptions({&quot;warn&quot;}))],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        n_estimators=100,</span>
<span class="gi">+        *,</span>
<span class="gi">+        criterion=&quot;gini&quot;,</span>
<span class="gi">+        max_depth=None,</span>
<span class="gi">+        min_samples_split=2,</span>
<span class="gi">+        min_samples_leaf=1,</span>
<span class="gi">+        min_weight_fraction_leaf=0.0,</span>
<span class="gi">+        max_features=&quot;sqrt&quot;,</span>
<span class="gi">+        max_leaf_nodes=None,</span>
<span class="gi">+        min_impurity_decrease=0.0,</span>
<span class="gi">+        bootstrap=&quot;warn&quot;,</span>
<span class="gi">+        oob_score=False,</span>
<span class="gi">+        sampling_strategy=&quot;warn&quot;,</span>
<span class="gi">+        replacement=&quot;warn&quot;,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        verbose=0,</span>
<span class="gi">+        warm_start=False,</span>
<span class="gi">+        class_weight=None,</span>
<span class="gi">+        ccp_alpha=0.0,</span>
<span class="gi">+        max_samples=None,</span>
<span class="gi">+        monotonic_cst=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        params_random_forest = {</span>
<span class="gi">+            &quot;criterion&quot;: criterion,</span>
<span class="gi">+            &quot;max_depth&quot;: max_depth,</span>
<span class="gi">+            &quot;n_estimators&quot;: n_estimators,</span>
<span class="gi">+            &quot;bootstrap&quot;: bootstrap,</span>
<span class="gi">+            &quot;oob_score&quot;: oob_score,</span>
<span class="gi">+            &quot;n_jobs&quot;: n_jobs,</span>
<span class="gi">+            &quot;random_state&quot;: random_state,</span>
<span class="gi">+            &quot;verbose&quot;: verbose,</span>
<span class="gi">+            &quot;warm_start&quot;: warm_start,</span>
<span class="gi">+            &quot;class_weight&quot;: class_weight,</span>
<span class="gi">+            &quot;min_samples_split&quot;: min_samples_split,</span>
<span class="gi">+            &quot;min_samples_leaf&quot;: min_samples_leaf,</span>
<span class="gi">+            &quot;min_weight_fraction_leaf&quot;: min_weight_fraction_leaf,</span>
<span class="gi">+            &quot;max_features&quot;: max_features,</span>
<span class="gi">+            &quot;max_leaf_nodes&quot;: max_leaf_nodes,</span>
<span class="gi">+            &quot;min_impurity_decrease&quot;: min_impurity_decrease,</span>
<span class="gi">+            &quot;ccp_alpha&quot;: ccp_alpha,</span>
<span class="gi">+            &quot;max_samples&quot;: max_samples,</span>
<span class="gi">+        }</span>
<span class="gi">+        # TODO: remove when the minimum supported version of scikit-learn will be 1.4</span>
<span class="gi">+        if parse_version(sklearn_version.base_version) &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+            # use scikit-learn support for monotonic constraints</span>
<span class="gi">+            params_random_forest[&quot;monotonic_cst&quot;] = monotonic_cst</span>
<span class="w"> </span>        else:
<span class="w"> </span>            if monotonic_cst is not None:
<span class="w"> </span>                raise ValueError(
<span class="gd">-                    &#39;Monotonic constraints are not supported for scikit-learn version &lt; 1.4.&#39;</span>
<span class="gd">-                    )</span>
<span class="gi">+                    &quot;Monotonic constraints are not supported for scikit-learn &quot;</span>
<span class="gi">+                    &quot;version &lt; 1.4.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            # create an attribute for compatibility with other scikit-learn tools such</span>
<span class="gi">+            # as HTML representation.</span>
<span class="w"> </span>            self.monotonic_cst = monotonic_cst
<span class="w"> </span>        super().__init__(**params_random_forest)
<span class="gi">+</span>
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.replacement = replacement

<span class="w"> </span>    def _validate_estimator(self, default=DecisionTreeClassifier()):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute, set the
<span class="w"> </span>        `estimator_` attribute.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(self, &quot;estimator&quot;):</span>
<span class="gi">+            base_estimator = self.estimator</span>
<span class="gi">+        else:</span>
<span class="gi">+            base_estimator = self.base_estimator</span>
<span class="gi">+</span>
<span class="gi">+        if base_estimator is not None:</span>
<span class="gi">+            self.estimator_ = clone(base_estimator)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(default)</span>
<span class="gi">+</span>
<span class="gi">+        self.base_sampler_ = RandomUnderSampler(</span>
<span class="gi">+            sampling_strategy=self._sampling_strategy,</span>
<span class="gi">+            replacement=self._replacement,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _make_sampler_estimator(self, random_state=None):
<span class="w"> </span>        &quot;&quot;&quot;Make and configure a copy of the `base_estimator_` attribute.
<span class="w"> </span>        Warning: This method should be used to properly instantiate new
<span class="w"> </span>        sub-estimators.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = clone(self.estimator_)</span>
<span class="gi">+        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})</span>
<span class="gi">+        sampler = clone(self.base_sampler_)</span>
<span class="gi">+</span>
<span class="gi">+        if random_state is not None:</span>
<span class="gi">+            _set_random_states(estimator, random_state)</span>
<span class="gi">+            _set_random_states(sampler, random_state)</span>
<span class="gi">+</span>
<span class="gi">+        return estimator, sampler</span>

<span class="w"> </span>    @_fit_context(prefer_skip_nested_validation=True)
<span class="w"> </span>    def fit(self, X, y, sample_weight=None):
<span class="gu">@@ -436,7 +571,238 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            The fitted instance.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        # TODO: remove in 0.13</span>
<span class="gi">+        if self.sampling_strategy == &quot;warn&quot;:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;The default of `sampling_strategy` will change from `&#39;auto&#39;` to &quot;</span>
<span class="gi">+                &quot;`&#39;all&#39;` in version 0.13. This change will follow the implementation &quot;</span>
<span class="gi">+                &quot;proposed in the original paper. Set to `&#39;all&#39;` to silence this &quot;</span>
<span class="gi">+                &quot;warning and adopt the future behaviour.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._sampling_strategy = self.sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+        if self.replacement == &quot;warn&quot;:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;The default of `replacement` will change from `False` to &quot;</span>
<span class="gi">+                &quot;`True` in version 0.13. This change will follow the implementation &quot;</span>
<span class="gi">+                &quot;proposed in the original paper. Set to `True` to silence this &quot;</span>
<span class="gi">+                &quot;warning and adopt the future behaviour.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._replacement = False</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._replacement = self.replacement</span>
<span class="gi">+</span>
<span class="gi">+        if self.bootstrap == &quot;warn&quot;:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;The default of `bootstrap` will change from `True` to &quot;</span>
<span class="gi">+                &quot;`False` in version 0.13. This change will follow the implementation &quot;</span>
<span class="gi">+                &quot;proposed in the original paper. Set to `False` to silence this &quot;</span>
<span class="gi">+                &quot;warning and adopt the future behaviour.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            self._bootstrap = True</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._bootstrap = self.bootstrap</span>
<span class="gi">+</span>
<span class="gi">+        # Validate or convert input data</span>
<span class="gi">+        if issparse(y):</span>
<span class="gi">+            raise ValueError(&quot;sparse multilabel-indicator for y is not supported.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # TODO: remove when the minimum supported version of scipy will be 1.4</span>
<span class="gi">+        # Support for missing values</span>
<span class="gi">+        if parse_version(sklearn_version.base_version) &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+            force_all_finite = False</span>
<span class="gi">+        else:</span>
<span class="gi">+            force_all_finite = True</span>
<span class="gi">+</span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X,</span>
<span class="gi">+            y,</span>
<span class="gi">+            multi_output=True,</span>
<span class="gi">+            accept_sparse=&quot;csc&quot;,</span>
<span class="gi">+            dtype=DTYPE,</span>
<span class="gi">+            force_all_finite=force_all_finite,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # TODO: remove when the minimum supported version of scikit-learn will be 1.4</span>
<span class="gi">+        if parse_version(sklearn_version.base_version) &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+            # _compute_missing_values_in_feature_mask checks if X has missing values and</span>
<span class="gi">+            # will raise an error if the underlying tree base estimator can&#39;t handle</span>
<span class="gi">+            # missing values. Only the criterion is required to determine if the tree</span>
<span class="gi">+            # supports missing values.</span>
<span class="gi">+            estimator = type(self.estimator)(criterion=self.criterion)</span>
<span class="gi">+            missing_values_in_feature_mask = (</span>
<span class="gi">+                estimator._compute_missing_values_in_feature_mask(</span>
<span class="gi">+                    X, estimator_name=self.__class__.__name__</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            missing_values_in_feature_mask = None</span>
<span class="gi">+</span>
<span class="gi">+        if sample_weight is not None:</span>
<span class="gi">+            sample_weight = _check_sample_weight(sample_weight, X)</span>
<span class="gi">+</span>
<span class="gi">+        self._n_features = X.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+        if issparse(X):</span>
<span class="gi">+            # Pre-sort indices to avoid that each individual tree of the</span>
<span class="gi">+            # ensemble sorts the indices.</span>
<span class="gi">+            X.sort_indices()</span>
<span class="gi">+</span>
<span class="gi">+        y = np.atleast_1d(y)</span>
<span class="gi">+        if y.ndim == 2 and y.shape[1] == 1:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;A column-vector y was passed when a 1d array was&quot;</span>
<span class="gi">+                &quot; expected. Please change the shape of y to &quot;</span>
<span class="gi">+                &quot;(n_samples,), for example using ravel().&quot;,</span>
<span class="gi">+                DataConversionWarning,</span>
<span class="gi">+                stacklevel=2,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if y.ndim == 1:</span>
<span class="gi">+            # reshape is necessary to preserve the data contiguity against vs</span>
<span class="gi">+            # [:, np.newaxis] that does not.</span>
<span class="gi">+            y = np.reshape(y, (-1, 1))</span>
<span class="gi">+</span>
<span class="gi">+        self.n_outputs_ = y.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+        y_encoded, expanded_class_weight = self._validate_y_class_weight(y)</span>
<span class="gi">+</span>
<span class="gi">+        if getattr(y, &quot;dtype&quot;, None) != DOUBLE or not y.flags.contiguous:</span>
<span class="gi">+            y_encoded = np.ascontiguousarray(y_encoded, dtype=DOUBLE)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self._sampling_strategy, dict):</span>
<span class="gi">+            self._sampling_strategy = {</span>
<span class="gi">+                np.where(self.classes_[0] == key)[0][0]: value</span>
<span class="gi">+                for key, value in check_sampling_strategy(</span>
<span class="gi">+                    self.sampling_strategy,</span>
<span class="gi">+                    y,</span>
<span class="gi">+                    &quot;under-sampling&quot;,</span>
<span class="gi">+                ).items()</span>
<span class="gi">+            }</span>
<span class="gi">+        else:</span>
<span class="gi">+            self._sampling_strategy = self._sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+        if expanded_class_weight is not None:</span>
<span class="gi">+            if sample_weight is not None:</span>
<span class="gi">+                sample_weight = sample_weight * expanded_class_weight</span>
<span class="gi">+            else:</span>
<span class="gi">+                sample_weight = expanded_class_weight</span>
<span class="gi">+</span>
<span class="gi">+        # Get bootstrap sample size</span>
<span class="gi">+        n_samples_bootstrap = _get_n_samples_bootstrap(</span>
<span class="gi">+            n_samples=X.shape[0], max_samples=self.max_samples</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Check parameters</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        if not self._bootstrap and self.oob_score:</span>
<span class="gi">+            raise ValueError(&quot;Out of bag estimation only available if bootstrap=True&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        if not self.warm_start or not hasattr(self, &quot;estimators_&quot;):</span>
<span class="gi">+            # Free allocated memory, if any</span>
<span class="gi">+            self.estimators_ = []</span>
<span class="gi">+            self.samplers_ = []</span>
<span class="gi">+            self.pipelines_ = []</span>
<span class="gi">+</span>
<span class="gi">+        n_more_estimators = self.n_estimators - len(self.estimators_)</span>
<span class="gi">+</span>
<span class="gi">+        if n_more_estimators &lt; 0:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;n_estimators=%d must be larger or equal to &quot;</span>
<span class="gi">+                &quot;len(estimators_)=%d when warm_start==True&quot;</span>
<span class="gi">+                % (self.n_estimators, len(self.estimators_))</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        elif n_more_estimators == 0:</span>
<span class="gi">+            warn(</span>
<span class="gi">+                &quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
<span class="gi">+                &quot;fit new trees.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            if self.warm_start and len(self.estimators_) &gt; 0:</span>
<span class="gi">+                # We draw from the random state to get the random state we</span>
<span class="gi">+                # would have got if we hadn&#39;t used a warm_start.</span>
<span class="gi">+                random_state.randint(MAX_INT, size=len(self.estimators_))</span>
<span class="gi">+</span>
<span class="gi">+            trees = []</span>
<span class="gi">+            samplers = []</span>
<span class="gi">+            for _ in range(n_more_estimators):</span>
<span class="gi">+                tree, sampler = self._make_sampler_estimator(random_state=random_state)</span>
<span class="gi">+                trees.append(tree)</span>
<span class="gi">+                samplers.append(sampler)</span>
<span class="gi">+</span>
<span class="gi">+            # Parallel loop: we prefer the threading backend as the Cython code</span>
<span class="gi">+            # for fitting the trees is internally releasing the Python GIL</span>
<span class="gi">+            # making threading more efficient than multiprocessing in</span>
<span class="gi">+            # that case. However, we respect any parallel_backend contexts set</span>
<span class="gi">+            # at a higher level, since correctness does not rely on using</span>
<span class="gi">+            # threads.</span>
<span class="gi">+            samplers_trees = Parallel(</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+                verbose=self.verbose,</span>
<span class="gi">+                prefer=&quot;threads&quot;,</span>
<span class="gi">+            )(</span>
<span class="gi">+                delayed(_local_parallel_build_trees)(</span>
<span class="gi">+                    s,</span>
<span class="gi">+                    t,</span>
<span class="gi">+                    self._bootstrap,</span>
<span class="gi">+                    X,</span>
<span class="gi">+                    y_encoded,</span>
<span class="gi">+                    sample_weight,</span>
<span class="gi">+                    i,</span>
<span class="gi">+                    len(trees),</span>
<span class="gi">+                    verbose=self.verbose,</span>
<span class="gi">+                    class_weight=self.class_weight,</span>
<span class="gi">+                    n_samples_bootstrap=n_samples_bootstrap,</span>
<span class="gi">+                    forest=self,</span>
<span class="gi">+                    missing_values_in_feature_mask=missing_values_in_feature_mask,</span>
<span class="gi">+                )</span>
<span class="gi">+                for i, (s, t) in enumerate(zip(samplers, trees))</span>
<span class="gi">+            )</span>
<span class="gi">+            samplers, trees = zip(*samplers_trees)</span>
<span class="gi">+</span>
<span class="gi">+            # Collect newly grown trees</span>
<span class="gi">+            self.estimators_.extend(trees)</span>
<span class="gi">+            self.samplers_.extend(samplers)</span>
<span class="gi">+</span>
<span class="gi">+            # Create pipeline with the fitted samplers and trees</span>
<span class="gi">+            self.pipelines_.extend(</span>
<span class="gi">+                [</span>
<span class="gi">+                    make_pipeline(deepcopy(s), deepcopy(t))</span>
<span class="gi">+                    for s, t in zip(samplers, trees)</span>
<span class="gi">+                ]</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if self.oob_score:</span>
<span class="gi">+            y_type = type_of_target(y)</span>
<span class="gi">+            if y_type in (&quot;multiclass-multioutput&quot;, &quot;unknown&quot;):</span>
<span class="gi">+                # FIXME: we could consider to support multiclass-multioutput if</span>
<span class="gi">+                # we introduce or reuse a constructor parameter (e.g.</span>
<span class="gi">+                # oob_score) allowing our user to pass a callable defining the</span>
<span class="gi">+                # scoring strategy on OOB sample.</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;The type of target cannot be used to compute OOB &quot;</span>
<span class="gi">+                    f&quot;estimates. Got {y_type} while only the following are &quot;</span>
<span class="gi">+                    &quot;supported: continuous, continuous-multioutput, binary, &quot;</span>
<span class="gi">+                    &quot;multiclass, multilabel-indicator.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            self._set_oob_score_and_attributes(X, y_encoded)</span>
<span class="gi">+</span>
<span class="gi">+        # Decapsulate classes_ attributes</span>
<span class="gi">+        if hasattr(self, &quot;classes_&quot;) and self.n_outputs_ == 1:</span>
<span class="gi">+            self.n_classes_ = self.n_classes_[0]</span>
<span class="gi">+            self.classes_ = self.classes_[0]</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def _set_oob_score_and_attributes(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Compute and set the OOB score and attributes.
<span class="gu">@@ -448,7 +814,15 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>
<span class="w"> </span>        y : ndarray of shape (n_samples, n_outputs)
<span class="w"> </span>            The target matrix.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.oob_decision_function_ = self._compute_oob_predictions(X, y)</span>
<span class="gi">+        if self.oob_decision_function_.shape[-1] == 1:</span>
<span class="gi">+            # drop the n_outputs axis if there is a single output</span>
<span class="gi">+            self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)</span>
<span class="gi">+        from sklearn.metrics import accuracy_score</span>
<span class="gi">+</span>
<span class="gi">+        self.oob_score_ = accuracy_score(</span>
<span class="gi">+            y, np.argmax(self.oob_decision_function_, axis=1)</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _compute_oob_predictions(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Compute and set the OOB score.
<span class="gu">@@ -462,12 +836,79 @@ class BalancedRandomForestClassifier(_ParamsValidationMixin,</span>

<span class="w"> </span>        Returns
<span class="w"> </span>        -------
<span class="gd">-        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or                 (n_samples, 1, n_outputs)</span>
<span class="gi">+        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \</span>
<span class="gi">+                (n_samples, 1, n_outputs)</span>
<span class="w"> </span>            The OOB predictions.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        # Prediction requires X to be in CSR format</span>
<span class="gi">+        if issparse(X):</span>
<span class="gi">+            X = X.tocsr()</span>
<span class="gi">+</span>
<span class="gi">+        n_samples = y.shape[0]</span>
<span class="gi">+        n_outputs = self.n_outputs_</span>
<span class="gi">+</span>
<span class="gi">+        if is_classifier(self) and hasattr(self, &quot;n_classes_&quot;):</span>
<span class="gi">+            # n_classes_ is a ndarray at this stage</span>
<span class="gi">+            # all the supported type of target will have the same number of</span>
<span class="gi">+            # classes in all outputs</span>
<span class="gi">+            oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)</span>
<span class="gi">+        else:</span>
<span class="gi">+            # for regression, n_classes_ does not exist and we create an empty</span>
<span class="gi">+            # axis to be consistent with the classification case and make</span>
<span class="gi">+            # the array operations compatible with the 2 settings</span>
<span class="gi">+            oob_pred_shape = (n_samples, 1, n_outputs)</span>
<span class="gi">+</span>
<span class="gi">+        oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)</span>
<span class="gi">+        n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)</span>
<span class="gi">+</span>
<span class="gi">+        for sampler, estimator in zip(self.samplers_, self.estimators_):</span>
<span class="gi">+            X_resample = X[sampler.sample_indices_]</span>
<span class="gi">+            y_resample = y[sampler.sample_indices_]</span>
<span class="gi">+</span>
<span class="gi">+            n_sample_subset = y_resample.shape[0]</span>
<span class="gi">+            n_samples_bootstrap = _get_n_samples_bootstrap(</span>
<span class="gi">+                n_sample_subset, self.max_samples</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            unsampled_indices = _generate_unsampled_indices(</span>
<span class="gi">+                estimator.random_state, n_sample_subset, n_samples_bootstrap</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            y_pred = self._get_oob_predictions(</span>
<span class="gi">+                estimator, X_resample[unsampled_indices, :]</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            indices = sampler.sample_indices_[unsampled_indices]</span>
<span class="gi">+            oob_pred[indices, ...] += y_pred</span>
<span class="gi">+            n_oob_pred[indices, :] += 1</span>
<span class="gi">+</span>
<span class="gi">+        for k in range(n_outputs):</span>
<span class="gi">+            if (n_oob_pred == 0).any():</span>
<span class="gi">+                warn(</span>
<span class="gi">+                    &quot;Some inputs do not have OOB scores. This probably means &quot;</span>
<span class="gi">+                    &quot;too few trees were used to compute any reliable OOB &quot;</span>
<span class="gi">+                    &quot;estimates.&quot;,</span>
<span class="gi">+                    UserWarning,</span>
<span class="gi">+                )</span>
<span class="gi">+                n_oob_pred[n_oob_pred == 0] = 1</span>
<span class="gi">+            oob_pred[..., k] /= n_oob_pred[..., [k]]</span>
<span class="gi">+</span>
<span class="gi">+        return oob_pred</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove when supporting scikit-learn&gt;=1.2</span>
<span class="w"> </span>    @property
<span class="w"> </span>    def n_features_(self):
<span class="w"> </span>        &quot;&quot;&quot;Number of features when ``fit`` is performed.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warn(</span>
<span class="gi">+            &quot;`n_features_` was deprecated in scikit-learn 1.0. This attribute will &quot;</span>
<span class="gi">+            &quot;not be accessible when the minimum supported version of scikit-learn &quot;</span>
<span class="gi">+            &quot;is 1.2.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.n_features_in_</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;multioutput&quot;: False,</span>
<span class="gi">+            &quot;multilabel&quot;: False,</span>
<span class="gi">+        }</span>
<span class="gh">diff --git a/imblearn/ensemble/_weight_boosting.py b/imblearn/ensemble/_weight_boosting.py</span>
<span class="gh">index 26f43c4..9da0225 100644</span>
<span class="gd">--- a/imblearn/ensemble/_weight_boosting.py</span>
<span class="gi">+++ b/imblearn/ensemble/_weight_boosting.py</span>
<span class="gu">@@ -1,6 +1,7 @@</span>
<span class="w"> </span>import copy
<span class="w"> </span>import numbers
<span class="w"> </span>from copy import deepcopy
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.base import clone
<span class="gu">@@ -10,6 +11,7 @@ from sklearn.tree import DecisionTreeClassifier</span>
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.validation import has_fit_parameter
<span class="gi">+</span>
<span class="w"> </span>from ..base import _ParamsValidationMixin
<span class="w"> </span>from ..pipeline import make_pipeline
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="gu">@@ -19,11 +21,14 @@ from ..utils._docstring import _random_state_docstring</span>
<span class="w"> </span>from ..utils._param_validation import Interval, StrOptions
<span class="w"> </span>from ..utils.fixes import _fit_context
<span class="w"> </span>from ._common import _adaboost_classifier_parameter_constraints
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, random_state=_random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class RUSBoostClassifier(_ParamsValidationMixin, AdaBoostClassifier):
<span class="w"> </span>    &quot;&quot;&quot;Random under-sampling integrated in the learning of AdaBoost.

<span class="gu">@@ -149,24 +154,49 @@ class RUSBoostClassifier(_ParamsValidationMixin, AdaBoostClassifier):</span>
<span class="w"> </span>    &gt;&gt;&gt; clf.predict(X)
<span class="w"> </span>    array([...])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    if sklearn_version &gt;= parse_version(&#39;1.4&#39;):</span>
<span class="gd">-        _parameter_constraints = copy.deepcopy(AdaBoostClassifier.</span>
<span class="gd">-            _parameter_constraints)</span>
<span class="gi">+</span>
<span class="gi">+    # make a deepcopy to not modify the original dictionary</span>
<span class="gi">+    if sklearn_version &gt;= parse_version(&quot;1.4&quot;):</span>
<span class="gi">+        _parameter_constraints = copy.deepcopy(</span>
<span class="gi">+            AdaBoostClassifier._parameter_constraints</span>
<span class="gi">+        )</span>
<span class="w"> </span>    else:
<span class="w"> </span>        _parameter_constraints = copy.deepcopy(
<span class="gd">-            _adaboost_classifier_parameter_constraints)</span>
<span class="gd">-    _parameter_constraints.update({&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), dict, callable],</span>
<span class="gd">-        &#39;replacement&#39;: [&#39;boolean&#39;]})</span>
<span class="gd">-    if &#39;base_estimator&#39; in _parameter_constraints:</span>
<span class="gd">-        del _parameter_constraints[&#39;base_estimator&#39;]</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, estimator=None, *, n_estimators=50, learning_rate=</span>
<span class="gd">-        1.0, algorithm=&#39;SAMME.R&#39;, sampling_strategy=&#39;auto&#39;, replacement=</span>
<span class="gd">-        False, random_state=None):</span>
<span class="gd">-        super().__init__(n_estimators=n_estimators, learning_rate=</span>
<span class="gd">-            learning_rate, algorithm=algorithm, random_state=random_state)</span>
<span class="gi">+            _adaboost_classifier_parameter_constraints</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints.update(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;sampling_strategy&quot;: [</span>
<span class="gi">+                Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+                StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+                dict,</span>
<span class="gi">+                callable,</span>
<span class="gi">+            ],</span>
<span class="gi">+            &quot;replacement&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    # TODO: remove when minimum supported version of scikit-learn is 1.4</span>
<span class="gi">+    if &quot;base_estimator&quot; in _parameter_constraints:</span>
<span class="gi">+        del _parameter_constraints[&quot;base_estimator&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        estimator=None,</span>
<span class="gi">+        *,</span>
<span class="gi">+        n_estimators=50,</span>
<span class="gi">+        learning_rate=1.0,</span>
<span class="gi">+        algorithm=&quot;SAMME.R&quot;,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        replacement=False,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            n_estimators=n_estimators,</span>
<span class="gi">+            learning_rate=learning_rate,</span>
<span class="gi">+            algorithm=algorithm,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.estimator = estimator
<span class="w"> </span>        self.sampling_strategy = sampling_strategy
<span class="w"> </span>        self.replacement = replacement
<span class="gu">@@ -193,26 +223,174 @@ class RUSBoostClassifier(_ParamsValidationMixin, AdaBoostClassifier):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Returns self.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        check_target_type(y)</span>
<span class="gi">+        self.samplers_ = []</span>
<span class="gi">+        self.pipelines_ = []</span>
<span class="gi">+        super().fit(X, y, sample_weight)</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Check the estimator and the n_estimator attribute.

<span class="w"> </span>        Sets the `estimator_` attributes.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        default = DecisionTreeClassifier(max_depth=1)</span>
<span class="gi">+        if self.estimator is not None:</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(default)</span>
<span class="gi">+</span>
<span class="gi">+        #  SAMME-R requires predict_proba-enabled estimators</span>
<span class="gi">+        if self.algorithm == &quot;SAMME.R&quot;:</span>
<span class="gi">+            if not hasattr(self.estimator_, &quot;predict_proba&quot;):</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    &quot;AdaBoostClassifier with algorithm=&#39;SAMME.R&#39; requires &quot;</span>
<span class="gi">+                    &quot;that the weak learner supports the calculation of class &quot;</span>
<span class="gi">+                    &quot;probabilities with a predict_proba method.\n&quot;</span>
<span class="gi">+                    &quot;Please change the base estimator or set &quot;</span>
<span class="gi">+                    &quot;algorithm=&#39;SAMME&#39; instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+        if not has_fit_parameter(self.estimator_, &quot;sample_weight&quot;):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;{self.estimator_.__class__.__name__} doesn&#39;t support sample_weight.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.base_sampler_ = RandomUnderSampler(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            replacement=self.replacement,</span>
<span class="gi">+        )</span>

<span class="w"> </span>    def _make_sampler_estimator(self, append=True, random_state=None):
<span class="w"> </span>        &quot;&quot;&quot;Make and configure a copy of the `base_estimator_` attribute.
<span class="w"> </span>        Warning: This method should be used to properly instantiate new
<span class="w"> </span>        sub-estimators.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator = clone(self.estimator_)</span>
<span class="gi">+        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})</span>
<span class="gi">+        sampler = clone(self.base_sampler_)</span>
<span class="gi">+</span>
<span class="gi">+        if random_state is not None:</span>
<span class="gi">+            _set_random_states(estimator, random_state)</span>
<span class="gi">+            _set_random_states(sampler, random_state)</span>
<span class="gi">+</span>
<span class="gi">+        if append:</span>
<span class="gi">+            self.estimators_.append(estimator)</span>
<span class="gi">+            self.samplers_.append(sampler)</span>
<span class="gi">+            self.pipelines_.append(</span>
<span class="gi">+                make_pipeline(deepcopy(sampler), deepcopy(estimator))</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        return estimator, sampler</span>

<span class="w"> </span>    def _boost_real(self, iboost, X, y, sample_weight, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Implement a single boost using the SAMME.R real algorithm.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator, sampler = self._make_sampler_estimator(random_state=random_state)</span>
<span class="gi">+</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+        sample_weight_res = _safe_indexing(sample_weight, sampler.sample_indices_)</span>
<span class="gi">+        estimator.fit(X_res, y_res, sample_weight=sample_weight_res)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict_proba = estimator.predict_proba(X)</span>
<span class="gi">+</span>
<span class="gi">+        if iboost == 0:</span>
<span class="gi">+            self.classes_ = getattr(estimator, &quot;classes_&quot;, None)</span>
<span class="gi">+            self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)</span>
<span class="gi">+</span>
<span class="gi">+        # Instances incorrectly classified</span>
<span class="gi">+        incorrect = y_predict != y</span>
<span class="gi">+</span>
<span class="gi">+        # Error fraction</span>
<span class="gi">+        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if classification is perfect</span>
<span class="gi">+        if estimator_error &lt;= 0:</span>
<span class="gi">+            return sample_weight, 1.0, 0.0</span>
<span class="gi">+</span>
<span class="gi">+        # Construct y coding as described in Zhu et al [2]:</span>
<span class="gi">+        #</span>
<span class="gi">+        #    y_k = 1 if c == k else -1 / (K - 1)</span>
<span class="gi">+        #</span>
<span class="gi">+        # where K == n_classes_ and c, k in [0, K) are indices along the second</span>
<span class="gi">+        # axis of the y coding with c being the index corresponding to the true</span>
<span class="gi">+        # class label.</span>
<span class="gi">+        n_classes = self.n_classes_</span>
<span class="gi">+        classes = self.classes_</span>
<span class="gi">+        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])</span>
<span class="gi">+        y_coding = y_codes.take(classes == y[:, np.newaxis])</span>
<span class="gi">+</span>
<span class="gi">+        # Displace zero probabilities so the log is defined.</span>
<span class="gi">+        # Also fix negative elements which may occur with</span>
<span class="gi">+        # negative sample weights.</span>
<span class="gi">+        proba = y_predict_proba  # alias for readability</span>
<span class="gi">+        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)</span>
<span class="gi">+</span>
<span class="gi">+        # Boost weight using multi-class AdaBoost SAMME.R alg</span>
<span class="gi">+        estimator_weight = (</span>
<span class="gi">+            -1.0</span>
<span class="gi">+            * self.learning_rate</span>
<span class="gi">+            * ((n_classes - 1.0) / n_classes)</span>
<span class="gi">+            * (y_coding * np.log(y_predict_proba)).sum(axis=1)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Only boost the weights if it will fit again</span>
<span class="gi">+        if not iboost == self.n_estimators - 1:</span>
<span class="gi">+            # Only boost positive weights</span>
<span class="gi">+            sample_weight *= np.exp(</span>
<span class="gi">+                estimator_weight * ((sample_weight &gt; 0) | (estimator_weight &lt; 0))</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        return sample_weight, 1.0, estimator_error</span>

<span class="w"> </span>    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Implement a single boost using the SAMME discrete algorithm.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        estimator, sampler = self._make_sampler_estimator(random_state=random_state)</span>
<span class="gi">+</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+        sample_weight_res = _safe_indexing(sample_weight, sampler.sample_indices_)</span>
<span class="gi">+        estimator.fit(X_res, y_res, sample_weight=sample_weight_res)</span>
<span class="gi">+</span>
<span class="gi">+        y_predict = estimator.predict(X)</span>
<span class="gi">+</span>
<span class="gi">+        if iboost == 0:</span>
<span class="gi">+            self.classes_ = getattr(estimator, &quot;classes_&quot;, None)</span>
<span class="gi">+            self.n_classes_ = len(self.classes_)</span>
<span class="gi">+</span>
<span class="gi">+        # Instances incorrectly classified</span>
<span class="gi">+        incorrect = y_predict != y</span>
<span class="gi">+</span>
<span class="gi">+        # Error fraction</span>
<span class="gi">+        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if classification is perfect</span>
<span class="gi">+        if estimator_error &lt;= 0:</span>
<span class="gi">+            return sample_weight, 1.0, 0.0</span>
<span class="gi">+</span>
<span class="gi">+        n_classes = self.n_classes_</span>
<span class="gi">+</span>
<span class="gi">+        # Stop if the error is at least as bad as random guessing</span>
<span class="gi">+        if estimator_error &gt;= 1.0 - (1.0 / n_classes):</span>
<span class="gi">+            self.estimators_.pop(-1)</span>
<span class="gi">+            self.samplers_.pop(-1)</span>
<span class="gi">+            self.pipelines_.pop(-1)</span>
<span class="gi">+            if len(self.estimators_) == 0:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;BaseClassifier in AdaBoostClassifier &quot;</span>
<span class="gi">+                    &quot;ensemble is worse than random, ensemble &quot;</span>
<span class="gi">+                    &quot;can not be fit.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return None, None, None</span>
<span class="gi">+</span>
<span class="gi">+        # Boost weight using multi-class AdaBoost SAMME alg</span>
<span class="gi">+        estimator_weight = self.learning_rate * (</span>
<span class="gi">+            np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Only boost the weights if I will fit again</span>
<span class="gi">+        if not iboost == self.n_estimators - 1:</span>
<span class="gi">+            # Only boost positive weights</span>
<span class="gi">+            sample_weight *= np.exp(estimator_weight * incorrect * (sample_weight &gt; 0))</span>
<span class="gi">+</span>
<span class="gi">+        return sample_weight, estimator_weight, estimator_error</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_bagging.py b/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gh">index 02d90ed..3825971 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_bagging.py</span>
<span class="gu">@@ -1,5 +1,10 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module ensemble classifiers.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>import sklearn
<span class="gu">@@ -12,23 +17,578 @@ from sklearn.model_selection import GridSearchCV, ParameterGrid, train_test_spli</span>
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier
<span class="w"> </span>from sklearn.svm import SVC
<span class="w"> </span>from sklearn.tree import DecisionTreeClassifier
<span class="gd">-from sklearn.utils._testing import assert_allclose, assert_array_almost_equal, assert_array_equal</span>
<span class="gi">+from sklearn.utils._testing import (</span>
<span class="gi">+    assert_allclose,</span>
<span class="gi">+    assert_array_almost_equal,</span>
<span class="gi">+    assert_array_equal,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn import FunctionSampler
<span class="w"> </span>from imblearn.datasets import make_imbalance
<span class="w"> </span>from imblearn.ensemble import BalancedBaggingClassifier
<span class="w"> </span>from imblearn.over_sampling import SMOTE, RandomOverSampler
<span class="w"> </span>from imblearn.pipeline import make_pipeline
<span class="w"> </span>from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="w"> </span>iris = load_iris()


<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;estimator&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        None,</span>
<span class="gi">+        DummyClassifier(strategy=&quot;prior&quot;),</span>
<span class="gi">+        Perceptron(max_iter=1000, tol=1e-3),</span>
<span class="gi">+        DecisionTreeClassifier(),</span>
<span class="gi">+        KNeighborsClassifier(),</span>
<span class="gi">+        SVC(gamma=&quot;scale&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;params&quot;,</span>
<span class="gi">+    ParameterGrid(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;max_samples&quot;: [0.5, 1.0],</span>
<span class="gi">+            &quot;max_features&quot;: [1, 2, 4],</span>
<span class="gi">+            &quot;bootstrap&quot;: [True, False],</span>
<span class="gi">+            &quot;bootstrap_features&quot;: [True, False],</span>
<span class="gi">+        }</span>
<span class="gi">+    ),</span>
<span class="gi">+)</span>
<span class="gi">+def test_balanced_bagging_classifier(estimator, params):</span>
<span class="gi">+    # Check classification for various parameter settings.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    bag = BalancedBaggingClassifier(estimator=estimator, random_state=0, **params).fit(</span>
<span class="gi">+        X_train, y_train</span>
<span class="gi">+    )</span>
<span class="gi">+    bag.predict(X_test)</span>
<span class="gi">+    bag.predict_proba(X_test)</span>
<span class="gi">+    bag.score(X_test, y_test)</span>
<span class="gi">+    if hasattr(estimator, &quot;decision_function&quot;):</span>
<span class="gi">+        bag.decision_function(X_test)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_bootstrap_samples():</span>
<span class="gi">+    # Test that bootstrapping samples generate non-perfect base estimators.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    estimator = DecisionTreeClassifier().fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    # without bootstrap, all trees are perfect on the training set</span>
<span class="gi">+    # disable the resampling by passing an empty dictionary.</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=DecisionTreeClassifier(),</span>
<span class="gi">+        max_samples=1.0,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+        n_estimators=10,</span>
<span class="gi">+        sampling_strategy={},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert ensemble.score(X_train, y_train) == estimator.score(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    # with bootstrap, trees are no longer perfect on the training set</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=DecisionTreeClassifier(),</span>
<span class="gi">+        max_samples=1.0,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert ensemble.score(X_train, y_train) &lt; estimator.score(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_bootstrap_features():</span>
<span class="gi">+    # Test that bootstrapping features may generate duplicate features.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=DecisionTreeClassifier(),</span>
<span class="gi">+        max_features=1.0,</span>
<span class="gi">+        bootstrap_features=False,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    for features in ensemble.estimators_features_:</span>
<span class="gi">+        assert np.unique(features).shape[0] == X.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=DecisionTreeClassifier(),</span>
<span class="gi">+        max_features=1.0,</span>
<span class="gi">+        bootstrap_features=True,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    unique_features = [</span>
<span class="gi">+        np.unique(features).shape[0] for features in ensemble.estimators_features_</span>
<span class="gi">+    ]</span>
<span class="gi">+    assert np.median(unique_features) &lt; X.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_probability():</span>
<span class="gi">+    # Predict probabilities.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    with np.errstate(divide=&quot;ignore&quot;, invalid=&quot;ignore&quot;):</span>
<span class="gi">+        # Normal case</span>
<span class="gi">+        ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+            estimator=DecisionTreeClassifier(), random_state=0</span>
<span class="gi">+        ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+        assert_array_almost_equal(</span>
<span class="gi">+            np.sum(ensemble.predict_proba(X_test), axis=1),</span>
<span class="gi">+            np.ones(len(X_test)),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        assert_array_almost_equal(</span>
<span class="gi">+            ensemble.predict_proba(X_test),</span>
<span class="gi">+            np.exp(ensemble.predict_log_proba(X_test)),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Degenerate case, where some classes are missing</span>
<span class="gi">+        ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+            estimator=LogisticRegression(solver=&quot;lbfgs&quot;),</span>
<span class="gi">+            random_state=0,</span>
<span class="gi">+            max_samples=5,</span>
<span class="gi">+        )</span>
<span class="gi">+        ensemble.fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+        assert_array_almost_equal(</span>
<span class="gi">+            np.sum(ensemble.predict_proba(X_test), axis=1),</span>
<span class="gi">+            np.ones(len(X_test)),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        assert_array_almost_equal(</span>
<span class="gi">+            ensemble.predict_proba(X_test),</span>
<span class="gi">+            np.exp(ensemble.predict_log_proba(X_test)),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_oob_score_classification():</span>
<span class="gi">+    # Check that oob prediction is a good estimation of the generalization</span>
<span class="gi">+    # error.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    for estimator in [DecisionTreeClassifier(), SVC(gamma=&quot;scale&quot;)]:</span>
<span class="gi">+        clf = BalancedBaggingClassifier(</span>
<span class="gi">+            estimator=estimator,</span>
<span class="gi">+            n_estimators=100,</span>
<span class="gi">+            bootstrap=True,</span>
<span class="gi">+            oob_score=True,</span>
<span class="gi">+            random_state=0,</span>
<span class="gi">+        ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+        test_score = clf.score(X_test, y_test)</span>
<span class="gi">+</span>
<span class="gi">+        assert abs(test_score - clf.oob_score_) &lt; 0.1</span>
<span class="gi">+</span>
<span class="gi">+        # Test with few estimators</span>
<span class="gi">+        with pytest.warns(UserWarning):</span>
<span class="gi">+            BalancedBaggingClassifier(</span>
<span class="gi">+                estimator=estimator,</span>
<span class="gi">+                n_estimators=1,</span>
<span class="gi">+                bootstrap=True,</span>
<span class="gi">+                oob_score=True,</span>
<span class="gi">+                random_state=0,</span>
<span class="gi">+            ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_single_estimator():</span>
<span class="gi">+    # Check singleton ensembles.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    clf1 = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=KNeighborsClassifier(),</span>
<span class="gi">+        n_estimators=1,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+        bootstrap_features=False,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    clf2 = make_pipeline(</span>
<span class="gi">+        RandomUnderSampler(random_state=clf1.estimators_[0].steps[0][1].random_state),</span>
<span class="gi">+        KNeighborsClassifier(),</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_equal(clf1.predict(X_test), clf2.predict(X_test))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_gridsearch():</span>
<span class="gi">+    # Check that bagging ensembles can be grid-searched.</span>
<span class="gi">+    # Transform iris into a binary classification task</span>
<span class="gi">+    X, y = iris.data, iris.target.copy()</span>
<span class="gi">+    y[y == 2] = 1</span>
<span class="gi">+</span>
<span class="gi">+    # Grid search with scoring based on decision_function</span>
<span class="gi">+    parameters = {&quot;n_estimators&quot;: (1, 2), &quot;estimator__C&quot;: (1, 2)}</span>
<span class="gi">+</span>
<span class="gi">+    GridSearchCV(</span>
<span class="gi">+        BalancedBaggingClassifier(SVC(gamma=&quot;scale&quot;)),</span>
<span class="gi">+        parameters,</span>
<span class="gi">+        cv=3,</span>
<span class="gi">+        scoring=&quot;roc_auc&quot;,</span>
<span class="gi">+    ).fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_estimator():</span>
<span class="gi">+    # Check estimator and its default values.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(None, n_jobs=3, random_state=0).fit(</span>
<span class="gi">+        X_train, y_train</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(ensemble.estimator_.steps[-1][1], DecisionTreeClassifier)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        DecisionTreeClassifier(), n_jobs=3, random_state=0</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(ensemble.estimator_.steps[-1][1], DecisionTreeClassifier)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = BalancedBaggingClassifier(</span>
<span class="gi">+        Perceptron(max_iter=1000, tol=1e-3), n_jobs=3, random_state=0</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(ensemble.estimator_.steps[-1][1], Perceptron)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_bagging_with_pipeline():</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    estimator = BalancedBaggingClassifier(</span>
<span class="gi">+        make_pipeline(SelectKBest(k=1), DecisionTreeClassifier()),</span>
<span class="gi">+        max_features=2,</span>
<span class="gi">+    )</span>
<span class="gi">+    estimator.fit(X, y).predict(X)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start(random_state=42):</span>
<span class="gi">+    # Test if fitting incrementally with warm start gives a forest of the</span>
<span class="gi">+    # right size and the same results as a normal fit.</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+</span>
<span class="gi">+    clf_ws = None</span>
<span class="gi">+    for n_estimators in [5, 10]:</span>
<span class="gi">+        if clf_ws is None:</span>
<span class="gi">+            clf_ws = BalancedBaggingClassifier(</span>
<span class="gi">+                n_estimators=n_estimators,</span>
<span class="gi">+                random_state=random_state,</span>
<span class="gi">+                warm_start=True,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            clf_ws.set_params(n_estimators=n_estimators)</span>
<span class="gi">+        clf_ws.fit(X, y)</span>
<span class="gi">+        assert len(clf_ws) == n_estimators</span>
<span class="gi">+</span>
<span class="gi">+    clf_no_ws = BalancedBaggingClassifier(</span>
<span class="gi">+        n_estimators=10, random_state=random_state, warm_start=False</span>
<span class="gi">+    )</span>
<span class="gi">+    clf_no_ws.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert {pipe.steps[-1][1].random_state for pipe in clf_ws} == {</span>
<span class="gi">+        pipe.steps[-1][1].random_state for pipe in clf_no_ws</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_smaller_n_estimators():</span>
<span class="gi">+    # Test if warm start&#39;ed second fit with smaller n_estimators raises error.</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    clf = BalancedBaggingClassifier(n_estimators=5, warm_start=True)</span>
<span class="gi">+    clf.fit(X, y)</span>
<span class="gi">+    clf.set_params(n_estimators=4)</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_equal_n_estimators():</span>
<span class="gi">+    # Test that nothing happens when fitting without increasing n_estimators</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)</span>
<span class="gi">+</span>
<span class="gi">+    clf = BalancedBaggingClassifier(n_estimators=5, warm_start=True, random_state=83)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    y_pred = clf.predict(X_test)</span>
<span class="gi">+    # modify X to nonsense values, this should not change anything</span>
<span class="gi">+    X_train += 1.0</span>
<span class="gi">+</span>
<span class="gi">+    warn_msg = &quot;Warm-start fitting without increasing n_estimators does not&quot;</span>
<span class="gi">+    with pytest.warns(UserWarning, match=warn_msg):</span>
<span class="gi">+        clf.fit(X_train, y_train)</span>
<span class="gi">+    assert_array_equal(y_pred, clf.predict(X_test))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_equivalence():</span>
<span class="gi">+    # warm started classifier with 5+5 estimators should be equivalent to</span>
<span class="gi">+    # one classifier with 10 estimators</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)</span>
<span class="gi">+</span>
<span class="gi">+    clf_ws = BalancedBaggingClassifier(</span>
<span class="gi">+        n_estimators=5, warm_start=True, random_state=3141</span>
<span class="gi">+    )</span>
<span class="gi">+    clf_ws.fit(X_train, y_train)</span>
<span class="gi">+    clf_ws.set_params(n_estimators=10)</span>
<span class="gi">+    clf_ws.fit(X_train, y_train)</span>
<span class="gi">+    y1 = clf_ws.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    clf = BalancedBaggingClassifier(</span>
<span class="gi">+        n_estimators=10, warm_start=False, random_state=3141</span>
<span class="gi">+    )</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+    y2 = clf.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_almost_equal(y1, y2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_with_oob_score_fails():</span>
<span class="gi">+    # Check using oob_score and warm_start simultaneously fails</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    clf = BalancedBaggingClassifier(n_estimators=5, warm_start=True, oob_score=True)</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_oob_score_removed_on_warm_start():</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=2000, random_state=1)</span>
<span class="gi">+</span>
<span class="gi">+    clf = BalancedBaggingClassifier(n_estimators=50, oob_score=True)</span>
<span class="gi">+    clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    clf.set_params(warm_start=True, oob_score=False, n_estimators=100)</span>
<span class="gi">+    clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(AttributeError):</span>
<span class="gi">+        getattr(clf, &quot;oob_score_&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_oob_score_consistency():</span>
<span class="gi">+    # Make sure OOB scores are identical when random_state, estimator, and</span>
<span class="gi">+    # training data are fixed and fitting is done twice</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=200, random_state=1)</span>
<span class="gi">+    bagging = BalancedBaggingClassifier(</span>
<span class="gi">+        KNeighborsClassifier(),</span>
<span class="gi">+        max_samples=0.5,</span>
<span class="gi">+        max_features=0.5,</span>
<span class="gi">+        oob_score=True,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    assert bagging.fit(X, y).oob_score_ == bagging.fit(X, y).oob_score_</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_estimators_samples():</span>
<span class="gi">+    # Check that format of estimators_samples_ is correct and that results</span>
<span class="gi">+    # generated at fit time can be identically reproduced at a later time</span>
<span class="gi">+    # using data saved in object attributes.</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=200, random_state=1)</span>
<span class="gi">+</span>
<span class="gi">+    # remap the y outside of the BalancedBaggingclassifier</span>
<span class="gi">+    # _, y = np.unique(y, return_inverse=True)</span>
<span class="gi">+    bagging = BalancedBaggingClassifier(</span>
<span class="gi">+        LogisticRegression(),</span>
<span class="gi">+        max_samples=0.5,</span>
<span class="gi">+        max_features=0.5,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+    )</span>
<span class="gi">+    bagging.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    # Get relevant attributes</span>
<span class="gi">+    estimators_samples = bagging.estimators_samples_</span>
<span class="gi">+    estimators_features = bagging.estimators_features_</span>
<span class="gi">+    estimators = bagging.estimators_</span>
<span class="gi">+</span>
<span class="gi">+    # Test for correct formatting</span>
<span class="gi">+    assert len(estimators_samples) == len(estimators)</span>
<span class="gi">+    assert len(estimators_samples[0]) == len(X) // 2</span>
<span class="gi">+    assert estimators_samples[0].dtype.kind == &quot;i&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # Re-fit single estimator to test for consistent sampling</span>
<span class="gi">+    estimator_index = 0</span>
<span class="gi">+    estimator_samples = estimators_samples[estimator_index]</span>
<span class="gi">+    estimator_features = estimators_features[estimator_index]</span>
<span class="gi">+    estimator = estimators[estimator_index]</span>
<span class="gi">+</span>
<span class="gi">+    X_train = (X[estimator_samples])[:, estimator_features]</span>
<span class="gi">+    y_train = y[estimator_samples]</span>
<span class="gi">+</span>
<span class="gi">+    orig_coefs = estimator.steps[-1][1].coef_</span>
<span class="gi">+    estimator.fit(X_train, y_train)</span>
<span class="gi">+    new_coefs = estimator.steps[-1][1].coef_</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(orig_coefs, new_coefs)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_max_samples_consistency():</span>
<span class="gi">+    # Make sure validated max_samples and original max_samples are identical</span>
<span class="gi">+    # when valid integer max_samples supplied by user</span>
<span class="gi">+    max_samples = 100</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=2 * max_samples, random_state=1)</span>
<span class="gi">+    bagging = BalancedBaggingClassifier(</span>
<span class="gi">+        KNeighborsClassifier(),</span>
<span class="gi">+        max_samples=max_samples,</span>
<span class="gi">+        max_features=0.5,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    bagging.fit(X, y)</span>
<span class="gi">+    assert bagging._max_samples == max_samples</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>class CountDecisionTreeClassifier(DecisionTreeClassifier):
<span class="w"> </span>    &quot;&quot;&quot;DecisionTreeClassifier that will memorize the number of samples seen
<span class="w"> </span>    at fit.&quot;&quot;&quot;

<span class="gi">+    def fit(self, X, y, sample_weight=None):</span>
<span class="gi">+        self.class_counts_ = Counter(y)</span>
<span class="gi">+        return super().fit(X, y, sample_weight=sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Number of distinct clusters&quot;)</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampler, n_samples_bootstrap&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (None, 15),</span>
<span class="gi">+        (RandomUnderSampler(), 15),  # under-sampling with sample_indices_</span>
<span class="gi">+        (</span>
<span class="gi">+            ClusterCentroids(estimator=KMeans(n_init=1)),</span>
<span class="gi">+            15,</span>
<span class="gi">+        ),  # under-sampling without sample_indices_</span>
<span class="gi">+        (RandomOverSampler(), 40),  # over-sampling with sample_indices_</span>
<span class="gi">+        (SMOTE(), 40),  # over-sampling without sample_indices_</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_balanced_bagging_classifier_samplers(sampler, n_samples_bootstrap):</span>
<span class="gi">+    # check that we can pass any kind of sampler to a bagging classifier</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+    clf = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=CountDecisionTreeClassifier(),</span>
<span class="gi">+        n_estimators=2,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+    clf.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    # check that we have balanced class with the right counts of class</span>
<span class="gi">+    # sample depending on the sampling strategy</span>
<span class="gi">+    assert_array_equal(</span>
<span class="gi">+        list(clf.estimators_[0][-1].class_counts_.values()), n_samples_bootstrap</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;replace&quot;, [True, False])</span>
<span class="gi">+def test_balanced_bagging_classifier_with_function_sampler(replace):</span>
<span class="gi">+    # check that we can provide a FunctionSampler in BalancedBaggingClassifier</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1_000,</span>
<span class="gi">+        n_features=10,</span>
<span class="gi">+        n_classes=2,</span>
<span class="gi">+        weights=[0.3, 0.7],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    def roughly_balanced_bagging(X, y, replace=False):</span>
<span class="gi">+        &quot;&quot;&quot;Implementation of Roughly Balanced Bagging for binary problem.&quot;&quot;&quot;</span>
<span class="gi">+        # find the minority and majority classes</span>
<span class="gi">+        class_counts = Counter(y)</span>
<span class="gi">+        majority_class = max(class_counts, key=class_counts.get)</span>
<span class="gi">+        minority_class = min(class_counts, key=class_counts.get)</span>
<span class="gi">+</span>
<span class="gi">+        # compute the number of sample to draw from the majority class using</span>
<span class="gi">+        # a negative binomial distribution</span>
<span class="gi">+        n_minority_class = class_counts[minority_class]</span>
<span class="gi">+        n_majority_resampled = np.random.negative_binomial(n=n_minority_class, p=0.5)</span>
<span class="gi">+</span>
<span class="gi">+        # draw randomly with or without replacement</span>
<span class="gi">+        majority_indices = np.random.choice(</span>
<span class="gi">+            np.flatnonzero(y == majority_class),</span>
<span class="gi">+            size=n_majority_resampled,</span>
<span class="gi">+            replace=replace,</span>
<span class="gi">+        )</span>
<span class="gi">+        minority_indices = np.random.choice(</span>
<span class="gi">+            np.flatnonzero(y == minority_class),</span>
<span class="gi">+            size=n_minority_class,</span>
<span class="gi">+            replace=replace,</span>
<span class="gi">+        )</span>
<span class="gi">+        indices = np.hstack([majority_indices, minority_indices])</span>
<span class="gi">+</span>
<span class="gi">+        return X[indices], y[indices]</span>
<span class="gi">+</span>
<span class="gi">+    # Roughly Balanced Bagging</span>
<span class="gi">+    rbb = BalancedBaggingClassifier(</span>
<span class="gi">+        estimator=CountDecisionTreeClassifier(random_state=0),</span>
<span class="gi">+        n_estimators=2,</span>
<span class="gi">+        sampler=FunctionSampler(</span>
<span class="gi">+            func=roughly_balanced_bagging, kw_args={&quot;replace&quot;: replace}</span>
<span class="gi">+        ),</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    rbb.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    for estimator in rbb.estimators_:</span>
<span class="gi">+        class_counts = estimator[-1].class_counts_</span>
<span class="gi">+        assert (class_counts[0] / class_counts[1]) &gt; 0.78</span>
<span class="gi">+</span>

<span class="w"> </span>def test_balanced_bagging_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = load_iris(return_X_y=True)</span>
<span class="gi">+    estimator = BalancedBaggingClassifier().fit(X, y)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`n_features_` was deprecated&quot;):</span>
<span class="gi">+        estimator.n_features_</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_easy_ensemble.py b/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gh">index 3b667a8..7dc0441 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_easy_ensemble.py</span>
<span class="gu">@@ -1,4 +1,8 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module easy ensemble.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>import sklearn
<span class="gu">@@ -8,21 +12,224 @@ from sklearn.feature_selection import SelectKBest</span>
<span class="w"> </span>from sklearn.model_selection import GridSearchCV, train_test_split
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn.datasets import make_imbalance
<span class="w"> </span>from imblearn.ensemble import EasyEnsembleClassifier
<span class="w"> </span>from imblearn.pipeline import make_pipeline
<span class="w"> </span>from imblearn.under_sampling import RandomUnderSampler
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="w"> </span>iris = load_iris()
<span class="gi">+</span>
<span class="gi">+# Generate a global dataset to use</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.5220963, 0.11349303], [0.59091459, 0.40692742], [</span>
<span class="gd">-    1.10915364, 0.05718352], [0.22039505, 0.26469445], [1.35269503, </span>
<span class="gd">-    0.44812421], [0.85117925, 1.0185556], [-2.10724436, 0.70263997], [-</span>
<span class="gd">-    0.23627356, 0.30254174], [-1.23195149, 0.15427291], [-0.58539673, </span>
<span class="gd">-    0.62515052]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.5220963, 0.11349303],</span>
<span class="gi">+        [0.59091459, 0.40692742],</span>
<span class="gi">+        [1.10915364, 0.05718352],</span>
<span class="gi">+        [0.22039505, 0.26469445],</span>
<span class="gi">+        [1.35269503, 0.44812421],</span>
<span class="gi">+        [0.85117925, 1.0185556],</span>
<span class="gi">+        [-2.10724436, 0.70263997],</span>
<span class="gi">+        [-0.23627356, 0.30254174],</span>
<span class="gi">+        [-1.23195149, 0.15427291],</span>
<span class="gi">+        [-0.58539673, 0.62515052],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 2, 2, 2, 1, 0, 1, 1, 1, 0])


<span class="gi">+@pytest.mark.parametrize(&quot;n_estimators&quot;, [10, 20])</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;estimator&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        AdaBoostClassifier(algorithm=&quot;SAMME&quot;, n_estimators=5),</span>
<span class="gi">+        AdaBoostClassifier(algorithm=&quot;SAMME&quot;, n_estimators=10),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_easy_ensemble_classifier(n_estimators, estimator):</span>
<span class="gi">+    # Check classification for various parameter settings.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    eec = EasyEnsembleClassifier(</span>
<span class="gi">+        n_estimators=n_estimators,</span>
<span class="gi">+        estimator=estimator,</span>
<span class="gi">+        n_jobs=-1,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+    )</span>
<span class="gi">+    eec.fit(X_train, y_train).score(X_test, y_test)</span>
<span class="gi">+    assert len(eec.estimators_) == n_estimators</span>
<span class="gi">+    for est in eec.estimators_:</span>
<span class="gi">+        assert len(est.named_steps[&quot;classifier&quot;]) == estimator.n_estimators</span>
<span class="gi">+    # test the different prediction function</span>
<span class="gi">+    eec.predict(X_test)</span>
<span class="gi">+    eec.predict_proba(X_test)</span>
<span class="gi">+    eec.predict_log_proba(X_test)</span>
<span class="gi">+    eec.decision_function(X_test)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_estimator():</span>
<span class="gi">+    # Check estimator and its default values.</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = EasyEnsembleClassifier(2, None, n_jobs=-1, random_state=0).fit(</span>
<span class="gi">+        X_train, y_train</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(ensemble.estimator_.steps[-1][1], AdaBoostClassifier)</span>
<span class="gi">+</span>
<span class="gi">+    ensemble = EasyEnsembleClassifier(</span>
<span class="gi">+        2, AdaBoostClassifier(algorithm=&quot;SAMME&quot;), n_jobs=-1, random_state=0</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(ensemble.estimator_.steps[-1][1], AdaBoostClassifier)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_bagging_with_pipeline():</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    estimator = EasyEnsembleClassifier(</span>
<span class="gi">+        n_estimators=2,</span>
<span class="gi">+        estimator=make_pipeline(</span>
<span class="gi">+            SelectKBest(k=1), AdaBoostClassifier(algorithm=&quot;SAMME&quot;)</span>
<span class="gi">+        ),</span>
<span class="gi">+    )</span>
<span class="gi">+    estimator.fit(X, y).predict(X)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start(random_state=42):</span>
<span class="gi">+    # Test if fitting incrementally with warm start gives a forest of the</span>
<span class="gi">+    # right size and the same results as a normal fit.</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+</span>
<span class="gi">+    clf_ws = None</span>
<span class="gi">+    for n_estimators in [5, 10]:</span>
<span class="gi">+        if clf_ws is None:</span>
<span class="gi">+            clf_ws = EasyEnsembleClassifier(</span>
<span class="gi">+                n_estimators=n_estimators,</span>
<span class="gi">+                random_state=random_state,</span>
<span class="gi">+                warm_start=True,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            clf_ws.set_params(n_estimators=n_estimators)</span>
<span class="gi">+        clf_ws.fit(X, y)</span>
<span class="gi">+        assert len(clf_ws) == n_estimators</span>
<span class="gi">+</span>
<span class="gi">+    clf_no_ws = EasyEnsembleClassifier(</span>
<span class="gi">+        n_estimators=10, random_state=random_state, warm_start=False</span>
<span class="gi">+    )</span>
<span class="gi">+    clf_no_ws.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert {pipe.steps[-1][1].random_state for pipe in clf_ws} == {</span>
<span class="gi">+        pipe.steps[-1][1].random_state for pipe in clf_no_ws</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_smaller_n_estimators():</span>
<span class="gi">+    # Test if warm start&#39;ed second fit with smaller n_estimators raises error.</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    clf = EasyEnsembleClassifier(n_estimators=5, warm_start=True)</span>
<span class="gi">+    clf.fit(X, y)</span>
<span class="gi">+    clf.set_params(n_estimators=4)</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        clf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_equal_n_estimators():</span>
<span class="gi">+    # Test that nothing happens when fitting without increasing n_estimators</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)</span>
<span class="gi">+</span>
<span class="gi">+    clf = EasyEnsembleClassifier(n_estimators=5, warm_start=True, random_state=83)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    y_pred = clf.predict(X_test)</span>
<span class="gi">+    # modify X to nonsense values, this should not change anything</span>
<span class="gi">+    X_train += 1.0</span>
<span class="gi">+</span>
<span class="gi">+    warn_msg = &quot;Warm-start fitting without increasing n_estimators&quot;</span>
<span class="gi">+    with pytest.warns(UserWarning, match=warn_msg):</span>
<span class="gi">+        clf.fit(X_train, y_train)</span>
<span class="gi">+    assert_array_equal(y_pred, clf.predict(X_test))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_warm_start_equivalence():</span>
<span class="gi">+    # warm started classifier with 5+5 estimators should be equivalent to</span>
<span class="gi">+    # one classifier with 10 estimators</span>
<span class="gi">+    X, y = make_hastie_10_2(n_samples=20, random_state=1)</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)</span>
<span class="gi">+</span>
<span class="gi">+    clf_ws = EasyEnsembleClassifier(n_estimators=5, warm_start=True, random_state=3141)</span>
<span class="gi">+    clf_ws.fit(X_train, y_train)</span>
<span class="gi">+    clf_ws.set_params(n_estimators=10)</span>
<span class="gi">+    clf_ws.fit(X_train, y_train)</span>
<span class="gi">+    y1 = clf_ws.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    clf = EasyEnsembleClassifier(n_estimators=10, warm_start=False, random_state=3141)</span>
<span class="gi">+    clf.fit(X_train, y_train)</span>
<span class="gi">+    y2 = clf.predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(y1, y2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_easy_ensemble_classifier_single_estimator():</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    clf1 = EasyEnsembleClassifier(n_estimators=1, random_state=0).fit(X_train, y_train)</span>
<span class="gi">+    clf2 = make_pipeline(</span>
<span class="gi">+        RandomUnderSampler(random_state=0),</span>
<span class="gi">+        AdaBoostClassifier(algorithm=&quot;SAMME&quot;, random_state=0),</span>
<span class="gi">+    ).fit(X_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_equal(clf1.predict(X_test), clf2.predict(X_test))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_easy_ensemble_classifier_grid_search():</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data,</span>
<span class="gi">+        iris.target,</span>
<span class="gi">+        sampling_strategy={0: 20, 1: 25, 2: 50},</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    parameters = {</span>
<span class="gi">+        &quot;n_estimators&quot;: [1, 2],</span>
<span class="gi">+        &quot;estimator__n_estimators&quot;: [3, 4],</span>
<span class="gi">+    }</span>
<span class="gi">+    grid_search = GridSearchCV(</span>
<span class="gi">+        EasyEnsembleClassifier(estimator=AdaBoostClassifier(algorithm=&quot;SAMME&quot;)),</span>
<span class="gi">+        parameters,</span>
<span class="gi">+        cv=5,</span>
<span class="gi">+    )</span>
<span class="gi">+    grid_search.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_easy_ensemble_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = load_iris(return_X_y=True)</span>
<span class="gi">+    estimator = EasyEnsembleClassifier().fit(X, y)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`n_features_` was deprecated&quot;):</span>
<span class="gi">+        estimator.n_features_</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_forest.py b/imblearn/ensemble/tests/test_forest.py</span>
<span class="gh">index 2742293..3719568 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_forest.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_forest.py</span>
<span class="gu">@@ -5,32 +5,340 @@ from sklearn.datasets import load_iris, make_classification</span>
<span class="w"> </span>from sklearn.model_selection import GridSearchCV, train_test_split
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn.ensemble import BalancedRandomForestClassifier
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gi">+@pytest.fixture</span>
<span class="gi">+def imbalanced_dataset():</span>
<span class="gi">+    return make_classification(</span>
<span class="gi">+        n_samples=10000,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        weights=[0.01, 0.05, 0.94],</span>
<span class="gi">+        class_sep=0.8,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest_error_warning_warm_start(imbalanced_dataset):</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=5, sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False</span>
<span class="gi">+    )</span>
<span class="gi">+    brf.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;must be larger or equal to&quot;):</span>
<span class="gi">+        brf.set_params(warm_start=True, n_estimators=2)</span>
<span class="gi">+        brf.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+    brf.set_params(n_estimators=10)</span>
<span class="gi">+    brf.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(UserWarning, match=&quot;Warm-start fitting without&quot;):</span>
<span class="gi">+        brf.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest(imbalanced_dataset):</span>
<span class="gi">+    n_estimators = 10</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=n_estimators,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+    )</span>
<span class="gi">+    brf.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+    assert len(brf.samplers_) == n_estimators</span>
<span class="gi">+    assert len(brf.estimators_) == n_estimators</span>
<span class="gi">+    assert len(brf.pipelines_) == n_estimators</span>
<span class="gi">+    assert len(brf.feature_importances_) == imbalanced_dataset[0].shape[1]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest_attributes(imbalanced_dataset):</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    n_estimators = 10</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=n_estimators,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+    )</span>
<span class="gi">+    brf.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    for idx in range(n_estimators):</span>
<span class="gi">+        X_res, y_res = brf.samplers_[idx].fit_resample(X, y)</span>
<span class="gi">+        X_res_2, y_res_2 = (</span>
<span class="gi">+            brf.pipelines_[idx].named_steps[&quot;randomundersampler&quot;].fit_resample(X, y)</span>
<span class="gi">+        )</span>
<span class="gi">+        assert_allclose(X_res, X_res_2)</span>
<span class="gi">+        assert_array_equal(y_res, y_res_2)</span>
<span class="gi">+</span>
<span class="gi">+        y_pred = brf.estimators_[idx].fit(X_res, y_res).predict(X)</span>
<span class="gi">+        y_pred_2 = brf.pipelines_[idx].fit(X, y).predict(X)</span>
<span class="gi">+        assert_array_equal(y_pred, y_pred_2)</span>
<span class="gi">+</span>
<span class="gi">+        y_pred = brf.estimators_[idx].fit(X_res, y_res).predict_proba(X)</span>
<span class="gi">+        y_pred_2 = brf.pipelines_[idx].fit(X, y).predict_proba(X)</span>
<span class="gi">+        assert_array_equal(y_pred, y_pred_2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest_sample_weight(imbalanced_dataset):</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    sample_weight = rng.rand(y.shape[0])</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=5,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+    )</span>
<span class="gi">+    brf.fit(X, y, sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Some inputs do not have OOB scores&quot;)</span>
<span class="gi">+def test_balanced_random_forest_oob(imbalanced_dataset):</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="gi">+        X, y, random_state=42, stratify=y</span>
<span class="gi">+    )</span>
<span class="gi">+    est = BalancedRandomForestClassifier(</span>
<span class="gi">+        oob_score=True,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        n_estimators=1000,</span>
<span class="gi">+        min_samples_leaf=2,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    est.fit(X_train, y_train)</span>
<span class="gi">+    test_score = est.score(X_test, y_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert abs(test_score - est.oob_score_) &lt; 0.1</span>
<span class="gi">+</span>
<span class="gi">+    # Check warning if not enough estimators</span>
<span class="gi">+    est = BalancedRandomForestClassifier(</span>
<span class="gi">+        oob_score=True,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        n_estimators=1,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.warns(UserWarning) and np.errstate(divide=&quot;ignore&quot;, invalid=&quot;ignore&quot;):</span>
<span class="gi">+        est.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest_grid_search(imbalanced_dataset):</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False</span>
<span class="gi">+    )</span>
<span class="gi">+    grid = GridSearchCV(brf, {&quot;n_estimators&quot;: (1, 2), &quot;max_depth&quot;: (1, 2)}, cv=3)</span>
<span class="gi">+    grid.fit(*imbalanced_dataset)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_little_tree_with_small_max_samples():</span>
<span class="gi">+    rng = np.random.RandomState(1)</span>
<span class="gi">+</span>
<span class="gi">+    X = rng.randn(10000, 2)</span>
<span class="gi">+    y = rng.randn(10000) &gt; 0</span>
<span class="gi">+</span>
<span class="gi">+    # First fit with no restriction on max samples</span>
<span class="gi">+    est1 = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=1,</span>
<span class="gi">+        random_state=rng,</span>
<span class="gi">+        max_samples=None,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    # Second fit with max samples restricted to just 2</span>
<span class="gi">+    est2 = BalancedRandomForestClassifier(</span>
<span class="gi">+        n_estimators=1,</span>
<span class="gi">+        random_state=rng,</span>
<span class="gi">+        max_samples=2,</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    est1.fit(X, y)</span>
<span class="gi">+    est2.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    tree1 = est1.estimators_[0].tree_</span>
<span class="gi">+    tree2 = est2.estimators_[0].tree_</span>
<span class="gi">+</span>
<span class="gi">+    msg = &quot;Tree without `max_samples` restriction should have more nodes&quot;</span>
<span class="gi">+    assert tree1.node_count &gt; tree2.node_count, msg</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_random_forest_pruning(imbalanced_dataset):</span>
<span class="gi">+    brf = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False</span>
<span class="gi">+    )</span>
<span class="gi">+    brf.fit(*imbalanced_dataset)</span>
<span class="gi">+    n_nodes_no_pruning = brf.estimators_[0].tree_.node_count</span>
<span class="gi">+</span>
<span class="gi">+    brf_pruned = BalancedRandomForestClassifier(</span>
<span class="gi">+        ccp_alpha=0.015, sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False</span>
<span class="gi">+    )</span>
<span class="gi">+    brf_pruned.fit(*imbalanced_dataset)</span>
<span class="gi">+    n_nodes_pruning = brf_pruned.estimators_[0].tree_.node_count</span>
<span class="gi">+</span>
<span class="gi">+    assert n_nodes_no_pruning &gt; n_nodes_pruning</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;ratio&quot;, [0.5, 0.1])</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Some inputs do not have OOB scores&quot;)</span>
<span class="gi">+def test_balanced_random_forest_oob_binomial(ratio):</span>
<span class="gi">+    # Regression test for #655: check that the oob score is closed to 0.5</span>
<span class="gi">+    # a binomial experiment.</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    n_samples = 1000</span>
<span class="gi">+    X = np.arange(n_samples).reshape(-1, 1)</span>
<span class="gi">+    y = rng.binomial(1, ratio, size=n_samples)</span>
<span class="gi">+</span>
<span class="gi">+    erf = BalancedRandomForestClassifier(</span>
<span class="gi">+        oob_score=True,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+        sampling_strategy=&quot;not minority&quot;,</span>
<span class="gi">+        replacement=False,</span>
<span class="gi">+        bootstrap=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    erf.fit(X, y)</span>
<span class="gi">+    assert np.abs(erf.oob_score_ - 0.5) &lt; 0.1</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_balanced_bagging_classifier_n_features():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing `n_features_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = load_iris(return_X_y=True)</span>
<span class="gi">+    estimator = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False</span>
<span class="gi">+    ).fit(X, y)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`n_features_` was deprecated&quot;):</span>
<span class="gi">+        estimator.n_features_</span>


<span class="gi">+# TODO: remove in 0.13</span>
<span class="w"> </span>def test_balanced_random_forest_change_behaviour(imbalanced_dataset):
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a change of behaviour for the parameters `sampling_strategy`
<span class="w"> </span>    and `replacement`.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    estimator = BalancedRandomForestClassifier(sampling_strategy=&quot;all&quot;, bootstrap=False)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The default of `replacement`&quot;):</span>
<span class="gi">+        estimator.fit(*imbalanced_dataset)</span>
<span class="gi">+    estimator = BalancedRandomForestClassifier(replacement=True, bootstrap=False)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The default of `sampling_strategy`&quot;):</span>
<span class="gi">+        estimator.fit(*imbalanced_dataset)</span>
<span class="gi">+    estimator = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;The default of `bootstrap`&quot;):</span>
<span class="gi">+        estimator.fit(*imbalanced_dataset)</span>


<span class="gd">-@pytest.mark.skipif(parse_version(sklearn_version.base_version) &lt;</span>
<span class="gd">-    parse_version(&#39;1.4&#39;), reason=&#39;scikit-learn should be &gt;= 1.4&#39;)</span>
<span class="gi">+@pytest.mark.skipif(</span>
<span class="gi">+    parse_version(sklearn_version.base_version) &lt; parse_version(&quot;1.4&quot;),</span>
<span class="gi">+    reason=&quot;scikit-learn should be &gt;= 1.4&quot;,</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_missing_values_is_resilient():
<span class="w"> </span>    &quot;&quot;&quot;Check that forest can deal with missing values and has decent performance.&quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    n_samples, n_features = 1000, 10</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=n_samples, n_features=n_features, random_state=rng</span>
<span class="gi">+    )</span>

<span class="gd">-@pytest.mark.skipif(parse_version(sklearn_version.base_version) &lt;</span>
<span class="gd">-    parse_version(&#39;1.4&#39;), reason=&#39;scikit-learn should be &gt;= 1.4&#39;)</span>
<span class="gi">+    # Create dataset with missing values</span>
<span class="gi">+    X_missing = X.copy()</span>
<span class="gi">+    X_missing[rng.choice([False, True], size=X.shape, p=[0.95, 0.05])] = np.nan</span>
<span class="gi">+    assert np.isnan(X_missing).any()</span>
<span class="gi">+</span>
<span class="gi">+    X_missing_train, X_missing_test, y_train, y_test = train_test_split(</span>
<span class="gi">+        X_missing, y, random_state=0</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    # Train forest with missing values</span>
<span class="gi">+    forest_with_missing = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+        random_state=rng,</span>
<span class="gi">+        n_estimators=50,</span>
<span class="gi">+    )</span>
<span class="gi">+    forest_with_missing.fit(X_missing_train, y_train)</span>
<span class="gi">+    score_with_missing = forest_with_missing.score(X_missing_test, y_test)</span>
<span class="gi">+</span>
<span class="gi">+    # Train forest without missing values</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span>
<span class="gi">+    forest = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+        bootstrap=False,</span>
<span class="gi">+        random_state=rng,</span>
<span class="gi">+        n_estimators=50,</span>
<span class="gi">+    )</span>
<span class="gi">+    forest.fit(X_train, y_train)</span>
<span class="gi">+    score_without_missing = forest.score(X_test, y_test)</span>
<span class="gi">+</span>
<span class="gi">+    # Score is still 80 percent of the forest&#39;s score that had no missing values</span>
<span class="gi">+    assert score_with_missing &gt;= 0.80 * score_without_missing</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.skipif(</span>
<span class="gi">+    parse_version(sklearn_version.base_version) &lt; parse_version(&quot;1.4&quot;),</span>
<span class="gi">+    reason=&quot;scikit-learn should be &gt;= 1.4&quot;,</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_missing_value_is_predictive():
<span class="w"> </span>    &quot;&quot;&quot;Check that the forest learns when missing values are only present for
<span class="w"> </span>    a predictive feature.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    n_samples = 300</span>
<span class="gi">+</span>
<span class="gi">+    X_non_predictive = rng.standard_normal(size=(n_samples, 10))</span>
<span class="gi">+    y = rng.randint(0, high=2, size=n_samples)</span>
<span class="gi">+</span>
<span class="gi">+    # Create a predictive feature using `y` and with some noise</span>
<span class="gi">+    X_random_mask = rng.choice([False, True], size=n_samples, p=[0.95, 0.05])</span>
<span class="gi">+    y_mask = y.astype(bool)</span>
<span class="gi">+    y_mask[X_random_mask] = ~y_mask[X_random_mask]</span>
<span class="gi">+</span>
<span class="gi">+    predictive_feature = rng.standard_normal(size=n_samples)</span>
<span class="gi">+    predictive_feature[y_mask] = np.nan</span>
<span class="gi">+    assert np.isnan(predictive_feature).any()</span>
<span class="gi">+</span>
<span class="gi">+    X_predictive = X_non_predictive.copy()</span>
<span class="gi">+    X_predictive[:, 5] = predictive_feature</span>
<span class="gi">+</span>
<span class="gi">+    (</span>
<span class="gi">+        X_predictive_train,</span>
<span class="gi">+        X_predictive_test,</span>
<span class="gi">+        X_non_predictive_train,</span>
<span class="gi">+        X_non_predictive_test,</span>
<span class="gi">+        y_train,</span>
<span class="gi">+        y_test,</span>
<span class="gi">+    ) = train_test_split(X_predictive, X_non_predictive, y, random_state=0)</span>
<span class="gi">+    forest_predictive = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False, random_state=0</span>
<span class="gi">+    ).fit(X_predictive_train, y_train)</span>
<span class="gi">+    forest_non_predictive = BalancedRandomForestClassifier(</span>
<span class="gi">+        sampling_strategy=&quot;all&quot;, replacement=True, bootstrap=False, random_state=0</span>
<span class="gi">+    ).fit(X_non_predictive_train, y_train)</span>
<span class="gi">+</span>
<span class="gi">+    predictive_test_score = forest_predictive.score(X_predictive_test, y_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert predictive_test_score &gt;= 0.75</span>
<span class="gi">+    assert predictive_test_score &gt;= forest_non_predictive.score(</span>
<span class="gi">+        X_non_predictive_test, y_test</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/ensemble/tests/test_weight_boosting.py b/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gh">index c1d0d1c..ad3dbca 100644</span>
<span class="gd">--- a/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gi">+++ b/imblearn/ensemble/tests/test_weight_boosting.py</span>
<span class="gu">@@ -5,5 +5,90 @@ from sklearn.datasets import make_classification</span>
<span class="w"> </span>from sklearn.model_selection import train_test_split
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn.ensemble import RUSBoostClassifier
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def imbalanced_dataset():</span>
<span class="gi">+    return make_classification(</span>
<span class="gi">+        n_samples=10000,</span>
<span class="gi">+        n_features=3,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        weights=[0.01, 0.05, 0.94],</span>
<span class="gi">+        class_sep=0.8,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;algorithm&quot;, [&quot;SAMME&quot;, &quot;SAMME.R&quot;])</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The SAMME.R algorithm (the default) is&quot;)</span>
<span class="gi">+def test_rusboost(imbalanced_dataset, algorithm):</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="gi">+        X, y, stratify=y, random_state=1</span>
<span class="gi">+    )</span>
<span class="gi">+    classes = np.unique(y)</span>
<span class="gi">+</span>
<span class="gi">+    n_estimators = 500</span>
<span class="gi">+    rusboost = RUSBoostClassifier(</span>
<span class="gi">+        n_estimators=n_estimators, algorithm=algorithm, random_state=0</span>
<span class="gi">+    )</span>
<span class="gi">+    rusboost.fit(X_train, y_train)</span>
<span class="gi">+    assert_array_equal(classes, rusboost.classes_)</span>
<span class="gi">+</span>
<span class="gi">+    # check that we have an ensemble of samplers and estimators with a</span>
<span class="gi">+    # consistent size</span>
<span class="gi">+    assert len(rusboost.estimators_) &gt; 1</span>
<span class="gi">+    assert len(rusboost.estimators_) == len(rusboost.samplers_)</span>
<span class="gi">+    assert len(rusboost.pipelines_) == len(rusboost.samplers_)</span>
<span class="gi">+</span>
<span class="gi">+    # each sampler in the ensemble should have different random state</span>
<span class="gi">+    assert len({sampler.random_state for sampler in rusboost.samplers_}) == len(</span>
<span class="gi">+        rusboost.samplers_</span>
<span class="gi">+    )</span>
<span class="gi">+    # each estimator in the ensemble should have different random state</span>
<span class="gi">+    assert len({est.random_state for est in rusboost.estimators_}) == len(</span>
<span class="gi">+        rusboost.estimators_</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    # check the consistency of the feature importances</span>
<span class="gi">+    assert len(rusboost.feature_importances_) == imbalanced_dataset[0].shape[1]</span>
<span class="gi">+</span>
<span class="gi">+    # check the consistency of the prediction outpus</span>
<span class="gi">+    y_pred = rusboost.predict_proba(X_test)</span>
<span class="gi">+    assert y_pred.shape[1] == len(classes)</span>
<span class="gi">+    assert rusboost.decision_function(X_test).shape[1] == len(classes)</span>
<span class="gi">+</span>
<span class="gi">+    score = rusboost.score(X_test, y_test)</span>
<span class="gi">+    assert score &gt; 0.6, f&quot;Failed with algorithm {algorithm} and score {score}&quot;</span>
<span class="gi">+</span>
<span class="gi">+    y_pred = rusboost.predict(X_test)</span>
<span class="gi">+    assert y_pred.shape == y_test.shape</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;algorithm&quot;, [&quot;SAMME&quot;, &quot;SAMME.R&quot;])</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The SAMME.R algorithm (the default) is&quot;)</span>
<span class="gi">+def test_rusboost_sample_weight(imbalanced_dataset, algorithm):</span>
<span class="gi">+    X, y = imbalanced_dataset</span>
<span class="gi">+    sample_weight = np.ones_like(y)</span>
<span class="gi">+    rusboost = RUSBoostClassifier(algorithm=algorithm, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    # Predictions should be the same when sample_weight are all ones</span>
<span class="gi">+    y_pred_sample_weight = rusboost.fit(X, y, sample_weight).predict(X)</span>
<span class="gi">+    y_pred_no_sample_weight = rusboost.fit(X, y).predict(X)</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_equal(y_pred_sample_weight, y_pred_no_sample_weight)</span>
<span class="gi">+</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    sample_weight = rng.rand(y.shape[0])</span>
<span class="gi">+    y_pred_sample_weight = rusboost.fit(X, y, sample_weight).predict(X)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(AssertionError):</span>
<span class="gi">+        assert_array_equal(y_pred_no_sample_weight, y_pred_sample_weight)</span>
<span class="gh">diff --git a/imblearn/exceptions.py b/imblearn/exceptions.py</span>
<span class="gh">index a78c1ad..1011d14 100644</span>
<span class="gd">--- a/imblearn/exceptions.py</span>
<span class="gi">+++ b/imblearn/exceptions.py</span>
<span class="gu">@@ -3,6 +3,9 @@ The :mod:`imblearn.exceptions` module includes all custom warnings and error</span>
<span class="w"> </span>classes and functions used across imbalanced-learn.
<span class="w"> </span>&quot;&quot;&quot;

<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>

<span class="w"> </span>def raise_isinstance_error(variable_name, possible_type, variable):
<span class="w"> </span>    &quot;&quot;&quot;Raise consistent error message for isinstance() function.
<span class="gu">@@ -23,4 +26,7 @@ def raise_isinstance_error(variable_name, possible_type, variable):</span>
<span class="w"> </span>    ValueError
<span class="w"> </span>        If the instance is not of the possible type.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    raise ValueError(</span>
<span class="gi">+        f&quot;{variable_name} has to be one of {possible_type}. &quot;</span>
<span class="gi">+        f&quot;Got {type(variable)} instead.&quot;</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/keras/_generator.py b/imblearn/keras/_generator.py</span>
<span class="gh">index f3de87c..ce49935 100644</span>
<span class="gd">--- a/imblearn/keras/_generator.py</span>
<span class="gi">+++ b/imblearn/keras/_generator.py</span>
<span class="gu">@@ -1,26 +1,67 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Implement generators for ``keras`` which will balance the data.&quot;&quot;&quot;


<span class="gi">+# This is a trick to avoid an error during tests collection with pytest. We</span>
<span class="gi">+# avoid the error when importing the package raise the error at the moment of</span>
<span class="gi">+# creating the instance.</span>
<span class="gi">+# This is a trick to avoid an error during tests collection with pytest. We</span>
<span class="gi">+# avoid the error when importing the package raise the error at the moment of</span>
<span class="gi">+# creating the instance.</span>
<span class="w"> </span>def import_keras():
<span class="w"> </span>    &quot;&quot;&quot;Try to import keras from keras and tensorflow.

<span class="w"> </span>    This is possible to import the sequence from keras or tensorflow.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def import_from_keras():</span>
<span class="gi">+        try:</span>
<span class="gi">+            import keras  # noqa</span>
<span class="gi">+</span>
<span class="gi">+            if hasattr(keras.utils, &quot;Sequence&quot;):</span>
<span class="gi">+                return (keras.utils.Sequence,), True</span>
<span class="gi">+            else:</span>
<span class="gi">+                return (keras.utils.data_utils.Sequence,), True</span>
<span class="gi">+        except ImportError:</span>
<span class="gi">+            return tuple(), False</span>
<span class="gi">+</span>
<span class="gi">+    def import_from_tensforflow():</span>
<span class="gi">+        try:</span>
<span class="gi">+            from tensorflow import keras</span>
<span class="gi">+</span>
<span class="gi">+            if hasattr(keras.utils, &quot;Sequence&quot;):</span>
<span class="gi">+                return (keras.utils.Sequence,), True</span>
<span class="gi">+            else:</span>
<span class="gi">+                return (keras.utils.data_utils.Sequence,), True</span>
<span class="gi">+        except ImportError:</span>
<span class="gi">+            return tuple(), False</span>
<span class="gi">+</span>
<span class="gi">+    ParentClassKeras, has_keras_k = import_from_keras()</span>
<span class="gi">+    ParentClassTensorflow, has_keras_tf = import_from_tensforflow()</span>
<span class="gi">+    has_keras = has_keras_k or has_keras_tf</span>
<span class="gi">+    if has_keras:</span>
<span class="gi">+        if has_keras_k:</span>
<span class="gi">+            ParentClass = ParentClassKeras</span>
<span class="gi">+        else:</span>
<span class="gi">+            ParentClass = ParentClassTensorflow</span>
<span class="gi">+    else:</span>
<span class="gi">+        ParentClass = (object,)</span>
<span class="gi">+    return ParentClass, has_keras</span>


<span class="w"> </span>ParentClass, HAS_KERAS = import_keras()
<span class="gd">-from scipy.sparse import issparse</span>
<span class="gd">-from sklearn.base import clone</span>
<span class="gd">-from sklearn.utils import _safe_indexing</span>
<span class="gd">-from sklearn.utils import check_random_state</span>
<span class="gd">-from ..tensorflow import balanced_batch_generator as tf_bbg</span>
<span class="gd">-from ..under_sampling import RandomUnderSampler</span>
<span class="gd">-from ..utils import Substitution</span>
<span class="gd">-from ..utils._docstring import _random_state_docstring</span>

<span class="gi">+from scipy.sparse import issparse  # noqa</span>
<span class="gi">+from sklearn.base import clone  # noqa</span>
<span class="gi">+from sklearn.utils import _safe_indexing  # noqa</span>
<span class="gi">+from sklearn.utils import check_random_state  # noqa</span>
<span class="gi">+</span>
<span class="gi">+from ..tensorflow import balanced_batch_generator as tf_bbg  # noqa</span>
<span class="gi">+from ..under_sampling import RandomUnderSampler  # noqa</span>
<span class="gi">+from ..utils import Substitution  # noqa</span>
<span class="gi">+from ..utils._docstring import _random_state_docstring  # noqa</span>

<span class="gd">-class BalancedBatchGenerator(*ParentClass):</span>
<span class="gi">+</span>
<span class="gi">+class BalancedBatchGenerator(*ParentClass):  # type: ignore</span>
<span class="w"> </span>    &quot;&quot;&quot;Create balanced batches when training a keras model.

<span class="w"> </span>    Create a keras ``Sequence`` which is given to ``fit``. The
<span class="gu">@@ -96,10 +137,21 @@ class BalancedBatchGenerator(*ParentClass):</span>
<span class="w"> </span>    ...     X, y, sampler=NearMiss(), batch_size=10, random_state=42)
<span class="w"> </span>    &gt;&gt;&gt; callback_history = model.fit(training_generator, epochs=10, verbose=0)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+    # flag for keras sequence duck-typing</span>
<span class="w"> </span>    use_sequence_api = True

<span class="gd">-    def __init__(self, X, y, *, sample_weight=None, sampler=None,</span>
<span class="gd">-        batch_size=32, keep_sparse=False, random_state=None):</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sample_weight=None,</span>
<span class="gi">+        sampler=None,</span>
<span class="gi">+        batch_size=32,</span>
<span class="gi">+        keep_sparse=False,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        if not HAS_KERAS:
<span class="w"> </span>            raise ImportError(&quot;&#39;No module named &#39;keras&#39;&quot;)
<span class="w"> </span>        self.X = X
<span class="gu">@@ -111,20 +163,39 @@ class BalancedBatchGenerator(*ParentClass):</span>
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self._sample()

<span class="gi">+    def _sample(self):</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        if self.sampler is None:</span>
<span class="gi">+            self.sampler_ = RandomUnderSampler(random_state=random_state)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.sampler_ = clone(self.sampler)</span>
<span class="gi">+        self.sampler_.fit_resample(self.X, self.y)</span>
<span class="gi">+        if not hasattr(self.sampler_, &quot;sample_indices_&quot;):</span>
<span class="gi">+            raise ValueError(&quot;&#39;sampler&#39; needs to have an attribute &#39;sample_indices_&#39;.&quot;)</span>
<span class="gi">+        self.indices_ = self.sampler_.sample_indices_</span>
<span class="gi">+        # shuffle the indices since the sampler are packing them by class</span>
<span class="gi">+        random_state.shuffle(self.indices_)</span>
<span class="gi">+</span>
<span class="w"> </span>    def __len__(self):
<span class="w"> </span>        return int(self.indices_.size // self.batch_size)

<span class="w"> </span>    def __getitem__(self, index):
<span class="gd">-        X_resampled = _safe_indexing(self.X, self.indices_[index * self.</span>
<span class="gd">-            batch_size:(index + 1) * self.batch_size])</span>
<span class="gd">-        y_resampled = _safe_indexing(self.y, self.indices_[index * self.</span>
<span class="gd">-            batch_size:(index + 1) * self.batch_size])</span>
<span class="gi">+        X_resampled = _safe_indexing(</span>
<span class="gi">+            self.X,</span>
<span class="gi">+            self.indices_[index * self.batch_size : (index + 1) * self.batch_size],</span>
<span class="gi">+        )</span>
<span class="gi">+        y_resampled = _safe_indexing(</span>
<span class="gi">+            self.y,</span>
<span class="gi">+            self.indices_[index * self.batch_size : (index + 1) * self.batch_size],</span>
<span class="gi">+        )</span>
<span class="w"> </span>        if issparse(X_resampled) and not self.keep_sparse:
<span class="w"> </span>            X_resampled = X_resampled.toarray()
<span class="w"> </span>        if self.sample_weight is not None:
<span class="gd">-            sample_weight_resampled = _safe_indexing(self.sample_weight,</span>
<span class="gd">-                self.indices_[index * self.batch_size:(index + 1) * self.</span>
<span class="gd">-                batch_size])</span>
<span class="gi">+            sample_weight_resampled = _safe_indexing(</span>
<span class="gi">+                self.sample_weight,</span>
<span class="gi">+                self.indices_[index * self.batch_size : (index + 1) * self.batch_size],</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="w"> </span>        if self.sample_weight is None:
<span class="w"> </span>            return X_resampled, y_resampled
<span class="w"> </span>        else:
<span class="gu">@@ -132,8 +203,16 @@ class BalancedBatchGenerator(*ParentClass):</span>


<span class="w"> </span>@Substitution(random_state=_random_state_docstring)
<span class="gd">-def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="gd">-    batch_size=32, keep_sparse=False, random_state=None):</span>
<span class="gi">+def balanced_batch_generator(</span>
<span class="gi">+    X,</span>
<span class="gi">+    y,</span>
<span class="gi">+    *,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+    sampler=None,</span>
<span class="gi">+    batch_size=32,</span>
<span class="gi">+    keep_sparse=False,</span>
<span class="gi">+    random_state=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Create a balanced batch generator to train keras model.

<span class="w"> </span>    Returns a generator --- as well as the number of step per epoch --- which
<span class="gu">@@ -204,4 +283,13 @@ def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="w"> </span>    ...                              steps_per_epoch=steps_per_epoch,
<span class="w"> </span>    ...                              epochs=10, verbose=0)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    return tf_bbg(</span>
<span class="gi">+        X=X,</span>
<span class="gi">+        y=y,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        batch_size=batch_size,</span>
<span class="gi">+        keep_sparse=keep_sparse,</span>
<span class="gi">+        random_state=random_state,</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/keras/tests/test_generator.py b/imblearn/keras/tests/test_generator.py</span>
<span class="gh">index 59ccc2c..a073d84 100644</span>
<span class="gd">--- a/imblearn/keras/tests/test_generator.py</span>
<span class="gi">+++ b/imblearn/keras/tests/test_generator.py</span>
<span class="gu">@@ -4,11 +4,144 @@ from scipy import sparse</span>
<span class="w"> </span>from sklearn.cluster import KMeans
<span class="w"> </span>from sklearn.datasets import load_iris
<span class="w"> </span>from sklearn.preprocessing import LabelBinarizer
<span class="gd">-keras = pytest.importorskip(&#39;keras&#39;)</span>
<span class="gd">-from keras.layers import Dense</span>
<span class="gd">-from keras.models import Sequential</span>
<span class="gd">-from imblearn.datasets import make_imbalance</span>
<span class="gd">-from imblearn.keras import BalancedBatchGenerator, balanced_batch_generator</span>
<span class="gd">-from imblearn.over_sampling import RandomOverSampler</span>
<span class="gd">-from imblearn.under_sampling import ClusterCentroids, NearMiss</span>
<span class="gi">+</span>
<span class="gi">+keras = pytest.importorskip(&quot;keras&quot;)</span>
<span class="gi">+from keras.layers import Dense  # noqa: E402</span>
<span class="gi">+from keras.models import Sequential  # noqa: E402</span>
<span class="gi">+</span>
<span class="gi">+from imblearn.datasets import make_imbalance  # noqa: E402</span>
<span class="gi">+from imblearn.keras import (</span>
<span class="gi">+    BalancedBatchGenerator,  # noqa: E402</span>
<span class="gi">+    balanced_batch_generator,  # noqa: E402</span>
<span class="gi">+)</span>
<span class="gi">+from imblearn.over_sampling import RandomOverSampler  # noqa: E402</span>
<span class="gi">+from imblearn.under_sampling import (</span>
<span class="gi">+    ClusterCentroids,  # noqa: E402</span>
<span class="gi">+    NearMiss,  # noqa: E402</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>3
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    iris = load_iris()</span>
<span class="gi">+    X, y = make_imbalance(</span>
<span class="gi">+        iris.data, iris.target, sampling_strategy={0: 30, 1: 50, 2: 40}</span>
<span class="gi">+    )</span>
<span class="gi">+    y = LabelBinarizer().fit_transform(y)</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _build_keras_model(n_classes, n_features):</span>
<span class="gi">+    model = Sequential()</span>
<span class="gi">+    model.add(Dense(n_classes, input_dim=n_features, activation=&quot;softmax&quot;))</span>
<span class="gi">+    model.compile(</span>
<span class="gi">+        optimizer=&quot;sgd&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]</span>
<span class="gi">+    )</span>
<span class="gi">+    return model</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_batch_generator_class_no_return_indices(data):</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;needs to have an attribute&quot;):</span>
<span class="gi">+        BalancedBatchGenerator(</span>
<span class="gi">+            *data, sampler=ClusterCentroids(estimator=KMeans(n_init=1)), batch_size=10</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:`wait_time` is not used&quot;)  # keras 2.2.4</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampler, sample_weight&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (None, None),</span>
<span class="gi">+        (RandomOverSampler(), None),</span>
<span class="gi">+        (NearMiss(), None),</span>
<span class="gi">+        (None, np.random.uniform(size=120)),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_balanced_batch_generator_class(data, sampler, sample_weight):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    model = _build_keras_model(y.shape[1], X.shape[1])</span>
<span class="gi">+    training_generator = BalancedBatchGenerator(</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        batch_size=10,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+    model.fit(training_generator, epochs=10)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;keep_sparse&quot;, [True, False])</span>
<span class="gi">+def test_balanced_batch_generator_class_sparse(data, keep_sparse):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    training_generator = BalancedBatchGenerator(</span>
<span class="gi">+        sparse.csr_matrix(X),</span>
<span class="gi">+        y,</span>
<span class="gi">+        batch_size=10,</span>
<span class="gi">+        keep_sparse=keep_sparse,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+    for idx in range(len(training_generator)):</span>
<span class="gi">+        X_batch, _ = training_generator.__getitem__(idx)</span>
<span class="gi">+        if keep_sparse:</span>
<span class="gi">+            assert sparse.issparse(X_batch)</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert not sparse.issparse(X_batch)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_balanced_batch_generator_function_no_return_indices(data):</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;needs to have an attribute&quot;):</span>
<span class="gi">+        balanced_batch_generator(</span>
<span class="gi">+            *data,</span>
<span class="gi">+            sampler=ClusterCentroids(estimator=KMeans(n_init=10)),</span>
<span class="gi">+            batch_size=10,</span>
<span class="gi">+            random_state=42,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:`wait_time` is not used&quot;)  # keras 2.2.4</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampler, sample_weight&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (None, None),</span>
<span class="gi">+        (RandomOverSampler(), None),</span>
<span class="gi">+        (NearMiss(), None),</span>
<span class="gi">+        (None, np.random.uniform(size=120)),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_balanced_batch_generator_function(data, sampler, sample_weight):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    model = _build_keras_model(y.shape[1], X.shape[1])</span>
<span class="gi">+    training_generator, steps_per_epoch = balanced_batch_generator(</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        batch_size=10,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+    model.fit(</span>
<span class="gi">+        training_generator,</span>
<span class="gi">+        steps_per_epoch=steps_per_epoch,</span>
<span class="gi">+        epochs=10,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;keep_sparse&quot;, [True, False])</span>
<span class="gi">+def test_balanced_batch_generator_function_sparse(data, keep_sparse):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    training_generator, steps_per_epoch = balanced_batch_generator(</span>
<span class="gi">+        sparse.csr_matrix(X),</span>
<span class="gi">+        y,</span>
<span class="gi">+        keep_sparse=keep_sparse,</span>
<span class="gi">+        batch_size=10,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+    for _ in range(steps_per_epoch):</span>
<span class="gi">+        X_batch, _ = next(training_generator)</span>
<span class="gi">+        if keep_sparse:</span>
<span class="gi">+            assert sparse.issparse(X_batch)</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert not sparse.issparse(X_batch)</span>
<span class="gh">diff --git a/imblearn/metrics/_classification.py b/imblearn/metrics/_classification.py</span>
<span class="gh">index 6723b08..489066d 100644</span>
<span class="gd">--- a/imblearn/metrics/_classification.py</span>
<span class="gi">+++ b/imblearn/metrics/_classification.py</span>
<span class="gu">@@ -1,3 +1,4 @@</span>
<span class="gi">+# coding: utf-8</span>
<span class="w"> </span>&quot;&quot;&quot;Metrics to assess performance on a classification task given class
<span class="w"> </span>predictions. The available metrics are complementary from the metrics available
<span class="w"> </span>in scikit-learn.
<span class="gu">@@ -8,10 +9,16 @@ the better</span>
<span class="w"> </span>Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:
<span class="w"> </span>the lower the better
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Dariusz Brzezinski</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import functools
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="w"> </span>from inspect import signature
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import scipy as sp
<span class="w"> </span>from sklearn.metrics import mean_absolute_error, precision_recall_fscore_support
<span class="gu">@@ -19,17 +26,35 @@ from sklearn.metrics._classification import _check_targets, _prf_divide</span>
<span class="w"> </span>from sklearn.preprocessing import LabelEncoder
<span class="w"> </span>from sklearn.utils.multiclass import unique_labels
<span class="w"> </span>from sklearn.utils.validation import check_consistent_length, column_or_1d
<span class="gi">+</span>
<span class="w"> </span>from ..utils._param_validation import Interval, StrOptions, validate_params


<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;labels&#39;: [&#39;array-like&#39;, None], &#39;pos_label&#39;: [str, numbers.Integral,</span>
<span class="gd">-    None], &#39;average&#39;: [None, StrOptions({&#39;binary&#39;, &#39;micro&#39;, &#39;macro&#39;,</span>
<span class="gd">-    &#39;weighted&#39;, &#39;samples&#39;})], &#39;warn_for&#39;: [&#39;array-like&#39;], &#39;sample_weight&#39;:</span>
<span class="gd">-    [&#39;array-like&#39;, None]}, prefer_skip_nested_validation=True)</span>
<span class="gd">-def sensitivity_specificity_support(y_true, y_pred, *, labels=None,</span>
<span class="gd">-    pos_label=1, average=None, warn_for=(&#39;sensitivity&#39;, &#39;specificity&#39;),</span>
<span class="gd">-    sample_weight=None):</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;labels&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;pos_label&quot;: [str, numbers.Integral, None],</span>
<span class="gi">+        &quot;average&quot;: [</span>
<span class="gi">+            None,</span>
<span class="gi">+            StrOptions({&quot;binary&quot;, &quot;micro&quot;, &quot;macro&quot;, &quot;weighted&quot;, &quot;samples&quot;}),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;warn_for&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def sensitivity_specificity_support(</span>
<span class="gi">+    y_true,</span>
<span class="gi">+    y_pred,</span>
<span class="gi">+    *,</span>
<span class="gi">+    labels=None,</span>
<span class="gi">+    pos_label=1,</span>
<span class="gi">+    average=None,</span>
<span class="gi">+    warn_for=(&quot;sensitivity&quot;, &quot;specificity&quot;),</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Compute sensitivity, specificity, and support for each class.

<span class="w"> </span>    The sensitivity is the ratio ``tp / (tp + fn)`` where ``tp`` is the number
<span class="gu">@@ -106,13 +131,16 @@ def sensitivity_specificity_support(y_true, y_pred, *, labels=None,</span>

<span class="w"> </span>    Returns
<span class="w"> </span>    -------
<span class="gd">-    sensitivity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+    sensitivity : float (if `average is None`) or ndarray of \</span>
<span class="gi">+            shape (n_unique_labels,)</span>
<span class="w"> </span>        The sensitivity metric.

<span class="gd">-    specificity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+    specificity : float (if `average is None`) or ndarray of \</span>
<span class="gi">+            shape (n_unique_labels,)</span>
<span class="w"> </span>        The specificity metric.

<span class="gd">-    support : int (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+    support : int (if `average is None`) or ndarray of \</span>
<span class="gi">+            shape (n_unique_labels,)</span>
<span class="w"> </span>        The number of occurrences of each label in ``y_true``.

<span class="w"> </span>    References
<span class="gu">@@ -133,16 +161,163 @@ def sensitivity_specificity_support(y_true, y_pred, *, labels=None,</span>
<span class="w"> </span>    &gt;&gt;&gt; sensitivity_specificity_support(y_true, y_pred, average=&#39;weighted&#39;)
<span class="w"> </span>    (0.33..., 0.66..., None)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;labels&#39;: [&#39;array-like&#39;, None], &#39;pos_label&#39;: [str, numbers.Integral,</span>
<span class="gd">-    None], &#39;average&#39;: [None, StrOptions({&#39;binary&#39;, &#39;micro&#39;, &#39;macro&#39;,</span>
<span class="gd">-    &#39;weighted&#39;, &#39;samples&#39;})], &#39;sample_weight&#39;: [&#39;array-like&#39;, None]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gd">-def sensitivity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="gd">-    &#39;binary&#39;, sample_weight=None):</span>
<span class="gi">+    average_options = (None, &quot;micro&quot;, &quot;macro&quot;, &quot;weighted&quot;, &quot;samples&quot;)</span>
<span class="gi">+    if average not in average_options and average != &quot;binary&quot;:</span>
<span class="gi">+        raise ValueError(&quot;average has to be one of &quot; + str(average_options))</span>
<span class="gi">+</span>
<span class="gi">+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+    present_labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    if average == &quot;binary&quot;:</span>
<span class="gi">+        if y_type == &quot;binary&quot;:</span>
<span class="gi">+            if pos_label not in present_labels:</span>
<span class="gi">+                if len(present_labels) &lt; 2:</span>
<span class="gi">+                    # Only negative labels</span>
<span class="gi">+                    return (0.0, 0.0, 0)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        &quot;pos_label=%r is not a valid label: %r&quot;</span>
<span class="gi">+                        % (pos_label, present_labels)</span>
<span class="gi">+                    )</span>
<span class="gi">+            labels = [pos_label]</span>
<span class="gi">+        else:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Target is %s but average=&#39;binary&#39;. Please &quot;</span>
<span class="gi">+                &quot;choose another average setting.&quot; % y_type</span>
<span class="gi">+            )</span>
<span class="gi">+    elif pos_label not in (None, 1):</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;Note that pos_label (set to %r) is ignored when &quot;</span>
<span class="gi">+            &quot;average != &#39;binary&#39; (got %r). You may use &quot;</span>
<span class="gi">+            &quot;labels=[pos_label] to specify a single positive class.&quot;</span>
<span class="gi">+            % (pos_label, average),</span>
<span class="gi">+            UserWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if labels is None:</span>
<span class="gi">+        labels = present_labels</span>
<span class="gi">+        n_labels = None</span>
<span class="gi">+    else:</span>
<span class="gi">+        n_labels = len(labels)</span>
<span class="gi">+        labels = np.hstack(</span>
<span class="gi">+            [labels, np.setdiff1d(present_labels, labels, assume_unique=True)]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # Calculate tp_sum, pred_sum, true_sum ###</span>
<span class="gi">+</span>
<span class="gi">+    if y_type.startswith(&quot;multilabel&quot;):</span>
<span class="gi">+        raise ValueError(&quot;imblearn does not support multilabel&quot;)</span>
<span class="gi">+    elif average == &quot;samples&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;Sample-based precision, recall, fscore is &quot;</span>
<span class="gi">+            &quot;not meaningful outside multilabel &quot;</span>
<span class="gi">+            &quot;classification. See the accuracy_score instead.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        le = LabelEncoder()</span>
<span class="gi">+        le.fit(labels)</span>
<span class="gi">+        y_true = le.transform(y_true)</span>
<span class="gi">+        y_pred = le.transform(y_pred)</span>
<span class="gi">+        sorted_labels = le.classes_</span>
<span class="gi">+</span>
<span class="gi">+        # labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
<span class="gi">+        tp = y_true == y_pred</span>
<span class="gi">+        tp_bins = y_true[tp]</span>
<span class="gi">+        if sample_weight is not None:</span>
<span class="gi">+            tp_bins_weights = np.asarray(sample_weight)[tp]</span>
<span class="gi">+        else:</span>
<span class="gi">+            tp_bins_weights = None</span>
<span class="gi">+</span>
<span class="gi">+        if len(tp_bins):</span>
<span class="gi">+            tp_sum = np.bincount(</span>
<span class="gi">+                tp_bins, weights=tp_bins_weights, minlength=len(labels)</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Pathological case</span>
<span class="gi">+            true_sum = pred_sum = tp_sum = np.zeros(len(labels))</span>
<span class="gi">+        if len(y_pred):</span>
<span class="gi">+            pred_sum = np.bincount(y_pred, weights=sample_weight, minlength=len(labels))</span>
<span class="gi">+        if len(y_true):</span>
<span class="gi">+            true_sum = np.bincount(y_true, weights=sample_weight, minlength=len(labels))</span>
<span class="gi">+</span>
<span class="gi">+        # Compute the true negative</span>
<span class="gi">+        tn_sum = y_true.size - (pred_sum + true_sum - tp_sum)</span>
<span class="gi">+</span>
<span class="gi">+        # Retain only selected labels</span>
<span class="gi">+        indices = np.searchsorted(sorted_labels, labels[:n_labels])</span>
<span class="gi">+        tp_sum = tp_sum[indices]</span>
<span class="gi">+        true_sum = true_sum[indices]</span>
<span class="gi">+        pred_sum = pred_sum[indices]</span>
<span class="gi">+        tn_sum = tn_sum[indices]</span>
<span class="gi">+</span>
<span class="gi">+    if average == &quot;micro&quot;:</span>
<span class="gi">+        tp_sum = np.array([tp_sum.sum()])</span>
<span class="gi">+        pred_sum = np.array([pred_sum.sum()])</span>
<span class="gi">+        true_sum = np.array([true_sum.sum()])</span>
<span class="gi">+        tn_sum = np.array([tn_sum.sum()])</span>
<span class="gi">+</span>
<span class="gi">+    # Finally, we have all our sufficient statistics. Divide! #</span>
<span class="gi">+</span>
<span class="gi">+    with np.errstate(divide=&quot;ignore&quot;, invalid=&quot;ignore&quot;):</span>
<span class="gi">+        # Divide, and on zero-division, set scores to 0 and warn:</span>
<span class="gi">+</span>
<span class="gi">+        # Oddly, we may get an &quot;invalid&quot; rather than a &quot;divide&quot; error</span>
<span class="gi">+        # here.</span>
<span class="gi">+        specificity = _prf_divide(</span>
<span class="gi">+            tn_sum,</span>
<span class="gi">+            tn_sum + pred_sum - tp_sum,</span>
<span class="gi">+            &quot;specificity&quot;,</span>
<span class="gi">+            &quot;predicted&quot;,</span>
<span class="gi">+            average,</span>
<span class="gi">+            warn_for,</span>
<span class="gi">+        )</span>
<span class="gi">+        sensitivity = _prf_divide(</span>
<span class="gi">+            tp_sum, true_sum, &quot;sensitivity&quot;, &quot;true&quot;, average, warn_for</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    # Average the results</span>
<span class="gi">+</span>
<span class="gi">+    if average == &quot;weighted&quot;:</span>
<span class="gi">+        weights = true_sum</span>
<span class="gi">+        if weights.sum() == 0:</span>
<span class="gi">+            return 0, 0, None</span>
<span class="gi">+    elif average == &quot;samples&quot;:</span>
<span class="gi">+        weights = sample_weight</span>
<span class="gi">+    else:</span>
<span class="gi">+        weights = None</span>
<span class="gi">+</span>
<span class="gi">+    if average is not None:</span>
<span class="gi">+        assert average != &quot;binary&quot; or len(specificity) == 1</span>
<span class="gi">+        specificity = np.average(specificity, weights=weights)</span>
<span class="gi">+        sensitivity = np.average(sensitivity, weights=weights)</span>
<span class="gi">+        true_sum = None  # return no support</span>
<span class="gi">+</span>
<span class="gi">+    return sensitivity, specificity, true_sum</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;labels&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;pos_label&quot;: [str, numbers.Integral, None],</span>
<span class="gi">+        &quot;average&quot;: [</span>
<span class="gi">+            None,</span>
<span class="gi">+            StrOptions({&quot;binary&quot;, &quot;micro&quot;, &quot;macro&quot;, &quot;weighted&quot;, &quot;samples&quot;}),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def sensitivity_score(</span>
<span class="gi">+    y_true,</span>
<span class="gi">+    y_pred,</span>
<span class="gi">+    *,</span>
<span class="gi">+    labels=None,</span>
<span class="gi">+    pos_label=1,</span>
<span class="gi">+    average=&quot;binary&quot;,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Compute the sensitivity.

<span class="w"> </span>    The sensitivity is the ratio ``tp / (tp + fn)`` where ``tp`` is the number
<span class="gu">@@ -204,7 +379,8 @@ def sensitivity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>

<span class="w"> </span>    Returns
<span class="w"> </span>    -------
<span class="gd">-    specificity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+    specificity : float (if `average is None`) or ndarray of \</span>
<span class="gi">+            shape (n_unique_labels,)</span>
<span class="w"> </span>        The specifcity metric.

<span class="w"> </span>    Examples
<span class="gu">@@ -222,16 +398,42 @@ def sensitivity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="w"> </span>    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([1., 0., 0.])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;labels&#39;: [&#39;array-like&#39;, None], &#39;pos_label&#39;: [str, numbers.Integral,</span>
<span class="gd">-    None], &#39;average&#39;: [None, StrOptions({&#39;binary&#39;, &#39;micro&#39;, &#39;macro&#39;,</span>
<span class="gd">-    &#39;weighted&#39;, &#39;samples&#39;})], &#39;sample_weight&#39;: [&#39;array-like&#39;, None]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gd">-def specificity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="gd">-    &#39;binary&#39;, sample_weight=None):</span>
<span class="gi">+    s, _, _ = sensitivity_specificity_support(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        pos_label=pos_label,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+        warn_for=(&quot;sensitivity&quot;,),</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    return s</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;labels&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;pos_label&quot;: [str, numbers.Integral, None],</span>
<span class="gi">+        &quot;average&quot;: [</span>
<span class="gi">+            None,</span>
<span class="gi">+            StrOptions({&quot;binary&quot;, &quot;micro&quot;, &quot;macro&quot;, &quot;weighted&quot;, &quot;samples&quot;}),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def specificity_score(</span>
<span class="gi">+    y_true,</span>
<span class="gi">+    y_pred,</span>
<span class="gi">+    *,</span>
<span class="gi">+    labels=None,</span>
<span class="gi">+    pos_label=1,</span>
<span class="gi">+    average=&quot;binary&quot;,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Compute the specificity.

<span class="w"> </span>    The specificity is the ratio ``tn / (tn + fp)`` where ``tn`` is the number
<span class="gu">@@ -293,7 +495,8 @@ def specificity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>

<span class="w"> </span>    Returns
<span class="w"> </span>    -------
<span class="gd">-    specificity : float (if `average is None`) or ndarray of             shape (n_unique_labels,)</span>
<span class="gi">+    specificity : float (if `average is None`) or ndarray of \</span>
<span class="gi">+            shape (n_unique_labels,)</span>
<span class="w"> </span>        The specificity metric.

<span class="w"> </span>    Examples
<span class="gu">@@ -311,17 +514,46 @@ def specificity_score(y_true, y_pred, *, labels=None, pos_label=1, average=</span>
<span class="w"> </span>    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([0.75, 0.5 , 0.75])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;labels&#39;: [&#39;array-like&#39;, None], &#39;pos_label&#39;: [str, numbers.Integral,</span>
<span class="gd">-    None], &#39;average&#39;: [None, StrOptions({&#39;binary&#39;, &#39;micro&#39;, &#39;macro&#39;,</span>
<span class="gd">-    &#39;weighted&#39;, &#39;samples&#39;, &#39;multiclass&#39;})], &#39;sample_weight&#39;: [&#39;array-like&#39;,</span>
<span class="gd">-    None], &#39;correction&#39;: [Interval(numbers.Real, 0, None, closed=&#39;left&#39;)]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gd">-def geometric_mean_score(y_true, y_pred, *, labels=None, pos_label=1,</span>
<span class="gd">-    average=&#39;multiclass&#39;, sample_weight=None, correction=0.0):</span>
<span class="gi">+    _, s, _ = sensitivity_specificity_support(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        pos_label=pos_label,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+        warn_for=(&quot;specificity&quot;,),</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    return s</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;labels&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;pos_label&quot;: [str, numbers.Integral, None],</span>
<span class="gi">+        &quot;average&quot;: [</span>
<span class="gi">+            None,</span>
<span class="gi">+            StrOptions(</span>
<span class="gi">+                {&quot;binary&quot;, &quot;micro&quot;, &quot;macro&quot;, &quot;weighted&quot;, &quot;samples&quot;, &quot;multiclass&quot;}</span>
<span class="gi">+            ),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;correction&quot;: [Interval(numbers.Real, 0, None, closed=&quot;left&quot;)],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def geometric_mean_score(</span>
<span class="gi">+    y_true,</span>
<span class="gi">+    y_pred,</span>
<span class="gi">+    *,</span>
<span class="gi">+    labels=None,</span>
<span class="gi">+    pos_label=1,</span>
<span class="gi">+    average=&quot;multiclass&quot;,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+    correction=0.0,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Compute the geometric mean.

<span class="w"> </span>    The geometric mean (G-mean) is the root of the product of class-wise
<span class="gu">@@ -435,11 +667,76 @@ def geometric_mean_score(y_true, y_pred, *, labels=None, pos_label=1,</span>
<span class="w"> </span>    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=None)
<span class="w"> </span>    array([0.866...,  0.       ,  0.       ])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;alpha&#39;: [numbers.Real], &#39;squared&#39;: [&#39;boolean&#39;]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gi">+    if average is None or average != &quot;multiclass&quot;:</span>
<span class="gi">+        sen, spe, _ = sensitivity_specificity_support(</span>
<span class="gi">+            y_true,</span>
<span class="gi">+            y_pred,</span>
<span class="gi">+            labels=labels,</span>
<span class="gi">+            pos_label=pos_label,</span>
<span class="gi">+            average=average,</span>
<span class="gi">+            warn_for=(&quot;specificity&quot;, &quot;specificity&quot;),</span>
<span class="gi">+            sample_weight=sample_weight,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        return np.sqrt(sen * spe)</span>
<span class="gi">+    else:</span>
<span class="gi">+        present_labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+        if labels is None:</span>
<span class="gi">+            labels = present_labels</span>
<span class="gi">+            n_labels = None</span>
<span class="gi">+        else:</span>
<span class="gi">+            n_labels = len(labels)</span>
<span class="gi">+            labels = np.hstack(</span>
<span class="gi">+                [labels, np.setdiff1d(present_labels, labels, assume_unique=True)]</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        le = LabelEncoder()</span>
<span class="gi">+        le.fit(labels)</span>
<span class="gi">+        y_true = le.transform(y_true)</span>
<span class="gi">+        y_pred = le.transform(y_pred)</span>
<span class="gi">+        sorted_labels = le.classes_</span>
<span class="gi">+</span>
<span class="gi">+        # labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
<span class="gi">+        tp = y_true == y_pred</span>
<span class="gi">+        tp_bins = y_true[tp]</span>
<span class="gi">+</span>
<span class="gi">+        if sample_weight is not None:</span>
<span class="gi">+            tp_bins_weights = np.asarray(sample_weight)[tp]</span>
<span class="gi">+        else:</span>
<span class="gi">+            tp_bins_weights = None</span>
<span class="gi">+</span>
<span class="gi">+        if len(tp_bins):</span>
<span class="gi">+            tp_sum = np.bincount(</span>
<span class="gi">+                tp_bins, weights=tp_bins_weights, minlength=len(labels)</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            # Pathological case</span>
<span class="gi">+            true_sum = tp_sum = np.zeros(len(labels))</span>
<span class="gi">+        if len(y_true):</span>
<span class="gi">+            true_sum = np.bincount(y_true, weights=sample_weight, minlength=len(labels))</span>
<span class="gi">+</span>
<span class="gi">+        # Retain only selected labels</span>
<span class="gi">+        indices = np.searchsorted(sorted_labels, labels[:n_labels])</span>
<span class="gi">+        tp_sum = tp_sum[indices]</span>
<span class="gi">+        true_sum = true_sum[indices]</span>
<span class="gi">+</span>
<span class="gi">+        with np.errstate(divide=&quot;ignore&quot;, invalid=&quot;ignore&quot;):</span>
<span class="gi">+            recall = _prf_divide(tp_sum, true_sum, &quot;recall&quot;, &quot;true&quot;, None, &quot;recall&quot;)</span>
<span class="gi">+        recall[recall == 0] = correction</span>
<span class="gi">+</span>
<span class="gi">+        with np.errstate(divide=&quot;ignore&quot;, invalid=&quot;ignore&quot;):</span>
<span class="gi">+            gmean = sp.stats.gmean(recall)</span>
<span class="gi">+        # old version of scipy return MaskedConstant instead of 0.0</span>
<span class="gi">+        if isinstance(gmean, np.ma.core.MaskedConstant):</span>
<span class="gi">+            return 0.0</span>
<span class="gi">+        return gmean</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {&quot;alpha&quot;: [numbers.Real], &quot;squared&quot;: [&quot;boolean&quot;]},</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="w"> </span>def make_index_balanced_accuracy(*, alpha=0.1, squared=True):
<span class="w"> </span>    &quot;&quot;&quot;Balance any scoring function using the index balanced accuracy.

<span class="gu">@@ -489,19 +786,91 @@ def make_index_balanced_accuracy(*, alpha=0.1, squared=True):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(gmean(y_true, y_pred, average=None))
<span class="w"> </span>    [0.44...  0.44...]
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;labels&#39;: [&#39;array-like&#39;, None], &#39;target_names&#39;: [&#39;array-like&#39;, None],</span>
<span class="gd">-    &#39;sample_weight&#39;: [&#39;array-like&#39;, None], &#39;digits&#39;: [Interval(numbers.</span>
<span class="gd">-    Integral, 0, None, closed=&#39;left&#39;)], &#39;alpha&#39;: [numbers.Real],</span>
<span class="gd">-    &#39;output_dict&#39;: [&#39;boolean&#39;], &#39;zero_division&#39;: [StrOptions({&#39;warn&#39;}),</span>
<span class="gd">-    Interval(numbers.Integral, 0, 1, closed=&#39;both&#39;)]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gd">-def classification_report_imbalanced(y_true, y_pred, *, labels=None,</span>
<span class="gd">-    target_names=None, sample_weight=None, digits=2, alpha=0.1, output_dict</span>
<span class="gd">-    =False, zero_division=&#39;warn&#39;):</span>
<span class="gi">+</span>
<span class="gi">+    def decorate(scoring_func):</span>
<span class="gi">+        @functools.wraps(scoring_func)</span>
<span class="gi">+        def compute_score(*args, **kwargs):</span>
<span class="gi">+            signature_scoring_func = signature(scoring_func)</span>
<span class="gi">+            params_scoring_func = set(signature_scoring_func.parameters.keys())</span>
<span class="gi">+</span>
<span class="gi">+            # check that the scoring function does not need a score</span>
<span class="gi">+            # and only a prediction</span>
<span class="gi">+            prohibitied_y_pred = set([&quot;y_score&quot;, &quot;y_prob&quot;, &quot;y2&quot;])</span>
<span class="gi">+            if prohibitied_y_pred.intersection(params_scoring_func):</span>
<span class="gi">+                raise AttributeError(</span>
<span class="gi">+                    f&quot;The function {scoring_func.__name__} has an unsupported&quot;</span>
<span class="gi">+                    f&quot; attribute. Metric with`y_pred` are the&quot;</span>
<span class="gi">+                    f&quot; only supported metrics is the only&quot;</span>
<span class="gi">+                    f&quot; supported.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            args_scoring_func = signature_scoring_func.bind(*args, **kwargs)</span>
<span class="gi">+            args_scoring_func.apply_defaults()</span>
<span class="gi">+            _score = scoring_func(*args_scoring_func.args, **args_scoring_func.kwargs)</span>
<span class="gi">+            if squared:</span>
<span class="gi">+                _score = np.power(_score, 2)</span>
<span class="gi">+</span>
<span class="gi">+            signature_sens_spec = signature(sensitivity_specificity_support)</span>
<span class="gi">+            params_sens_spec = set(signature_sens_spec.parameters.keys())</span>
<span class="gi">+            common_params = params_sens_spec.intersection(</span>
<span class="gi">+                set(args_scoring_func.arguments.keys())</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            args_sens_spec = {k: args_scoring_func.arguments[k] for k in common_params}</span>
<span class="gi">+</span>
<span class="gi">+            if scoring_func.__name__ == &quot;geometric_mean_score&quot;:</span>
<span class="gi">+                if &quot;average&quot; in args_sens_spec:</span>
<span class="gi">+                    if args_sens_spec[&quot;average&quot;] == &quot;multiclass&quot;:</span>
<span class="gi">+                        args_sens_spec[&quot;average&quot;] = &quot;macro&quot;</span>
<span class="gi">+            elif (</span>
<span class="gi">+                scoring_func.__name__ == &quot;accuracy_score&quot;</span>
<span class="gi">+                or scoring_func.__name__ == &quot;jaccard_score&quot;</span>
<span class="gi">+            ):</span>
<span class="gi">+                # We do not support multilabel so the only average supported</span>
<span class="gi">+                # is binary</span>
<span class="gi">+                args_sens_spec[&quot;average&quot;] = &quot;binary&quot;</span>
<span class="gi">+</span>
<span class="gi">+            sensitivity, specificity, _ = sensitivity_specificity_support(</span>
<span class="gi">+                **args_sens_spec</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            dominance = sensitivity - specificity</span>
<span class="gi">+            return (1.0 + alpha * dominance) * _score</span>
<span class="gi">+</span>
<span class="gi">+        return compute_score</span>
<span class="gi">+</span>
<span class="gi">+    return decorate</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;labels&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;target_names&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+        &quot;digits&quot;: [Interval(numbers.Integral, 0, None, closed=&quot;left&quot;)],</span>
<span class="gi">+        &quot;alpha&quot;: [numbers.Real],</span>
<span class="gi">+        &quot;output_dict&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;zero_division&quot;: [</span>
<span class="gi">+            StrOptions({&quot;warn&quot;}),</span>
<span class="gi">+            Interval(numbers.Integral, 0, 1, closed=&quot;both&quot;),</span>
<span class="gi">+        ],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="gi">+def classification_report_imbalanced(</span>
<span class="gi">+    y_true,</span>
<span class="gi">+    y_pred,</span>
<span class="gi">+    *,</span>
<span class="gi">+    labels=None,</span>
<span class="gi">+    target_names=None,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+    digits=2,</span>
<span class="gi">+    alpha=0.1,</span>
<span class="gi">+    output_dict=False,</span>
<span class="gi">+    zero_division=&quot;warn&quot;,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Build a classification report based on metrics used with imbalanced dataset.

<span class="w"> </span>    Specific metrics have been proposed to evaluate the classification
<span class="gu">@@ -571,21 +940,140 @@ def classification_report_imbalanced(y_true, y_pred, *, labels=None,</span>
<span class="w"> </span>    &gt;&gt;&gt; y_true = [0, 1, 2, 2, 2]
<span class="w"> </span>    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 1]
<span class="w"> </span>    &gt;&gt;&gt; target_names = [&#39;class 0&#39;, &#39;class 1&#39;, &#39;class 2&#39;]
<span class="gd">-    &gt;&gt;&gt; print(classification_report_imbalanced(y_true, y_pred,     target_names=target_names))</span>
<span class="gd">-                       pre       rec       spe        f1       geo       iba       sup</span>
<span class="gi">+    &gt;&gt;&gt; print(classification_report_imbalanced(y_true, y_pred, \</span>
<span class="gi">+    target_names=target_names))</span>
<span class="gi">+                       pre       rec       spe        f1       geo       iba\</span>
<span class="gi">+       sup</span>
<span class="w"> </span>    &lt;BLANKLINE&gt;
<span class="gd">-        class 0       0.50      1.00      0.75      0.67      0.87      0.77         1</span>
<span class="gd">-        class 1       0.00      0.00      0.75      0.00      0.00      0.00         1</span>
<span class="gd">-        class 2       1.00      0.67      1.00      0.80      0.82      0.64         3</span>
<span class="gi">+        class 0       0.50      1.00      0.75      0.67      0.87      0.77\</span>
<span class="gi">+         1</span>
<span class="gi">+        class 1       0.00      0.00      0.75      0.00      0.00      0.00\</span>
<span class="gi">+         1</span>
<span class="gi">+        class 2       1.00      0.67      1.00      0.80      0.82      0.64\</span>
<span class="gi">+         3</span>
<span class="w"> </span>    &lt;BLANKLINE&gt;
<span class="gd">-    avg / total       0.70      0.60      0.90      0.61      0.66      0.54         5</span>
<span class="gi">+    avg / total       0.70      0.60      0.90      0.61      0.66      0.54\</span>
<span class="gi">+         5</span>
<span class="w"> </span>    &lt;BLANKLINE&gt;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>

<span class="gd">-@validate_params({&#39;y_true&#39;: [&#39;array-like&#39;], &#39;y_pred&#39;: [&#39;array-like&#39;],</span>
<span class="gd">-    &#39;sample_weight&#39;: [&#39;array-like&#39;, None]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    if labels is None:</span>
<span class="gi">+        labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+    else:</span>
<span class="gi">+        labels = np.asarray(labels)</span>
<span class="gi">+</span>
<span class="gi">+    last_line_heading = &quot;avg / total&quot;</span>
<span class="gi">+</span>
<span class="gi">+    if target_names is None:</span>
<span class="gi">+        target_names = [f&quot;{label}&quot; for label in labels]</span>
<span class="gi">+    name_width = max(len(cn) for cn in target_names)</span>
<span class="gi">+    width = max(name_width, len(last_line_heading), digits)</span>
<span class="gi">+</span>
<span class="gi">+    headers = [&quot;pre&quot;, &quot;rec&quot;, &quot;spe&quot;, &quot;f1&quot;, &quot;geo&quot;, &quot;iba&quot;, &quot;sup&quot;]</span>
<span class="gi">+    fmt = &quot;%% %ds&quot; % width  # first column: class name</span>
<span class="gi">+    fmt += &quot;  &quot;</span>
<span class="gi">+    fmt += &quot; &quot;.join([&quot;% 9s&quot; for _ in headers])</span>
<span class="gi">+    fmt += &quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    headers = [&quot;&quot;] + headers</span>
<span class="gi">+    report = fmt % tuple(headers)</span>
<span class="gi">+    report += &quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # Compute the different metrics</span>
<span class="gi">+    # Precision/recall/f1</span>
<span class="gi">+    precision, recall, f1, support = precision_recall_fscore_support(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        average=None,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+        zero_division=zero_division,</span>
<span class="gi">+    )</span>
<span class="gi">+    # Specificity</span>
<span class="gi">+    specificity = specificity_score(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        average=None,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+    # Geometric mean</span>
<span class="gi">+    geo_mean = geometric_mean_score(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        average=None,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+    # Index balanced accuracy</span>
<span class="gi">+    iba_gmean = make_index_balanced_accuracy(alpha=alpha, squared=True)(</span>
<span class="gi">+        geometric_mean_score</span>
<span class="gi">+    )</span>
<span class="gi">+    iba = iba_gmean(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=labels,</span>
<span class="gi">+        average=None,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    report_dict = {}</span>
<span class="gi">+    for i, label in enumerate(labels):</span>
<span class="gi">+        report_dict_label = {}</span>
<span class="gi">+        values = [target_names[i]]</span>
<span class="gi">+        for score_name, score_value in zip(</span>
<span class="gi">+            headers[1:-1],</span>
<span class="gi">+            [</span>
<span class="gi">+                precision[i],</span>
<span class="gi">+                recall[i],</span>
<span class="gi">+                specificity[i],</span>
<span class="gi">+                f1[i],</span>
<span class="gi">+                geo_mean[i],</span>
<span class="gi">+                iba[i],</span>
<span class="gi">+            ],</span>
<span class="gi">+        ):</span>
<span class="gi">+            values += [&quot;{0:0.{1}f}&quot;.format(score_value, digits)]</span>
<span class="gi">+            report_dict_label[score_name] = score_value</span>
<span class="gi">+        values += [f&quot;{support[i]}&quot;]</span>
<span class="gi">+        report_dict_label[headers[-1]] = support[i]</span>
<span class="gi">+        report += fmt % tuple(values)</span>
<span class="gi">+</span>
<span class="gi">+        report_dict[target_names[i]] = report_dict_label</span>
<span class="gi">+</span>
<span class="gi">+    report += &quot;\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # compute averages</span>
<span class="gi">+    values = [last_line_heading]</span>
<span class="gi">+    for score_name, score_value in zip(</span>
<span class="gi">+        headers[1:-1],</span>
<span class="gi">+        [</span>
<span class="gi">+            np.average(precision, weights=support),</span>
<span class="gi">+            np.average(recall, weights=support),</span>
<span class="gi">+            np.average(specificity, weights=support),</span>
<span class="gi">+            np.average(f1, weights=support),</span>
<span class="gi">+            np.average(geo_mean, weights=support),</span>
<span class="gi">+            np.average(iba, weights=support),</span>
<span class="gi">+        ],</span>
<span class="gi">+    ):</span>
<span class="gi">+        values += [&quot;{0:0.{1}f}&quot;.format(score_value, digits)]</span>
<span class="gi">+        report_dict[f&quot;avg_{score_name}&quot;] = score_value</span>
<span class="gi">+    values += [f&quot;{np.sum(support)}&quot;]</span>
<span class="gi">+    report += fmt % tuple(values)</span>
<span class="gi">+    report_dict[&quot;total_support&quot;] = np.sum(support)</span>
<span class="gi">+</span>
<span class="gi">+    if output_dict:</span>
<span class="gi">+        return report_dict</span>
<span class="gi">+    return report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {</span>
<span class="gi">+        &quot;y_true&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;y_pred&quot;: [&quot;array-like&quot;],</span>
<span class="gi">+        &quot;sample_weight&quot;: [&quot;array-like&quot;, None],</span>
<span class="gi">+    },</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="w"> </span>def macro_averaged_mean_absolute_error(y_true, y_pred, *, sample_weight=None):
<span class="w"> </span>    &quot;&quot;&quot;Compute Macro-Averaged MAE for imbalanced ordinal classification.

<span class="gu">@@ -630,4 +1118,23 @@ def macro_averaged_mean_absolute_error(y_true, y_pred, *, sample_weight=None):</span>
<span class="w"> </span>    &gt;&gt;&gt; macro_averaged_mean_absolute_error(y_true_imbalanced, y_pred)
<span class="w"> </span>    0.16...
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    _, y_true, y_pred = _check_targets(y_true, y_pred)</span>
<span class="gi">+    if sample_weight is not None:</span>
<span class="gi">+        sample_weight = column_or_1d(sample_weight)</span>
<span class="gi">+    else:</span>
<span class="gi">+        sample_weight = np.ones(y_true.shape)</span>
<span class="gi">+    check_consistent_length(y_true, y_pred, sample_weight)</span>
<span class="gi">+    labels = unique_labels(y_true, y_pred)</span>
<span class="gi">+    mae = []</span>
<span class="gi">+    for possible_class in labels:</span>
<span class="gi">+        indices = np.flatnonzero(y_true == possible_class)</span>
<span class="gi">+</span>
<span class="gi">+        mae.append(</span>
<span class="gi">+            mean_absolute_error(</span>
<span class="gi">+                y_true[indices],</span>
<span class="gi">+                y_pred[indices],</span>
<span class="gi">+                sample_weight=sample_weight[indices],</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    return np.sum(mae) / len(mae)</span>
<span class="gh">diff --git a/imblearn/metrics/pairwise.py b/imblearn/metrics/pairwise.py</span>
<span class="gh">index d73edae..11f654f 100644</span>
<span class="gd">--- a/imblearn/metrics/pairwise.py</span>
<span class="gi">+++ b/imblearn/metrics/pairwise.py</span>
<span class="gu">@@ -1,24 +1,30 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Metrics to perform pairwise computation.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy.spatial import distance_matrix
<span class="w"> </span>from sklearn.base import BaseEstimator
<span class="w"> </span>from sklearn.utils import check_consistent_length
<span class="w"> </span>from sklearn.utils.multiclass import unique_labels
<span class="w"> </span>from sklearn.utils.validation import check_is_fitted
<span class="gi">+</span>
<span class="w"> </span>from ..base import _ParamsValidationMixin
<span class="w"> </span>from ..utils._param_validation import StrOptions


<span class="w"> </span>class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):
<span class="gd">-    &quot;&quot;&quot;Class implementing the Value Difference Metric.</span>
<span class="gi">+    r&quot;&quot;&quot;Class implementing the Value Difference Metric.</span>

<span class="w"> </span>    This metric computes the distance between samples containing only
<span class="w"> </span>    categorical features. The distance between feature values of two samples is
<span class="w"> </span>    defined as:

<span class="w"> </span>    .. math::
<span class="gd">-       \\delta(x, y) = \\sum_{c=1}^{C} |p(c|x_{f}) - p(c|y_{f})|^{k} \\ ,</span>
<span class="gi">+       \delta(x, y) = \sum_{c=1}^{C} |p(c|x_{f}) - p(c|y_{f})|^{k} \ ,</span>

<span class="w"> </span>    where :math:`x` and :math:`y` are two samples and :math:`f` a given
<span class="w"> </span>    feature, :math:`C` is the number of classes, :math:`p(c|x_{f})` is the
<span class="gu">@@ -30,7 +36,7 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>    subsequently defined as:

<span class="w"> </span>    .. math::
<span class="gd">-       \\Delta(X, Y) = \\sum_{f=1}^{F} \\delta(X_{f}, Y_{f})^{r} \\ ,</span>
<span class="gi">+       \Delta(X, Y) = \sum_{f=1}^{F} \delta(X_{f}, Y_{f})^{r} \ ,</span>

<span class="w"> </span>    where :math:`F` is the number of feature and :math:`r` an exponent usually
<span class="w"> </span>    defined equal to 1 or 2.
<span class="gu">@@ -112,10 +118,13 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>           [0.04,  0.  ,  1.44],
<span class="w"> </span>           [1.96,  1.44,  0.  ]])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {&#39;n_categories&#39;: [StrOptions({&#39;auto&#39;}),</span>
<span class="gd">-        &#39;array-like&#39;], &#39;k&#39;: [numbers.Integral], &#39;r&#39;: [numbers.Integral]}</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;n_categories&quot;: [StrOptions({&quot;auto&quot;}), &quot;array-like&quot;],</span>
<span class="gi">+        &quot;k&quot;: [numbers.Integral],</span>
<span class="gi">+        &quot;r&quot;: [numbers.Integral],</span>
<span class="gi">+    }</span>

<span class="gd">-    def __init__(self, *, n_categories=&#39;auto&#39;, k=1, r=2):</span>
<span class="gi">+    def __init__(self, *, n_categories=&quot;auto&quot;, k=1, r=2):</span>
<span class="w"> </span>        self.n_categories = n_categories
<span class="w"> </span>        self.k = k
<span class="w"> </span>        self.r = r
<span class="gu">@@ -137,7 +146,47 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>        self : object
<span class="w"> </span>            Return the instance itself.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self._validate_params()</span>
<span class="gi">+        check_consistent_length(X, y)</span>
<span class="gi">+        X, y = self._validate_data(X, y, reset=True, dtype=np.int32)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.n_categories, str) and self.n_categories == &quot;auto&quot;:</span>
<span class="gi">+            # categories are expected to be encoded from 0 to n_categories - 1</span>
<span class="gi">+            self.n_categories_ = X.max(axis=0) + 1</span>
<span class="gi">+        else:</span>
<span class="gi">+            if len(self.n_categories) != self.n_features_in_:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;The length of n_categories is not consistent with the &quot;</span>
<span class="gi">+                    f&quot;number of feature in X. Got {len(self.n_categories)} &quot;</span>
<span class="gi">+                    f&quot;elements in n_categories and {self.n_features_in_} in &quot;</span>
<span class="gi">+                    f&quot;X.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            self.n_categories_ = np.array(self.n_categories, copy=False)</span>
<span class="gi">+        classes = unique_labels(y)</span>
<span class="gi">+</span>
<span class="gi">+        # list of length n_features of ndarray (n_categories, n_classes)</span>
<span class="gi">+        # compute the counts</span>
<span class="gi">+        self.proba_per_class_ = [</span>
<span class="gi">+            np.empty(shape=(n_cat, len(classes)), dtype=np.float64)</span>
<span class="gi">+            for n_cat in self.n_categories_</span>
<span class="gi">+        ]</span>
<span class="gi">+        for feature_idx in range(self.n_features_in_):</span>
<span class="gi">+            for klass_idx, klass in enumerate(classes):</span>
<span class="gi">+                self.proba_per_class_[feature_idx][:, klass_idx] = np.bincount(</span>
<span class="gi">+                    X[y == klass, feature_idx],</span>
<span class="gi">+                    minlength=self.n_categories_[feature_idx],</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        # normalize by the summing over the classes</span>
<span class="gi">+        with np.errstate(invalid=&quot;ignore&quot;):</span>
<span class="gi">+            # silence potential warning due to in-place division by zero</span>
<span class="gi">+            for feature_idx in range(self.n_features_in_):</span>
<span class="gi">+                self.proba_per_class_[feature_idx] /= (</span>
<span class="gi">+                    self.proba_per_class_[feature_idx].sum(axis=1).reshape(-1, 1)</span>
<span class="gi">+                )</span>
<span class="gi">+                np.nan_to_num(self.proba_per_class_[feature_idx], copy=False)</span>
<span class="gi">+</span>
<span class="gi">+        return self</span>

<span class="w"> </span>    def pairwise(self, X, Y=None):
<span class="w"> </span>        &quot;&quot;&quot;Compute the VDM distance pairwise.
<span class="gu">@@ -157,4 +206,29 @@ class ValueDifferenceMetric(_ParamsValidationMixin, BaseEstimator):</span>
<span class="w"> </span>        distance_matrix : ndarray of shape (n_samples, n_samples)
<span class="w"> </span>            The VDM pairwise distance.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        check_is_fitted(self)</span>
<span class="gi">+        X = self._validate_data(X, reset=False, dtype=np.int32)</span>
<span class="gi">+        n_samples_X = X.shape[0]</span>
<span class="gi">+</span>
<span class="gi">+        if Y is not None:</span>
<span class="gi">+            Y = self._validate_data(Y, reset=False, dtype=np.int32)</span>
<span class="gi">+            n_samples_Y = Y.shape[0]</span>
<span class="gi">+        else:</span>
<span class="gi">+            n_samples_Y = n_samples_X</span>
<span class="gi">+</span>
<span class="gi">+        distance = np.zeros(shape=(n_samples_X, n_samples_Y), dtype=np.float64)</span>
<span class="gi">+        for feature_idx in range(self.n_features_in_):</span>
<span class="gi">+            proba_feature_X = self.proba_per_class_[feature_idx][X[:, feature_idx]]</span>
<span class="gi">+            if Y is not None:</span>
<span class="gi">+                proba_feature_Y = self.proba_per_class_[feature_idx][Y[:, feature_idx]]</span>
<span class="gi">+            else:</span>
<span class="gi">+                proba_feature_Y = proba_feature_X</span>
<span class="gi">+            distance += (</span>
<span class="gi">+                distance_matrix(proba_feature_X, proba_feature_Y, p=self.k) ** self.r</span>
<span class="gi">+            )</span>
<span class="gi">+        return distance</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;requires_positive_X&quot;: True,  # X should be encoded with OrdinalEncoder</span>
<span class="gi">+        }</span>
<span class="gh">diff --git a/imblearn/metrics/tests/test_classification.py b/imblearn/metrics/tests/test_classification.py</span>
<span class="gh">index 7fce78f..8169cee 100644</span>
<span class="gd">--- a/imblearn/metrics/tests/test_classification.py</span>
<span class="gi">+++ b/imblearn/metrics/tests/test_classification.py</span>
<span class="gu">@@ -1,15 +1,47 @@</span>
<span class="gi">+# coding: utf-8</span>
<span class="w"> </span>&quot;&quot;&quot;Testing the metric for classification with imbalanced dataset&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from functools import partial
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn import datasets, svm
<span class="gd">-from sklearn.metrics import accuracy_score, average_precision_score, brier_score_loss, cohen_kappa_score, jaccard_score, precision_score, recall_score, roc_auc_score</span>
<span class="gi">+from sklearn.metrics import (</span>
<span class="gi">+    accuracy_score,</span>
<span class="gi">+    average_precision_score,</span>
<span class="gi">+    brier_score_loss,</span>
<span class="gi">+    cohen_kappa_score,</span>
<span class="gi">+    jaccard_score,</span>
<span class="gi">+    precision_score,</span>
<span class="gi">+    recall_score,</span>
<span class="gi">+    roc_auc_score,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.preprocessing import label_binarize
<span class="gd">-from sklearn.utils._testing import assert_allclose, assert_array_equal, assert_no_warnings</span>
<span class="gi">+from sklearn.utils._testing import (</span>
<span class="gi">+    assert_allclose,</span>
<span class="gi">+    assert_array_equal,</span>
<span class="gi">+    assert_no_warnings,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.utils.validation import check_random_state
<span class="gd">-from imblearn.metrics import classification_report_imbalanced, geometric_mean_score, macro_averaged_mean_absolute_error, make_index_balanced_accuracy, sensitivity_score, sensitivity_specificity_support, specificity_score</span>
<span class="gi">+</span>
<span class="gi">+from imblearn.metrics import (</span>
<span class="gi">+    classification_report_imbalanced,</span>
<span class="gi">+    geometric_mean_score,</span>
<span class="gi">+    macro_averaged_mean_absolute_error,</span>
<span class="gi">+    make_index_balanced_accuracy,</span>
<span class="gi">+    sensitivity_score,</span>
<span class="gi">+    sensitivity_specificity_support,</span>
<span class="gi">+    specificity_score,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 42
<span class="gd">-R_TOL = 0.01</span>
<span class="gi">+R_TOL = 1e-2</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Utilities for testing</span>


<span class="w"> </span>def make_prediction(dataset=None, binary=False):
<span class="gu">@@ -17,4 +49,505 @@ def make_prediction(dataset=None, binary=False):</span>
<span class="w"> </span>    If binary is True restrict to a binary classification problem instead of a
<span class="w"> </span>    multiclass classification problem
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    if dataset is None:</span>
<span class="gi">+        # import some data to play with</span>
<span class="gi">+        dataset = datasets.load_iris()</span>
<span class="gi">+</span>
<span class="gi">+    X = dataset.data</span>
<span class="gi">+    y = dataset.target</span>
<span class="gi">+</span>
<span class="gi">+    if binary:</span>
<span class="gi">+        # restrict to a binary classification task</span>
<span class="gi">+        X, y = X[y &lt; 2], y[y &lt; 2]</span>
<span class="gi">+</span>
<span class="gi">+    n_samples, n_features = X.shape</span>
<span class="gi">+    p = np.arange(n_samples)</span>
<span class="gi">+</span>
<span class="gi">+    rng = check_random_state(37)</span>
<span class="gi">+    rng.shuffle(p)</span>
<span class="gi">+    X, y = X[p], y[p]</span>
<span class="gi">+    half = int(n_samples / 2)</span>
<span class="gi">+</span>
<span class="gi">+    # add noisy features to make the problem harder and avoid perfect results</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]</span>
<span class="gi">+</span>
<span class="gi">+    # run classifier, get class probabilities and label predictions</span>
<span class="gi">+    clf = svm.SVC(kernel=&quot;linear&quot;, probability=True, random_state=0)</span>
<span class="gi">+    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])</span>
<span class="gi">+</span>
<span class="gi">+    if binary:</span>
<span class="gi">+        # only interested in probabilities of the positive case</span>
<span class="gi">+        # XXX: do we really want a special API for the binary case?</span>
<span class="gi">+        probas_pred = probas_pred[:, 1]</span>
<span class="gi">+</span>
<span class="gi">+    y_pred = clf.predict(X[half:])</span>
<span class="gi">+    y_true = y[half:]</span>
<span class="gi">+</span>
<span class="gi">+    return y_true, y_pred, probas_pred</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+###############################################################################</span>
<span class="gi">+# Tests</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sensitivity_specificity_score_binary():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    # detailed measures for each class</span>
<span class="gi">+    sen, spe, sup = sensitivity_specificity_support(y_true, y_pred, average=None)</span>
<span class="gi">+    assert_allclose(sen, [0.88, 0.68], rtol=R_TOL)</span>
<span class="gi">+    assert_allclose(spe, [0.68, 0.88], rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(sup, [25, 25])</span>
<span class="gi">+</span>
<span class="gi">+    # individual scoring function that can be used for grid search: in the</span>
<span class="gi">+    # binary class case the score is the value of the measure for the positive</span>
<span class="gi">+    # class (e.g. label == 1). This is deprecated for average != &#39;binary&#39;.</span>
<span class="gi">+    for kwargs in ({}, {&quot;average&quot;: &quot;binary&quot;}):</span>
<span class="gi">+        sen = assert_no_warnings(sensitivity_score, y_true, y_pred, **kwargs)</span>
<span class="gi">+        assert sen == pytest.approx(0.68, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+        spe = assert_no_warnings(specificity_score, y_true, y_pred, **kwargs)</span>
<span class="gi">+        assert spe == pytest.approx(0.88, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Specificity is ill-defined&quot;)</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;y_pred, expected_sensitivity, expected_specificity&quot;,</span>
<span class="gi">+    [(([1, 1], [1, 1]), 1.0, 0.0), (([-1, -1], [-1, -1]), 0.0, 0.0)],</span>
<span class="gi">+)</span>
<span class="gi">+def test_sensitivity_specificity_f_binary_single_class(</span>
<span class="gi">+    y_pred, expected_sensitivity, expected_specificity</span>
<span class="gi">+):</span>
<span class="gi">+    # Such a case may occur with non-stratified cross-validation</span>
<span class="gi">+    assert sensitivity_score(*y_pred) == expected_sensitivity</span>
<span class="gi">+    assert specificity_score(*y_pred) == expected_specificity</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;average, expected_specificty&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (None, [1.0, 0.67, 1.0, 1.0, 1.0]),</span>
<span class="gi">+        (&quot;macro&quot;, np.mean([1.0, 0.67, 1.0, 1.0, 1.0])),</span>
<span class="gi">+        (&quot;micro&quot;, 15 / 16),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_sensitivity_specificity_extra_labels(average, expected_specificty):</span>
<span class="gi">+    y_true = [1, 3, 3, 2]</span>
<span class="gi">+    y_pred = [1, 1, 3, 2]</span>
<span class="gi">+</span>
<span class="gi">+    actual = specificity_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average=average)</span>
<span class="gi">+    assert_allclose(expected_specificty, actual, rtol=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sensitivity_specificity_ignored_labels():</span>
<span class="gi">+    y_true = [1, 1, 2, 3]</span>
<span class="gi">+    y_pred = [1, 3, 3, 3]</span>
<span class="gi">+</span>
<span class="gi">+    specificity_13 = partial(specificity_score, y_true, y_pred, labels=[1, 3])</span>
<span class="gi">+    specificity_all = partial(specificity_score, y_true, y_pred, labels=None)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose([1.0, 0.33], specificity_13(average=None), rtol=R_TOL)</span>
<span class="gi">+    assert_allclose(np.mean([1.0, 0.33]), specificity_13(average=&quot;macro&quot;), rtol=R_TOL)</span>
<span class="gi">+    assert_allclose(</span>
<span class="gi">+        np.average([1.0, 0.33], weights=[2.0, 1.0]),</span>
<span class="gi">+        specificity_13(average=&quot;weighted&quot;),</span>
<span class="gi">+        rtol=R_TOL,</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(3.0 / (3.0 + 2.0), specificity_13(average=&quot;micro&quot;), rtol=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+    # ensure the above were meaningful tests:</span>
<span class="gi">+    for each in [&quot;macro&quot;, &quot;weighted&quot;, &quot;micro&quot;]:</span>
<span class="gi">+        assert specificity_13(average=each) != specificity_all(average=each)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sensitivity_specificity_error_multilabels():</span>
<span class="gi">+    y_true = [1, 3, 3, 2]</span>
<span class="gi">+    y_pred = [1, 1, 3, 2]</span>
<span class="gi">+    y_true_bin = label_binarize(y_true, classes=np.arange(5))</span>
<span class="gi">+    y_pred_bin = label_binarize(y_pred, classes=np.arange(5))</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        sensitivity_score(y_true_bin, y_pred_bin)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sensitivity_specificity_support_errors():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    # Bad pos_label</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        sensitivity_specificity_support(y_true, y_pred, pos_label=2, average=&quot;binary&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # Bad average option</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        sensitivity_specificity_support([0, 1, 2], [1, 2, 0], average=&quot;mega&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sensitivity_specificity_unused_pos_label():</span>
<span class="gi">+    # but average != &#39;binary&#39;; even if data is binary</span>
<span class="gi">+    msg = r&quot;use labels=\[pos_label\] to specify a single&quot;</span>
<span class="gi">+    with pytest.warns(UserWarning, match=msg):</span>
<span class="gi">+        sensitivity_specificity_support(</span>
<span class="gi">+            [1, 2, 1], [1, 2, 2], pos_label=2, average=&quot;macro&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_geometric_mean_support_binary():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    # compute the geometric mean for the binary problem</span>
<span class="gi">+    geo_mean = geometric_mean_score(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(geo_mean, 0.77, rtol=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Recall is ill-defined&quot;)</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;y_true, y_pred, correction, expected_gmean&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ([0, 0, 1, 1], [0, 0, 1, 1], 0.0, 1.0),</span>
<span class="gi">+        ([0, 0, 0, 0], [1, 1, 1, 1], 0.0, 0.0),</span>
<span class="gi">+        ([0, 0, 0, 0], [0, 0, 0, 0], 0.001, 1.0),</span>
<span class="gi">+        ([0, 0, 0, 0], [1, 1, 1, 1], 0.001, 0.001),</span>
<span class="gi">+        ([0, 0, 1, 1], [0, 1, 1, 0], 0.001, 0.5),</span>
<span class="gi">+        (</span>
<span class="gi">+            [0, 1, 2, 0, 1, 2],</span>
<span class="gi">+            [0, 2, 1, 0, 0, 1],</span>
<span class="gi">+            0.001,</span>
<span class="gi">+            (0.001**2) ** (1 / 3),</span>
<span class="gi">+        ),</span>
<span class="gi">+        ([0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], 0.001, 1),</span>
<span class="gi">+        ([0, 1, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], 0.001, (0.5 * 0.75) ** 0.5),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_geometric_mean_multiclass(y_true, y_pred, correction, expected_gmean):</span>
<span class="gi">+    gmean = geometric_mean_score(y_true, y_pred, correction=correction)</span>
<span class="gi">+    assert gmean == pytest.approx(expected_gmean, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:Recall is ill-defined&quot;)</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;y_true, y_pred, average, expected_gmean&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ([0, 1, 2, 0, 1, 2], [0, 2, 1, 0, 0, 1], &quot;macro&quot;, 0.471),</span>
<span class="gi">+        ([0, 1, 2, 0, 1, 2], [0, 2, 1, 0, 0, 1], &quot;micro&quot;, 0.471),</span>
<span class="gi">+        ([0, 1, 2, 0, 1, 2], [0, 2, 1, 0, 0, 1], &quot;weighted&quot;, 0.471),</span>
<span class="gi">+        ([0, 1, 2, 0, 1, 2], [0, 2, 1, 0, 0, 1], None, [0.8660254, 0.0, 0.0]),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_geometric_mean_average(y_true, y_pred, average, expected_gmean):</span>
<span class="gi">+    gmean = geometric_mean_score(y_true, y_pred, average=average)</span>
<span class="gi">+    assert gmean == pytest.approx(expected_gmean, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;y_true, y_pred, sample_weight, average, expected_gmean&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ([0, 1, 2, 0, 1, 2], [0, 1, 1, 0, 0, 1], None, &quot;multiclass&quot;, 0.707),</span>
<span class="gi">+        (</span>
<span class="gi">+            [0, 1, 2, 0, 1, 2],</span>
<span class="gi">+            [0, 1, 1, 0, 0, 1],</span>
<span class="gi">+            [1, 2, 1, 1, 2, 1],</span>
<span class="gi">+            &quot;multiclass&quot;,</span>
<span class="gi">+            0.707,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            [0, 1, 2, 0, 1, 2],</span>
<span class="gi">+            [0, 1, 1, 0, 0, 1],</span>
<span class="gi">+            [1, 2, 1, 1, 2, 1],</span>
<span class="gi">+            &quot;weighted&quot;,</span>
<span class="gi">+            0.333,</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_geometric_mean_sample_weight(</span>
<span class="gi">+    y_true, y_pred, sample_weight, average, expected_gmean</span>
<span class="gi">+):</span>
<span class="gi">+    gmean = geometric_mean_score(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=[0, 1],</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+        average=average,</span>
<span class="gi">+    )</span>
<span class="gi">+    assert gmean == pytest.approx(expected_gmean, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;average, expected_gmean&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (&quot;multiclass&quot;, 0.41),</span>
<span class="gi">+        (None, [0.85, 0.29, 0.7]),</span>
<span class="gi">+        (&quot;macro&quot;, 0.68),</span>
<span class="gi">+        (&quot;weighted&quot;, 0.65),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_geometric_mean_score_prediction(average, expected_gmean):</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    gmean = geometric_mean_score(y_true, y_pred, average=average)</span>
<span class="gi">+    assert gmean == pytest.approx(expected_gmean, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iba_geo_mean_binary():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    iba_gmean = make_index_balanced_accuracy(alpha=0.5, squared=True)(</span>
<span class="gi">+        geometric_mean_score</span>
<span class="gi">+    )</span>
<span class="gi">+    iba = iba_gmean(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(iba, 0.5948, rtol=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _format_report(report):</span>
<span class="gi">+    return &quot; &quot;.join(report.split())</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_multiclass():</span>
<span class="gi">+    iris = datasets.load_iris()</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    # print classification report with class names</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup setosa 0.83 0.79 0.92 &quot;</span>
<span class="gi">+        &quot;0.81 0.85 0.72 24 versicolor 0.33 0.10 0.86 0.15 &quot;</span>
<span class="gi">+        &quot;0.29 0.08 31 virginica 0.42 0.90 0.55 0.57 0.70 &quot;</span>
<span class="gi">+        &quot;0.51 20 avg / total 0.51 0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    report = classification_report_imbalanced(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=np.arange(len(iris.target_names)),</span>
<span class="gi">+        target_names=iris.target_names,</span>
<span class="gi">+    )</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+    # print classification report with label detection</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup 0 0.83 0.79 0.92 0.81 &quot;</span>
<span class="gi">+        &quot;0.85 0.72 24 1 0.33 0.10 0.86 0.15 0.29 0.08 31 &quot;</span>
<span class="gi">+        &quot;2 0.42 0.90 0.55 0.57 0.70 0.51 20 avg / total &quot;</span>
<span class="gi">+        &quot;0.51 0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    report = classification_report_imbalanced(y_true, y_pred)</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_multiclass_with_digits():</span>
<span class="gi">+    iris = datasets.load_iris()</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    # print classification report with class names</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup setosa 0.82609 0.79167 &quot;</span>
<span class="gi">+        &quot;0.92157 0.80851 0.85415 0.72010 24 versicolor &quot;</span>
<span class="gi">+        &quot;0.33333 0.09677 0.86364 0.15000 0.28910 0.07717 &quot;</span>
<span class="gi">+        &quot;31 virginica 0.41860 0.90000 0.54545 0.57143 0.70065 &quot;</span>
<span class="gi">+        &quot;0.50831 20 avg / total 0.51375 0.53333 0.79733 &quot;</span>
<span class="gi">+        &quot;0.47310 0.57966 0.39788 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    report = classification_report_imbalanced(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=np.arange(len(iris.target_names)),</span>
<span class="gi">+        target_names=iris.target_names,</span>
<span class="gi">+        digits=5,</span>
<span class="gi">+    )</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+    # print classification report with label detection</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup 0 0.83 0.79 0.92 0.81 &quot;</span>
<span class="gi">+        &quot;0.85 0.72 24 1 0.33 0.10 0.86 0.15 0.29 0.08 31 &quot;</span>
<span class="gi">+        &quot;2 0.42 0.90 0.55 0.57 0.70 0.51 20 avg / total 0.51 &quot;</span>
<span class="gi">+        &quot;0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    report = classification_report_imbalanced(y_true, y_pred)</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_multiclass_with_string_label():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    y_true = np.array([&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;])[y_true]</span>
<span class="gi">+    y_pred = np.array([&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;])[y_pred]</span>
<span class="gi">+</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup blue 0.83 0.79 0.92 0.81 &quot;</span>
<span class="gi">+        &quot;0.85 0.72 24 green 0.33 0.10 0.86 0.15 0.29 0.08 31 &quot;</span>
<span class="gi">+        &quot;red 0.42 0.90 0.55 0.57 0.70 0.51 20 avg / total &quot;</span>
<span class="gi">+        &quot;0.51 0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    report = classification_report_imbalanced(y_true, y_pred)</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup a 0.83 0.79 0.92 0.81 0.85 &quot;</span>
<span class="gi">+        &quot;0.72 24 b 0.33 0.10 0.86 0.15 0.29 0.08 31 c 0.42 &quot;</span>
<span class="gi">+        &quot;0.90 0.55 0.57 0.70 0.51 20 avg / total 0.51 0.53 &quot;</span>
<span class="gi">+        &quot;0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    report = classification_report_imbalanced(</span>
<span class="gi">+        y_true, y_pred, target_names=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_multiclass_with_unicode_label():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    labels = np.array([&quot;blue\xa2&quot;, &quot;green\xa2&quot;, &quot;red\xa2&quot;])</span>
<span class="gi">+    y_true = labels[y_true]</span>
<span class="gi">+    y_pred = labels[y_pred]</span>
<span class="gi">+</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup blue 0.83 0.79 0.92 0.81 &quot;</span>
<span class="gi">+        &quot;0.85 0.72 24 green 0.33 0.10 0.86 0.15 0.29 0.08 31 &quot;</span>
<span class="gi">+        &quot;red 0.42 0.90 0.55 0.57 0.70 0.51 20 avg / total &quot;</span>
<span class="gi">+        &quot;0.51 0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    report = classification_report_imbalanced(y_true, y_pred)</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_multiclass_with_long_string_label():</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    labels = np.array([&quot;blue&quot;, &quot;green&quot; * 5, &quot;red&quot;])</span>
<span class="gi">+    y_true = labels[y_true]</span>
<span class="gi">+    y_pred = labels[y_pred]</span>
<span class="gi">+</span>
<span class="gi">+    expected_report = (</span>
<span class="gi">+        &quot;pre rec spe f1 geo iba sup blue 0.83 0.79 0.92 0.81 &quot;</span>
<span class="gi">+        &quot;0.85 0.72 24 greengreengreengreengreen 0.33 0.10 &quot;</span>
<span class="gi">+        &quot;0.86 0.15 0.29 0.08 31 red 0.42 0.90 0.55 0.57 0.70 &quot;</span>
<span class="gi">+        &quot;0.51 20 avg / total 0.51 0.53 0.80 0.47 0.58 0.40 75&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    report = classification_report_imbalanced(y_true, y_pred)</span>
<span class="gi">+    assert _format_report(report) == expected_report</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;score, expected_score&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (accuracy_score, 0.54756),</span>
<span class="gi">+        (jaccard_score, 0.33176),</span>
<span class="gi">+        (precision_score, 0.65025),</span>
<span class="gi">+        (recall_score, 0.41616),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_iba_sklearn_metrics(score, expected_score):</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    score_iba = make_index_balanced_accuracy(alpha=0.5, squared=True)(score)</span>
<span class="gi">+    score = score_iba(y_true, y_pred)</span>
<span class="gi">+    assert score == pytest.approx(expected_score)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;score_loss&quot;,</span>
<span class="gi">+    [average_precision_score, brier_score_loss, cohen_kappa_score, roc_auc_score],</span>
<span class="gi">+)</span>
<span class="gi">+def test_iba_error_y_score_prob_error(score_loss):</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(binary=True)</span>
<span class="gi">+</span>
<span class="gi">+    aps = make_index_balanced_accuracy(alpha=0.5, squared=True)(score_loss)</span>
<span class="gi">+    with pytest.raises(AttributeError):</span>
<span class="gi">+        aps(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_dict_with_target_names():</span>
<span class="gi">+    iris = datasets.load_iris()</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)</span>
<span class="gi">+</span>
<span class="gi">+    report = classification_report_imbalanced(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=np.arange(len(iris.target_names)),</span>
<span class="gi">+        target_names=iris.target_names,</span>
<span class="gi">+        output_dict=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    outer_keys = set(report.keys())</span>
<span class="gi">+    inner_keys = set(report[&quot;setosa&quot;].keys())</span>
<span class="gi">+</span>
<span class="gi">+    expected_outer_keys = {</span>
<span class="gi">+        &quot;setosa&quot;,</span>
<span class="gi">+        &quot;versicolor&quot;,</span>
<span class="gi">+        &quot;virginica&quot;,</span>
<span class="gi">+        &quot;avg_pre&quot;,</span>
<span class="gi">+        &quot;avg_rec&quot;,</span>
<span class="gi">+        &quot;avg_spe&quot;,</span>
<span class="gi">+        &quot;avg_f1&quot;,</span>
<span class="gi">+        &quot;avg_geo&quot;,</span>
<span class="gi">+        &quot;avg_iba&quot;,</span>
<span class="gi">+        &quot;total_support&quot;,</span>
<span class="gi">+    }</span>
<span class="gi">+    expected_inner_keys = {&quot;spe&quot;, &quot;f1&quot;, &quot;sup&quot;, &quot;rec&quot;, &quot;geo&quot;, &quot;iba&quot;, &quot;pre&quot;}</span>
<span class="gi">+</span>
<span class="gi">+    assert outer_keys == expected_outer_keys</span>
<span class="gi">+    assert inner_keys == expected_inner_keys</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_classification_report_imbalanced_dict_without_target_names():</span>
<span class="gi">+    iris = datasets.load_iris()</span>
<span class="gi">+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)</span>
<span class="gi">+    print(iris.target_names)</span>
<span class="gi">+    report = classification_report_imbalanced(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        labels=np.arange(len(iris.target_names)),</span>
<span class="gi">+        output_dict=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    print(report.keys())</span>
<span class="gi">+    outer_keys = set(report.keys())</span>
<span class="gi">+    inner_keys = set(report[&quot;0&quot;].keys())</span>
<span class="gi">+</span>
<span class="gi">+    expected_outer_keys = {</span>
<span class="gi">+        &quot;0&quot;,</span>
<span class="gi">+        &quot;1&quot;,</span>
<span class="gi">+        &quot;2&quot;,</span>
<span class="gi">+        &quot;avg_pre&quot;,</span>
<span class="gi">+        &quot;avg_rec&quot;,</span>
<span class="gi">+        &quot;avg_spe&quot;,</span>
<span class="gi">+        &quot;avg_f1&quot;,</span>
<span class="gi">+        &quot;avg_geo&quot;,</span>
<span class="gi">+        &quot;avg_iba&quot;,</span>
<span class="gi">+        &quot;total_support&quot;,</span>
<span class="gi">+    }</span>
<span class="gi">+    expected_inner_keys = {&quot;spe&quot;, &quot;f1&quot;, &quot;sup&quot;, &quot;rec&quot;, &quot;geo&quot;, &quot;iba&quot;, &quot;pre&quot;}</span>
<span class="gi">+</span>
<span class="gi">+    assert outer_keys == expected_outer_keys</span>
<span class="gi">+    assert inner_keys == expected_inner_keys</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;y_true, y_pred, expected_ma_mae&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ([1, 1, 1, 2, 2, 2], [1, 2, 1, 2, 1, 2], 0.333),</span>
<span class="gi">+        ([1, 1, 1, 1, 1, 2], [1, 2, 1, 2, 1, 2], 0.2),</span>
<span class="gi">+        ([1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 3, 1, 2, 1, 1, 2, 3, 3], 0.555),</span>
<span class="gi">+        ([1, 1, 1, 1, 1, 1, 2, 3, 3], [1, 3, 1, 2, 1, 1, 2, 3, 3], 0.166),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_macro_averaged_mean_absolute_error(y_true, y_pred, expected_ma_mae):</span>
<span class="gi">+    ma_mae = macro_averaged_mean_absolute_error(y_true, y_pred)</span>
<span class="gi">+    assert ma_mae == pytest.approx(expected_ma_mae, rel=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_macro_averaged_mean_absolute_error_sample_weight():</span>
<span class="gi">+    y_true = [1, 1, 1, 2, 2, 2]</span>
<span class="gi">+    y_pred = [1, 2, 1, 2, 1, 2]</span>
<span class="gi">+</span>
<span class="gi">+    ma_mae_no_weights = macro_averaged_mean_absolute_error(y_true, y_pred)</span>
<span class="gi">+</span>
<span class="gi">+    sample_weight = [1, 1, 1, 1, 1, 1]</span>
<span class="gi">+    ma_mae_unit_weights = macro_averaged_mean_absolute_error(</span>
<span class="gi">+        y_true,</span>
<span class="gi">+        y_pred,</span>
<span class="gi">+        sample_weight=sample_weight,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    assert ma_mae_unit_weights == pytest.approx(ma_mae_no_weights)</span>
<span class="gh">diff --git a/imblearn/metrics/tests/test_pairwise.py b/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gh">index ccbfede..d724591 100644</span>
<span class="gd">--- a/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gi">+++ b/imblearn/metrics/tests/test_pairwise.py</span>
<span class="gu">@@ -1,7 +1,172 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the metrics that perform pairwise distance computation.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.exceptions import NotFittedError
<span class="w"> </span>from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
<span class="w"> </span>from sklearn.utils._testing import _convert_container
<span class="gi">+</span>
<span class="w"> </span>from imblearn.metrics.pairwise import ValueDifferenceMetric
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+</span>
<span class="gi">+    feature_1 = [&quot;A&quot;] * 10 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 30</span>
<span class="gi">+    feature_2 = [&quot;A&quot;] * 40 + [&quot;B&quot;] * 20</span>
<span class="gi">+    feature_3 = [&quot;A&quot;] * 20 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 10 + [&quot;D&quot;] * 10</span>
<span class="gi">+    X = np.array([feature_1, feature_2, feature_3], dtype=object).T</span>
<span class="gi">+    rng.shuffle(X)</span>
<span class="gi">+    y = rng.randint(low=0, high=2, size=X.shape[0])</span>
<span class="gi">+    y_labels = np.array([&quot;not apple&quot;, &quot;apple&quot;], dtype=object)</span>
<span class="gi">+    y = y_labels[y]</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;dtype&quot;, [np.int32, np.int64, np.float32, np.float64])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;k, r&quot;, [(1, 1), (1, 2), (2, 1), (2, 2)])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;y_type&quot;, [&quot;list&quot;, &quot;array&quot;])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;encode_label&quot;, [True, False])</span>
<span class="gi">+def test_value_difference_metric(data, dtype, k, r, y_type, encode_label):</span>
<span class="gi">+    # Check basic feature of the metric:</span>
<span class="gi">+    # * the shape of the distance matrix is (n_samples, n_samples)</span>
<span class="gi">+    # * computing pairwise distance of X is the same than explicitely between</span>
<span class="gi">+    #   X and X.</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    y = _convert_container(y, y_type)</span>
<span class="gi">+    if encode_label:</span>
<span class="gi">+        y = LabelEncoder().fit_transform(y)</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=dtype)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+    vdm = ValueDifferenceMetric(k=k, r=r)</span>
<span class="gi">+    vdm.fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+    dist_1 = vdm.pairwise(X_encoded)</span>
<span class="gi">+    dist_2 = vdm.pairwise(X_encoded, X_encoded)</span>
<span class="gi">+</span>
<span class="gi">+    np.testing.assert_allclose(dist_1, dist_2)</span>
<span class="gi">+    assert dist_1.shape == (X.shape[0], X.shape[0])</span>
<span class="gi">+    assert dist_2.shape == (X.shape[0], X.shape[0])</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;dtype&quot;, [np.int32, np.int64, np.float32, np.float64])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;k, r&quot;, [(1, 1), (1, 2), (2, 1), (2, 2)])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;y_type&quot;, [&quot;list&quot;, &quot;array&quot;])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;encode_label&quot;, [True, False])</span>
<span class="gi">+def test_value_difference_metric_property(dtype, k, r, y_type, encode_label):</span>
<span class="gi">+    # Check the property of the vdm distance. Let&#39;s check the property</span>
<span class="gi">+    # described in &quot;Improved Heterogeneous Distance Functions&quot;, D.R. Wilson and</span>
<span class="gi">+    # T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34</span>
<span class="gi">+    # https://arxiv.org/pdf/cs/9701101.pdf</span>
<span class="gi">+    #</span>
<span class="gi">+    # &quot;if an attribute color has three values red, green and blue, and the</span>
<span class="gi">+    # application is to identify whether or not an object is an apple, red and</span>
<span class="gi">+    # green would be considered closer than red and blue because the former two</span>
<span class="gi">+    # both have similar correlations with the output class apple.&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # defined our feature</span>
<span class="gi">+    X = np.array([&quot;green&quot;] * 10 + [&quot;red&quot;] * 10 + [&quot;blue&quot;] * 10).reshape(-1, 1)</span>
<span class="gi">+    # 0 - not an apple / 1 - an apple</span>
<span class="gi">+    y = np.array([1] * 8 + [0] * 5 + [1] * 7 + [0] * 9 + [1])</span>
<span class="gi">+    y_labels = np.array([&quot;not apple&quot;, &quot;apple&quot;], dtype=object)</span>
<span class="gi">+    y = y_labels[y]</span>
<span class="gi">+    y = _convert_container(y, y_type)</span>
<span class="gi">+    if encode_label:</span>
<span class="gi">+        y = LabelEncoder().fit_transform(y)</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=dtype)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+    vdm = ValueDifferenceMetric(k=k, r=r)</span>
<span class="gi">+    vdm.fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+    sample_green = encoder.transform([[&quot;green&quot;]])</span>
<span class="gi">+    sample_red = encoder.transform([[&quot;red&quot;]])</span>
<span class="gi">+    sample_blue = encoder.transform([[&quot;blue&quot;]])</span>
<span class="gi">+</span>
<span class="gi">+    for sample in (sample_green, sample_red, sample_blue):</span>
<span class="gi">+        # computing the distance between a sample of the same category should</span>
<span class="gi">+        # give a null distance</span>
<span class="gi">+        dist = vdm.pairwise(sample).squeeze()</span>
<span class="gi">+        assert dist == pytest.approx(0)</span>
<span class="gi">+</span>
<span class="gi">+    # check the property explained in the introduction example</span>
<span class="gi">+    dist_1 = vdm.pairwise(sample_green, sample_red).squeeze()</span>
<span class="gi">+    dist_2 = vdm.pairwise(sample_blue, sample_red).squeeze()</span>
<span class="gi">+    dist_3 = vdm.pairwise(sample_blue, sample_green).squeeze()</span>
<span class="gi">+</span>
<span class="gi">+    # green and red are very close</span>
<span class="gi">+    # blue is closer to red than green</span>
<span class="gi">+    assert dist_1 &lt; dist_2</span>
<span class="gi">+    assert dist_1 &lt; dist_3</span>
<span class="gi">+    assert dist_2 &lt; dist_3</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_categories(data):</span>
<span class="gi">+    # Check that &quot;auto&quot; is equivalent to provide the number categories</span>
<span class="gi">+    # beforehand</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=np.int32)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+    n_categories = np.array([len(cat) for cat in encoder.categories_])</span>
<span class="gi">+</span>
<span class="gi">+    vdm_auto = ValueDifferenceMetric().fit(X_encoded, y)</span>
<span class="gi">+    vdm_categories = ValueDifferenceMetric(n_categories=n_categories)</span>
<span class="gi">+    vdm_categories.fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+    np.testing.assert_array_equal(vdm_auto.n_categories_, n_categories)</span>
<span class="gi">+    np.testing.assert_array_equal(vdm_auto.n_categories_, vdm_categories.n_categories_)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_categories_error(data):</span>
<span class="gi">+    # Check that we raise an error if n_categories is inconsistent with the</span>
<span class="gi">+    # number of features in X</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=np.int32)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+    n_categories = [1, 2]</span>
<span class="gi">+</span>
<span class="gi">+    vdm = ValueDifferenceMetric(n_categories=n_categories)</span>
<span class="gi">+    err_msg = &quot;The length of n_categories is not consistent with the number&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        vdm.fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_metric_missing_categories(data):</span>
<span class="gi">+    # Check that we don&#39;t get issue when a category is missing between 0</span>
<span class="gi">+    # n_categories - 1</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=np.int32)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+    n_categories = np.array([len(cat) for cat in encoder.categories_])</span>
<span class="gi">+</span>
<span class="gi">+    # remove a categories that could be between 0 and n_categories</span>
<span class="gi">+    X_encoded[X_encoded[:, -1] == 1] = 0</span>
<span class="gi">+    np.testing.assert_array_equal(np.unique(X_encoded[:, -1]), [0, 2, 3])</span>
<span class="gi">+</span>
<span class="gi">+    vdm = ValueDifferenceMetric(n_categories=n_categories)</span>
<span class="gi">+    vdm.fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+    for n_cats, proba in zip(n_categories, vdm.proba_per_class_):</span>
<span class="gi">+        assert proba.shape == (n_cats, len(np.unique(y)))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_value_difference_value_unfitted(data):</span>
<span class="gi">+    # Check that we raise a NotFittedError when `fit` is not not called before</span>
<span class="gi">+    # pairwise.</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=np.int32)</span>
<span class="gi">+    X_encoded = encoder.fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(NotFittedError):</span>
<span class="gi">+        ValueDifferenceMetric().pairwise(X_encoded)</span>
<span class="gh">diff --git a/imblearn/metrics/tests/test_score_objects.py b/imblearn/metrics/tests/test_score_objects.py</span>
<span class="gh">index f80a93e..10a1ced 100644</span>
<span class="gd">--- a/imblearn/metrics/tests/test_score_objects.py</span>
<span class="gi">+++ b/imblearn/metrics/tests/test_score_objects.py</span>
<span class="gu">@@ -1,8 +1,78 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for score&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_blobs
<span class="w"> </span>from sklearn.linear_model import LogisticRegression
<span class="w"> </span>from sklearn.metrics import make_scorer
<span class="w"> </span>from sklearn.model_selection import GridSearchCV, train_test_split
<span class="gd">-from imblearn.metrics import geometric_mean_score, make_index_balanced_accuracy, sensitivity_score, specificity_score</span>
<span class="gd">-R_TOL = 0.01</span>
<span class="gi">+</span>
<span class="gi">+from imblearn.metrics import (</span>
<span class="gi">+    geometric_mean_score,</span>
<span class="gi">+    make_index_balanced_accuracy,</span>
<span class="gi">+    sensitivity_score,</span>
<span class="gi">+    specificity_score,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+R_TOL = 1e-2</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    X, y = make_blobs(random_state=0, centers=2)</span>
<span class="gi">+    return train_test_split(X, y, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;score, expected_score&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (sensitivity_score, 0.90),</span>
<span class="gi">+        (specificity_score, 0.90),</span>
<span class="gi">+        (geometric_mean_score, 0.90),</span>
<span class="gi">+        (make_index_balanced_accuracy()(geometric_mean_score), 0.82),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+@pytest.mark.parametrize(&quot;average&quot;, [&quot;macro&quot;, &quot;weighted&quot;, &quot;micro&quot;])</span>
<span class="gi">+def test_scorer_common_average(data, score, expected_score, average):</span>
<span class="gi">+    X_train, X_test, y_train, _ = data</span>
<span class="gi">+</span>
<span class="gi">+    scorer = make_scorer(score, pos_label=None, average=average)</span>
<span class="gi">+    grid = GridSearchCV(</span>
<span class="gi">+        LogisticRegression(),</span>
<span class="gi">+        param_grid={&quot;C&quot;: [1, 10]},</span>
<span class="gi">+        scoring=scorer,</span>
<span class="gi">+        cv=3,</span>
<span class="gi">+    )</span>
<span class="gi">+    grid.fit(X_train, y_train).predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert grid.best_score_ &gt;= expected_score</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;score, average, expected_score&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (sensitivity_score, &quot;binary&quot;, 0.94),</span>
<span class="gi">+        (specificity_score, &quot;binary&quot;, 0.89),</span>
<span class="gi">+        (geometric_mean_score, &quot;multiclass&quot;, 0.90),</span>
<span class="gi">+        (</span>
<span class="gi">+            make_index_balanced_accuracy()(geometric_mean_score),</span>
<span class="gi">+            &quot;multiclass&quot;,</span>
<span class="gi">+            0.82,</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_scorer_default_average(data, score, average, expected_score):</span>
<span class="gi">+    X_train, X_test, y_train, _ = data</span>
<span class="gi">+</span>
<span class="gi">+    scorer = make_scorer(score, pos_label=1, average=average)</span>
<span class="gi">+    grid = GridSearchCV(</span>
<span class="gi">+        LogisticRegression(),</span>
<span class="gi">+        param_grid={&quot;C&quot;: [1, 10]},</span>
<span class="gi">+        scoring=scorer,</span>
<span class="gi">+        cv=3,</span>
<span class="gi">+    )</span>
<span class="gi">+    grid.fit(X_train, y_train).predict(X_test)</span>
<span class="gi">+</span>
<span class="gi">+    assert grid.best_score_ &gt;= expected_score</span>
<span class="gh">diff --git a/imblearn/over_sampling/_adasyn.py b/imblearn/over_sampling/_adasyn.py</span>
<span class="gh">index d8159df..54e88b7 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_adasyn.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_adasyn.py</span>
<span class="gu">@@ -1,18 +1,27 @@</span>
<span class="gd">-&quot;&quot;&quot;Class to perform over-sampling using ADASYN.&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;Class to perform over-sampling using ADASYN.&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ..utils import Substitution, check_neighbors_object
<span class="w"> </span>from ..utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ..utils._param_validation import HasMethods, Interval
<span class="w"> </span>from .base import BaseOverSampler


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class ADASYN(BaseOverSampler):
<span class="w"> </span>    &quot;&quot;&quot;Oversample using Adaptive Synthetic (ADASYN) algorithm.

<span class="gu">@@ -112,13 +121,24 @@ class ADASYN(BaseOverSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 904, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseOverSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;n_jobs&#39;: [numbers.Integral, None]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        n_neighbors=5, n_jobs=None):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseOverSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        n_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="gu">@@ -126,4 +146,88 @@ class ADASYN(BaseOverSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create the necessary objects for ADASYN&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(</span>
<span class="gi">+            &quot;n_neighbors&quot;, self.n_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = [X.copy()]</span>
<span class="gi">+        y_resampled = [y.copy()]</span>
<span class="gi">+</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_.fit(X)</span>
<span class="gi">+            nns = self.nn_.kneighbors(X_class, return_distance=False)[:, 1:]</span>
<span class="gi">+            # The ratio is computed using a one-vs-rest manner. Using majority</span>
<span class="gi">+            # in multi-class would lead to slightly different results at the</span>
<span class="gi">+            # cost of introducing a new parameter.</span>
<span class="gi">+            n_neighbors = self.nn_.n_neighbors - 1</span>
<span class="gi">+            ratio_nn = np.sum(y[nns] != class_sample, axis=1) / n_neighbors</span>
<span class="gi">+            if not np.sum(ratio_nn):</span>
<span class="gi">+                raise RuntimeError(</span>
<span class="gi">+                    &quot;Not any neigbours belong to the majority&quot;</span>
<span class="gi">+                    &quot; class. This case will induce a NaN case&quot;</span>
<span class="gi">+                    &quot; with a division by zero. ADASYN is not&quot;</span>
<span class="gi">+                    &quot; suited for this specific dataset.&quot;</span>
<span class="gi">+                    &quot; Use SMOTE instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            ratio_nn /= np.sum(ratio_nn)</span>
<span class="gi">+            n_samples_generate = np.rint(ratio_nn * n_samples).astype(int)</span>
<span class="gi">+            # rounding may cause new amount for n_samples</span>
<span class="gi">+            n_samples = np.sum(n_samples_generate)</span>
<span class="gi">+            if not n_samples:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;No samples will be generated with the provided ratio settings.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # the nearest neighbors need to be fitted only on the current class</span>
<span class="gi">+            # to find the class NN to generate new samples</span>
<span class="gi">+            self.nn_.fit(X_class)</span>
<span class="gi">+            nns = self.nn_.kneighbors(X_class, return_distance=False)[:, 1:]</span>
<span class="gi">+</span>
<span class="gi">+            enumerated_class_indices = np.arange(len(target_class_indices))</span>
<span class="gi">+            rows = np.repeat(enumerated_class_indices, n_samples_generate)</span>
<span class="gi">+            cols = random_state.choice(n_neighbors, size=n_samples)</span>
<span class="gi">+            diffs = X_class[nns[rows, cols]] - X_class[rows]</span>
<span class="gi">+            steps = random_state.uniform(size=(n_samples, 1))</span>
<span class="gi">+</span>
<span class="gi">+            if sparse.issparse(X):</span>
<span class="gi">+                sparse_func = type(X).__name__</span>
<span class="gi">+                steps = getattr(sparse, sparse_func)(steps)</span>
<span class="gi">+                X_new = X_class[rows] + steps.multiply(diffs)</span>
<span class="gi">+            else:</span>
<span class="gi">+                X_new = X_class[rows] + steps * diffs</span>
<span class="gi">+</span>
<span class="gi">+            X_new = X_new.astype(X.dtype)</span>
<span class="gi">+            y_new = np.full(n_samples, fill_value=class_sample, dtype=y.dtype)</span>
<span class="gi">+            X_resampled.append(X_new)</span>
<span class="gi">+            y_resampled.append(y_new)</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_resampled = sparse.vstack(X_resampled, format=X.format)</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = np.vstack(X_resampled)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;X_types&quot;: [&quot;2darray&quot;],</span>
<span class="gi">+        }</span>
<span class="gh">diff --git a/imblearn/over_sampling/_random_over_sampler.py b/imblearn/over_sampling/_random_over_sampler.py</span>
<span class="gh">index cffe043..63b5a66 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_random_over_sampler.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_random_over_sampler.py</span>
<span class="gu">@@ -1,10 +1,17 @@</span>
<span class="gd">-&quot;&quot;&quot;Class to perform random over-sampling.&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;Class to perform random over-sampling.&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections.abc import Mapping
<span class="w"> </span>from numbers import Real
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_array, check_random_state
<span class="w"> </span>from sklearn.utils.sparsefuncs import mean_variance_axis
<span class="gi">+</span>
<span class="w"> </span>from ..utils import Substitution, check_target_type
<span class="w"> </span>from ..utils._docstring import _random_state_docstring
<span class="w"> </span>from ..utils._param_validation import Interval
<span class="gu">@@ -12,8 +19,10 @@ from ..utils._validation import _check_X</span>
<span class="w"> </span>from .base import BaseOverSampler


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, random_state=_random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class RandomOverSampler(BaseOverSampler):
<span class="w"> </span>    &quot;&quot;&quot;Class to perform random over-sampling.

<span class="gu">@@ -127,12 +136,125 @@ class RandomOverSampler(BaseOverSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseOverSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;shrinkage&#39;: [Interval(Real, 0, None,</span>
<span class="gd">-        closed=&#39;left&#39;), dict, None]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        shrinkage=None):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseOverSampler._parameter_constraints,</span>
<span class="gi">+        &quot;shrinkage&quot;: [Interval(Real, 0, None, closed=&quot;left&quot;), dict, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        shrinkage=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.shrinkage = shrinkage
<span class="gi">+</span>
<span class="gi">+    def _check_X_y(self, X, y):</span>
<span class="gi">+        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X = _check_X(X)</span>
<span class="gi">+        self._check_n_features(X, reset=True)</span>
<span class="gi">+        self._check_feature_names(X, reset=True)</span>
<span class="gi">+        return X, y, binarize_y</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(self.shrinkage, Real):</span>
<span class="gi">+            self.shrinkage_ = {</span>
<span class="gi">+                klass: self.shrinkage for klass in self.sampling_strategy_</span>
<span class="gi">+            }</span>
<span class="gi">+        elif self.shrinkage is None or isinstance(self.shrinkage, Mapping):</span>
<span class="gi">+            self.shrinkage_ = self.shrinkage</span>
<span class="gi">+</span>
<span class="gi">+        if self.shrinkage_ is not None:</span>
<span class="gi">+            missing_shrinkage_keys = (</span>
<span class="gi">+                self.sampling_strategy_.keys() - self.shrinkage_.keys()</span>
<span class="gi">+            )</span>
<span class="gi">+            if missing_shrinkage_keys:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;`shrinkage` should contain a shrinkage factor for &quot;</span>
<span class="gi">+                    f&quot;each class that will be resampled. The missing &quot;</span>
<span class="gi">+                    f&quot;classes are: {repr(missing_shrinkage_keys)}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            for klass, shrink_factor in self.shrinkage_.items():</span>
<span class="gi">+                if shrink_factor &lt; 0:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;The shrinkage factor needs to be &gt;= 0. &quot;</span>
<span class="gi">+                        f&quot;Got {shrink_factor} for class {klass}.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+            # smoothed bootstrap imposes to make numerical operation; we need</span>
<span class="gi">+            # to be sure to have only numerical data in X</span>
<span class="gi">+            try:</span>
<span class="gi">+                X = check_array(X, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;], dtype=&quot;numeric&quot;)</span>
<span class="gi">+            except ValueError as exc:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;When shrinkage is not None, X needs to contain only &quot;</span>
<span class="gi">+                    &quot;numerical data to later generate a smoothed bootstrap &quot;</span>
<span class="gi">+                    &quot;sample.&quot;</span>
<span class="gi">+                ) from exc</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = [X.copy()]</span>
<span class="gi">+        y_resampled = [y.copy()]</span>
<span class="gi">+</span>
<span class="gi">+        sample_indices = range(X.shape[0])</span>
<span class="gi">+        for class_sample, num_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            bootstrap_indices = random_state.choice(</span>
<span class="gi">+                target_class_indices,</span>
<span class="gi">+                size=num_samples,</span>
<span class="gi">+                replace=True,</span>
<span class="gi">+            )</span>
<span class="gi">+            sample_indices = np.append(sample_indices, bootstrap_indices)</span>
<span class="gi">+            if self.shrinkage_ is not None:</span>
<span class="gi">+                # generate a smoothed bootstrap with a perturbation</span>
<span class="gi">+                n_samples, n_features = X.shape</span>
<span class="gi">+                smoothing_constant = (4 / ((n_features + 2) * n_samples)) ** (</span>
<span class="gi">+                    1 / (n_features + 4)</span>
<span class="gi">+                )</span>
<span class="gi">+                if sparse.issparse(X):</span>
<span class="gi">+                    _, X_class_variance = mean_variance_axis(</span>
<span class="gi">+                        X[target_class_indices, :],</span>
<span class="gi">+                        axis=0,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    X_class_scale = np.sqrt(X_class_variance, out=X_class_variance)</span>
<span class="gi">+                else:</span>
<span class="gi">+                    X_class_scale = np.std(X[target_class_indices, :], axis=0)</span>
<span class="gi">+                smoothing_matrix = np.diagflat(</span>
<span class="gi">+                    self.shrinkage_[class_sample] * smoothing_constant * X_class_scale</span>
<span class="gi">+                )</span>
<span class="gi">+                X_new = random_state.randn(num_samples, n_features)</span>
<span class="gi">+                X_new = X_new.dot(smoothing_matrix) + X[bootstrap_indices, :]</span>
<span class="gi">+                if sparse.issparse(X):</span>
<span class="gi">+                    X_new = sparse.csr_matrix(X_new, dtype=X.dtype)</span>
<span class="gi">+                X_resampled.append(X_new)</span>
<span class="gi">+            else:</span>
<span class="gi">+                # generate a bootstrap</span>
<span class="gi">+                X_resampled.append(_safe_indexing(X, bootstrap_indices))</span>
<span class="gi">+</span>
<span class="gi">+            y_resampled.append(_safe_indexing(y, bootstrap_indices))</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = np.array(sample_indices)</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_resampled = sparse.vstack(X_resampled, format=X.format)</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = np.vstack(X_resampled)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;X_types&quot;: [&quot;2darray&quot;, &quot;string&quot;, &quot;sparse&quot;, &quot;dataframe&quot;],</span>
<span class="gi">+            &quot;sample_indices&quot;: True,</span>
<span class="gi">+            &quot;allow_nan&quot;: True,</span>
<span class="gi">+            &quot;_xfail_checks&quot;: {</span>
<span class="gi">+                &quot;check_complex_data&quot;: &quot;Robust to this type of data.&quot;,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/base.py b/imblearn/over_sampling/_smote/base.py</span>
<span class="gh">index 968941c..8ef9029 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/base.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/base.py</span>
<span class="gu">@@ -1,17 +1,32 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Base class and original SMOTE methods for over-sampling&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Fernando Nogueira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+#          Dzianis Dudnik</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import math
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.exceptions import DataConversionWarning
<span class="w"> </span>from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
<span class="gd">-from sklearn.utils import _safe_indexing, check_array, check_random_state</span>
<span class="gi">+from sklearn.utils import (</span>
<span class="gi">+    _safe_indexing,</span>
<span class="gi">+    check_array,</span>
<span class="gi">+    check_random_state,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gd">-from sklearn.utils.sparsefuncs_fast import csr_mean_variance_axis0</span>
<span class="gi">+from sklearn.utils.sparsefuncs_fast import (</span>
<span class="gi">+    csr_mean_variance_axis0,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.utils.validation import _num_features
<span class="gi">+</span>
<span class="w"> </span>from ...metrics.pairwise import ValueDifferenceMetric
<span class="w"> </span>from ...utils import Substitution, check_neighbors_object, check_target_type
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="gu">@@ -19,8 +34,9 @@ from ...utils._param_validation import HasMethods, Interval, StrOptions</span>
<span class="w"> </span>from ...utils._validation import _check_X
<span class="w"> </span>from ...utils.fixes import _is_pandas_df, _mode
<span class="w"> </span>from ..base import BaseOverSampler
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__).base_version
<span class="gd">-if parse_version(sklearn_version) &lt; parse_version(&#39;1.5&#39;):</span>
<span class="gi">+if parse_version(sklearn_version) &lt; parse_version(&quot;1.5&quot;):</span>
<span class="w"> </span>    from sklearn.utils import _get_column_indices
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.utils._indexing import _get_column_indices
<span class="gu">@@ -28,13 +44,23 @@ else:</span>

<span class="w"> </span>class BaseSMOTE(BaseOverSampler):
<span class="w"> </span>    &quot;&quot;&quot;Base class for the different SMOTE algorithms.&quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseOverSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;k_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;n_jobs&#39;: [numbers.Integral, None]}</span>

<span class="gd">-    def __init__(self, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        k_neighbors=5, n_jobs=None):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseOverSampler._parameter_constraints,</span>
<span class="gi">+        &quot;k_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.k_neighbors = k_neighbors
<span class="gu">@@ -44,10 +70,13 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        &quot;&quot;&quot;Check the NN estimators shared across the different SMOTE
<span class="w"> </span>        algorithms.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_k_ = check_neighbors_object(</span>
<span class="gi">+            &quot;k_neighbors&quot;, self.k_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>

<span class="gd">-    def _make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples,</span>
<span class="gd">-        step_size=1.0, y=None):</span>
<span class="gi">+    def _make_samples(</span>
<span class="gi">+        self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size=1.0, y=None</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;A support function that returns artificial samples constructed along
<span class="w"> </span>        the line connecting nearest neighbours.

<span class="gu">@@ -88,21 +117,32 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        y_new : ndarray of shape (n_samples_new,)
<span class="w"> </span>            Target values for synthetic samples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        samples_indices = random_state.randint(low=0, high=nn_num.size, size=n_samples)</span>
<span class="gi">+</span>
<span class="gi">+        # np.newaxis for backwards compatability with random_state</span>
<span class="gi">+        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]</span>
<span class="gi">+        rows = np.floor_divide(samples_indices, nn_num.shape[1])</span>
<span class="gi">+        cols = np.mod(samples_indices, nn_num.shape[1])</span>
<span class="gi">+</span>
<span class="gi">+        X_new = self._generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)</span>
<span class="gi">+        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)</span>
<span class="gi">+        return X_new, y_new</span>

<span class="gd">-    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps,</span>
<span class="gd">-        y_type=None, y=None):</span>
<span class="gd">-        &quot;&quot;&quot;Generate a synthetic sample.</span>
<span class="gi">+    def _generate_samples(</span>
<span class="gi">+        self, X, nn_data, nn_num, rows, cols, steps, y_type=None, y=None</span>
<span class="gi">+    ):</span>
<span class="gi">+        r&quot;&quot;&quot;Generate a synthetic sample.</span>

<span class="w"> </span>        The rule for the generation is:

<span class="w"> </span>        .. math::
<span class="gd">-           \\mathbf{s_{s}} = \\mathbf{s_{i}} + \\mathcal{u}(0, 1) \\times</span>
<span class="gd">-           (\\mathbf{s_{i}} - \\mathbf{s_{nn}}) \\,</span>
<span class="gi">+           \mathbf{s_{s}} = \mathbf{s_{i}} + \mathcal{u}(0, 1) \times</span>
<span class="gi">+           (\mathbf{s_{i}} - \mathbf{s_{nn}}) \,</span>

<span class="gd">-        where \\mathbf{s_{s}} is the new synthetic samples, \\mathbf{s_{i}} is</span>
<span class="gd">-        the current sample, \\mathbf{s_{nn}} is a randomly selected neighbors of</span>
<span class="gd">-        \\mathbf{s_{i}} and \\mathcal{u}(0, 1) is a random number between [0, 1).</span>
<span class="gi">+        where \mathbf{s_{s}} is the new synthetic samples, \mathbf{s_{i}} is</span>
<span class="gi">+        the current sample, \mathbf{s_{nn}} is a randomly selected neighbors of</span>
<span class="gi">+        \mathbf{s_{i}} and \mathcal{u}(0, 1) is a random number between [0, 1).</span>

<span class="w"> </span>        Parameters
<span class="w"> </span>        ----------
<span class="gu">@@ -139,10 +179,24 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        X_new : {ndarray, sparse matrix} of shape (n_samples, n_features)
<span class="w"> </span>            Synthetically generated samples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def _in_danger_noise(self, nn_estimator, samples, target_class, y, kind</span>
<span class="gd">-        =&#39;danger&#39;):</span>
<span class="gi">+        diffs = nn_data[nn_num[rows, cols]] - X[rows]</span>
<span class="gi">+        if y is not None:  # only entering for BorderlineSMOTE-2</span>
<span class="gi">+            random_state = check_random_state(self.random_state)</span>
<span class="gi">+            mask_pair_samples = y[nn_num[rows, cols]] != y_type</span>
<span class="gi">+            diffs[mask_pair_samples] *= random_state.uniform(</span>
<span class="gi">+                low=0.0, high=0.5, size=(mask_pair_samples.sum(), 1)</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            sparse_func = type(X).__name__</span>
<span class="gi">+            steps = getattr(sparse, sparse_func)(steps)</span>
<span class="gi">+            X_new = X[rows] + steps.multiply(diffs)</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_new = X[rows] + steps * diffs</span>
<span class="gi">+</span>
<span class="gi">+        return X_new.astype(X.dtype)</span>
<span class="gi">+</span>
<span class="gi">+    def _in_danger_noise(self, nn_estimator, samples, target_class, y, kind=&quot;danger&quot;):</span>
<span class="w"> </span>        &quot;&quot;&quot;Estimate if a set of sample are in danger or noise.

<span class="w"> </span>        Used by BorderlineSMOTE and SVMSMOTE.
<span class="gu">@@ -174,12 +228,26 @@ class BaseSMOTE(BaseOverSampler):</span>
<span class="w"> </span>        output : ndarray of shape (n_samples,)
<span class="w"> </span>            A boolean array where True refer to samples in danger or noise.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+        x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]</span>
<span class="gi">+        nn_label = (y[x] != target_class).astype(int)</span>
<span class="gi">+        n_maj = np.sum(nn_label, axis=1)</span>
<span class="gi">+</span>
<span class="gi">+        if kind == &quot;danger&quot;:</span>
<span class="gi">+            # Samples are in danger for m/2 &lt;= m&#39; &lt; m</span>
<span class="gi">+            return np.bitwise_and(</span>
<span class="gi">+                n_maj &gt;= (nn_estimator.n_neighbors - 1) / 2,</span>
<span class="gi">+                n_maj &lt; nn_estimator.n_neighbors - 1,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:  # kind == &quot;noise&quot;:</span>
<span class="gi">+            # Samples are noise for m = m&#39;</span>
<span class="gi">+            return n_maj == nn_estimator.n_neighbors - 1</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SMOTE(BaseSMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Class to perform over-sampling using SMOTE.

<span class="gu">@@ -281,15 +349,64 @@ class SMOTE(BaseSMOTE):</span>
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        k_neighbors=5, n_jobs=None):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = [X.copy()]</span>
<span class="gi">+        y_resampled = [y.copy()]</span>
<span class="gi">+</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_k_.fit(X_class)</span>
<span class="gi">+            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]</span>
<span class="gi">+            X_new, y_new = self._make_samples(</span>
<span class="gi">+                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0</span>
<span class="gi">+            )</span>
<span class="gi">+            X_resampled.append(X_new)</span>
<span class="gi">+            y_resampled.append(y_new)</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_resampled = sparse.vstack(X_resampled, format=X.format)</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = np.vstack(X_resampled)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SMOTENC(SMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Synthetic Minority Over-sampling Technique for Nominal and Continuous.

<span class="gu">@@ -303,7 +420,8 @@ class SMOTENC(SMOTE):</span>

<span class="w"> </span>    Parameters
<span class="w"> </span>    ----------
<span class="gd">-    categorical_features : &quot;infer&quot; or array-like of shape (n_cat_features,) or             (n_features,), dtype={{bool, int, str}}</span>
<span class="gi">+    categorical_features : &quot;infer&quot; or array-like of shape (n_cat_features,) or \</span>
<span class="gi">+            (n_features,), dtype={{bool, int, str}}</span>
<span class="w"> </span>        Specified which features are categorical. Can either be:

<span class="w"> </span>        - &quot;auto&quot; (default) to automatically detect categorical features. Only
<span class="gu">@@ -444,17 +562,34 @@ class SMOTENC(SMOTE):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(f&#39;Resampled dataset samples per class {{Counter(y_res)}}&#39;)
<span class="w"> </span>    Resampled dataset samples per class Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _required_parameters = [&#39;categorical_features&#39;]</span>
<span class="gd">-    _parameter_constraints: dict = {**SMOTE._parameter_constraints,</span>
<span class="gd">-        &#39;categorical_features&#39;: [&#39;array-like&#39;, StrOptions({&#39;auto&#39;})],</span>
<span class="gd">-        &#39;categorical_encoder&#39;: [HasMethods([&#39;fit_transform&#39;,</span>
<span class="gd">-        &#39;inverse_transform&#39;]), None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, categorical_features, *, categorical_encoder=None,</span>
<span class="gd">-        sampling_strategy=&#39;auto&#39;, random_state=None, k_neighbors=5, n_jobs=None</span>
<span class="gd">-        ):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    _required_parameters = [&quot;categorical_features&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **SMOTE._parameter_constraints,</span>
<span class="gi">+        &quot;categorical_features&quot;: [&quot;array-like&quot;, StrOptions({&quot;auto&quot;})],</span>
<span class="gi">+        &quot;categorical_encoder&quot;: [</span>
<span class="gi">+            HasMethods([&quot;fit_transform&quot;, &quot;inverse_transform&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        categorical_features,</span>
<span class="gi">+        *,</span>
<span class="gi">+        categorical_encoder=None,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.categorical_features = categorical_features
<span class="w"> </span>        self.categorical_encoder = categorical_encoder

<span class="gu">@@ -462,14 +597,170 @@ class SMOTENC(SMOTE):</span>
<span class="w"> </span>        &quot;&quot;&quot;Overwrite the checking to let pass some string for categorical
<span class="w"> </span>        features.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X = _check_X(X)</span>
<span class="gi">+        self._check_n_features(X, reset=True)</span>
<span class="gi">+        self._check_feature_names(X, reset=True)</span>
<span class="gi">+        return X, y, binarize_y</span>

<span class="w"> </span>    def _validate_column_types(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Compute the indices of the categorical and continuous features.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.categorical_features == &quot;auto&quot;:</span>
<span class="gi">+            if not _is_pandas_df(X):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;When `categorical_features=&#39;auto&#39;`, the input data &quot;</span>
<span class="gi">+                    f&quot;should be a pandas.DataFrame. Got {type(X)} instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            import pandas as pd  # safely import pandas now</span>
<span class="gi">+</span>
<span class="gi">+            are_columns_categorical = np.array(</span>
<span class="gi">+                [isinstance(col_dtype, pd.CategoricalDtype) for col_dtype in X.dtypes]</span>
<span class="gi">+            )</span>
<span class="gi">+            self.categorical_features_ = np.flatnonzero(are_columns_categorical)</span>
<span class="gi">+            self.continuous_features_ = np.flatnonzero(~are_columns_categorical)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.categorical_features_ = np.array(</span>
<span class="gi">+                _get_column_indices(X, self.categorical_features)</span>
<span class="gi">+            )</span>
<span class="gi">+            self.continuous_features_ = np.setdiff1d(</span>
<span class="gi">+                np.arange(self.n_features_), self.categorical_features_</span>
<span class="gi">+            )</span>

<span class="gd">-    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps,</span>
<span class="gd">-        y_type, y=None):</span>
<span class="gi">+    def _validate_estimator(self):</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+        if self.categorical_features_.size == self.n_features_in_:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;SMOTE-NC is not designed to work only with categorical &quot;</span>
<span class="gi">+                &quot;features. It requires some numerical features.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        elif self.categorical_features_.size == 0:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;SMOTE-NC is not designed to work only with numerical &quot;</span>
<span class="gi">+                &quot;features. It requires some categorical features.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.n_features_ = _num_features(X)</span>
<span class="gi">+        self._validate_column_types(X)</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_continuous = _safe_indexing(X, self.continuous_features_, axis=1)</span>
<span class="gi">+        X_continuous = check_array(X_continuous, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;])</span>
<span class="gi">+        X_categorical = _safe_indexing(X, self.categorical_features_, axis=1)</span>
<span class="gi">+        if X_continuous.dtype.name != &quot;object&quot;:</span>
<span class="gi">+            dtype_ohe = X_continuous.dtype</span>
<span class="gi">+        else:</span>
<span class="gi">+            dtype_ohe = np.float64</span>
<span class="gi">+</span>
<span class="gi">+        if self.categorical_encoder is None:</span>
<span class="gi">+            self.categorical_encoder_ = OneHotEncoder(</span>
<span class="gi">+                handle_unknown=&quot;ignore&quot;, dtype=dtype_ohe</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.categorical_encoder_ = clone(self.categorical_encoder)</span>
<span class="gi">+</span>
<span class="gi">+        # the input of the OneHotEncoder needs to be dense</span>
<span class="gi">+        X_ohe = self.categorical_encoder_.fit_transform(</span>
<span class="gi">+            X_categorical.toarray() if sparse.issparse(X_categorical) else X_categorical</span>
<span class="gi">+        )</span>
<span class="gi">+        if not sparse.issparse(X_ohe):</span>
<span class="gi">+            X_ohe = sparse.csr_matrix(X_ohe, dtype=dtype_ohe)</span>
<span class="gi">+</span>
<span class="gi">+        X_encoded = sparse.hstack((X_continuous, X_ohe), format=&quot;csr&quot;, dtype=dtype_ohe)</span>
<span class="gi">+        X_resampled = [X_encoded.copy()]</span>
<span class="gi">+        y_resampled = [y.copy()]</span>
<span class="gi">+</span>
<span class="gi">+        # SMOTE resampling starts here</span>
<span class="gi">+        self.median_std_ = {}</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X_encoded, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            _, var = csr_mean_variance_axis0(</span>
<span class="gi">+                X_class[:, : self.continuous_features_.size]</span>
<span class="gi">+            )</span>
<span class="gi">+            self.median_std_[class_sample] = np.median(np.sqrt(var))</span>
<span class="gi">+</span>
<span class="gi">+            # In the edge case where the median of the std is equal to 0, the 1s</span>
<span class="gi">+            # entries will be also nullified. In this case, we store the original</span>
<span class="gi">+            # categorical encoding which will be later used for inverting the OHE</span>
<span class="gi">+            if math.isclose(self.median_std_[class_sample], 0):</span>
<span class="gi">+                # This variable will be used when generating data</span>
<span class="gi">+                self._X_categorical_minority_encoded = X_class[</span>
<span class="gi">+                    :, self.continuous_features_.size :</span>
<span class="gi">+                ].toarray()</span>
<span class="gi">+</span>
<span class="gi">+            # we can replace the 1 entries of the categorical features with the</span>
<span class="gi">+            # median of the standard deviation. It will ensure that whenever</span>
<span class="gi">+            # distance is computed between 2 samples, the difference will be equal</span>
<span class="gi">+            # to the median of the standard deviation as in the original paper.</span>
<span class="gi">+            X_class_categorical = X_class[:, self.continuous_features_.size :]</span>
<span class="gi">+            # With one-hot encoding, the median will be repeated twice. We need</span>
<span class="gi">+            # to divide by sqrt(2) such that we only have one median value</span>
<span class="gi">+            # contributing to the Euclidean distance</span>
<span class="gi">+            X_class_categorical.data[:] = self.median_std_[class_sample] / np.sqrt(2)</span>
<span class="gi">+            X_class[:, self.continuous_features_.size :] = X_class_categorical</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_k_.fit(X_class)</span>
<span class="gi">+            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]</span>
<span class="gi">+            X_new, y_new = self._make_samples(</span>
<span class="gi">+                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0</span>
<span class="gi">+            )</span>
<span class="gi">+            X_resampled.append(X_new)</span>
<span class="gi">+            y_resampled.append(y_new)</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = sparse.vstack(X_resampled, format=X_encoded.format)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+        # SMOTE resampling ends here</span>
<span class="gi">+</span>
<span class="gi">+        # reverse the encoding of the categorical features</span>
<span class="gi">+        X_res_cat = X_resampled[:, self.continuous_features_.size :]</span>
<span class="gi">+        X_res_cat.data = np.ones_like(X_res_cat.data)</span>
<span class="gi">+        X_res_cat_dec = self.categorical_encoder_.inverse_transform(X_res_cat)</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_resampled = sparse.hstack(</span>
<span class="gi">+                (</span>
<span class="gi">+                    X_resampled[:, : self.continuous_features_.size],</span>
<span class="gi">+                    X_res_cat_dec,</span>
<span class="gi">+                ),</span>
<span class="gi">+                format=&quot;csr&quot;,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = np.hstack(</span>
<span class="gi">+                (</span>
<span class="gi">+                    X_resampled[:, : self.continuous_features_.size].toarray(),</span>
<span class="gi">+                    X_res_cat_dec,</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        indices_reordered = np.argsort(</span>
<span class="gi">+            np.hstack((self.continuous_features_, self.categorical_features_))</span>
<span class="gi">+        )</span>
<span class="gi">+        if sparse.issparse(X_resampled):</span>
<span class="gi">+            # the matrix is supposed to be in the CSR format after the stacking</span>
<span class="gi">+            col_indices = X_resampled.indices.copy()</span>
<span class="gi">+            for idx, col_idx in enumerate(indices_reordered):</span>
<span class="gi">+                mask = X_resampled.indices == col_idx</span>
<span class="gi">+                col_indices[mask] = idx</span>
<span class="gi">+            X_resampled.indices = col_indices</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = X_resampled[:, indices_reordered]</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps, y_type, y=None):</span>
<span class="w"> </span>        &quot;&quot;&quot;Generate a synthetic sample with an additional steps for the
<span class="w"> </span>        categorical features.

<span class="gu">@@ -477,17 +768,59 @@ class SMOTENC(SMOTE):</span>
<span class="w"> </span>        categorical features are mapped to the most frequent nearest neighbors
<span class="w"> </span>        of the majority class.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        rng = check_random_state(self.random_state)</span>
<span class="gi">+        X_new = super()._generate_samples(X, nn_data, nn_num, rows, cols, steps)</span>
<span class="gi">+        # change in sparsity structure more efficient with LIL than CSR</span>
<span class="gi">+        X_new = X_new.tolil() if sparse.issparse(X_new) else X_new</span>
<span class="gi">+</span>
<span class="gi">+        # convert to dense array since scipy.sparse doesn&#39;t handle 3D</span>
<span class="gi">+        nn_data = nn_data.toarray() if sparse.issparse(nn_data) else nn_data</span>
<span class="gi">+</span>
<span class="gi">+        # In the case that the median std was equal to zeros, we have to</span>
<span class="gi">+        # create non-null entry based on the encoded of OHE</span>
<span class="gi">+        if math.isclose(self.median_std_[y_type], 0):</span>
<span class="gi">+            nn_data[</span>
<span class="gi">+                :, self.continuous_features_.size :</span>
<span class="gi">+            ] = self._X_categorical_minority_encoded</span>
<span class="gi">+</span>
<span class="gi">+        all_neighbors = nn_data[nn_num[rows]]</span>
<span class="gi">+</span>
<span class="gi">+        categories_size = [self.continuous_features_.size] + [</span>
<span class="gi">+            cat.size for cat in self.categorical_encoder_.categories_</span>
<span class="gi">+        ]</span>
<span class="gi">+</span>
<span class="gi">+        for start_idx, end_idx in zip(</span>
<span class="gi">+            np.cumsum(categories_size)[:-1], np.cumsum(categories_size)[1:]</span>
<span class="gi">+        ):</span>
<span class="gi">+            col_maxs = all_neighbors[:, :, start_idx:end_idx].sum(axis=1)</span>
<span class="gi">+            # tie breaking argmax</span>
<span class="gi">+            is_max = np.isclose(col_maxs, col_maxs.max(axis=1, keepdims=True))</span>
<span class="gi">+            max_idxs = rng.permutation(np.argwhere(is_max))</span>
<span class="gi">+            xs, idx_sels = np.unique(max_idxs[:, 0], return_index=True)</span>
<span class="gi">+            col_sels = max_idxs[idx_sels, 1]</span>
<span class="gi">+</span>
<span class="gi">+            ys = start_idx + col_sels</span>
<span class="gi">+            X_new[:, start_idx:end_idx] = 0</span>
<span class="gi">+            X_new[xs, ys] = 1</span>
<span class="gi">+</span>
<span class="gi">+        return X_new</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def ohe_(self):
<span class="w"> </span>        &quot;&quot;&quot;One-hot encoder used to encode the categorical features.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;&#39;ohe_&#39; attribute has been deprecated in 0.11 and will be removed &quot;</span>
<span class="gi">+            &quot;in 0.13. Use &#39;categorical_encoder_&#39; instead.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.categorical_encoder_</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SMOTEN(SMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Synthetic Minority Over-sampling Technique for Nominal.

<span class="gu">@@ -595,20 +928,128 @@ class SMOTEN(SMOTE):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(f&quot;Class counts after resampling {{Counter(y_res)}}&quot;)
<span class="w"> </span>    Class counts after resampling Counter({{0: 40, 1: 40}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**SMOTE._parameter_constraints,</span>
<span class="gd">-        &#39;categorical_encoder&#39;: [HasMethods([&#39;fit_transform&#39;,</span>
<span class="gd">-        &#39;inverse_transform&#39;]), None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, categorical_encoder=None, *, sampling_strategy=</span>
<span class="gd">-        &#39;auto&#39;, random_state=None, k_neighbors=5, n_jobs=None):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **SMOTE._parameter_constraints,</span>
<span class="gi">+        &quot;categorical_encoder&quot;: [</span>
<span class="gi">+            HasMethods([&quot;fit_transform&quot;, &quot;inverse_transform&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        categorical_encoder=None,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.categorical_encoder = categorical_encoder

<span class="w"> </span>    def _check_X_y(self, X, y):
<span class="w"> </span>        &quot;&quot;&quot;Check should accept strings and not sparse matrices.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X,</span>
<span class="gi">+            y,</span>
<span class="gi">+            reset=True,</span>
<span class="gi">+            dtype=None,</span>
<span class="gi">+            accept_sparse=[&quot;csr&quot;, &quot;csc&quot;],</span>
<span class="gi">+        )</span>
<span class="gi">+        return X, y, binarize_y</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Force to use precomputed distance matrix.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+        self.nn_k_.set_params(metric=&quot;precomputed&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def _make_samples(self, X_class, klass, y_dtype, nn_indices, n_samples):</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        # generate sample indices that will be used to generate new samples</span>
<span class="gi">+        samples_indices = random_state.choice(</span>
<span class="gi">+            np.arange(X_class.shape[0]), size=n_samples, replace=True</span>
<span class="gi">+        )</span>
<span class="gi">+        # for each drawn samples, select its k-neighbors and generate a sample</span>
<span class="gi">+        # where for each feature individually, each category generated is the</span>
<span class="gi">+        # most common category</span>
<span class="gi">+        X_new = np.squeeze(</span>
<span class="gi">+            _mode(X_class[nn_indices[samples_indices]], axis=1).mode, axis=1</span>
<span class="gi">+        )</span>
<span class="gi">+        y_new = np.full(n_samples, fill_value=klass, dtype=y_dtype)</span>
<span class="gi">+        return X_new, y_new</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_sparse_format = X.format</span>
<span class="gi">+            X = X.toarray()</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;Passing a sparse matrix to SMOTEN is not really efficient since it is&quot;</span>
<span class="gi">+                &quot; converted to a dense array internally.&quot;,</span>
<span class="gi">+                DataConversionWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_sparse_format = None</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = [X.copy()]</span>
<span class="gi">+        y_resampled = [y.copy()]</span>
<span class="gi">+</span>
<span class="gi">+        if self.categorical_encoder is None:</span>
<span class="gi">+            self.categorical_encoder_ = OrdinalEncoder(dtype=np.int32)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.categorical_encoder_ = clone(self.categorical_encoder)</span>
<span class="gi">+        X_encoded = self.categorical_encoder_.fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+        vdm = ValueDifferenceMetric(</span>
<span class="gi">+            n_categories=[len(cat) for cat in self.categorical_encoder_.categories_]</span>
<span class="gi">+        ).fit(X_encoded, y)</span>
<span class="gi">+</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X_encoded, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            X_class_dist = vdm.pairwise(X_class)</span>
<span class="gi">+            self.nn_k_.fit(X_class_dist)</span>
<span class="gi">+            # the kneigbors search will include the sample itself which is</span>
<span class="gi">+            # expected from the original algorithm</span>
<span class="gi">+            nn_indices = self.nn_k_.kneighbors(X_class_dist, return_distance=False)</span>
<span class="gi">+            X_new, y_new = self._make_samples(</span>
<span class="gi">+                X_class, class_sample, y.dtype, nn_indices, n_samples</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            X_new = self.categorical_encoder_.inverse_transform(X_new)</span>
<span class="gi">+            X_resampled.append(X_new)</span>
<span class="gi">+            y_resampled.append(y_new)</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = np.vstack(X_resampled)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        if X_sparse_format == &quot;csr&quot;:</span>
<span class="gi">+            return sparse.csr_matrix(X_resampled), y_resampled</span>
<span class="gi">+        elif X_sparse_format == &quot;csc&quot;:</span>
<span class="gi">+            return sparse.csc_matrix(X_resampled), y_resampled</span>
<span class="gi">+        else:</span>
<span class="gi">+            return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;X_types&quot;: [&quot;2darray&quot;, &quot;dataframe&quot;, &quot;string&quot;]}</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/cluster.py b/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gh">index 31fb344..2852cfd 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/cluster.py</span>
<span class="gu">@@ -1,12 +1,20 @@</span>
<span class="w"> </span>&quot;&quot;&quot;SMOTE variant employing some clustering before the generation.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Fernando Nogueira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import math
<span class="w"> </span>import numbers
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.cluster import MiniBatchKMeans
<span class="w"> </span>from sklearn.metrics import pairwise_distances
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval, StrOptions
<span class="gu">@@ -14,9 +22,11 @@ from ..base import BaseOverSampler</span>
<span class="w"> </span>from .base import BaseSMOTE


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class KMeansSMOTE(BaseSMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Apply a KMeans clustering before to over-sample using SMOTE.

<span class="gu">@@ -135,21 +145,163 @@ class KMeansSMOTE(BaseSMOTE):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&quot;More 0 samples: %s&quot; % ((y_res == 0).sum() &gt; (y == 0).sum()))
<span class="w"> </span>    More 0 samples: True
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseSMOTE._parameter_constraints,</span>
<span class="gd">-        &#39;kmeans_estimator&#39;: [HasMethods([&#39;fit&#39;, &#39;predict&#39;]), Interval(</span>
<span class="gd">-        numbers.Integral, 1, None, closed=&#39;left&#39;), None],</span>
<span class="gd">-        &#39;cluster_balance_threshold&#39;: [StrOptions({&#39;auto&#39;}), numbers.Real],</span>
<span class="gd">-        &#39;density_exponent&#39;: [StrOptions({&#39;auto&#39;}), numbers.Real]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        k_neighbors=2, n_jobs=None, kmeans_estimator=None,</span>
<span class="gd">-        cluster_balance_threshold=&#39;auto&#39;, density_exponent=&#39;auto&#39;):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseSMOTE._parameter_constraints,</span>
<span class="gi">+        &quot;kmeans_estimator&quot;: [</span>
<span class="gi">+            HasMethods([&quot;fit&quot;, &quot;predict&quot;]),</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;cluster_balance_threshold&quot;: [StrOptions({&quot;auto&quot;}), numbers.Real],</span>
<span class="gi">+        &quot;density_exponent&quot;: [StrOptions({&quot;auto&quot;}), numbers.Real],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=2,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        kmeans_estimator=None,</span>
<span class="gi">+        cluster_balance_threshold=&quot;auto&quot;,</span>
<span class="gi">+        density_exponent=&quot;auto&quot;,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.kmeans_estimator = kmeans_estimator
<span class="w"> </span>        self.cluster_balance_threshold = cluster_balance_threshold
<span class="w"> </span>        self.density_exponent = density_exponent

<span class="gi">+    def _validate_estimator(self):</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+        if self.kmeans_estimator is None:</span>
<span class="gi">+            self.kmeans_estimator_ = MiniBatchKMeans(random_state=self.random_state)</span>
<span class="gi">+        elif isinstance(self.kmeans_estimator, int):</span>
<span class="gi">+            self.kmeans_estimator_ = MiniBatchKMeans(</span>
<span class="gi">+                n_clusters=self.kmeans_estimator,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.kmeans_estimator_ = clone(self.kmeans_estimator)</span>
<span class="gi">+</span>
<span class="gi">+        self.cluster_balance_threshold_ = (</span>
<span class="gi">+            self.cluster_balance_threshold</span>
<span class="gi">+            if self.kmeans_estimator_.n_clusters != 1</span>
<span class="gi">+            else -np.inf</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="w"> </span>    def _find_cluster_sparsity(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Compute the cluster sparsity.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        euclidean_distances = pairwise_distances(</span>
<span class="gi">+            X, metric=&quot;euclidean&quot;, n_jobs=self.n_jobs</span>
<span class="gi">+        )</span>
<span class="gi">+        # negate diagonal elements</span>
<span class="gi">+        for ind in range(X.shape[0]):</span>
<span class="gi">+            euclidean_distances[ind, ind] = 0</span>
<span class="gi">+</span>
<span class="gi">+        non_diag_elements = (X.shape[0] ** 2) - X.shape[0]</span>
<span class="gi">+        mean_distance = euclidean_distances.sum() / non_diag_elements</span>
<span class="gi">+        exponent = (</span>
<span class="gi">+            math.log(X.shape[0], 1.6) ** 1.8 * 0.16</span>
<span class="gi">+            if self.density_exponent == &quot;auto&quot;</span>
<span class="gi">+            else self.density_exponent</span>
<span class="gi">+        )</span>
<span class="gi">+        return (mean_distance**exponent) / X.shape[0]</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        X_resampled = X.copy()</span>
<span class="gi">+        y_resampled = y.copy()</span>
<span class="gi">+        total_inp_samples = sum(self.sampling_strategy_.values())</span>
<span class="gi">+</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            X_clusters = self.kmeans_estimator_.fit_predict(X)</span>
<span class="gi">+            valid_clusters = []</span>
<span class="gi">+            cluster_sparsities = []</span>
<span class="gi">+</span>
<span class="gi">+            # identify cluster which are answering the requirements</span>
<span class="gi">+            for cluster_idx in range(self.kmeans_estimator_.n_clusters):</span>
<span class="gi">+                cluster_mask = np.flatnonzero(X_clusters == cluster_idx)</span>
<span class="gi">+</span>
<span class="gi">+                if cluster_mask.size == 0:</span>
<span class="gi">+                    # empty cluster</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                X_cluster = _safe_indexing(X, cluster_mask)</span>
<span class="gi">+                y_cluster = _safe_indexing(y, cluster_mask)</span>
<span class="gi">+</span>
<span class="gi">+                cluster_class_mean = (y_cluster == class_sample).mean()</span>
<span class="gi">+</span>
<span class="gi">+                if self.cluster_balance_threshold_ == &quot;auto&quot;:</span>
<span class="gi">+                    balance_threshold = n_samples / total_inp_samples / 2</span>
<span class="gi">+                else:</span>
<span class="gi">+                    balance_threshold = self.cluster_balance_threshold_</span>
<span class="gi">+</span>
<span class="gi">+                # the cluster is already considered balanced</span>
<span class="gi">+                if cluster_class_mean &lt; balance_threshold:</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                # not enough samples to apply SMOTE</span>
<span class="gi">+                anticipated_samples = cluster_class_mean * X_cluster.shape[0]</span>
<span class="gi">+                if anticipated_samples &lt; self.nn_k_.n_neighbors:</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                X_cluster_class = _safe_indexing(</span>
<span class="gi">+                    X_cluster, np.flatnonzero(y_cluster == class_sample)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                valid_clusters.append(cluster_mask)</span>
<span class="gi">+                cluster_sparsities.append(self._find_cluster_sparsity(X_cluster_class))</span>
<span class="gi">+</span>
<span class="gi">+            cluster_sparsities = np.array(cluster_sparsities)</span>
<span class="gi">+            cluster_weights = cluster_sparsities / cluster_sparsities.sum()</span>
<span class="gi">+</span>
<span class="gi">+            if not valid_clusters:</span>
<span class="gi">+                raise RuntimeError(</span>
<span class="gi">+                    f&quot;No clusters found with sufficient samples of &quot;</span>
<span class="gi">+                    f&quot;class {class_sample}. Try lowering the &quot;</span>
<span class="gi">+                    f&quot;cluster_balance_threshold or increasing the number of &quot;</span>
<span class="gi">+                    f&quot;clusters.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            for valid_cluster_idx, valid_cluster in enumerate(valid_clusters):</span>
<span class="gi">+                X_cluster = _safe_indexing(X, valid_cluster)</span>
<span class="gi">+                y_cluster = _safe_indexing(y, valid_cluster)</span>
<span class="gi">+</span>
<span class="gi">+                X_cluster_class = _safe_indexing(</span>
<span class="gi">+                    X_cluster, np.flatnonzero(y_cluster == class_sample)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                self.nn_k_.fit(X_cluster_class)</span>
<span class="gi">+                nns = self.nn_k_.kneighbors(X_cluster_class, return_distance=False)[</span>
<span class="gi">+                    :, 1:</span>
<span class="gi">+                ]</span>
<span class="gi">+</span>
<span class="gi">+                cluster_n_samples = int(</span>
<span class="gi">+                    math.ceil(n_samples * cluster_weights[valid_cluster_idx])</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                X_new, y_new = self._make_samples(</span>
<span class="gi">+                    X_cluster_class,</span>
<span class="gi">+                    y.dtype,</span>
<span class="gi">+                    class_sample,</span>
<span class="gi">+                    X_cluster_class,</span>
<span class="gi">+                    nns,</span>
<span class="gi">+                    cluster_n_samples,</span>
<span class="gi">+                    1.0,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                stack = [np.vstack, sparse.vstack][int(sparse.issparse(X_new))]</span>
<span class="gi">+                X_resampled = stack((X_resampled, X_new))</span>
<span class="gi">+                y_resampled = np.hstack((y_resampled, y_new))</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/filter.py b/imblearn/over_sampling/_smote/filter.py</span>
<span class="gh">index 454b627..2916b68 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/filter.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/filter.py</span>
<span class="gu">@@ -1,11 +1,20 @@</span>
<span class="gd">-&quot;&quot;&quot;SMOTE variant applying some filtering before the generation process.&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;SMOTE variant applying some filtering before the generation process.&quot;&quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Fernando Nogueira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+#          Dzianis Dudnik</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.svm import SVC
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution, check_neighbors_object
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval, StrOptions
<span class="gu">@@ -13,9 +22,11 @@ from ..base import BaseOverSampler</span>
<span class="w"> </span>from .base import BaseSMOTE


<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class BorderlineSMOTE(BaseSMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Over-sampling using Borderline SMOTE.

<span class="gu">@@ -145,22 +156,104 @@ class BorderlineSMOTE(BaseSMOTE):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseSMOTE._parameter_constraints,</span>
<span class="gd">-        &#39;m_neighbors&#39;: [Interval(numbers.Integral, 1, None, closed=&#39;left&#39;),</span>
<span class="gd">-        HasMethods([&#39;kneighbors&#39;, &#39;kneighbors_graph&#39;])], &#39;kind&#39;: [</span>
<span class="gd">-        StrOptions({&#39;borderline-1&#39;, &#39;borderline-2&#39;})]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        k_neighbors=5, n_jobs=None, m_neighbors=10, kind=&#39;borderline-1&#39;):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseSMOTE._parameter_constraints,</span>
<span class="gi">+        &quot;m_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;kind&quot;: [StrOptions({&quot;borderline-1&quot;, &quot;borderline-2&quot;})],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        m_neighbors=10,</span>
<span class="gi">+        kind=&quot;borderline-1&quot;,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.m_neighbors = m_neighbors
<span class="w"> </span>        self.kind = kind

<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseOverSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+    def _validate_estimator(self):</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+        self.nn_m_ = check_neighbors_object(</span>
<span class="gi">+            &quot;m_neighbors&quot;, self.m_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = X.copy()</span>
<span class="gi">+        y_resampled = y.copy()</span>
<span class="gi">+</span>
<span class="gi">+        self.in_danger_indices = {}</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_m_.fit(X)</span>
<span class="gi">+            mask_danger = self._in_danger_noise(</span>
<span class="gi">+                self.nn_m_, X_class, class_sample, y, kind=&quot;danger&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            if not any(mask_danger):</span>
<span class="gi">+                continue</span>
<span class="gi">+            X_danger = _safe_indexing(X_class, mask_danger)</span>
<span class="gi">+            self.in_danger_indices[class_sample] = target_class_indices[mask_danger]</span>
<span class="gi">+</span>
<span class="gi">+            if self.kind == &quot;borderline-1&quot;:</span>
<span class="gi">+                X_to_sample_from = X_class  # consider the positive class only</span>
<span class="gi">+                y_to_check_neighbors = None</span>
<span class="gi">+            else:  # self.kind == &quot;borderline-2&quot;</span>
<span class="gi">+                X_to_sample_from = X  # consider the whole dataset</span>
<span class="gi">+                y_to_check_neighbors = y</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_k_.fit(X_to_sample_from)</span>
<span class="gi">+            nns = self.nn_k_.kneighbors(X_danger, return_distance=False)[:, 1:]</span>
<span class="gi">+            X_new, y_new = self._make_samples(</span>
<span class="gi">+                X_danger,</span>
<span class="gi">+                y.dtype,</span>
<span class="gi">+                class_sample,</span>
<span class="gi">+                X_to_sample_from,</span>
<span class="gi">+                nns,</span>
<span class="gi">+                n_samples,</span>
<span class="gi">+                y=y_to_check_neighbors,</span>
<span class="gi">+            )</span>
<span class="gi">+            if sparse.issparse(X_new):</span>
<span class="gi">+                X_resampled = sparse.vstack([X_resampled, X_new])</span>
<span class="gi">+            else:</span>
<span class="gi">+                X_resampled = np.vstack((X_resampled, X_new))</span>
<span class="gi">+            y_resampled = np.hstack((y_resampled, y_new))</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class SVMSMOTE(BaseSMOTE):
<span class="w"> </span>    &quot;&quot;&quot;Over-sampling using SVM-SMOTE.

<span class="gu">@@ -295,17 +388,151 @@ class SVMSMOTE(BaseSMOTE):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 900, 1: 900}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseSMOTE._parameter_constraints,</span>
<span class="gd">-        &#39;m_neighbors&#39;: [Interval(numbers.Integral, 1, None, closed=&#39;left&#39;),</span>
<span class="gd">-        HasMethods([&#39;kneighbors&#39;, &#39;kneighbors_graph&#39;])], &#39;svm_estimator&#39;: [</span>
<span class="gd">-        HasMethods([&#39;fit&#39;, &#39;predict&#39;]), None], &#39;out_step&#39;: [Interval(</span>
<span class="gd">-        numbers.Real, 0, 1, closed=&#39;both&#39;)]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        k_neighbors=5, n_jobs=None, m_neighbors=10, svm_estimator=None,</span>
<span class="gd">-        out_step=0.5):</span>
<span class="gd">-        super().__init__(sampling_strategy=sampling_strategy, random_state=</span>
<span class="gd">-            random_state, k_neighbors=k_neighbors, n_jobs=n_jobs)</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseSMOTE._parameter_constraints,</span>
<span class="gi">+        &quot;m_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;svm_estimator&quot;: [HasMethods([&quot;fit&quot;, &quot;predict&quot;]), None],</span>
<span class="gi">+        &quot;out_step&quot;: [Interval(numbers.Real, 0, 1, closed=&quot;both&quot;)],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+        m_neighbors=10,</span>
<span class="gi">+        svm_estimator=None,</span>
<span class="gi">+        out_step=0.5,</span>
<span class="gi">+    ):</span>
<span class="gi">+        super().__init__(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+            k_neighbors=k_neighbors,</span>
<span class="gi">+            n_jobs=n_jobs,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        self.m_neighbors = m_neighbors
<span class="w"> </span>        self.svm_estimator = svm_estimator
<span class="w"> </span>        self.out_step = out_step
<span class="gi">+</span>
<span class="gi">+    def _validate_estimator(self):</span>
<span class="gi">+        super()._validate_estimator()</span>
<span class="gi">+        self.nn_m_ = check_neighbors_object(</span>
<span class="gi">+            &quot;m_neighbors&quot;, self.m_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        if self.svm_estimator is None:</span>
<span class="gi">+            self.svm_estimator_ = SVC(gamma=&quot;scale&quot;, random_state=self.random_state)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.svm_estimator_ = clone(self.svm_estimator)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # FIXME: to be removed in 0.12</span>
<span class="gi">+        if self.n_jobs is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The parameter `n_jobs` has been deprecated in 0.10 and will be &quot;</span>
<span class="gi">+                &quot;removed in 0.12. You can pass an nearest neighbors estimator where &quot;</span>
<span class="gi">+                &quot;`n_jobs` is already set instead.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        X_resampled = X.copy()</span>
<span class="gi">+        y_resampled = y.copy()</span>
<span class="gi">+</span>
<span class="gi">+        for class_sample, n_samples in self.sampling_strategy_.items():</span>
<span class="gi">+            if n_samples == 0:</span>
<span class="gi">+                continue</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == class_sample)</span>
<span class="gi">+            X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+            self.svm_estimator_.fit(X, y)</span>
<span class="gi">+            if not hasattr(self.svm_estimator_, &quot;support_&quot;):</span>
<span class="gi">+                raise RuntimeError(</span>
<span class="gi">+                    &quot;`svm_estimator` is required to exposed a `support_` fitted &quot;</span>
<span class="gi">+                    &quot;attribute. Such estimator belongs to the familly of Support &quot;</span>
<span class="gi">+                    &quot;Vector Machine.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            support_index = self.svm_estimator_.support_[</span>
<span class="gi">+                y[self.svm_estimator_.support_] == class_sample</span>
<span class="gi">+            ]</span>
<span class="gi">+            support_vector = _safe_indexing(X, support_index)</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_m_.fit(X)</span>
<span class="gi">+            noise_bool = self._in_danger_noise(</span>
<span class="gi">+                self.nn_m_, support_vector, class_sample, y, kind=&quot;noise&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            support_vector = _safe_indexing(</span>
<span class="gi">+                support_vector, np.flatnonzero(np.logical_not(noise_bool))</span>
<span class="gi">+            )</span>
<span class="gi">+            if support_vector.shape[0] == 0:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;All support vectors are considered as noise. SVM-SMOTE is not &quot;</span>
<span class="gi">+                    &quot;adapted to your dataset. Try another SMOTE variant.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            danger_bool = self._in_danger_noise(</span>
<span class="gi">+                self.nn_m_, support_vector, class_sample, y, kind=&quot;danger&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+            safety_bool = np.logical_not(danger_bool)</span>
<span class="gi">+</span>
<span class="gi">+            self.nn_k_.fit(X_class)</span>
<span class="gi">+            fractions = random_state.beta(10, 10)</span>
<span class="gi">+            n_generated_samples = int(fractions * (n_samples + 1))</span>
<span class="gi">+            if np.count_nonzero(danger_bool) &gt; 0:</span>
<span class="gi">+                nns = self.nn_k_.kneighbors(</span>
<span class="gi">+                    _safe_indexing(support_vector, np.flatnonzero(danger_bool)),</span>
<span class="gi">+                    return_distance=False,</span>
<span class="gi">+                )[:, 1:]</span>
<span class="gi">+</span>
<span class="gi">+                X_new_1, y_new_1 = self._make_samples(</span>
<span class="gi">+                    _safe_indexing(support_vector, np.flatnonzero(danger_bool)),</span>
<span class="gi">+                    y.dtype,</span>
<span class="gi">+                    class_sample,</span>
<span class="gi">+                    X_class,</span>
<span class="gi">+                    nns,</span>
<span class="gi">+                    n_generated_samples,</span>
<span class="gi">+                    step_size=1.0,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if np.count_nonzero(safety_bool) &gt; 0:</span>
<span class="gi">+                nns = self.nn_k_.kneighbors(</span>
<span class="gi">+                    _safe_indexing(support_vector, np.flatnonzero(safety_bool)),</span>
<span class="gi">+                    return_distance=False,</span>
<span class="gi">+                )[:, 1:]</span>
<span class="gi">+</span>
<span class="gi">+                X_new_2, y_new_2 = self._make_samples(</span>
<span class="gi">+                    _safe_indexing(support_vector, np.flatnonzero(safety_bool)),</span>
<span class="gi">+                    y.dtype,</span>
<span class="gi">+                    class_sample,</span>
<span class="gi">+                    X_class,</span>
<span class="gi">+                    nns,</span>
<span class="gi">+                    n_samples - n_generated_samples,</span>
<span class="gi">+                    step_size=-self.out_step,</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if np.count_nonzero(danger_bool) &gt; 0 and np.count_nonzero(safety_bool) &gt; 0:</span>
<span class="gi">+                if sparse.issparse(X_resampled):</span>
<span class="gi">+                    X_resampled = sparse.vstack([X_resampled, X_new_1, X_new_2])</span>
<span class="gi">+                else:</span>
<span class="gi">+                    X_resampled = np.vstack((X_resampled, X_new_1, X_new_2))</span>
<span class="gi">+                y_resampled = np.concatenate((y_resampled, y_new_1, y_new_2), axis=0)</span>
<span class="gi">+            elif np.count_nonzero(danger_bool) == 0:</span>
<span class="gi">+                if sparse.issparse(X_resampled):</span>
<span class="gi">+                    X_resampled = sparse.vstack([X_resampled, X_new_2])</span>
<span class="gi">+                else:</span>
<span class="gi">+                    X_resampled = np.vstack((X_resampled, X_new_2))</span>
<span class="gi">+                y_resampled = np.concatenate((y_resampled, y_new_2), axis=0)</span>
<span class="gi">+            elif np.count_nonzero(safety_bool) == 0:</span>
<span class="gi">+                if sparse.issparse(X_resampled):</span>
<span class="gi">+                    X_resampled = sparse.vstack([X_resampled, X_new_1])</span>
<span class="gi">+                else:</span>
<span class="gi">+                    X_resampled = np.vstack((X_resampled, X_new_1))</span>
<span class="gi">+                y_resampled = np.concatenate((y_resampled, y_new_1), axis=0)</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_borderline_smote.py b/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gh">index b11e0ea..0d85c4d 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_borderline_smote.py</span>
<span class="gu">@@ -1,17 +1,36 @@</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.linear_model import LogisticRegression
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import BorderlineSMOTE


<span class="gd">-@pytest.mark.parametrize(&#39;kind&#39;, [&#39;borderline-1&#39;, &#39;borderline-2&#39;])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;kind&quot;, [&quot;borderline-1&quot;, &quot;borderline-2&quot;])</span>
<span class="w"> </span>def test_borderline_smote_no_in_danger_samples(kind):
<span class="w"> </span>    &quot;&quot;&quot;Check that the algorithm behave properly even on a dataset without any sample
<span class="w"> </span>    in danger.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=500,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.2, 0.7],</span>
<span class="gi">+        class_sep=1.5,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = BorderlineSMOTE(kind=kind, m_neighbors=3, k_neighbors=5, random_state=0)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X, X_res)</span>
<span class="gi">+    assert_allclose(y, y_res)</span>
<span class="gi">+    assert not smote.in_danger_indices</span>


<span class="w"> </span>def test_borderline_smote_kind():
<span class="gu">@@ -21,4 +40,71 @@ def test_borderline_smote_kind():</span>
<span class="w"> </span>    &quot;borderline-1&quot;. We generate an example where a logistic regression will perform
<span class="w"> </span>    worse on &quot;borderline-2&quot; than on &quot;borderline-1&quot;.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=500,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.2, 0.7],</span>
<span class="gi">+        class_sep=1.0,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = BorderlineSMOTE(</span>
<span class="gi">+        kind=&quot;borderline-1&quot;, m_neighbors=9, k_neighbors=5, random_state=0</span>
<span class="gi">+    )</span>
<span class="gi">+    X_res_borderline_1, y_res_borderline_1 = smote.fit_resample(X, y)</span>
<span class="gi">+    smote.set_params(kind=&quot;borderline-2&quot;)</span>
<span class="gi">+    X_res_borderline_2, y_res_borderline_2 = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    score_borderline_1 = (</span>
<span class="gi">+        LogisticRegression()</span>
<span class="gi">+        .fit(X_res_borderline_1, y_res_borderline_1)</span>
<span class="gi">+        .score(X_res_borderline_1, y_res_borderline_1)</span>
<span class="gi">+    )</span>
<span class="gi">+    score_borderline_2 = (</span>
<span class="gi">+        LogisticRegression()</span>
<span class="gi">+        .fit(X_res_borderline_2, y_res_borderline_2)</span>
<span class="gi">+        .score(X_res_borderline_2, y_res_borderline_2)</span>
<span class="gi">+    )</span>
<span class="gi">+    assert score_borderline_1 &gt; score_borderline_2</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_borderline_smote_in_danger():</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=500,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.2, 0.7],</span>
<span class="gi">+        class_sep=0.8,</span>
<span class="gi">+        random_state=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = BorderlineSMOTE(</span>
<span class="gi">+        kind=&quot;borderline-1&quot;,</span>
<span class="gi">+        m_neighbors=9,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    _, y_res_1 = smote.fit_resample(X, y)</span>
<span class="gi">+    in_danger_indices_borderline_1 = smote.in_danger_indices</span>
<span class="gi">+    smote.set_params(kind=&quot;borderline-2&quot;)</span>
<span class="gi">+    _, y_res_2 = smote.fit_resample(X, y)</span>
<span class="gi">+    in_danger_indices_borderline_2 = smote.in_danger_indices</span>
<span class="gi">+</span>
<span class="gi">+    for key1, key2 in zip(</span>
<span class="gi">+        in_danger_indices_borderline_1, in_danger_indices_borderline_2</span>
<span class="gi">+    ):</span>
<span class="gi">+        assert_array_equal(</span>
<span class="gi">+            in_danger_indices_borderline_1[key1], in_danger_indices_borderline_2[key2]</span>
<span class="gi">+        )</span>
<span class="gi">+    assert len(in_danger_indices_borderline_1) == len(in_danger_indices_borderline_2)</span>
<span class="gi">+    counter = Counter(y_res_1)</span>
<span class="gi">+    assert counter[0] == counter[1] == counter[2]</span>
<span class="gi">+    counter = Counter(y_res_2)</span>
<span class="gi">+    assert counter[0] == counter[1] == counter[2]</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_kmeans_smote.py b/imblearn/over_sampling/_smote/tests/test_kmeans_smote.py</span>
<span class="gh">index e69baba..71fa47c 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_kmeans_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_kmeans_smote.py</span>
<span class="gu">@@ -4,4 +4,105 @@ from sklearn.cluster import KMeans, MiniBatchKMeans</span>
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import SMOTE, KMeansSMOTE
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    X = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_kmeans_smote(data):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    kmeans_smote = KMeansSMOTE(</span>
<span class="gi">+        kmeans_estimator=1,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+        cluster_balance_threshold=0.0,</span>
<span class="gi">+        k_neighbors=5,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = SMOTE(random_state=42)</span>
<span class="gi">+</span>
<span class="gi">+    X_res_1, y_res_1 = kmeans_smote.fit_resample(X, y)</span>
<span class="gi">+    X_res_2, y_res_2 = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X_res_1, X_res_2)</span>
<span class="gi">+    assert_array_equal(y_res_1, y_res_2)</span>
<span class="gi">+</span>
<span class="gi">+    assert kmeans_smote.nn_k_.n_neighbors == 6</span>
<span class="gi">+    assert kmeans_smote.kmeans_estimator_.n_clusters == 1</span>
<span class="gi">+    assert &quot;batch_size&quot; in kmeans_smote.kmeans_estimator_.get_params()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+@pytest.mark.parametrize(&quot;k_neighbors&quot;, [2, NearestNeighbors(n_neighbors=3)])</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;kmeans_estimator&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        3,</span>
<span class="gi">+        KMeans(n_clusters=3, n_init=1, random_state=42),</span>
<span class="gi">+        MiniBatchKMeans(n_clusters=3, n_init=1, random_state=42),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_sample_kmeans_custom(data, k_neighbors, kmeans_estimator):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    kmeans_smote = KMeansSMOTE(</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+        kmeans_estimator=kmeans_estimator,</span>
<span class="gi">+        k_neighbors=k_neighbors,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)</span>
<span class="gi">+    assert X_resampled.shape == (24, 2)</span>
<span class="gi">+    assert y_resampled.shape == (24,)</span>
<span class="gi">+</span>
<span class="gi">+    assert kmeans_smote.nn_k_.n_neighbors == 3</span>
<span class="gi">+    assert kmeans_smote.kmeans_estimator_.n_clusters == 3</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_sample_kmeans_not_enough_clusters(data):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    smote = KMeansSMOTE(cluster_balance_threshold=10, random_state=42)</span>
<span class="gi">+    with pytest.raises(RuntimeError):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;density_exponent&quot;, [&quot;auto&quot;, 10])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;cluster_balance_threshold&quot;, [&quot;auto&quot;, 0.1])</span>
<span class="gi">+def test_sample_kmeans_density_estimation(density_exponent, cluster_balance_threshold):</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=10_000, n_classes=2, weights=[0.3, 0.7], random_state=42</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = KMeansSMOTE(</span>
<span class="gi">+        kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=42),</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        density_exponent=density_exponent,</span>
<span class="gi">+        cluster_balance_threshold=cluster_balance_threshold,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote.fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_smote.py b/imblearn/over_sampling/_smote/tests/test_smote.py</span>
<span class="gh">index f8343f1..060ac8c 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_smote.py</span>
<span class="gu">@@ -1,17 +1,149 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module SMOTE.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import SMOTE
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.11622591, -0.0317206], [0.77481731, 0.60935141], [</span>
<span class="gd">-    1.25192108, -0.22367336], [0.53366841, -0.30312976], [1.52091956, -</span>
<span class="gd">-    0.49283504], [-0.28162401, -2.10400981], [0.83680821, 1.72827342], [</span>
<span class="gd">-    0.3084254, 0.33299982], [0.70472253, -0.73309052], [0.28893132, -</span>
<span class="gd">-    0.38761769], [1.15514042, 0.0129463], [0.88407872, 0.35454207], [</span>
<span class="gd">-    1.31301027, -0.92648734], [-1.11515198, -0.93689695], [-0.18410027, -</span>
<span class="gd">-    0.45194484], [0.9281014, 0.53085498], [-0.14374509, 0.27370049], [-</span>
<span class="gd">-    0.41635887, -0.38299653], [0.08711622, 0.93259929], [1.70580611, -</span>
<span class="gd">-    0.11219234]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.11622591, -0.0317206],</span>
<span class="gi">+        [0.77481731, 0.60935141],</span>
<span class="gi">+        [1.25192108, -0.22367336],</span>
<span class="gi">+        [0.53366841, -0.30312976],</span>
<span class="gi">+        [1.52091956, -0.49283504],</span>
<span class="gi">+        [-0.28162401, -2.10400981],</span>
<span class="gi">+        [0.83680821, 1.72827342],</span>
<span class="gi">+        [0.3084254, 0.33299982],</span>
<span class="gi">+        [0.70472253, -0.73309052],</span>
<span class="gi">+        [0.28893132, -0.38761769],</span>
<span class="gi">+        [1.15514042, 0.0129463],</span>
<span class="gi">+        [0.88407872, 0.35454207],</span>
<span class="gi">+        [1.31301027, -0.92648734],</span>
<span class="gi">+        [-1.11515198, -0.93689695],</span>
<span class="gi">+        [-0.18410027, -0.45194484],</span>
<span class="gi">+        [0.9281014, 0.53085498],</span>
<span class="gi">+        [-0.14374509, 0.27370049],</span>
<span class="gi">+        [-0.41635887, -0.38299653],</span>
<span class="gi">+        [0.08711622, 0.93259929],</span>
<span class="gi">+        [1.70580611, -0.11219234],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular():</span>
<span class="gi">+    smote = SMOTE(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+            [0.29307743, -0.14670439],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571668],</span>
<span class="gi">+            [0.66052536, -0.28246517],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular_half():</span>
<span class="gi">+    sampling_strategy = {0: 9, 1: 12}</span>
<span class="gi">+    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+            [0.36784496, -0.1953161],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sample_regular_with_nn():</span>
<span class="gi">+    nn_k = NearestNeighbors(n_neighbors=6)</span>
<span class="gi">+    smote = SMOTE(random_state=RND_SEED, k_neighbors=nn_k)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+            [0.29307743, -0.14670439],</span>
<span class="gi">+            [0.84976473, -0.15570176],</span>
<span class="gi">+            [0.61319159, -0.11571668],</span>
<span class="gi">+            [0.66052536, -0.28246517],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_smote_nc.py b/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gh">index 06080ca..1314ea9 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_smote_nc.py</span>
<span class="gu">@@ -1,5 +1,11 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module SMOTENC.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+#          Dzianis Dudnik</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>import sklearn
<span class="gu">@@ -8,26 +14,301 @@ from sklearn.datasets import make_classification</span>
<span class="w"> </span>from sklearn.preprocessing import OneHotEncoder
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import SMOTENC
<span class="gd">-from imblearn.utils.estimator_checks import _set_checking_parameters, check_param_validation</span>
<span class="gi">+from imblearn.utils.estimator_checks import (</span>
<span class="gi">+    _set_checking_parameters,</span>
<span class="gi">+    check_param_validation,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gi">+def data_heterogneous_ordered():</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((30, 4), dtype=object)</span>
<span class="gi">+    # create 2 random continuous feature</span>
<span class="gi">+    X[:, :2] = rng.randn(30, 2)</span>
<span class="gi">+    # create a categorical feature using some string</span>
<span class="gi">+    X[:, 2] = rng.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=30).astype(object)</span>
<span class="gi">+    # create a categorical feature using some integer</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=30)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 20)</span>
<span class="gi">+    # return the categories</span>
<span class="gi">+    return X, y, [2, 3]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def data_heterogneous_unordered():</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((30, 4), dtype=object)</span>
<span class="gi">+    # create 2 random continuous feature</span>
<span class="gi">+    X[:, [1, 2]] = rng.randn(30, 2)</span>
<span class="gi">+    # create a categorical feature using some string</span>
<span class="gi">+    X[:, 0] = rng.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=30).astype(object)</span>
<span class="gi">+    # create a categorical feature using some integer</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=30)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 20)</span>
<span class="gi">+    # return the categories</span>
<span class="gi">+    return X, y, [0, 3]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def data_heterogneous_masked():</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((30, 4), dtype=object)</span>
<span class="gi">+    # create 2 random continuous feature</span>
<span class="gi">+    X[:, [1, 2]] = rng.randn(30, 2)</span>
<span class="gi">+    # create a categorical feature using some string</span>
<span class="gi">+    X[:, 0] = rng.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=30).astype(object)</span>
<span class="gi">+    # create a categorical feature using some integer</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=30)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 20)</span>
<span class="gi">+    # return the categories</span>
<span class="gi">+    return X, y, [True, False, False, True]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def data_heterogneous_unordered_multiclass():</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((50, 4), dtype=object)</span>
<span class="gi">+    # create 2 random continuous feature</span>
<span class="gi">+    X[:, [1, 2]] = rng.randn(50, 2)</span>
<span class="gi">+    # create a categorical feature using some string</span>
<span class="gi">+    X[:, 0] = rng.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=50).astype(object)</span>
<span class="gi">+    # create a categorical feature using some integer</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=50)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 15 + [2] * 25)</span>
<span class="gi">+    # return the categories</span>
<span class="gi">+    return X, y, [0, 3]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def data_sparse(format):</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((30, 4), dtype=np.float64)</span>
<span class="gi">+    # create 2 random continuous feature</span>
<span class="gi">+    X[:, [1, 2]] = rng.randn(30, 2)</span>
<span class="gi">+    # create a categorical feature using some string</span>
<span class="gi">+    X[:, 0] = rng.randint(3, size=30)</span>
<span class="gi">+    # create a categorical feature using some integer</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=30)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 20)</span>
<span class="gi">+    X = sparse.csr_matrix(X) if format == &quot;csr&quot; else sparse.csc_matrix(X)</span>
<span class="gi">+    return X, y, [0, 3]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_error():</span>
<span class="gi">+    X, y, _ = data_heterogneous_unordered()</span>
<span class="gi">+    categorical_features = [0, 10]</span>
<span class="gi">+    smote = SMOTENC(random_state=0, categorical_features=categorical_features)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;all features must be in&quot;):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;data&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        data_heterogneous_ordered(),</span>
<span class="gi">+        data_heterogneous_unordered(),</span>
<span class="gi">+        data_heterogneous_masked(),</span>
<span class="gi">+        data_sparse(&quot;csr&quot;),</span>
<span class="gi">+        data_sparse(&quot;csc&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_smotenc(data):</span>
<span class="gi">+    X, y, categorical_features = data</span>
<span class="gi">+    smote = SMOTENC(random_state=0, categorical_features=categorical_features)</span>
<span class="gi">+    X_resampled, y_resampled = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_resampled.dtype == X.dtype</span>
<span class="gi">+</span>
<span class="gi">+    categorical_features = np.array(categorical_features)</span>
<span class="gi">+    if categorical_features.dtype == bool:</span>
<span class="gi">+        categorical_features = np.flatnonzero(categorical_features)</span>
<span class="gi">+    for cat_idx in categorical_features:</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            assert set(X[:, cat_idx].data) == set(X_resampled[:, cat_idx].data)</span>
<span class="gi">+            assert X[:, cat_idx].dtype == X_resampled[:, cat_idx].dtype</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert set(X[:, cat_idx]) == set(X_resampled[:, cat_idx])</span>
<span class="gi">+            assert X[:, cat_idx].dtype == X_resampled[:, cat_idx].dtype</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(smote.median_std_, dict)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# part of the common test which apply to SMOTE-NC even if it is not default</span>
<span class="gi">+# constructible</span>
<span class="gi">+def test_smotenc_check_target_type():</span>
<span class="gi">+    X, _, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    y = np.linspace(0, 1, 30)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;Unknown label type&quot;):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    y = rng.randint(2, size=(20, 3))</span>
<span class="gi">+    msg = &quot;Multilabel and multioutput targets are not supported.&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=msg):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_samplers_one_label():</span>
<span class="gi">+    X, _, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    y = np.zeros(30)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;needs to have more than 1 class&quot;):</span>
<span class="gi">+        smote.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_fit():</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    smote.fit_resample(X, y)</span>
<span class="gi">+    assert hasattr(</span>
<span class="gi">+        smote, &quot;sampling_strategy_&quot;</span>
<span class="gi">+    ), &quot;No fitted attribute sampling_strategy_&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_fit_resample():</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    target_stats = Counter(y)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    _, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    _ = Counter(y_res)</span>
<span class="gi">+    n_samples = max(target_stats.values())</span>
<span class="gi">+    assert all(value &gt;= n_samples for value in Counter(y_res).values())</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_fit_resample_sampling_strategy():</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered_multiclass()</span>
<span class="gi">+    expected_stat = Counter(y)[1]</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    sampling_strategy = {2: 25, 0: 25}</span>
<span class="gi">+    smote.set_params(sampling_strategy=sampling_strategy)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    assert Counter(y_res)[1] == expected_stat</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_pandas():</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    # Check that the samplers handle pandas dataframe and pandas series</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered_multiclass()</span>
<span class="gi">+    X_pd = pd.DataFrame(X)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    X_res_pd, y_res_pd = smote.fit_resample(X_pd, y)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    assert_array_equal(X_res_pd.to_numpy(), X_res)</span>
<span class="gi">+    assert_allclose(y_res_pd, y_res)</span>
<span class="gi">+    assert set(smote.median_std_.keys()) == {0, 1}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smotenc_preserve_dtype():</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=50,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_informative=4,</span>
<span class="gi">+        weights=[0.2, 0.3, 0.5],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    # Cast X and y to not default dtype</span>
<span class="gi">+    X = X.astype(np.float32)</span>
<span class="gi">+    y = y.astype(np.int32)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=[1], random_state=0)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    assert X.dtype == X_res.dtype, &quot;X dtype is not preserved&quot;</span>
<span class="gi">+    assert y.dtype == y_res.dtype, &quot;y dtype is not preserved&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;categorical_features&quot;, [[True, True, True], [0, 1, 2]])</span>
<span class="gi">+def test_smotenc_raising_error_all_categorical(categorical_features):</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_features=3,</span>
<span class="gi">+        n_informative=1,</span>
<span class="gi">+        n_redundant=1,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features)</span>
<span class="gi">+    err_msg = &quot;SMOTE-NC is not designed to work only with categorical features&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smote_nc_with_null_median_std():</span>
<span class="gi">+    # Non-regression test for #662</span>
<span class="gi">+    # https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662</span>
<span class="gi">+    data = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [1, 2, 1, &quot;A&quot;],</span>
<span class="gi">+            [2, 1, 2, &quot;A&quot;],</span>
<span class="gi">+            [2, 1, 2, &quot;A&quot;],</span>
<span class="gi">+            [1, 2, 3, &quot;B&quot;],</span>
<span class="gi">+            [1, 2, 4, &quot;C&quot;],</span>
<span class="gi">+            [1, 2, 5, &quot;C&quot;],</span>
<span class="gi">+            [1, 2, 4, &quot;C&quot;],</span>
<span class="gi">+            [1, 2, 4, &quot;C&quot;],</span>
<span class="gi">+            [1, 2, 4, &quot;C&quot;],</span>
<span class="gi">+        ],</span>
<span class="gi">+        dtype=&quot;object&quot;,</span>
<span class="gi">+    )</span>
<span class="gi">+    labels = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            &quot;class_1&quot;,</span>
<span class="gi">+            &quot;class_1&quot;,</span>
<span class="gi">+            &quot;class_1&quot;,</span>
<span class="gi">+            &quot;class_1&quot;,</span>
<span class="gi">+            &quot;class_2&quot;,</span>
<span class="gi">+            &quot;class_2&quot;,</span>
<span class="gi">+            &quot;class_3&quot;,</span>
<span class="gi">+            &quot;class_3&quot;,</span>
<span class="gi">+            &quot;class_3&quot;,</span>
<span class="gi">+        ],</span>
<span class="gi">+        dtype=object,</span>
<span class="gi">+    )</span>
<span class="gi">+    smote = SMOTENC(categorical_features=[3], k_neighbors=1, random_state=0)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(data, labels)</span>
<span class="gi">+    # check that the categorical feature is not random but correspond to the</span>
<span class="gi">+    # categories seen in the minority class samples</span>
<span class="gi">+    assert_array_equal(X_res[-3:, -1], np.array([&quot;C&quot;, &quot;C&quot;, &quot;C&quot;], dtype=object))</span>
<span class="gi">+    assert smote.median_std_ == {&quot;class_2&quot;: 0.0, &quot;class_3&quot;: 0.0}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_smotenc_categorical_encoder():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can pass our own categorical encoder.&quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    # TODO: only use `sparse_output` when sklearn &gt;= 1.2</span>
<span class="gi">+    param = &quot;sparse&quot; if sklearn_version &lt; parse_version(&quot;1.2&quot;) else &quot;sparse_output&quot;</span>
<span class="gi">+</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    smote.fit_resample(X, y)</span>

<span class="gi">+    assert getattr(smote.categorical_encoder_, param) is True</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OneHotEncoder()</span>
<span class="gi">+    encoder.set_params(**{param: False})</span>
<span class="gi">+    smote.set_params(categorical_encoder=encoder).fit_resample(X, y)</span>
<span class="gi">+    assert smote.categorical_encoder is encoder</span>
<span class="gi">+    assert smote.categorical_encoder_ is not encoder</span>
<span class="gi">+    assert getattr(smote.categorical_encoder_, param) is False</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# TODO(0.13): remove this test</span>
<span class="w"> </span>def test_smotenc_deprecation_ohe_():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a deprecation warning when using `ohe_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y, categorical_features = data_heterogneous_unordered()</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;&#39;ohe_&#39; attribute has been deprecated&quot;):</span>
<span class="gi">+        smote.ohe_</span>


<span class="w"> </span>def test_smotenc_param_validation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we validate the parameters correctly since this estimator requires
<span class="w"> </span>    a specific parameter.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    categorical_features = [0]</span>
<span class="gi">+    smote = SMOTENC(categorical_features=categorical_features, random_state=0)</span>
<span class="gi">+    name = smote.__class__.__name__</span>
<span class="gi">+    _set_checking_parameters(smote)</span>
<span class="gi">+    check_param_validation(name, smote)</span>


<span class="w"> </span>def test_smotenc_bool_categorical():
<span class="gu">@@ -37,23 +318,102 @@ def test_smotenc_bool_categorical():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/974
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;c&quot;: pd.Categorical([x for x in &quot;abbacaba&quot; * 3]),</span>
<span class="gi">+            &quot;f&quot;: [0.3, 0.5, 0.1, 0.2] * 6,</span>
<span class="gi">+            &quot;b&quot;: [False, False, True] * 8,</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    y = pd.DataFrame({&quot;out&quot;: [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0] * 2})</span>
<span class="gi">+    smote = SMOTENC(categorical_features=[0])</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    pd.testing.assert_series_equal(X_res.dtypes, X.dtypes)</span>
<span class="gi">+    assert len(X_res) == len(y_res)</span>
<span class="gi">+</span>
<span class="gi">+    smote.set_params(categorical_features=[0, 2])</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    pd.testing.assert_series_equal(X_res.dtypes, X.dtypes)</span>
<span class="gi">+    assert len(X_res) == len(y_res)</span>
<span class="gi">+</span>
<span class="gi">+    X = X.astype({&quot;b&quot;: &quot;category&quot;})</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    pd.testing.assert_series_equal(X_res.dtypes, X.dtypes)</span>
<span class="gi">+    assert len(X_res) == len(y_res)</span>


<span class="w"> </span>def test_smotenc_categorical_features_str():
<span class="w"> </span>    &quot;&quot;&quot;Check that we support array-like of strings for `categorical_features` using
<span class="w"> </span>    pandas dataframe.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;A&quot;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
<span class="gi">+            &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;] * 5,</span>
<span class="gi">+            &quot;C&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] * 3 + [&quot;a&quot;],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    X = pd.concat([X] * 10, ignore_index=True)</span>
<span class="gi">+    y = np.array([0] * 70 + [1] * 30)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=[&quot;B&quot;, &quot;C&quot;], random_state=0)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    assert X_res[&quot;B&quot;].isin([&quot;a&quot;, &quot;b&quot;]).all()</span>
<span class="gi">+    assert X_res[&quot;C&quot;].isin([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]).all()</span>
<span class="gi">+    counter = Counter(y_res)</span>
<span class="gi">+    assert counter[0] == counter[1] == 70</span>
<span class="gi">+    assert_array_equal(smote.categorical_features_, [1, 2])</span>
<span class="gi">+    assert_array_equal(smote.continuous_features_, [0])</span>


<span class="w"> </span>def test_smotenc_categorical_features_auto():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can automatically detect categorical features based on pandas
<span class="w"> </span>    dataframe.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;A&quot;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
<span class="gi">+            &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;] * 5,</span>
<span class="gi">+            &quot;C&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] * 3 + [&quot;a&quot;],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    X = pd.concat([X] * 10, ignore_index=True)</span>
<span class="gi">+    X[&quot;B&quot;] = X[&quot;B&quot;].astype(&quot;category&quot;)</span>
<span class="gi">+    X[&quot;C&quot;] = X[&quot;C&quot;].astype(&quot;category&quot;)</span>
<span class="gi">+    y = np.array([0] * 70 + [1] * 30)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=&quot;auto&quot;, random_state=0)</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+    assert X_res[&quot;B&quot;].isin([&quot;a&quot;, &quot;b&quot;]).all()</span>
<span class="gi">+    assert X_res[&quot;C&quot;].isin([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]).all()</span>
<span class="gi">+    counter = Counter(y_res)</span>
<span class="gi">+    assert counter[0] == counter[1] == 70</span>
<span class="gi">+    assert_array_equal(smote.categorical_features_, [1, 2])</span>
<span class="gi">+    assert_array_equal(smote.continuous_features_, [0])</span>


<span class="w"> </span>def test_smote_nc_categorical_features_auto_error():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a proper error when we cannot use the `&#39;auto&#39;` mode.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;A&quot;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
<span class="gi">+            &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;] * 5,</span>
<span class="gi">+            &quot;C&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] * 3 + [&quot;a&quot;],</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0] * 70 + [1] * 30)</span>
<span class="gi">+    smote = SMOTENC(categorical_features=&quot;auto&quot;, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;the input data should be a pandas.DataFrame&quot;):</span>
<span class="gi">+        smote.fit_resample(X.to_numpy(), y)</span>
<span class="gi">+</span>
<span class="gi">+    err_msg = &quot;SMOTE-NC is not designed to work only with numerical features&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        smote.fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_smoten.py b/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gh">index b4fceeb..2e30e3f 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_smoten.py</span>
<span class="gu">@@ -3,19 +3,93 @@ import pytest</span>
<span class="w"> </span>from sklearn.exceptions import DataConversionWarning
<span class="w"> </span>from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
<span class="w"> </span>from sklearn.utils._testing import _convert_container
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import SMOTEN


<span class="gd">-@pytest.mark.parametrize(&#39;sparse_format&#39;, [&#39;sparse_csr&#39;, &#39;sparse_csc&#39;])</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+</span>
<span class="gi">+    feature_1 = [&quot;A&quot;] * 10 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 30</span>
<span class="gi">+    feature_2 = [&quot;A&quot;] * 40 + [&quot;B&quot;] * 20</span>
<span class="gi">+    feature_3 = [&quot;A&quot;] * 20 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 10 + [&quot;D&quot;] * 10</span>
<span class="gi">+    X = np.array([feature_1, feature_2, feature_3], dtype=object).T</span>
<span class="gi">+    rng.shuffle(X)</span>
<span class="gi">+    y = np.array([0] * 20 + [1] * 40, dtype=np.int32)</span>
<span class="gi">+    y_labels = np.array([&quot;not apple&quot;, &quot;apple&quot;], dtype=object)</span>
<span class="gi">+    y = y_labels[y]</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smoten(data):</span>
<span class="gi">+    # overall check for SMOTEN</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    sampler = SMOTEN(random_state=0)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape == (80, 3)</span>
<span class="gi">+    assert y_res.shape == (80,)</span>
<span class="gi">+    assert isinstance(sampler.categorical_encoder_, OrdinalEncoder)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_smoten_resampling():</span>
<span class="gi">+    # check if the SMOTEN resample data as expected</span>
<span class="gi">+    # we generate data such that &quot;not apple&quot; will be the minority class and</span>
<span class="gi">+    # samples from this class will be generated. We will force the &quot;blue&quot;</span>
<span class="gi">+    # category to be associated with this class. Therefore, the new generated</span>
<span class="gi">+    # samples should as well be from the &quot;blue&quot; category.</span>
<span class="gi">+    X = np.array([&quot;green&quot;] * 5 + [&quot;red&quot;] * 10 + [&quot;blue&quot;] * 7, dtype=object).reshape(</span>
<span class="gi">+        -1, 1</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array(</span>
<span class="gi">+        [&quot;apple&quot;] * 5</span>
<span class="gi">+        + [&quot;not apple&quot;] * 3</span>
<span class="gi">+        + [&quot;apple&quot;] * 7</span>
<span class="gi">+        + [&quot;not apple&quot;] * 5</span>
<span class="gi">+        + [&quot;apple&quot;] * 2,</span>
<span class="gi">+        dtype=object,</span>
<span class="gi">+    )</span>
<span class="gi">+    sampler = SMOTEN(random_state=0)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    X_generated, y_generated = X_res[X.shape[0] :], y_res[X.shape[0] :]</span>
<span class="gi">+    np.testing.assert_array_equal(X_generated, &quot;blue&quot;)</span>
<span class="gi">+    np.testing.assert_array_equal(y_generated, &quot;not apple&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;sparse_format&quot;, [&quot;sparse_csr&quot;, &quot;sparse_csc&quot;])</span>
<span class="w"> </span>def test_smoten_sparse_input(data, sparse_format):
<span class="w"> </span>    &quot;&quot;&quot;Check that we handle sparse input in SMOTEN even if it is not efficient.

<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/971
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    X = OneHotEncoder().fit_transform(X).toarray()</span>
<span class="gi">+    X = _convert_container(X, sparse_format)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(DataConversionWarning, match=&quot;is not really efficient&quot;):</span>
<span class="gi">+        X_res, y_res = SMOTEN(random_state=0).fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.format == X.format</span>
<span class="gi">+    assert X_res.shape[0] == len(y_res)</span>


<span class="w"> </span>def test_smoten_categorical_encoder(data):
<span class="w"> </span>    &quot;&quot;&quot;Check that `categorical_encoder` is used when provided.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    sampler = SMOTEN(random_state=0)</span>
<span class="gi">+    sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(sampler.categorical_encoder_, OrdinalEncoder)</span>
<span class="gi">+    assert sampler.categorical_encoder_.dtype == np.int32</span>
<span class="gi">+</span>
<span class="gi">+    encoder = OrdinalEncoder(dtype=np.int64)</span>
<span class="gi">+    sampler.set_params(categorical_encoder=encoder).fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(sampler.categorical_encoder_, OrdinalEncoder)</span>
<span class="gi">+    assert sampler.categorical_encoder is encoder</span>
<span class="gi">+    assert sampler.categorical_encoder_ is not encoder</span>
<span class="gi">+    assert sampler.categorical_encoder_.dtype == np.int64</span>
<span class="gh">diff --git a/imblearn/over_sampling/_smote/tests/test_svm_smote.py b/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gh">index dd43004..49e01f6 100644</span>
<span class="gd">--- a/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gi">+++ b/imblearn/over_sampling/_smote/tests/test_svm_smote.py</span>
<span class="gu">@@ -5,13 +5,63 @@ from sklearn.linear_model import LogisticRegression</span>
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.svm import SVC
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import SVMSMOTE


<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    X = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_svm_smote(data):</span>
<span class="gi">+    svm_smote = SVMSMOTE(random_state=42)</span>
<span class="gi">+    svm_smote_nn = SVMSMOTE(</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+        k_neighbors=NearestNeighbors(n_neighbors=6),</span>
<span class="gi">+        m_neighbors=NearestNeighbors(n_neighbors=11),</span>
<span class="gi">+        svm_estimator=SVC(gamma=&quot;scale&quot;, random_state=42),</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    X_res_1, y_res_1 = svm_smote.fit_resample(*data)</span>
<span class="gi">+    X_res_2, y_res_2 = svm_smote_nn.fit_resample(*data)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X_res_1, X_res_2)</span>
<span class="gi">+    assert_array_equal(y_res_1, y_res_2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_svm_smote_not_svm(data):
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a proper error if passing an estimator that does not
<span class="w"> </span>    expose a `support_` fitted attribute.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    err_msg = &quot;`svm_estimator` is required to exposed a `support_` fitted attribute.&quot;</span>
<span class="gi">+    with pytest.raises(RuntimeError, match=err_msg):</span>
<span class="gi">+        SVMSMOTE(svm_estimator=LogisticRegression()).fit_resample(*data)</span>


<span class="w"> </span>def test_svm_smote_all_noise(data):
<span class="gu">@@ -21,4 +71,18 @@ def test_svm_smote_all_noise(data):</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/742
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        class_sep=0.001,</span>
<span class="gi">+        weights=[0.004, 0.451, 0.545],</span>
<span class="gi">+        n_informative=3,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        flip_y=0,</span>
<span class="gi">+        n_features=3,</span>
<span class="gi">+        n_clusters_per_class=2,</span>
<span class="gi">+        n_samples=1000,</span>
<span class="gi">+        random_state=10,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;SVM-SMOTE is not adapted to your dataset&quot;):</span>
<span class="gi">+        SVMSMOTE(k_neighbors=4, random_state=42).fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/base.py b/imblearn/over_sampling/base.py</span>
<span class="gh">index f71cf78..fbd982b 100644</span>
<span class="gd">--- a/imblearn/over_sampling/base.py</span>
<span class="gi">+++ b/imblearn/over_sampling/base.py</span>
<span class="gu">@@ -1,8 +1,13 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Base class for the over-sampling method.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>from collections.abc import Mapping
<span class="gi">+</span>
<span class="w"> </span>from ..base import BaseSampler
<span class="w"> </span>from ..utils._param_validation import Interval, StrOptions

<span class="gu">@@ -13,9 +18,10 @@ class BaseOverSampler(BaseSampler):</span>
<span class="w"> </span>    Warning: This class should not be used directly. Use the derive classes
<span class="w"> </span>    instead.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;over-sampling&#39;</span>
<span class="gd">-    _sampling_strategy_docstring = (</span>
<span class="gd">-        &quot;&quot;&quot;sampling_strategy : float, str, dict or callable, default=&#39;auto&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_type = &quot;over-sampling&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_strategy_docstring = &quot;&quot;&quot;sampling_strategy : float, str, dict or callable, default=&#39;auto&#39;</span>
<span class="w"> </span>        Sampling information to resample the data set.

<span class="w"> </span>        - When ``float``, it corresponds to the desired ratio of the number of
<span class="gu">@@ -50,9 +56,14 @@ class BaseOverSampler(BaseSampler):</span>
<span class="w"> </span>        - When callable, function taking ``y`` and returns a ``dict``. The keys
<span class="w"> </span>          correspond to the targeted classes. The values correspond to the
<span class="w"> </span>          desired number of samples for each class.
<span class="gd">-        &quot;&quot;&quot;</span>
<span class="gd">-        .strip())</span>
<span class="gd">-    _parameter_constraints: dict = {&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;minority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), Mapping, callable],</span>
<span class="gd">-        &#39;random_state&#39;: [&#39;random_state&#39;]}</span>
<span class="gi">+        &quot;&quot;&quot;.strip()  # noqa: E501</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;sampling_strategy&quot;: [</span>
<span class="gi">+            Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+            StrOptions({&quot;auto&quot;, &quot;minority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+            Mapping,</span>
<span class="gi">+            callable,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gh">diff --git a/imblearn/over_sampling/tests/test_adasyn.py b/imblearn/over_sampling/tests/test_adasyn.py</span>
<span class="gh">index ea8f98e..4df6362 100644</span>
<span class="gd">--- a/imblearn/over_sampling/tests/test_adasyn.py</span>
<span class="gi">+++ b/imblearn/over_sampling/tests/test_adasyn.py</span>
<span class="gu">@@ -1,17 +1,121 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module under sampler.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import ADASYN
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.11622591, -0.0317206], [0.77481731, 0.60935141], [</span>
<span class="gd">-    1.25192108, -0.22367336], [0.53366841, -0.30312976], [1.52091956, -</span>
<span class="gd">-    0.49283504], [-0.28162401, -2.10400981], [0.83680821, 1.72827342], [</span>
<span class="gd">-    0.3084254, 0.33299982], [0.70472253, -0.73309052], [0.28893132, -</span>
<span class="gd">-    0.38761769], [1.15514042, 0.0129463], [0.88407872, 0.35454207], [</span>
<span class="gd">-    1.31301027, -0.92648734], [-1.11515198, -0.93689695], [-0.18410027, -</span>
<span class="gd">-    0.45194484], [0.9281014, 0.53085498], [-0.14374509, 0.27370049], [-</span>
<span class="gd">-    0.41635887, -0.38299653], [0.08711622, 0.93259929], [1.70580611, -</span>
<span class="gd">-    0.11219234]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.11622591, -0.0317206],</span>
<span class="gi">+        [0.77481731, 0.60935141],</span>
<span class="gi">+        [1.25192108, -0.22367336],</span>
<span class="gi">+        [0.53366841, -0.30312976],</span>
<span class="gi">+        [1.52091956, -0.49283504],</span>
<span class="gi">+        [-0.28162401, -2.10400981],</span>
<span class="gi">+        [0.83680821, 1.72827342],</span>
<span class="gi">+        [0.3084254, 0.33299982],</span>
<span class="gi">+        [0.70472253, -0.73309052],</span>
<span class="gi">+        [0.28893132, -0.38761769],</span>
<span class="gi">+        [1.15514042, 0.0129463],</span>
<span class="gi">+        [0.88407872, 0.35454207],</span>
<span class="gi">+        [1.31301027, -0.92648734],</span>
<span class="gi">+        [-1.11515198, -0.93689695],</span>
<span class="gi">+        [-0.18410027, -0.45194484],</span>
<span class="gi">+        [0.9281014, 0.53085498],</span>
<span class="gi">+        [-0.14374509, 0.27370049],</span>
<span class="gi">+        [-0.41635887, -0.38299653],</span>
<span class="gi">+        [0.08711622, 0.93259929],</span>
<span class="gi">+        [1.70580611, -0.11219234],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_ada_init():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    ada = ADASYN(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    assert ada.random_state == RND_SEED</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_ada_fit_resample():</span>
<span class="gi">+    ada = ADASYN(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ada.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+            [0.88161986, -0.2829741],</span>
<span class="gi">+            [0.35681689, -0.18814597],</span>
<span class="gi">+            [1.4148276, 0.05308106],</span>
<span class="gi">+            [0.3136591, -0.31327875],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_ada_fit_resample_nn_obj():</span>
<span class="gi">+    nn = NearestNeighbors(n_neighbors=6)</span>
<span class="gi">+    ada = ADASYN(random_state=RND_SEED, n_neighbors=nn)</span>
<span class="gi">+    X_resampled, y_resampled = ada.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.11622591, -0.0317206],</span>
<span class="gi">+            [0.77481731, 0.60935141],</span>
<span class="gi">+            [1.25192108, -0.22367336],</span>
<span class="gi">+            [0.53366841, -0.30312976],</span>
<span class="gi">+            [1.52091956, -0.49283504],</span>
<span class="gi">+            [-0.28162401, -2.10400981],</span>
<span class="gi">+            [0.83680821, 1.72827342],</span>
<span class="gi">+            [0.3084254, 0.33299982],</span>
<span class="gi">+            [0.70472253, -0.73309052],</span>
<span class="gi">+            [0.28893132, -0.38761769],</span>
<span class="gi">+            [1.15514042, 0.0129463],</span>
<span class="gi">+            [0.88407872, 0.35454207],</span>
<span class="gi">+            [1.31301027, -0.92648734],</span>
<span class="gi">+            [-1.11515198, -0.93689695],</span>
<span class="gi">+            [-0.18410027, -0.45194484],</span>
<span class="gi">+            [0.9281014, 0.53085498],</span>
<span class="gi">+            [-0.14374509, 0.27370049],</span>
<span class="gi">+            [-0.41635887, -0.38299653],</span>
<span class="gi">+            [0.08711622, 0.93259929],</span>
<span class="gi">+            [1.70580611, -0.11219234],</span>
<span class="gi">+            [0.88161986, -0.2829741],</span>
<span class="gi">+            [0.35681689, -0.18814597],</span>
<span class="gi">+            [1.4148276, 0.05308106],</span>
<span class="gi">+            [0.3136591, -0.31327875],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gh">diff --git a/imblearn/over_sampling/tests/test_common.py b/imblearn/over_sampling/tests/test_common.py</span>
<span class="gh">index e0a133d..cdd85c1 100644</span>
<span class="gd">--- a/imblearn/over_sampling/tests/test_common.py</span>
<span class="gi">+++ b/imblearn/over_sampling/tests/test_common.py</span>
<span class="gu">@@ -1,6 +1,145 @@</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.cluster import MiniBatchKMeans
<span class="gd">-from imblearn.over_sampling import ADASYN, SMOTE, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, KMeansSMOTE</span>
<span class="gi">+</span>
<span class="gi">+from imblearn.over_sampling import (</span>
<span class="gi">+    ADASYN,</span>
<span class="gi">+    SMOTE,</span>
<span class="gi">+    SMOTEN,</span>
<span class="gi">+    SMOTENC,</span>
<span class="gi">+    SVMSMOTE,</span>
<span class="gi">+    BorderlineSMOTE,</span>
<span class="gi">+    KMeansSMOTE,</span>
<span class="gi">+)</span>
<span class="w"> </span>from imblearn.utils.testing import _CustomNearestNeighbors
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def numerical_data():</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    X = rng.randn(100, 2)</span>
<span class="gi">+    y = np.repeat([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0], 5)</span>
<span class="gi">+</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def categorical_data():</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+</span>
<span class="gi">+    feature_1 = [&quot;A&quot;] * 10 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 30</span>
<span class="gi">+    feature_2 = [&quot;A&quot;] * 40 + [&quot;B&quot;] * 20</span>
<span class="gi">+    feature_3 = [&quot;A&quot;] * 20 + [&quot;B&quot;] * 20 + [&quot;C&quot;] * 10 + [&quot;D&quot;] * 10</span>
<span class="gi">+    X = np.array([feature_1, feature_2, feature_3], dtype=object).T</span>
<span class="gi">+    rng.shuffle(X)</span>
<span class="gi">+    y = np.array([0] * 20 + [1] * 40, dtype=np.int32)</span>
<span class="gi">+    y_labels = np.array([&quot;not apple&quot;, &quot;apple&quot;], dtype=object)</span>
<span class="gi">+    y = y_labels[y]</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def heterogeneous_data():</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = np.empty((30, 4), dtype=object)</span>
<span class="gi">+    X[:, :2] = rng.randn(30, 2)</span>
<span class="gi">+    X[:, 2] = rng.choice([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], size=30).astype(object)</span>
<span class="gi">+    X[:, 3] = rng.randint(3, size=30)</span>
<span class="gi">+    y = np.array([0] * 10 + [1] * 20)</span>
<span class="gi">+    return X, y, [2, 3]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;smote&quot;, [BorderlineSMOTE(), SVMSMOTE()], ids=[&quot;borderline&quot;, &quot;svm&quot;]</span>
<span class="gi">+)</span>
<span class="gi">+def test_smote_m_neighbors(numerical_data, smote):</span>
<span class="gi">+    # check that m_neighbors is properly set. Regression test for:</span>
<span class="gi">+    # https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568</span>
<span class="gi">+    X, y = numerical_data</span>
<span class="gi">+    _ = smote.fit_resample(X, y)</span>
<span class="gi">+    assert smote.nn_k_.n_neighbors == 6</span>
<span class="gi">+    assert smote.nn_m_.n_neighbors == 11</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;smote, neighbor_estimator_name&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (ADASYN(random_state=0), &quot;n_neighbors&quot;),</span>
<span class="gi">+        (BorderlineSMOTE(random_state=0), &quot;k_neighbors&quot;),</span>
<span class="gi">+        (</span>
<span class="gi">+            KMeansSMOTE(</span>
<span class="gi">+                kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0),</span>
<span class="gi">+                random_state=1,</span>
<span class="gi">+            ),</span>
<span class="gi">+            &quot;k_neighbors&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (SMOTE(random_state=0), &quot;k_neighbors&quot;),</span>
<span class="gi">+        (SVMSMOTE(random_state=0), &quot;k_neighbors&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+    ids=[&quot;adasyn&quot;, &quot;borderline&quot;, &quot;kmeans&quot;, &quot;smote&quot;, &quot;svm&quot;],</span>
<span class="gi">+)</span>
<span class="gi">+def test_numerical_smote_custom_nn(numerical_data, smote, neighbor_estimator_name):</span>
<span class="gi">+    X, y = numerical_data</span>
<span class="gi">+    params = {</span>
<span class="gi">+        neighbor_estimator_name: _CustomNearestNeighbors(n_neighbors=5),</span>
<span class="gi">+    }</span>
<span class="gi">+    smote.set_params(**params)</span>
<span class="gi">+    X_res, _ = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape[0] &gt;= 120</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_categorical_smote_k_custom_nn(categorical_data):</span>
<span class="gi">+    X, y = categorical_data</span>
<span class="gi">+    smote = SMOTEN(k_neighbors=_CustomNearestNeighbors(n_neighbors=5))</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape == (80, 3)</span>
<span class="gi">+    assert Counter(y_res) == {&quot;apple&quot;: 40, &quot;not apple&quot;: 40}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_heterogeneous_smote_k_custom_nn(heterogeneous_data):</span>
<span class="gi">+    X, y, categorical_features = heterogeneous_data</span>
<span class="gi">+    smote = SMOTENC(</span>
<span class="gi">+        categorical_features, k_neighbors=_CustomNearestNeighbors(n_neighbors=5)</span>
<span class="gi">+    )</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape == (40, 4)</span>
<span class="gi">+    assert Counter(y_res) == {0: 20, 1: 20}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;smote&quot;,</span>
<span class="gi">+    [BorderlineSMOTE(random_state=0), SVMSMOTE(random_state=0)],</span>
<span class="gi">+    ids=[&quot;borderline&quot;, &quot;svm&quot;],</span>
<span class="gi">+)</span>
<span class="gi">+def test_numerical_smote_extra_custom_nn(numerical_data, smote):</span>
<span class="gi">+    X, y = numerical_data</span>
<span class="gi">+    smote.set_params(m_neighbors=_CustomNearestNeighbors(n_neighbors=5))</span>
<span class="gi">+    X_res, y_res = smote.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape == (120, 2)</span>
<span class="gi">+    assert Counter(y_res) == {0: 60, 1: 60}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# FIXME: to be removed in 0.12</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampler&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ADASYN(random_state=0),</span>
<span class="gi">+        BorderlineSMOTE(random_state=0),</span>
<span class="gi">+        SMOTE(random_state=0),</span>
<span class="gi">+        SMOTEN(random_state=0),</span>
<span class="gi">+        SMOTENC([0], random_state=0),</span>
<span class="gi">+        SVMSMOTE(random_state=0),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_n_jobs_deprecation_warning(numerical_data, sampler):</span>
<span class="gi">+    X, y = numerical_data</span>
<span class="gi">+    sampler.set_params(n_jobs=2)</span>
<span class="gi">+    warning_msg = &quot;The parameter `n_jobs` has been deprecated&quot;</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=warning_msg):</span>
<span class="gi">+        sampler.fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/over_sampling/tests/test_random_over_sampler.py b/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gh">index c239b21..efa40c8 100644</span>
<span class="gd">--- a/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gi">+++ b/imblearn/over_sampling/tests/test_random_over_sampler.py</span>
<span class="gu">@@ -1,25 +1,292 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module under sampler.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="w"> </span>from datetime import datetime
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="gd">-from sklearn.utils._testing import _convert_container, assert_allclose, assert_array_equal</span>
<span class="gi">+from sklearn.utils._testing import (</span>
<span class="gi">+    _convert_container,</span>
<span class="gi">+    assert_allclose,</span>
<span class="gi">+    assert_array_equal,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>from imblearn.over_sampling import RandomOverSampler
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0


<span class="gd">-@pytest.mark.parametrize(&#39;sampling_strategy&#39;, [&#39;auto&#39;, &#39;minority&#39;,</span>
<span class="gd">-    &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;])</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    X = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.04352327, -0.20515826],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.20792588, 1.49407907],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.22950086, 0.33367433],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.09125309, -0.85409574],</span>
<span class="gi">+            [0.12372842, 0.6536186],</span>
<span class="gi">+            [0.13347175, 0.12167502],</span>
<span class="gi">+            [0.094035, -2.55298982],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    Y = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1])</span>
<span class="gi">+    return X, Y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_ros_init():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    assert ros.random_state == RND_SEED</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;params&quot;, [{&quot;shrinkage&quot;: None}, {&quot;shrinkage&quot;: 0}, {&quot;shrinkage&quot;: {0: 0}}]</span>
<span class="gi">+)</span>
<span class="gi">+@pytest.mark.parametrize(&quot;X_type&quot;, [&quot;array&quot;, &quot;dataframe&quot;])</span>
<span class="gi">+def test_ros_fit_resample(X_type, data, params):</span>
<span class="gi">+    X, Y = data</span>
<span class="gi">+    X_ = _convert_container(X, X_type)</span>
<span class="gi">+    ros = RandomOverSampler(**params, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X_, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.04352327, -0.20515826],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.20792588, 1.49407907],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.22950086, 0.33367433],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.09125309, -0.85409574],</span>
<span class="gi">+            [0.12372842, 0.6536186],</span>
<span class="gi">+            [0.13347175, 0.12167502],</span>
<span class="gi">+            [0.094035, -2.55298982],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0])</span>
<span class="gi">+</span>
<span class="gi">+    if X_type == &quot;dataframe&quot;:</span>
<span class="gi">+        assert hasattr(X_resampled, &quot;loc&quot;)</span>
<span class="gi">+        # FIXME: we should use to_numpy with pandas &gt;= 0.25</span>
<span class="gi">+        X_resampled = X_resampled.values</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+    if params[&quot;shrinkage&quot;] is None:</span>
<span class="gi">+        assert ros.shrinkage_ is None</span>
<span class="gi">+    else:</span>
<span class="gi">+        assert ros.shrinkage_ == {0: 0}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;params&quot;, [{&quot;shrinkage&quot;: None}, {&quot;shrinkage&quot;: 0}])</span>
<span class="gi">+def test_ros_fit_resample_half(data, params):</span>
<span class="gi">+    X, Y = data</span>
<span class="gi">+    sampling_strategy = {0: 3, 1: 7}</span>
<span class="gi">+    ros = RandomOverSampler(</span>
<span class="gi">+        **params, sampling_strategy=sampling_strategy, random_state=RND_SEED</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X, Y)</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.04352327, -0.20515826],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.20792588, 1.49407907],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.22950086, 0.33367433],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.09125309, -0.85409574],</span>
<span class="gi">+            [0.12372842, 0.6536186],</span>
<span class="gi">+            [0.13347175, 0.12167502],</span>
<span class="gi">+            [0.094035, -2.55298982],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1])</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+    if params[&quot;shrinkage&quot;] is None:</span>
<span class="gi">+        assert ros.shrinkage_ is None</span>
<span class="gi">+    else:</span>
<span class="gi">+        assert ros.shrinkage_ == {0: 0, 1: 0}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;params&quot;, [{&quot;shrinkage&quot;: None}, {&quot;shrinkage&quot;: 0}])</span>
<span class="gi">+def test_multiclass_fit_resample(data, params):</span>
<span class="gi">+    # check the random over-sampling with a multiclass problem</span>
<span class="gi">+    X, Y = data</span>
<span class="gi">+    y = Y.copy()</span>
<span class="gi">+    y[5] = 2</span>
<span class="gi">+    y[6] = 2</span>
<span class="gi">+    ros = RandomOverSampler(**params, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = ros.fit_resample(X, y)</span>
<span class="gi">+    count_y_res = Counter(y_resampled)</span>
<span class="gi">+    assert count_y_res[0] == 5</span>
<span class="gi">+    assert count_y_res[1] == 5</span>
<span class="gi">+    assert count_y_res[2] == 5</span>
<span class="gi">+</span>
<span class="gi">+    if params[&quot;shrinkage&quot;] is None:</span>
<span class="gi">+        assert ros.shrinkage_ is None</span>
<span class="gi">+    else:</span>
<span class="gi">+        assert ros.shrinkage_ == {0: 0, 2: 0}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_over_sampling_heterogeneous_data():</span>
<span class="gi">+    # check that resampling with heterogeneous dtype is working with basic</span>
<span class="gi">+    # resampling</span>
<span class="gi">+    X_hetero = np.array(</span>
<span class="gi">+        [[&quot;xxx&quot;, 1, 1.0], [&quot;yyy&quot;, 2, 2.0], [&quot;zzz&quot;, 3, 3.0]], dtype=object</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 0, 1])</span>
<span class="gi">+    ros = RandomOverSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_res, y_res = ros.fit_resample(X_hetero, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape[0] == 4</span>
<span class="gi">+    assert y_res.shape[0] == 4</span>
<span class="gi">+    assert X_res.dtype == object</span>
<span class="gi">+    assert X_res[-1, 0] in X_hetero[:, 0]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_over_sampling_nan_inf(data):</span>
<span class="gi">+    # check that we can oversample even with missing or infinite data</span>
<span class="gi">+    # regression tests for #605</span>
<span class="gi">+    X, Y = data</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    n_not_finite = X.shape[0] // 3</span>
<span class="gi">+    row_indices = rng.choice(np.arange(X.shape[0]), size=n_not_finite)</span>
<span class="gi">+    col_indices = rng.randint(0, X.shape[1], size=n_not_finite)</span>
<span class="gi">+    not_finite_values = rng.choice([np.nan, np.inf], size=n_not_finite)</span>
<span class="gi">+</span>
<span class="gi">+    X_ = X.copy()</span>
<span class="gi">+    X_[row_indices, col_indices] = not_finite_values</span>
<span class="gi">+</span>
<span class="gi">+    ros = RandomOverSampler(random_state=0)</span>
<span class="gi">+    X_res, y_res = ros.fit_resample(X_, Y)</span>
<span class="gi">+</span>
<span class="gi">+    assert y_res.shape == (14,)</span>
<span class="gi">+    assert X_res.shape == (14, 2)</span>
<span class="gi">+    assert np.any(~np.isfinite(X_res))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_over_sampling_heterogeneous_data_smoothed_bootstrap():</span>
<span class="gi">+    # check that we raise an error when heterogeneous dtype data are given</span>
<span class="gi">+    # and a smoothed bootstrap is requested</span>
<span class="gi">+    X_hetero = np.array(</span>
<span class="gi">+        [[&quot;xxx&quot;, 1, 1.0], [&quot;yyy&quot;, 2, 2.0], [&quot;zzz&quot;, 3, 3.0]], dtype=object</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 0, 1])</span>
<span class="gi">+    ros = RandomOverSampler(shrinkage=1, random_state=RND_SEED)</span>
<span class="gi">+    err_msg = &quot;When shrinkage is not None, X needs to contain only numerical&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        ros.fit_resample(X_hetero, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;X_type&quot;, [&quot;dataframe&quot;, &quot;array&quot;, &quot;sparse_csr&quot;, &quot;sparse_csc&quot;])</span>
<span class="gi">+def test_random_over_sampler_smoothed_bootstrap(X_type, data):</span>
<span class="gi">+    # check that smoothed bootstrap is working for numerical array</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    sampler = RandomOverSampler(shrinkage=1)</span>
<span class="gi">+    X = _convert_container(X, X_type)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert y_res.shape == (14,)</span>
<span class="gi">+    assert X_res.shape == (14, 2)</span>
<span class="gi">+</span>
<span class="gi">+    if X_type == &quot;dataframe&quot;:</span>
<span class="gi">+        assert hasattr(X_res, &quot;loc&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_over_sampler_equivalence_shrinkage(data):</span>
<span class="gi">+    # check that a shrinkage factor of 0 is equivalent to not create a smoothed</span>
<span class="gi">+    # bootstrap</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    ros_not_shrink = RandomOverSampler(shrinkage=0, random_state=0)</span>
<span class="gi">+    ros_hard_bootstrap = RandomOverSampler(shrinkage=None, random_state=0)</span>
<span class="gi">+</span>
<span class="gi">+    X_res_not_shrink, y_res_not_shrink = ros_not_shrink.fit_resample(X, y)</span>
<span class="gi">+    X_res, y_res = ros_hard_bootstrap.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X_res_not_shrink, X_res)</span>
<span class="gi">+    assert_allclose(y_res_not_shrink, y_res)</span>
<span class="gi">+</span>
<span class="gi">+    assert y_res.shape == (14,)</span>
<span class="gi">+    assert X_res.shape == (14, 2)</span>
<span class="gi">+    assert y_res_not_shrink.shape == (14,)</span>
<span class="gi">+    assert X_res_not_shrink.shape == (14, 2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_over_sampler_shrinkage_behaviour(data):</span>
<span class="gi">+    # check the behaviour of the shrinkage parameter</span>
<span class="gi">+    # the covariance of the data generated with the larger shrinkage factor</span>
<span class="gi">+    # should also be larger.</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    ros = RandomOverSampler(shrinkage=1, random_state=0)</span>
<span class="gi">+    X_res_shink_1, y_res_shrink_1 = ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    ros.set_params(shrinkage=5)</span>
<span class="gi">+    X_res_shink_5, y_res_shrink_5 = ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    disperstion_shrink_1 = np.linalg.det(np.cov(X_res_shink_1[y_res_shrink_1 == 0].T))</span>
<span class="gi">+    disperstion_shrink_5 = np.linalg.det(np.cov(X_res_shink_5[y_res_shrink_5 == 0].T))</span>
<span class="gi">+</span>
<span class="gi">+    assert disperstion_shrink_1 &lt; disperstion_shrink_5</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;shrinkage, err_msg&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ({}, &quot;`shrinkage` should contain a shrinkage factor for each class&quot;),</span>
<span class="gi">+        ({0: -1}, &quot;The shrinkage factor needs to be &gt;= 0&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_random_over_sampler_shrinkage_error(data, shrinkage, err_msg):</span>
<span class="gi">+    # check the validation of the shrinkage parameter</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    ros = RandomOverSampler(shrinkage=shrinkage)</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy&quot;, [&quot;auto&quot;, &quot;minority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;]</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_random_over_sampler_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseOverSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=100,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.3, 0.6],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    RandomOverSampler(sampling_strategy=sampling_strategy).fit_resample(X, y)</span>


<span class="w"> </span>def test_random_over_sampling_datetime():
<span class="w"> </span>    &quot;&quot;&quot;Check that we don&#39;t convert input data and only sample from it.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    X = pd.DataFrame({&quot;label&quot;: [0, 0, 0, 1], &quot;td&quot;: [datetime.now()] * 4})</span>
<span class="gi">+    y = X[&quot;label&quot;]</span>
<span class="gi">+    ros = RandomOverSampler(random_state=0)</span>
<span class="gi">+    X_res, y_res = ros.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    pd.testing.assert_series_equal(X_res.dtypes, X.dtypes)</span>
<span class="gi">+    pd.testing.assert_index_equal(X_res.index, y_res.index)</span>
<span class="gi">+    assert_array_equal(y_res.to_numpy(), np.array([0, 0, 0, 1, 1, 1]))</span>


<span class="w"> </span>def test_random_over_sampler_full_nat():
<span class="gu">@@ -28,4 +295,18 @@ def test_random_over_sampler_full_nat():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;col_str&quot;: [&quot;abc&quot;, &quot;def&quot;, &quot;xyz&quot;],</span>
<span class="gi">+            &quot;col_timedelta&quot;: pd.to_timedelta([np.nan, np.nan, np.nan]),</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 0, 1])</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = RandomOverSampler().fit_resample(X, y)</span>
<span class="gi">+    assert X_res.shape == (4, 2)</span>
<span class="gi">+    assert y_res.shape == (4,)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res[&quot;col_timedelta&quot;].dtype == &quot;timedelta64[ns]&quot;</span>
<span class="gh">diff --git a/imblearn/pipeline.py b/imblearn/pipeline.py</span>
<span class="gh">index 6c4f580..7453446 100644</span>
<span class="gd">--- a/imblearn/pipeline.py</span>
<span class="gi">+++ b/imblearn/pipeline.py</span>
<span class="gu">@@ -1,7 +1,17 @@</span>
<span class="gd">-&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;</span>
<span class="w"> </span>The :mod:`imblearn.pipeline` module implements utilities to build a
<span class="w"> </span>composite estimator, as a chain of transforms, samples and estimators.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Adapted from scikit-learn</span>
<span class="gi">+</span>
<span class="gi">+# Author: Edouard Duchesnay</span>
<span class="gi">+#         Gael Varoquaux</span>
<span class="gi">+#         Virgile Fritsch</span>
<span class="gi">+#         Alexandre Gramfort</span>
<span class="gi">+#         Lars Buitinck</span>
<span class="gi">+#         Christos Aridas</span>
<span class="gi">+#         Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: BSD</span>
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn import pipeline
<span class="w"> </span>from sklearn.base import clone
<span class="gu">@@ -9,14 +19,25 @@ from sklearn.utils import Bunch</span>
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.metaestimators import available_if
<span class="w"> </span>from sklearn.utils.validation import check_memory
<span class="gi">+</span>
<span class="w"> </span>from .base import _ParamsValidationMixin
<span class="gd">-from .utils._metadata_requests import METHODS, MetadataRouter, MethodMapping, _raise_for_params, _routing_enabled, process_routing</span>
<span class="gi">+from .utils._metadata_requests import (</span>
<span class="gi">+    METHODS,</span>
<span class="gi">+    MetadataRouter,</span>
<span class="gi">+    MethodMapping,</span>
<span class="gi">+    _raise_for_params,</span>
<span class="gi">+    _routing_enabled,</span>
<span class="gi">+    process_routing,</span>
<span class="gi">+)</span>
<span class="w"> </span>from .utils._param_validation import HasMethods, validate_params
<span class="w"> </span>from .utils.fixes import _fit_context
<span class="gd">-METHODS.append(&#39;fit_resample&#39;)</span>
<span class="gd">-__all__ = [&#39;Pipeline&#39;, &#39;make_pipeline&#39;]</span>
<span class="gi">+</span>
<span class="gi">+METHODS.append(&quot;fit_resample&quot;)</span>
<span class="gi">+</span>
<span class="gi">+__all__ = [&quot;Pipeline&quot;, &quot;make_pipeline&quot;]</span>
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__).base_version
<span class="gd">-if parse_version(sklearn_version) &lt; parse_version(&#39;1.5&#39;):</span>
<span class="gi">+if parse_version(sklearn_version) &lt; parse_version(&quot;1.5&quot;):</span>
<span class="w"> </span>    from sklearn.utils import _print_elapsed_time
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.utils._user_interface import _print_elapsed_time
<span class="gu">@@ -128,20 +149,138 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>    weighted avg       0.99      0.98      0.98       250
<span class="w"> </span>    &lt;BLANKLINE&gt;
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {&#39;steps&#39;: &#39;no_validation&#39;, &#39;memory&#39;: [</span>
<span class="gd">-        None, str, HasMethods([&#39;cache&#39;])], &#39;verbose&#39;: [&#39;boolean&#39;]}</span>

<span class="gd">-    def _iter(self, with_final=True, filter_passthrough=True,</span>
<span class="gd">-        filter_resample=True):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;steps&quot;: &quot;no_validation&quot;,  # validated in `_validate_steps`</span>
<span class="gi">+        &quot;memory&quot;: [None, str, HasMethods([&quot;cache&quot;])],</span>
<span class="gi">+        &quot;verbose&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    # BaseEstimator interface</span>
<span class="gi">+</span>
<span class="gi">+    def _validate_steps(self):</span>
<span class="gi">+        names, estimators = zip(*self.steps)</span>
<span class="gi">+</span>
<span class="gi">+        # validate names</span>
<span class="gi">+        self._validate_names(names)</span>
<span class="gi">+</span>
<span class="gi">+        # validate estimators</span>
<span class="gi">+        transformers = estimators[:-1]</span>
<span class="gi">+        estimator = estimators[-1]</span>
<span class="gi">+</span>
<span class="gi">+        for t in transformers:</span>
<span class="gi">+            if t is None or t == &quot;passthrough&quot;:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            is_transfomer = hasattr(t, &quot;fit&quot;) and hasattr(t, &quot;transform&quot;)</span>
<span class="gi">+            is_sampler = hasattr(t, &quot;fit_resample&quot;)</span>
<span class="gi">+            is_not_transfomer_or_sampler = not (is_transfomer or is_sampler)</span>
<span class="gi">+</span>
<span class="gi">+            if is_not_transfomer_or_sampler:</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    &quot;All intermediate steps of the chain should &quot;</span>
<span class="gi">+                    &quot;be estimators that implement fit and transform or &quot;</span>
<span class="gi">+                    &quot;fit_resample (but not both) or be a string &#39;passthrough&#39; &quot;</span>
<span class="gi">+                    &quot;&#39;%s&#39; (type %s) doesn&#39;t)&quot; % (t, type(t))</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if is_transfomer and is_sampler:</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    &quot;All intermediate steps of the chain should &quot;</span>
<span class="gi">+                    &quot;be estimators that implement fit and transform or &quot;</span>
<span class="gi">+                    &quot;fit_resample.&quot;</span>
<span class="gi">+                    &quot; &#39;%s&#39; implements both)&quot; % (t)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if isinstance(t, pipeline.Pipeline):</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    &quot;All intermediate steps of the chain should not be Pipelines&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        # We allow last estimator to be None as an identity transformation</span>
<span class="gi">+        if (</span>
<span class="gi">+            estimator is not None</span>
<span class="gi">+            and estimator != &quot;passthrough&quot;</span>
<span class="gi">+            and not hasattr(estimator, &quot;fit&quot;)</span>
<span class="gi">+        ):</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                &quot;Last step of Pipeline should implement fit or be &quot;</span>
<span class="gi">+                &quot;the string &#39;passthrough&#39;. &#39;%s&#39; (type %s) doesn&#39;t&quot;</span>
<span class="gi">+                % (estimator, type(estimator))</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _iter(self, with_final=True, filter_passthrough=True, filter_resample=True):</span>
<span class="w"> </span>        &quot;&quot;&quot;Generate (idx, (name, trans)) tuples from self.steps.

<span class="w"> </span>        When `filter_passthrough` is `True`, &#39;passthrough&#39; and None
<span class="w"> </span>        transformers are filtered out. When `filter_resample` is `True`,
<span class="w"> </span>        estimator with a method `fit_resample` are filtered out.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @_fit_context(prefer_skip_nested_validation=False)</span>
<span class="gi">+        it = super()._iter(with_final, filter_passthrough)</span>
<span class="gi">+        if filter_resample:</span>
<span class="gi">+            return filter(lambda x: not hasattr(x[-1], &quot;fit_resample&quot;), it)</span>
<span class="gi">+        else:</span>
<span class="gi">+            return it</span>
<span class="gi">+</span>
<span class="gi">+    # Estimator interface</span>
<span class="gi">+</span>
<span class="gi">+    # def _fit(self, X, y=None, **fit_params_steps):</span>
<span class="gi">+    def _fit(self, X, y=None, routed_params=None):</span>
<span class="gi">+        self.steps = list(self.steps)</span>
<span class="gi">+        self._validate_steps()</span>
<span class="gi">+        # Setup the memory</span>
<span class="gi">+        memory = check_memory(self.memory)</span>
<span class="gi">+</span>
<span class="gi">+        fit_transform_one_cached = memory.cache(_fit_transform_one)</span>
<span class="gi">+        fit_resample_one_cached = memory.cache(_fit_resample_one)</span>
<span class="gi">+</span>
<span class="gi">+        for step_idx, name, transformer in self._iter(</span>
<span class="gi">+            with_final=False, filter_passthrough=False, filter_resample=False</span>
<span class="gi">+        ):</span>
<span class="gi">+            if transformer is None or transformer == &quot;passthrough&quot;:</span>
<span class="gi">+                with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(step_idx)):</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+            if hasattr(memory, &quot;location&quot;) and memory.location is None:</span>
<span class="gi">+                # we do not clone when caching is disabled to</span>
<span class="gi">+                # preserve backward compatibility</span>
<span class="gi">+                cloned_transformer = transformer</span>
<span class="gi">+            else:</span>
<span class="gi">+                cloned_transformer = clone(transformer)</span>
<span class="gi">+</span>
<span class="gi">+            # Fit or load from cache the current transformer</span>
<span class="gi">+            if hasattr(cloned_transformer, &quot;transform&quot;) or hasattr(</span>
<span class="gi">+                cloned_transformer, &quot;fit_transform&quot;</span>
<span class="gi">+            ):</span>
<span class="gi">+                X, fitted_transformer = fit_transform_one_cached(</span>
<span class="gi">+                    cloned_transformer,</span>
<span class="gi">+                    X,</span>
<span class="gi">+                    y,</span>
<span class="gi">+                    None,</span>
<span class="gi">+                    message_clsname=&quot;Pipeline&quot;,</span>
<span class="gi">+                    message=self._log_message(step_idx),</span>
<span class="gi">+                    params=routed_params[name],</span>
<span class="gi">+                )</span>
<span class="gi">+            elif hasattr(cloned_transformer, &quot;fit_resample&quot;):</span>
<span class="gi">+                X, y, fitted_transformer = fit_resample_one_cached(</span>
<span class="gi">+                    cloned_transformer,</span>
<span class="gi">+                    X,</span>
<span class="gi">+                    y,</span>
<span class="gi">+                    message_clsname=&quot;Pipeline&quot;,</span>
<span class="gi">+                    message=self._log_message(step_idx),</span>
<span class="gi">+                    params=routed_params[name],</span>
<span class="gi">+                )</span>
<span class="gi">+            # Replace the transformer of the step with the fitted</span>
<span class="gi">+            # transformer. This is necessary when loading the transformer</span>
<span class="gi">+            # from the cache.</span>
<span class="gi">+            self.steps[step_idx] = (name, fitted_transformer)</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>
<span class="gi">+    # The `fit_*` methods need to be overridden to support the samplers.</span>
<span class="gi">+    @_fit_context(</span>
<span class="gi">+        # estimators in Pipeline.steps are not validated yet</span>
<span class="gi">+        prefer_skip_nested_validation=False</span>
<span class="gi">+    )</span>
<span class="w"> </span>    def fit(self, X, y=None, **params):
<span class="w"> </span>        &quot;&quot;&quot;Fit the model.

<span class="gu">@@ -186,10 +325,26 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        self : Pipeline
<span class="w"> </span>            This estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        routed_params = self._check_method_params(method=&quot;fit&quot;, props=params)</span>
<span class="gi">+        Xt, yt = self._fit(X, y, routed_params)</span>
<span class="gi">+        with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(len(self.steps) - 1)):</span>
<span class="gi">+            if self._final_estimator != &quot;passthrough&quot;:</span>
<span class="gi">+                last_step_params = routed_params[self.steps[-1][0]]</span>
<span class="gi">+                self._final_estimator.fit(Xt, yt, **last_step_params[&quot;fit&quot;])</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def _can_fit_transform(self):</span>
<span class="gi">+        return (</span>
<span class="gi">+            self._final_estimator == &quot;passthrough&quot;</span>
<span class="gi">+            or hasattr(self._final_estimator, &quot;transform&quot;)</span>
<span class="gi">+            or hasattr(self._final_estimator, &quot;fit_transform&quot;)</span>
<span class="gi">+        )</span>

<span class="w"> </span>    @available_if(_can_fit_transform)
<span class="gd">-    @_fit_context(prefer_skip_nested_validation=False)</span>
<span class="gi">+    @_fit_context(</span>
<span class="gi">+        # estimators in Pipeline.steps are not validated yet</span>
<span class="gi">+        prefer_skip_nested_validation=False</span>
<span class="gi">+    )</span>
<span class="w"> </span>    def fit_transform(self, X, y=None, **params):
<span class="w"> </span>        &quot;&quot;&quot;Fit the model and transform with the final estimator.

<span class="gu">@@ -233,9 +388,24 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        Xt : array-like of shape (n_samples, n_transformed_features)
<span class="w"> </span>            Transformed samples.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;predict&#39;))</span>
<span class="gi">+        routed_params = self._check_method_params(method=&quot;fit_transform&quot;, props=params)</span>
<span class="gi">+        Xt, yt = self._fit(X, y, routed_params)</span>
<span class="gi">+</span>
<span class="gi">+        last_step = self._final_estimator</span>
<span class="gi">+        with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(len(self.steps) - 1)):</span>
<span class="gi">+            if last_step == &quot;passthrough&quot;:</span>
<span class="gi">+                return Xt</span>
<span class="gi">+            last_step_params = routed_params[self.steps[-1][0]]</span>
<span class="gi">+            if hasattr(last_step, &quot;fit_transform&quot;):</span>
<span class="gi">+                return last_step.fit_transform(</span>
<span class="gi">+                    Xt, yt, **last_step_params[&quot;fit_transform&quot;]</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                return last_step.fit(Xt, y, **last_step_params[&quot;fit&quot;]).transform(</span>
<span class="gi">+                    Xt, **last_step_params[&quot;transform&quot;]</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;predict&quot;))</span>
<span class="w"> </span>    def predict(self, X, **params):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `predict` with the final estimator.

<span class="gu">@@ -282,10 +452,29 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_pred : ndarray
<span class="w"> </span>            Result of calling `predict` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+</span>
<span class="gi">+        if not _routing_enabled():</span>
<span class="gi">+            for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+                Xt = transform.transform(Xt)</span>
<span class="gi">+            return self.steps[-1][1].predict(Xt, **params)</span>
<span class="gi">+</span>
<span class="gi">+        # metadata routing enabled</span>
<span class="gi">+        routed_params = process_routing(self, &quot;predict&quot;, **params)</span>
<span class="gi">+        for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transform.transform(Xt, **routed_params[name].transform)</span>
<span class="gi">+        return self.steps[-1][1].predict(Xt, **routed_params[self.steps[-1][0]].predict)</span>
<span class="gi">+</span>
<span class="gi">+    def _can_fit_resample(self):</span>
<span class="gi">+        return self._final_estimator == &quot;passthrough&quot; or hasattr(</span>
<span class="gi">+            self._final_estimator, &quot;fit_resample&quot;</span>
<span class="gi">+        )</span>

<span class="w"> </span>    @available_if(_can_fit_resample)
<span class="gd">-    @_fit_context(prefer_skip_nested_validation=False)</span>
<span class="gi">+    @_fit_context(</span>
<span class="gi">+        # estimators in Pipeline.steps are not validated yet</span>
<span class="gi">+        prefer_skip_nested_validation=False</span>
<span class="gi">+    )</span>
<span class="w"> </span>    def fit_resample(self, X, y=None, **params):
<span class="w"> </span>        &quot;&quot;&quot;Fit the model and sample with the final estimator.

<span class="gu">@@ -332,10 +521,23 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        yt : array-like of shape (n_samples, n_transformed_features)
<span class="w"> </span>            Transformed target.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;fit_predict&#39;))</span>
<span class="gd">-    @_fit_context(prefer_skip_nested_validation=False)</span>
<span class="gi">+        routed_params = self._check_method_params(method=&quot;fit_resample&quot;, props=params)</span>
<span class="gi">+        Xt, yt = self._fit(X, y, routed_params)</span>
<span class="gi">+        last_step = self._final_estimator</span>
<span class="gi">+        with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(len(self.steps) - 1)):</span>
<span class="gi">+            if last_step == &quot;passthrough&quot;:</span>
<span class="gi">+                return Xt</span>
<span class="gi">+            last_step_params = routed_params[self.steps[-1][0]]</span>
<span class="gi">+            if hasattr(last_step, &quot;fit_resample&quot;):</span>
<span class="gi">+                return last_step.fit_resample(</span>
<span class="gi">+                    Xt, yt, **last_step_params[&quot;fit_resample&quot;]</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;fit_predict&quot;))</span>
<span class="gi">+    @_fit_context(</span>
<span class="gi">+        # estimators in Pipeline.steps are not validated yet</span>
<span class="gi">+        prefer_skip_nested_validation=False</span>
<span class="gi">+    )</span>
<span class="w"> </span>    def fit_predict(self, X, y=None, **params):
<span class="w"> </span>        &quot;&quot;&quot;Apply `fit_predict` of last step in pipeline after transforms.

<span class="gu">@@ -385,9 +587,20 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_pred : ndarray of shape (n_samples,)
<span class="w"> </span>            The predicted target.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;predict_proba&#39;))</span>
<span class="gi">+        routed_params = self._check_method_params(method=&quot;fit_predict&quot;, props=params)</span>
<span class="gi">+        Xt, yt = self._fit(X, y, routed_params)</span>
<span class="gi">+</span>
<span class="gi">+        params_last_step = routed_params[self.steps[-1][0]]</span>
<span class="gi">+        with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(len(self.steps) - 1)):</span>
<span class="gi">+            y_pred = self.steps[-1][-1].fit_predict(</span>
<span class="gi">+                Xt, yt, **params_last_step.get(&quot;fit_predict&quot;, {})</span>
<span class="gi">+            )</span>
<span class="gi">+        return y_pred</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: remove the following methods when the minimum scikit-learn &gt;= 1.4</span>
<span class="gi">+    # They do not depend on resampling but we need to redefine them for the</span>
<span class="gi">+    # compatibility with the metadata routing framework.</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;predict_proba&quot;))</span>
<span class="w"> </span>    def predict_proba(self, X, **params):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `predict_proba` with the final estimator.

<span class="gu">@@ -429,9 +642,22 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_proba : ndarray of shape (n_samples, n_classes)
<span class="w"> </span>            Result of calling `predict_proba` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;decision_function&#39;))</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+</span>
<span class="gi">+        if not _routing_enabled():</span>
<span class="gi">+            for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+                Xt = transform.transform(Xt)</span>
<span class="gi">+            return self.steps[-1][1].predict_proba(Xt, **params)</span>
<span class="gi">+</span>
<span class="gi">+        # metadata routing enabled</span>
<span class="gi">+        routed_params = process_routing(self, &quot;predict_proba&quot;, **params)</span>
<span class="gi">+        for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transform.transform(Xt, **routed_params[name].transform)</span>
<span class="gi">+        return self.steps[-1][1].predict_proba(</span>
<span class="gi">+            Xt, **routed_params[self.steps[-1][0]].predict_proba</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;decision_function&quot;))</span>
<span class="w"> </span>    def decision_function(self, X, **params):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `decision_function` with the final estimator.

<span class="gu">@@ -461,9 +687,22 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_score : ndarray of shape (n_samples, n_classes)
<span class="w"> </span>            Result of calling `decision_function` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;score_samples&#39;))</span>
<span class="gi">+        _raise_for_params(params, self, &quot;decision_function&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # not branching here since params is only available if</span>
<span class="gi">+        # enable_metadata_routing=True</span>
<span class="gi">+        routed_params = process_routing(self, &quot;decision_function&quot;, **params)</span>
<span class="gi">+</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+        for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transform.transform(</span>
<span class="gi">+                Xt, **routed_params.get(name, {}).get(&quot;transform&quot;, {})</span>
<span class="gi">+            )</span>
<span class="gi">+        return self.steps[-1][1].decision_function(</span>
<span class="gi">+            Xt, **routed_params.get(self.steps[-1][0], {}).get(&quot;decision_function&quot;, {})</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;score_samples&quot;))</span>
<span class="w"> </span>    def score_samples(self, X):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `score_samples` with the final estimator.

<span class="gu">@@ -483,9 +722,12 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_score : ndarray of shape (n_samples,)
<span class="w"> </span>            Result of calling `score_samples` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+        for _, _, transformer in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transformer.transform(Xt)</span>
<span class="gi">+        return self.steps[-1][1].score_samples(Xt)</span>

<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;predict_log_proba&#39;))</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;predict_log_proba&quot;))</span>
<span class="w"> </span>    def predict_log_proba(self, X, **params):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `predict_log_proba` with the final estimator.

<span class="gu">@@ -527,7 +769,25 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        y_log_proba : ndarray of shape (n_samples, n_classes)
<span class="w"> </span>            Result of calling `predict_log_proba` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+</span>
<span class="gi">+        if not _routing_enabled():</span>
<span class="gi">+            for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+                Xt = transform.transform(Xt)</span>
<span class="gi">+            return self.steps[-1][1].predict_log_proba(Xt, **params)</span>
<span class="gi">+</span>
<span class="gi">+        # metadata routing enabled</span>
<span class="gi">+        routed_params = process_routing(self, &quot;predict_log_proba&quot;, **params)</span>
<span class="gi">+        for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transform.transform(Xt, **routed_params[name].transform)</span>
<span class="gi">+        return self.steps[-1][1].predict_log_proba(</span>
<span class="gi">+            Xt, **routed_params[self.steps[-1][0]].predict_log_proba</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _can_transform(self):</span>
<span class="gi">+        return self._final_estimator == &quot;passthrough&quot; or hasattr(</span>
<span class="gi">+            self._final_estimator, &quot;transform&quot;</span>
<span class="gi">+        )</span>

<span class="w"> </span>    @available_if(_can_transform)
<span class="w"> </span>    def transform(self, X, **params):
<span class="gu">@@ -562,7 +822,18 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        Xt : ndarray of shape (n_samples, n_transformed_features)
<span class="w"> </span>            Transformed data.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        _raise_for_params(params, self, &quot;transform&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # not branching here since params is only available if</span>
<span class="gi">+        # enable_metadata_routing=True</span>
<span class="gi">+        routed_params = process_routing(self, &quot;transform&quot;, **params)</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+        for _, name, transform in self._iter():</span>
<span class="gi">+            Xt = transform.transform(Xt, **routed_params[name].transform)</span>
<span class="gi">+        return Xt</span>
<span class="gi">+</span>
<span class="gi">+    def _can_inverse_transform(self):</span>
<span class="gi">+        return all(hasattr(t, &quot;inverse_transform&quot;) for _, _, t in self._iter())</span>

<span class="w"> </span>    @available_if(_can_inverse_transform)
<span class="w"> </span>    def inverse_transform(self, Xt, **params):
<span class="gu">@@ -594,9 +865,19 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>            Inverse transformed data, that is, data in the original feature
<span class="w"> </span>            space.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    @available_if(pipeline._final_estimator_has(&#39;score&#39;))</span>
<span class="gi">+        _raise_for_params(params, self, &quot;inverse_transform&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        # we don&#39;t have to branch here, since params is only non-empty if</span>
<span class="gi">+        # enable_metadata_routing=True.</span>
<span class="gi">+        routed_params = process_routing(self, &quot;inverse_transform&quot;, **params)</span>
<span class="gi">+        reverse_iter = reversed(list(self._iter()))</span>
<span class="gi">+        for _, name, transform in reverse_iter:</span>
<span class="gi">+            Xt = transform.inverse_transform(</span>
<span class="gi">+                Xt, **routed_params[name].inverse_transform</span>
<span class="gi">+            )</span>
<span class="gi">+        return Xt</span>
<span class="gi">+</span>
<span class="gi">+    @available_if(pipeline._final_estimator_has(&quot;score&quot;))</span>
<span class="w"> </span>    def score(self, X, y=None, sample_weight=None, **params):
<span class="w"> </span>        &quot;&quot;&quot;Transform the data, and apply `score` with the final estimator.

<span class="gu">@@ -633,8 +914,27 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>        score : float
<span class="w"> </span>            Result of calling `score` on the final estimator.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+        if not _routing_enabled():</span>
<span class="gi">+            for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+                Xt = transform.transform(Xt)</span>
<span class="gi">+            score_params = {}</span>
<span class="gi">+            if sample_weight is not None:</span>
<span class="gi">+                score_params[&quot;sample_weight&quot;] = sample_weight</span>
<span class="gi">+            return self.steps[-1][1].score(Xt, y, **score_params)</span>
<span class="gi">+</span>
<span class="gi">+        # metadata routing is enabled.</span>
<span class="gi">+        routed_params = process_routing(</span>
<span class="gi">+            self, &quot;score&quot;, sample_weight=sample_weight, **params</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        Xt = X</span>
<span class="gi">+        for _, name, transform in self._iter(with_final=False):</span>
<span class="gi">+            Xt = transform.transform(Xt, **routed_params[name].transform)</span>
<span class="gi">+        return self.steps[-1][1].score(Xt, y, **routed_params[self.steps[-1][0]].score)</span>
<span class="gi">+</span>
<span class="gi">+    # TODO: once scikit-learn &gt;= 1.4, the following function should be simplified by</span>
<span class="gi">+    # calling `super().get_metadata_routing()`</span>
<span class="w"> </span>    def get_metadata_routing(self):
<span class="w"> </span>        &quot;&quot;&quot;Get metadata routing of this object.

<span class="gu">@@ -647,7 +947,116 @@ class Pipeline(_ParamsValidationMixin, pipeline.Pipeline):</span>
<span class="w"> </span>            A :class:`~utils.metadata_routing.MetadataRouter` encapsulating
<span class="w"> </span>            routing information.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        router = MetadataRouter(owner=self.__class__.__name__)</span>
<span class="gi">+</span>
<span class="gi">+        # first we add all steps except the last one</span>
<span class="gi">+        for _, name, trans in self._iter(with_final=False, filter_passthrough=True):</span>
<span class="gi">+            method_mapping = MethodMapping()</span>
<span class="gi">+            # fit, fit_predict, and fit_transform call fit_transform if it</span>
<span class="gi">+            # exists, or else fit and transform</span>
<span class="gi">+            if hasattr(trans, &quot;fit_transform&quot;):</span>
<span class="gi">+                (</span>
<span class="gi">+                    method_mapping.add(caller=&quot;fit&quot;, callee=&quot;fit_transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_transform&quot;, callee=&quot;fit_transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_predict&quot;, callee=&quot;fit_transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_resample&quot;, callee=&quot;fit_transform&quot;)</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                (</span>
<span class="gi">+                    method_mapping.add(caller=&quot;fit&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_transform&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_transform&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_predict&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_predict&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_resample&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                    .add(caller=&quot;fit_resample&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            (</span>
<span class="gi">+                method_mapping.add(caller=&quot;predict&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;predict&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;predict_proba&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;decision_function&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;predict_log_proba&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;transform&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;inverse_transform&quot;, callee=&quot;inverse_transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;score&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;fit_resample&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+            router.add(method_mapping=method_mapping, **{name: trans})</span>
<span class="gi">+</span>
<span class="gi">+        final_name, final_est = self.steps[-1]</span>
<span class="gi">+        if final_est is None or final_est == &quot;passthrough&quot;:</span>
<span class="gi">+            return router</span>
<span class="gi">+</span>
<span class="gi">+        # then we add the last step</span>
<span class="gi">+        method_mapping = MethodMapping()</span>
<span class="gi">+        if hasattr(final_est, &quot;fit_transform&quot;):</span>
<span class="gi">+            (</span>
<span class="gi">+                method_mapping.add(caller=&quot;fit_transform&quot;, callee=&quot;fit_transform&quot;).add(</span>
<span class="gi">+                    caller=&quot;fit_resample&quot;, callee=&quot;fit_transform&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            (</span>
<span class="gi">+                method_mapping.add(caller=&quot;fit&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                .add(caller=&quot;fit&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+                .add(caller=&quot;fit_resample&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+                .add(caller=&quot;fit_resample&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+            )</span>
<span class="gi">+        (</span>
<span class="gi">+            method_mapping.add(caller=&quot;fit&quot;, callee=&quot;fit&quot;)</span>
<span class="gi">+            .add(caller=&quot;predict&quot;, callee=&quot;predict&quot;)</span>
<span class="gi">+            .add(caller=&quot;fit_predict&quot;, callee=&quot;fit_predict&quot;)</span>
<span class="gi">+            .add(caller=&quot;predict_proba&quot;, callee=&quot;predict_proba&quot;)</span>
<span class="gi">+            .add(caller=&quot;decision_function&quot;, callee=&quot;decision_function&quot;)</span>
<span class="gi">+            .add(caller=&quot;predict_log_proba&quot;, callee=&quot;predict_log_proba&quot;)</span>
<span class="gi">+            .add(caller=&quot;transform&quot;, callee=&quot;transform&quot;)</span>
<span class="gi">+            .add(caller=&quot;inverse_transform&quot;, callee=&quot;inverse_transform&quot;)</span>
<span class="gi">+            .add(caller=&quot;score&quot;, callee=&quot;score&quot;)</span>
<span class="gi">+            .add(caller=&quot;fit_resample&quot;, callee=&quot;fit_resample&quot;)</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        router.add(method_mapping=method_mapping, **{final_name: final_est})</span>
<span class="gi">+        return router</span>
<span class="gi">+</span>
<span class="gi">+    def _check_method_params(self, method, props, **kwargs):</span>
<span class="gi">+        if _routing_enabled():</span>
<span class="gi">+            routed_params = process_routing(self, method, **props, **kwargs)</span>
<span class="gi">+            return routed_params</span>
<span class="gi">+        else:</span>
<span class="gi">+            fit_params_steps = Bunch(</span>
<span class="gi">+                **{</span>
<span class="gi">+                    name: Bunch(**{method: {} for method in METHODS})</span>
<span class="gi">+                    for name, step in self.steps</span>
<span class="gi">+                    if step is not None</span>
<span class="gi">+                }</span>
<span class="gi">+            )</span>
<span class="gi">+            for pname, pval in props.items():</span>
<span class="gi">+                if &quot;__&quot; not in pname:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        &quot;Pipeline.fit does not accept the {} parameter. &quot;</span>
<span class="gi">+                        &quot;You can pass parameters to specific steps of your &quot;</span>
<span class="gi">+                        &quot;pipeline using the stepname__parameter format, e.g. &quot;</span>
<span class="gi">+                        &quot;`Pipeline.fit(X, y, logisticregression__sample_weight&quot;</span>
<span class="gi">+                        &quot;=sample_weight)`.&quot;.format(pname)</span>
<span class="gi">+                    )</span>
<span class="gi">+                step, param = pname.split(&quot;__&quot;, 1)</span>
<span class="gi">+                fit_params_steps[step][&quot;fit&quot;][param] = pval</span>
<span class="gi">+                # without metadata routing, fit_transform and fit_predict</span>
<span class="gi">+                # get all the same params and pass it to the last fit.</span>
<span class="gi">+                fit_params_steps[step][&quot;fit_transform&quot;][param] = pval</span>
<span class="gi">+                fit_params_steps[step][&quot;fit_predict&quot;][param] = pval</span>
<span class="gi">+            return fit_params_steps</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _fit_resample_one(sampler, X, y, message_clsname=&quot;&quot;, message=None, params=None):</span>
<span class="gi">+    with _print_elapsed_time(message_clsname, message):</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y, **params.get(&quot;fit_resample&quot;, {}))</span>
<span class="gi">+</span>
<span class="gi">+        return X_res, y_res, sampler</span>


<span class="w"> </span>def _transform_one(transformer, X, y, weight, params):
<span class="gu">@@ -672,11 +1081,16 @@ def _transform_one(transformer, X, y, weight, params):</span>

<span class="w"> </span>        This should be of the form ``process_routing()[&quot;step_name&quot;]``.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    res = transformer.transform(X, **params.transform)</span>
<span class="gi">+    # if we have a weight for this transformer, multiply output</span>
<span class="gi">+    if weight is None:</span>
<span class="gi">+        return res</span>
<span class="gi">+    return res * weight</span>


<span class="gd">-def _fit_transform_one(transformer, X, y, weight, message_clsname=&#39;&#39;,</span>
<span class="gd">-    message=None, params=None):</span>
<span class="gi">+def _fit_transform_one(</span>
<span class="gi">+    transformer, X, y, weight, message_clsname=&quot;&quot;, message=None, params=None</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned
<span class="w"> </span>    with the fitted transformer. If ``weight`` is not ``None``, the result will
<span class="gu">@@ -684,11 +1098,24 @@ def _fit_transform_one(transformer, X, y, weight, message_clsname=&#39;&#39;,</span>

<span class="w"> </span>    ``params`` needs to be of the form ``process_routing()[&quot;step_name&quot;]``.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@validate_params({&#39;memory&#39;: [None, str, HasMethods([&#39;cache&#39;])], &#39;verbose&#39;:</span>
<span class="gd">-    [&#39;boolean&#39;]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    params = params or {}</span>
<span class="gi">+    with _print_elapsed_time(message_clsname, message):</span>
<span class="gi">+        if hasattr(transformer, &quot;fit_transform&quot;):</span>
<span class="gi">+            res = transformer.fit_transform(X, y, **params.get(&quot;fit_transform&quot;, {}))</span>
<span class="gi">+        else:</span>
<span class="gi">+            res = transformer.fit(X, y, **params.get(&quot;fit&quot;, {})).transform(</span>
<span class="gi">+                X, **params.get(&quot;transform&quot;, {})</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    if weight is None:</span>
<span class="gi">+        return res, transformer</span>
<span class="gi">+    return res * weight, transformer</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {&quot;memory&quot;: [None, str, HasMethods([&quot;cache&quot;])], &quot;verbose&quot;: [&quot;boolean&quot;]},</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="w"> </span>def make_pipeline(*steps, memory=None, verbose=False):
<span class="w"> </span>    &quot;&quot;&quot;Construct a Pipeline from the given estimators.

<span class="gu">@@ -733,4 +1160,4 @@ def make_pipeline(*steps, memory=None, verbose=False):</span>
<span class="w"> </span>    Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),
<span class="w"> </span>                    (&#39;gaussiannb&#39;, GaussianNB())])
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return Pipeline(pipeline._name_estimators(steps), memory=memory, verbose=verbose)</span>
<span class="gh">diff --git a/imblearn/tensorflow/_generator.py b/imblearn/tensorflow/_generator.py</span>
<span class="gh">index c55dd52..7e50322 100644</span>
<span class="gd">--- a/imblearn/tensorflow/_generator.py</span>
<span class="gi">+++ b/imblearn/tensorflow/_generator.py</span>
<span class="gu">@@ -1,15 +1,25 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Implement generators for ``tensorflow`` which will balance the data.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>from scipy.sparse import issparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ..under_sampling import RandomUnderSampler
<span class="w"> </span>from ..utils import Substitution
<span class="w"> </span>from ..utils._docstring import _random_state_docstring


<span class="w"> </span>@Substitution(random_state=_random_state_docstring)
<span class="gd">-def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="gd">-    batch_size=32, keep_sparse=False, random_state=None):</span>
<span class="gi">+def balanced_batch_generator(</span>
<span class="gi">+    X,</span>
<span class="gi">+    y,</span>
<span class="gi">+    *,</span>
<span class="gi">+    sample_weight=None,</span>
<span class="gi">+    sampler=None,</span>
<span class="gi">+    batch_size=32,</span>
<span class="gi">+    keep_sparse=False,</span>
<span class="gi">+    random_state=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Create a balanced batch generator to train tensorflow model.

<span class="w"> </span>    Returns a generator --- as well as the number of step per epoch --- to
<span class="gu">@@ -53,4 +63,35 @@ def balanced_batch_generator(X, y, *, sample_weight=None, sampler=None,</span>
<span class="w"> </span>    steps_per_epoch : int
<span class="w"> </span>        The number of samples per epoch.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    random_state = check_random_state(random_state)</span>
<span class="gi">+    if sampler is None:</span>
<span class="gi">+        sampler_ = RandomUnderSampler(random_state=random_state)</span>
<span class="gi">+    else:</span>
<span class="gi">+        sampler_ = clone(sampler)</span>
<span class="gi">+    sampler_.fit_resample(X, y)</span>
<span class="gi">+    if not hasattr(sampler_, &quot;sample_indices_&quot;):</span>
<span class="gi">+        raise ValueError(&quot;&#39;sampler&#39; needs to have an attribute &#39;sample_indices_&#39;.&quot;)</span>
<span class="gi">+    indices = sampler_.sample_indices_</span>
<span class="gi">+    # shuffle the indices since the sampler are packing them by class</span>
<span class="gi">+    random_state.shuffle(indices)</span>
<span class="gi">+</span>
<span class="gi">+    def generator(X, y, sample_weight, indices, batch_size):</span>
<span class="gi">+        while True:</span>
<span class="gi">+            for index in range(0, len(indices), batch_size):</span>
<span class="gi">+                X_res = _safe_indexing(X, indices[index : index + batch_size])</span>
<span class="gi">+                y_res = _safe_indexing(y, indices[index : index + batch_size])</span>
<span class="gi">+                if issparse(X_res) and not keep_sparse:</span>
<span class="gi">+                    X_res = X_res.toarray()</span>
<span class="gi">+                if sample_weight is None:</span>
<span class="gi">+                    yield X_res, y_res</span>
<span class="gi">+                else:</span>
<span class="gi">+                    sw_res = _safe_indexing(</span>
<span class="gi">+                        sample_weight, indices[index : index + batch_size]</span>
<span class="gi">+                    )</span>
<span class="gi">+                    yield X_res, y_res, sw_res</span>
<span class="gi">+</span>
<span class="gi">+    return (</span>
<span class="gi">+        generator(X, y, sample_weight, indices, batch_size),</span>
<span class="gi">+        int(indices.size // batch_size),</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/tensorflow/tests/test_generator.py b/imblearn/tensorflow/tests/test_generator.py</span>
<span class="gh">index 752979f..e0c7a91 100644</span>
<span class="gd">--- a/imblearn/tensorflow/tests/test_generator.py</span>
<span class="gi">+++ b/imblearn/tensorflow/tests/test_generator.py</span>
<span class="gu">@@ -3,8 +3,169 @@ import pytest</span>
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.datasets import load_iris
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from imblearn.datasets import make_imbalance
<span class="w"> </span>from imblearn.over_sampling import RandomOverSampler
<span class="w"> </span>from imblearn.tensorflow import balanced_batch_generator
<span class="w"> </span>from imblearn.under_sampling import NearMiss
<span class="gd">-tf = pytest.importorskip(&#39;tensorflow&#39;)</span>
<span class="gi">+</span>
<span class="gi">+tf = pytest.importorskip(&quot;tensorflow&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture</span>
<span class="gi">+def data():</span>
<span class="gi">+    X, y = load_iris(return_X_y=True)</span>
<span class="gi">+    X, y = make_imbalance(X, y, sampling_strategy={0: 30, 1: 50, 2: 40})</span>
<span class="gi">+    X = X.astype(np.float32)</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_balanced_batch_generator_tf_1_X_X(dataset, sampler):</span>
<span class="gi">+    X, y = dataset</span>
<span class="gi">+    batch_size = 10</span>
<span class="gi">+    training_generator, steps_per_epoch = balanced_batch_generator(</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+        sample_weight=None,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        batch_size=batch_size,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    learning_rate = 0.01</span>
<span class="gi">+    epochs = 10</span>
<span class="gi">+    input_size = X.shape[1]</span>
<span class="gi">+    output_size = 3</span>
<span class="gi">+</span>
<span class="gi">+    # helper functions</span>
<span class="gi">+    def init_weights(shape):</span>
<span class="gi">+        return tf.Variable(tf.random_normal(shape, stddev=0.01))</span>
<span class="gi">+</span>
<span class="gi">+    def accuracy(y_true, y_pred):</span>
<span class="gi">+        return np.mean(np.argmax(y_pred, axis=1) == y_true)</span>
<span class="gi">+</span>
<span class="gi">+    # input and output</span>
<span class="gi">+    data = tf.placeholder(&quot;float32&quot;, shape=[None, input_size])</span>
<span class="gi">+    targets = tf.placeholder(&quot;int32&quot;, shape=[None])</span>
<span class="gi">+</span>
<span class="gi">+    # build the model and weights</span>
<span class="gi">+    W = init_weights([input_size, output_size])</span>
<span class="gi">+    b = init_weights([output_size])</span>
<span class="gi">+    out_act = tf.nn.sigmoid(tf.matmul(data, W) + b)</span>
<span class="gi">+</span>
<span class="gi">+    # build the loss, predict, and train operator</span>
<span class="gi">+    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span>
<span class="gi">+        logits=out_act, labels=targets</span>
<span class="gi">+    )</span>
<span class="gi">+    loss = tf.reduce_sum(cross_entropy)</span>
<span class="gi">+    optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span>
<span class="gi">+    train_op = optimizer.minimize(loss)</span>
<span class="gi">+    predict = tf.nn.softmax(out_act)</span>
<span class="gi">+</span>
<span class="gi">+    # Initialization of all variables in the graph</span>
<span class="gi">+    init = tf.global_variables_initializer()</span>
<span class="gi">+</span>
<span class="gi">+    with tf.Session() as sess:</span>
<span class="gi">+        sess.run(init)</span>
<span class="gi">+</span>
<span class="gi">+        for e in range(epochs):</span>
<span class="gi">+            for i in range(steps_per_epoch):</span>
<span class="gi">+                X_batch, y_batch = next(training_generator)</span>
<span class="gi">+                sess.run(</span>
<span class="gi">+                    [train_op, loss],</span>
<span class="gi">+                    feed_dict={data: X_batch, targets: y_batch},</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # For each epoch, run accuracy on train and test</span>
<span class="gi">+            predicts_train = sess.run(predict, feed_dict={data: X})</span>
<span class="gi">+            print(f&quot;epoch: {e} train accuracy: {accuracy(y, predicts_train):.3f}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_balanced_batch_generator_tf_2_X_X_compat_1_X_X(dataset, sampler):</span>
<span class="gi">+    tf.compat.v1.disable_eager_execution()</span>
<span class="gi">+</span>
<span class="gi">+    X, y = dataset</span>
<span class="gi">+    batch_size = 10</span>
<span class="gi">+    training_generator, steps_per_epoch = balanced_batch_generator(</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+        sample_weight=None,</span>
<span class="gi">+        sampler=sampler,</span>
<span class="gi">+        batch_size=batch_size,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    learning_rate = 0.01</span>
<span class="gi">+    epochs = 10</span>
<span class="gi">+    input_size = X.shape[1]</span>
<span class="gi">+    output_size = 3</span>
<span class="gi">+</span>
<span class="gi">+    # helper functions</span>
<span class="gi">+    def init_weights(shape):</span>
<span class="gi">+        return tf.Variable(tf.random.normal(shape, stddev=0.01))</span>
<span class="gi">+</span>
<span class="gi">+    def accuracy(y_true, y_pred):</span>
<span class="gi">+        return np.mean(np.argmax(y_pred, axis=1) == y_true)</span>
<span class="gi">+</span>
<span class="gi">+    # input and output</span>
<span class="gi">+    data = tf.compat.v1.placeholder(&quot;float32&quot;, shape=[None, input_size])</span>
<span class="gi">+    targets = tf.compat.v1.placeholder(&quot;int32&quot;, shape=[None])</span>
<span class="gi">+</span>
<span class="gi">+    # build the model and weights</span>
<span class="gi">+    W = init_weights([input_size, output_size])</span>
<span class="gi">+    b = init_weights([output_size])</span>
<span class="gi">+    out_act = tf.nn.sigmoid(tf.matmul(data, W) + b)</span>
<span class="gi">+</span>
<span class="gi">+    # build the loss, predict, and train operator</span>
<span class="gi">+    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</span>
<span class="gi">+        logits=out_act, labels=targets</span>
<span class="gi">+    )</span>
<span class="gi">+    loss = tf.reduce_sum(input_tensor=cross_entropy)</span>
<span class="gi">+    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)</span>
<span class="gi">+    train_op = optimizer.minimize(loss)</span>
<span class="gi">+    predict = tf.nn.softmax(out_act)</span>
<span class="gi">+</span>
<span class="gi">+    # Initialization of all variables in the graph</span>
<span class="gi">+    init = tf.compat.v1.global_variables_initializer()</span>
<span class="gi">+</span>
<span class="gi">+    with tf.compat.v1.Session() as sess:</span>
<span class="gi">+        sess.run(init)</span>
<span class="gi">+</span>
<span class="gi">+        for e in range(epochs):</span>
<span class="gi">+            for i in range(steps_per_epoch):</span>
<span class="gi">+                X_batch, y_batch = next(training_generator)</span>
<span class="gi">+                sess.run(</span>
<span class="gi">+                    [train_op, loss],</span>
<span class="gi">+                    feed_dict={data: X_batch, targets: y_batch},</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # For each epoch, run accuracy on train and test</span>
<span class="gi">+            predicts_train = sess.run(predict, feed_dict={data: X})</span>
<span class="gi">+            print(f&quot;epoch: {e} train accuracy: {accuracy(y, predicts_train):.3f}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;sampler&quot;, [None, NearMiss(), RandomOverSampler()])</span>
<span class="gi">+def test_balanced_batch_generator(data, sampler):</span>
<span class="gi">+    if parse_version(tf.__version__) &lt; parse_version(&quot;2.0.0&quot;):</span>
<span class="gi">+        check_balanced_batch_generator_tf_1_X_X(data, sampler)</span>
<span class="gi">+    else:</span>
<span class="gi">+        check_balanced_batch_generator_tf_2_X_X_compat_1_X_X(data, sampler)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;keep_sparse&quot;, [True, False])</span>
<span class="gi">+def test_balanced_batch_generator_function_sparse(data, keep_sparse):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    training_generator, steps_per_epoch = balanced_batch_generator(</span>
<span class="gi">+        sparse.csr_matrix(X),</span>
<span class="gi">+        y,</span>
<span class="gi">+        keep_sparse=keep_sparse,</span>
<span class="gi">+        batch_size=10,</span>
<span class="gi">+        random_state=42,</span>
<span class="gi">+    )</span>
<span class="gi">+    for idx in range(steps_per_epoch):</span>
<span class="gi">+        X_batch, y_batch = next(training_generator)</span>
<span class="gi">+        if keep_sparse:</span>
<span class="gi">+            assert sparse.issparse(X_batch)</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert not sparse.issparse(X_batch)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py b/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gh">index 8b6d169..5e2ca3a 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_generation/_cluster_centroids.py</span>
<span class="gu">@@ -1,20 +1,30 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling by generating centroids based on
<span class="w"> </span>clustering.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Fernando Nogueira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.cluster import KMeans
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, StrOptions
<span class="w"> </span>from ..base import BaseUnderSampler
<span class="gd">-VOTING_KIND = &#39;auto&#39;, &#39;hard&#39;, &#39;soft&#39;</span>
<span class="gi">+</span>
<span class="gi">+VOTING_KIND = (&quot;auto&quot;, &quot;hard&quot;, &quot;soft&quot;)</span>


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, random_state=_random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class ClusterCentroids(BaseUnderSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample by generating centroids based on clustering methods.

<span class="gu">@@ -103,13 +113,22 @@ class ClusterCentroids(BaseUnderSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{...}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseUnderSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;estimator&#39;: [HasMethods([&#39;fit&#39;, &#39;predict&#39;]</span>
<span class="gd">-        ), None], &#39;voting&#39;: [StrOptions({&#39;auto&#39;, &#39;hard&#39;, &#39;soft&#39;})],</span>
<span class="gd">-        &#39;random_state&#39;: [&#39;random_state&#39;]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        estimator=None, voting=&#39;auto&#39;):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseUnderSampler._parameter_constraints,</span>
<span class="gi">+        &quot;estimator&quot;: [HasMethods([&quot;fit&quot;, &quot;predict&quot;]), None],</span>
<span class="gi">+        &quot;voting&quot;: [StrOptions({&quot;auto&quot;, &quot;hard&quot;, &quot;soft&quot;})],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        estimator=None,</span>
<span class="gi">+        voting=&quot;auto&quot;,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.estimator = estimator
<span class="gu">@@ -117,4 +136,70 @@ class ClusterCentroids(BaseUnderSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the KMeans estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.estimator is None:</span>
<span class="gi">+            self.estimator_ = KMeans(random_state=self.random_state)</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+            if &quot;n_clusters&quot; not in self.estimator_.get_params():</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;`estimator` should be a clustering estimator exposing a parameter&quot;</span>
<span class="gi">+                    &quot; `n_clusters` and a fitted parameter `cluster_centers_`.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+    def _generate_sample(self, X, y, centroids, target_class):</span>
<span class="gi">+        if self.voting_ == &quot;hard&quot;:</span>
<span class="gi">+            nearest_neighbors = NearestNeighbors(n_neighbors=1)</span>
<span class="gi">+            nearest_neighbors.fit(X, y)</span>
<span class="gi">+            indices = nearest_neighbors.kneighbors(centroids, return_distance=False)</span>
<span class="gi">+            X_new = _safe_indexing(X, np.squeeze(indices))</span>
<span class="gi">+        else:</span>
<span class="gi">+            if sparse.issparse(X):</span>
<span class="gi">+                X_new = sparse.csr_matrix(centroids, dtype=X.dtype)</span>
<span class="gi">+            else:</span>
<span class="gi">+                X_new = centroids</span>
<span class="gi">+        y_new = np.array([target_class] * centroids.shape[0], dtype=y.dtype)</span>
<span class="gi">+</span>
<span class="gi">+        return X_new, y_new</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        if self.voting == &quot;auto&quot;:</span>
<span class="gi">+            self.voting_ = &quot;hard&quot; if sparse.issparse(X) else &quot;soft&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.voting_ = self.voting</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled, y_resampled = [], []</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            target_class_indices = np.flatnonzero(y == target_class)</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                self.estimator_.set_params(**{&quot;n_clusters&quot;: n_samples})</span>
<span class="gi">+                self.estimator_.fit(_safe_indexing(X, target_class_indices))</span>
<span class="gi">+                if not hasattr(self.estimator_, &quot;cluster_centers_&quot;):</span>
<span class="gi">+                    raise RuntimeError(</span>
<span class="gi">+                        &quot;`estimator` should be a clustering estimator exposing a &quot;</span>
<span class="gi">+                        &quot;fitted parameter `cluster_centers_`.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                X_new, y_new = self._generate_sample(</span>
<span class="gi">+                    _safe_indexing(X, target_class_indices),</span>
<span class="gi">+                    _safe_indexing(y, target_class_indices),</span>
<span class="gi">+                    self.estimator_.cluster_centers_,</span>
<span class="gi">+                    target_class,</span>
<span class="gi">+                )</span>
<span class="gi">+                X_resampled.append(X_new)</span>
<span class="gi">+                y_resampled.append(y_new)</span>
<span class="gi">+            else:</span>
<span class="gi">+                X_resampled.append(_safe_indexing(X, target_class_indices))</span>
<span class="gi">+                y_resampled.append(_safe_indexing(y, target_class_indices))</span>
<span class="gi">+</span>
<span class="gi">+        if sparse.issparse(X):</span>
<span class="gi">+            X_resampled = sparse.vstack(X_resampled)</span>
<span class="gi">+        else:</span>
<span class="gi">+            X_resampled = np.vstack(X_resampled)</span>
<span class="gi">+        y_resampled = np.hstack(y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, np.array(y_resampled, dtype=y.dtype)</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: False}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_generation/tests/test_cluster_centroids.py b/imblearn/under_sampling/_prototype_generation/tests/test_cluster_centroids.py</span>
<span class="gh">index df6ac55..b51e350 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_generation/tests/test_cluster_centroids.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_generation/tests/test_cluster_centroids.py</span>
<span class="gu">@@ -1,17 +1,163 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module cluster centroids.&quot;&quot;&quot;
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.cluster import KMeans
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.linear_model import LogisticRegression
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import ClusterCentroids
<span class="w"> </span>from imblearn.utils.testing import _CustomClusterer
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.04352327, -0.20515826], [0.92923648, 0.76103773], [</span>
<span class="gd">-    0.20792588, 1.49407907], [0.47104475, 0.44386323], [0.22950086, </span>
<span class="gd">-    0.33367433], [0.15490546, 0.3130677], [0.09125309, -0.85409574], [</span>
<span class="gd">-    0.12372842, 0.6536186], [0.13347175, 0.12167502], [0.094035, -2.55298982]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.04352327, -0.20515826],</span>
<span class="gi">+        [0.92923648, 0.76103773],</span>
<span class="gi">+        [0.20792588, 1.49407907],</span>
<span class="gi">+        [0.47104475, 0.44386323],</span>
<span class="gi">+        [0.22950086, 0.33367433],</span>
<span class="gi">+        [0.15490546, 0.3130677],</span>
<span class="gi">+        [0.09125309, -0.85409574],</span>
<span class="gi">+        [0.12372842, 0.6536186],</span>
<span class="gi">+        [0.13347175, 0.12167502],</span>
<span class="gi">+        [0.094035, -2.55298982],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1])
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;X, expected_voting&quot;, [(X, &quot;soft&quot;), (sparse.csr_matrix(X), &quot;hard&quot;)]</span>
<span class="gi">+)</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_fit_resample_check_voting(X, expected_voting):</span>
<span class="gi">+    cc = ClusterCentroids(random_state=RND_SEED)</span>
<span class="gi">+    cc.fit_resample(X, Y)</span>
<span class="gi">+    assert cc.voting_ == expected_voting</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_fit_resample_auto():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    cc = ClusterCentroids(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = cc.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (6, 2)</span>
<span class="gi">+    assert y_resampled.shape == (6,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_fit_resample_half():</span>
<span class="gi">+    sampling_strategy = {0: 3, 1: 6}</span>
<span class="gi">+    cc = ClusterCentroids(sampling_strategy=sampling_strategy, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = cc.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (9, 2)</span>
<span class="gi">+    assert y_resampled.shape == (9,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_multiclass_fit_resample():</span>
<span class="gi">+    y = Y.copy()</span>
<span class="gi">+    y[5] = 2</span>
<span class="gi">+    y[6] = 2</span>
<span class="gi">+    cc = ClusterCentroids(random_state=RND_SEED)</span>
<span class="gi">+    _, y_resampled = cc.fit_resample(X, y)</span>
<span class="gi">+    count_y_res = Counter(y_resampled)</span>
<span class="gi">+    assert count_y_res[0] == 2</span>
<span class="gi">+    assert count_y_res[1] == 2</span>
<span class="gi">+    assert count_y_res[2] == 2</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_fit_resample_object():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    cluster = KMeans(random_state=RND_SEED, n_init=1)</span>
<span class="gi">+    cc = ClusterCentroids(</span>
<span class="gi">+        sampling_strategy=sampling_strategy,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+        estimator=cluster,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    X_resampled, y_resampled = cc.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (6, 2)</span>
<span class="gi">+    assert y_resampled.shape == (6,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_fit_hard_voting():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    voting = &quot;hard&quot;</span>
<span class="gi">+    cluster = KMeans(random_state=RND_SEED, n_init=1)</span>
<span class="gi">+    cc = ClusterCentroids(</span>
<span class="gi">+        sampling_strategy=sampling_strategy,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+        estimator=cluster,</span>
<span class="gi">+        voting=voting,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    X_resampled, y_resampled = cc.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (6, 2)</span>
<span class="gi">+    assert y_resampled.shape == (6,)</span>
<span class="gi">+    for x in X_resampled:</span>
<span class="gi">+        assert np.any(np.all(x == X, axis=1))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.filterwarnings(&quot;ignore:The default value of `n_init` will change&quot;)</span>
<span class="gi">+def test_cluster_centroids_hard_target_class():</span>
<span class="gi">+    # check that the samples selecting by the hard voting corresponds to the</span>
<span class="gi">+    # targeted class</span>
<span class="gi">+    # non-regression test for:</span>
<span class="gi">+    # https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=1,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        weights=[0.3, 0.7],</span>
<span class="gi">+        class_sep=0.01,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    cc = ClusterCentroids(voting=&quot;hard&quot;, random_state=0)</span>
<span class="gi">+    X_res, y_res = cc.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    minority_class_indices = np.flatnonzero(y == 0)</span>
<span class="gi">+    X_minority_class = X[minority_class_indices]</span>
<span class="gi">+</span>
<span class="gi">+    resampled_majority_class_indices = np.flatnonzero(y_res == 1)</span>
<span class="gi">+    X_res_majority = X_res[resampled_majority_class_indices]</span>
<span class="gi">+</span>
<span class="gi">+    sample_from_minority_in_majority = [</span>
<span class="gi">+        np.all(np.isclose(selected_sample, minority_sample))</span>
<span class="gi">+        for selected_sample in X_res_majority</span>
<span class="gi">+        for minority_sample in X_minority_class</span>
<span class="gi">+    ]</span>
<span class="gi">+    assert sum(sample_from_minority_in_majority) == 0</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_cluster_centroids_custom_clusterer():</span>
<span class="gi">+    clusterer = _CustomClusterer()</span>
<span class="gi">+    cc = ClusterCentroids(estimator=clusterer, random_state=RND_SEED)</span>
<span class="gi">+    cc.fit_resample(X, Y)</span>
<span class="gi">+    assert isinstance(cc.estimator_.cluster_centers_, np.ndarray)</span>
<span class="gi">+</span>
<span class="gi">+    clusterer = _CustomClusterer(expose_cluster_centers=False)</span>
<span class="gi">+    cc = ClusterCentroids(estimator=clusterer, random_state=RND_SEED)</span>
<span class="gi">+    err_msg = (</span>
<span class="gi">+        &quot;`estimator` should be a clustering estimator exposing a fitted parameter &quot;</span>
<span class="gi">+        &quot;`cluster_centers_`.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.raises(RuntimeError, match=err_msg):</span>
<span class="gi">+        cc.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    clusterer = LogisticRegression()</span>
<span class="gi">+    cc = ClusterCentroids(estimator=clusterer, random_state=RND_SEED)</span>
<span class="gi">+    err_msg = (</span>
<span class="gi">+        &quot;`estimator` should be a clustering estimator exposing a parameter &quot;</span>
<span class="gi">+        &quot;`n_clusters` and a fitted parameter `cluster_centers_`.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        cc.fit_resample(X, Y)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py b/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gh">index 5bd5434..ac012a0 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py</span>
<span class="gu">@@ -1,22 +1,31 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling based on the condensed nearest neighbour
<span class="w"> </span>method.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy.sparse import issparse
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval
<span class="w"> </span>from ..base import BaseCleaningSampler


<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class CondensedNearestNeighbour(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the condensed nearest neighbour method.

<span class="gu">@@ -103,25 +112,41 @@ class CondensedNearestNeighbour(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; from collections import Counter  # doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; from sklearn.datasets import fetch_openml  # doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; from sklearn.preprocessing import scale  # doctest: +SKIP
<span class="gd">-    &gt;&gt;&gt; from imblearn.under_sampling import CondensedNearestNeighbour  # doctest: +SKIP</span>
<span class="gi">+    &gt;&gt;&gt; from imblearn.under_sampling import \</span>
<span class="gi">+CondensedNearestNeighbour  # doctest: +SKIP</span>
<span class="w"> </span>    &gt;&gt;&gt; X, y = fetch_openml(&#39;diabetes&#39;, version=1, return_X_y=True)  # doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; X = scale(X)  # doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Original dataset shape %s&#39; % Counter(y))  # doctest: +SKIP
<span class="gd">-    Original dataset shape Counter({{&#39;tested_negative&#39;: 500,         &#39;tested_positive&#39;: 268}})  # doctest: +SKIP</span>
<span class="gi">+    Original dataset shape Counter({{&#39;tested_negative&#39;: 500, \</span>
<span class="gi">+        &#39;tested_positive&#39;: 268}})  # doctest: +SKIP</span>
<span class="w"> </span>    &gt;&gt;&gt; cnn = CondensedNearestNeighbour(random_state=42)  # doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; X_res, y_res = cnn.fit_resample(X, y)  #doctest: +SKIP
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))  # doctest: +SKIP
<span class="gd">-    Resampled dataset shape Counter({{&#39;tested_positive&#39;: 268,         &#39;tested_negative&#39;: 181}})  # doctest: +SKIP</span>
<span class="gi">+    Resampled dataset shape Counter({{&#39;tested_positive&#39;: 268, \</span>
<span class="gi">+        &#39;tested_negative&#39;: 181}})  # doctest: +SKIP</span>
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;]), None], &#39;n_seeds_S&#39;: [Interval(numbers.</span>
<span class="gd">-        Integral, 1, None, closed=&#39;left&#39;)], &#39;n_jobs&#39;: [numbers.Integral,</span>
<span class="gd">-        None], &#39;random_state&#39;: [&#39;random_state&#39;]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        n_neighbors=None, n_seeds_S=1, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_seeds_S&quot;: [Interval(numbers.Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        n_neighbors=None,</span>
<span class="gi">+        n_seeds_S=1,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="gu">@@ -130,9 +155,107 @@ class CondensedNearestNeighbour(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.n_neighbors is None:</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=1, n_jobs=self.n_jobs)</span>
<span class="gi">+        elif isinstance(self.n_neighbors, numbers.Integral):</span>
<span class="gi">+            estimator = KNeighborsClassifier(</span>
<span class="gi">+                n_neighbors=self.n_neighbors, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        elif isinstance(self.n_neighbors, KNeighborsClassifier):</span>
<span class="gi">+            estimator = clone(self.n_neighbors)</span>
<span class="gi">+</span>
<span class="gi">+        return estimator</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        estimator = self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        self.estimators_ = []</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                # Randomly get one sample from the majority class</span>
<span class="gi">+                # Generate the index to select</span>
<span class="gi">+                idx_maj = np.flatnonzero(y == target_class)</span>
<span class="gi">+                idx_maj_sample = idx_maj[</span>
<span class="gi">+                    random_state.randint(</span>
<span class="gi">+                        low=0,</span>
<span class="gi">+                        high=target_stats[target_class],</span>
<span class="gi">+                        size=self.n_seeds_S,</span>
<span class="gi">+                    )</span>
<span class="gi">+                ]</span>
<span class="gi">+</span>
<span class="gi">+                # Create the set C - One majority samples and all minority</span>
<span class="gi">+                C_indices = np.append(</span>
<span class="gi">+                    np.flatnonzero(y == class_minority), idx_maj_sample</span>
<span class="gi">+                )</span>
<span class="gi">+                C_x = _safe_indexing(X, C_indices)</span>
<span class="gi">+                C_y = _safe_indexing(y, C_indices)</span>
<span class="gi">+</span>
<span class="gi">+                # Create the set S - all majority samples</span>
<span class="gi">+                S_indices = np.flatnonzero(y == target_class)</span>
<span class="gi">+                S_x = _safe_indexing(X, S_indices)</span>
<span class="gi">+                S_y = _safe_indexing(y, S_indices)</span>
<span class="gi">+</span>
<span class="gi">+                # fit knn on C</span>
<span class="gi">+                self.estimators_.append(clone(estimator).fit(C_x, C_y))</span>
<span class="gi">+</span>
<span class="gi">+                good_classif_label = idx_maj_sample.copy()</span>
<span class="gi">+                # Check each sample in S if we keep it or drop it</span>
<span class="gi">+                for idx_sam, (x_sam, y_sam) in enumerate(zip(S_x, S_y)):</span>
<span class="gi">+                    # Do not select sample which are already well classified</span>
<span class="gi">+                    if idx_sam in good_classif_label:</span>
<span class="gi">+                        continue</span>
<span class="gi">+</span>
<span class="gi">+                    # Classify on S</span>
<span class="gi">+                    if not issparse(x_sam):</span>
<span class="gi">+                        x_sam = x_sam.reshape(1, -1)</span>
<span class="gi">+                    pred_y = self.estimators_[-1].predict(x_sam)</span>
<span class="gi">+</span>
<span class="gi">+                    # If the prediction do not agree with the true label</span>
<span class="gi">+                    # append it in C_x</span>
<span class="gi">+                    if y_sam != pred_y:</span>
<span class="gi">+                        # Keep the index for later</span>
<span class="gi">+                        idx_maj_sample = np.append(idx_maj_sample, idx_maj[idx_sam])</span>
<span class="gi">+</span>
<span class="gi">+                        # Update C</span>
<span class="gi">+                        C_indices = np.append(C_indices, idx_maj[idx_sam])</span>
<span class="gi">+                        C_x = _safe_indexing(X, C_indices)</span>
<span class="gi">+                        C_y = _safe_indexing(y, C_indices)</span>
<span class="gi">+</span>
<span class="gi">+                        # fit a knn on C</span>
<span class="gi">+                        self.estimators_[-1].fit(C_x, C_y)</span>
<span class="gi">+</span>
<span class="gi">+                        # This experimental to speed up the search</span>
<span class="gi">+                        # Classify all the element in S and avoid to test the</span>
<span class="gi">+                        # well classified elements</span>
<span class="gi">+                        pred_S_y = self.estimators_[-1].predict(S_x)</span>
<span class="gi">+                        good_classif_label = np.unique(</span>
<span class="gi">+                            np.append(idx_maj_sample, np.flatnonzero(pred_S_y == S_y))</span>
<span class="gi">+                        )</span>
<span class="gi">+</span>
<span class="gi">+                idx_under = np.concatenate((idx_under, idx_maj_sample), axis=0)</span>
<span class="gi">+            else:</span>
<span class="gi">+                idx_under = np.concatenate(</span>
<span class="gi">+                    (idx_under, np.flatnonzero(y == target_class)), axis=0</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Last fitted k-NN estimator.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`estimator_` attribute has been deprecated in 0.12 and will be &quot;</span>
<span class="gi">+            &quot;removed in 0.14. Use `estimators_` instead.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.estimators_[-1]</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py b/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gh">index 067fe55..38abd4b 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py</span>
<span class="gu">@@ -1,19 +1,30 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Classes to perform under-sampling based on the edited nearest neighbour
<span class="w"> </span>method.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Dayvid Oliveira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution, check_neighbors_object
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval, StrOptions
<span class="w"> </span>from ...utils.fixes import _mode
<span class="w"> </span>from ..base import BaseCleaningSampler
<span class="gd">-SEL_KIND = &#39;all&#39;, &#39;mode&#39;</span>
<span class="gi">+</span>
<span class="gi">+SEL_KIND = (&quot;all&quot;, &quot;mode&quot;)</span>


<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class EditedNearestNeighbours(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the edited nearest neighbour method.

<span class="gu">@@ -111,14 +122,25 @@ class EditedNearestNeighbours(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 887, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;kind_sel&#39;: [StrOptions({&#39;all&#39;, &#39;mode&#39;})],</span>
<span class="gd">-        &#39;n_jobs&#39;: [numbers.Integral, None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, n_neighbors=3, kind_sel</span>
<span class="gd">-        =&#39;all&#39;, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;kind_sel&quot;: [StrOptions({&quot;all&quot;, &quot;mode&quot;})],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        n_neighbors=3,</span>
<span class="gi">+        kind_sel=&quot;all&quot;,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.kind_sel = kind_sel
<span class="gu">@@ -126,11 +148,55 @@ class EditedNearestNeighbours(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Validate the estimator created in the ENN.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+        self.nn_ = check_neighbors_object(</span>
<span class="gi">+            &quot;n_neighbors&quot;, self.n_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+        self.nn_.set_params(**{&quot;n_jobs&quot;: self.n_jobs})</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        self.nn_.fit(X)</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                target_class_indices = np.flatnonzero(y == target_class)</span>
<span class="gi">+                X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+                y_class = _safe_indexing(y, target_class_indices)</span>
<span class="gi">+                nnhood_idx = self.nn_.kneighbors(X_class, return_distance=False)[:, 1:]</span>
<span class="gi">+                nnhood_label = y[nnhood_idx]</span>
<span class="gi">+                if self.kind_sel == &quot;mode&quot;:</span>
<span class="gi">+                    nnhood_label, _ = _mode(nnhood_label, axis=1)</span>
<span class="gi">+                    nnhood_bool = np.ravel(nnhood_label) == y_class</span>
<span class="gi">+                elif self.kind_sel == &quot;all&quot;:</span>
<span class="gi">+                    nnhood_label = nnhood_label == target_class</span>
<span class="gi">+                    nnhood_bool = np.all(nnhood_label, axis=1)</span>
<span class="gi">+                index_target_class = np.flatnonzero(nnhood_bool)</span>
<span class="gi">+            else:</span>
<span class="gi">+                index_target_class = slice(None)</span>
<span class="gi">+</span>
<span class="gi">+            idx_under = np.concatenate(</span>
<span class="gi">+                (</span>
<span class="gi">+                    idx_under,</span>
<span class="gi">+                    np.flatnonzero(y == target_class)[index_target_class],</span>
<span class="gi">+                ),</span>
<span class="gi">+                axis=0,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class RepeatedEditedNearestNeighbours(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the repeated edited nearest neighbour method.

<span class="gu">@@ -241,15 +307,27 @@ class RepeatedEditedNearestNeighbours(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 887, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;max_iter&#39;: [Interval(numbers.Integral, 1,</span>
<span class="gd">-        None, closed=&#39;left&#39;)], &#39;kind_sel&#39;: [StrOptions({&#39;all&#39;, &#39;mode&#39;})],</span>
<span class="gd">-        &#39;n_jobs&#39;: [numbers.Integral, None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, n_neighbors=3, max_iter</span>
<span class="gd">-        =100, kind_sel=&#39;all&#39;, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;max_iter&quot;: [Interval(numbers.Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+        &quot;kind_sel&quot;: [StrOptions({&quot;all&quot;, &quot;mode&quot;})],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        n_neighbors=3,</span>
<span class="gi">+        max_iter=100,</span>
<span class="gi">+        kind_sel=&quot;all&quot;,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.kind_sel = kind_sel
<span class="gu">@@ -258,11 +336,88 @@ class RepeatedEditedNearestNeighbours(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+        self.nn_ = check_neighbors_object(</span>
<span class="gi">+            &quot;n_neighbors&quot;, self.n_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            n_neighbors=self.nn_,</span>
<span class="gi">+            kind_sel=self.kind_sel,</span>
<span class="gi">+            n_jobs=self.n_jobs,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_, y_ = X, y</span>
<span class="gi">+        self.sample_indices_ = np.arange(X.shape[0], dtype=int)</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+</span>
<span class="gi">+        for n_iter in range(self.max_iter):</span>
<span class="gi">+            prev_len = y_.shape[0]</span>
<span class="gi">+            X_enn, y_enn = self.enn_.fit_resample(X_, y_)</span>
<span class="gi">+</span>
<span class="gi">+            # Check the stopping criterion</span>
<span class="gi">+            # 1. If there is no changes for the vector y</span>
<span class="gi">+            # 2. If the number of samples in the other class become inferior to</span>
<span class="gi">+            # the number of samples in the majority class</span>
<span class="gi">+            # 3. If one of the class is disappearing</span>
<span class="gi">+</span>
<span class="gi">+            # Case 1</span>
<span class="gi">+            b_conv = prev_len == y_enn.shape[0]</span>
<span class="gi">+</span>
<span class="gi">+            # Case 2</span>
<span class="gi">+            stats_enn = Counter(y_enn)</span>
<span class="gi">+            count_non_min = np.array(</span>
<span class="gi">+                [</span>
<span class="gi">+                    val</span>
<span class="gi">+                    for val, key in zip(stats_enn.values(), stats_enn.keys())</span>
<span class="gi">+                    if key != class_minority</span>
<span class="gi">+                ]</span>
<span class="gi">+            )</span>
<span class="gi">+            b_min_bec_maj = np.any(count_non_min &lt; target_stats[class_minority])</span>
<span class="gi">+</span>
<span class="gi">+            # Case 3</span>
<span class="gi">+            b_remove_maj_class = len(stats_enn) &lt; len(target_stats)</span>
<span class="gi">+</span>
<span class="gi">+            (</span>
<span class="gi">+                X_,</span>
<span class="gi">+                y_,</span>
<span class="gi">+            ) = (</span>
<span class="gi">+                X_enn,</span>
<span class="gi">+                y_enn,</span>
<span class="gi">+            )</span>
<span class="gi">+            self.sample_indices_ = self.sample_indices_[self.enn_.sample_indices_]</span>
<span class="gi">+</span>
<span class="gi">+            if b_conv or b_min_bec_maj or b_remove_maj_class:</span>
<span class="gi">+                if b_conv:</span>
<span class="gi">+                    (</span>
<span class="gi">+                        X_,</span>
<span class="gi">+                        y_,</span>
<span class="gi">+                    ) = (</span>
<span class="gi">+                        X_enn,</span>
<span class="gi">+                        y_enn,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    self.sample_indices_ = self.sample_indices_[</span>
<span class="gi">+                        self.enn_.sample_indices_</span>
<span class="gi">+                    ]</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+        self.n_iter_ = n_iter + 1</span>
<span class="gi">+        X_resampled, y_resampled = X_, y_</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class AllKNN(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the AllKNN method.

<span class="gu">@@ -372,14 +527,27 @@ class AllKNN(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 887, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;kind_sel&#39;: [StrOptions({&#39;all&#39;, &#39;mode&#39;})],</span>
<span class="gd">-        &#39;allow_minority&#39;: [&#39;boolean&#39;], &#39;n_jobs&#39;: [numbers.Integral, None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, n_neighbors=3, kind_sel</span>
<span class="gd">-        =&#39;all&#39;, allow_minority=False, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;kind_sel&quot;: [StrOptions({&quot;all&quot;, &quot;mode&quot;})],</span>
<span class="gi">+        &quot;allow_minority&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        n_neighbors=3,</span>
<span class="gi">+        kind_sel=&quot;all&quot;,</span>
<span class="gi">+        allow_minority=False,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.kind_sel = kind_sel
<span class="gu">@@ -388,4 +556,68 @@ class AllKNN(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create objects required by AllKNN&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        self.nn_ = check_neighbors_object(</span>
<span class="gi">+            &quot;n_neighbors&quot;, self.n_neighbors, additional_neighbor=1</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        self.enn_ = EditedNearestNeighbours(</span>
<span class="gi">+            sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+            n_neighbors=self.nn_,</span>
<span class="gi">+            kind_sel=self.kind_sel,</span>
<span class="gi">+            n_jobs=self.n_jobs,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        X_, y_ = X, y</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = np.arange(X.shape[0], dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        for curr_size_ngh in range(1, self.nn_.n_neighbors):</span>
<span class="gi">+            self.enn_.n_neighbors = curr_size_ngh</span>
<span class="gi">+</span>
<span class="gi">+            X_enn, y_enn = self.enn_.fit_resample(X_, y_)</span>
<span class="gi">+</span>
<span class="gi">+            # Check the stopping criterion</span>
<span class="gi">+            # 1. If the number of samples in the other class become inferior to</span>
<span class="gi">+            # the number of samples in the majority class</span>
<span class="gi">+            # 2. If one of the class is disappearing</span>
<span class="gi">+            # Case 1else:</span>
<span class="gi">+</span>
<span class="gi">+            stats_enn = Counter(y_enn)</span>
<span class="gi">+            count_non_min = np.array(</span>
<span class="gi">+                [</span>
<span class="gi">+                    val</span>
<span class="gi">+                    for val, key in zip(stats_enn.values(), stats_enn.keys())</span>
<span class="gi">+                    if key != class_minority</span>
<span class="gi">+                ]</span>
<span class="gi">+            )</span>
<span class="gi">+            b_min_bec_maj = np.any(count_non_min &lt; target_stats[class_minority])</span>
<span class="gi">+            if self.allow_minority:</span>
<span class="gi">+                # overwrite b_min_bec_maj</span>
<span class="gi">+                b_min_bec_maj = False</span>
<span class="gi">+</span>
<span class="gi">+            # Case 2</span>
<span class="gi">+            b_remove_maj_class = len(stats_enn) &lt; len(target_stats)</span>
<span class="gi">+</span>
<span class="gi">+            (</span>
<span class="gi">+                X_,</span>
<span class="gi">+                y_,</span>
<span class="gi">+            ) = (</span>
<span class="gi">+                X_enn,</span>
<span class="gi">+                y_enn,</span>
<span class="gi">+            )</span>
<span class="gi">+            self.sample_indices_ = self.sample_indices_[self.enn_.sample_indices_]</span>
<span class="gi">+</span>
<span class="gi">+            if b_min_bec_maj or b_remove_maj_class:</span>
<span class="gi">+                break</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled, y_resampled = X_, y_</span>
<span class="gi">+</span>
<span class="gi">+        return X_resampled, y_resampled</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py b/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gh">index c858b97..dac3f3c 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_instance_hardness_threshold.py</span>
<span class="gu">@@ -1,22 +1,32 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling based on the instance hardness
<span class="w"> </span>threshold.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Dayvid Oliveira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.base import clone, is_classifier
<span class="w"> </span>from sklearn.ensemble import RandomForestClassifier
<span class="w"> </span>from sklearn.ensemble._base import _set_random_states
<span class="w"> </span>from sklearn.model_selection import StratifiedKFold, cross_val_predict
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods
<span class="w"> </span>from ..base import BaseUnderSampler


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class InstanceHardnessThreshold(BaseUnderSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the instance hardness threshold.

<span class="gu">@@ -98,13 +108,27 @@ class InstanceHardnessThreshold(BaseUnderSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 5..., 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseUnderSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;estimator&#39;: [HasMethods([&#39;fit&#39;,</span>
<span class="gd">-        &#39;predict_proba&#39;]), None], &#39;cv&#39;: [&#39;cv_object&#39;], &#39;n_jobs&#39;: [numbers.</span>
<span class="gd">-        Integral, None], &#39;random_state&#39;: [&#39;random_state&#39;]}</span>

<span class="gd">-    def __init__(self, *, estimator=None, sampling_strategy=&#39;auto&#39;,</span>
<span class="gd">-        random_state=None, cv=5, n_jobs=None):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseUnderSampler._parameter_constraints,</span>
<span class="gi">+        &quot;estimator&quot;: [</span>
<span class="gi">+            HasMethods([&quot;fit&quot;, &quot;predict_proba&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;cv&quot;: [&quot;cv_object&quot;],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        estimator=None,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        cv=5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.estimator = estimator
<span class="gu">@@ -113,4 +137,68 @@ class InstanceHardnessThreshold(BaseUnderSampler):</span>

<span class="w"> </span>    def _validate_estimator(self, random_state):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the classifier&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        if (</span>
<span class="gi">+            self.estimator is not None</span>
<span class="gi">+            and is_classifier(self.estimator)</span>
<span class="gi">+            and hasattr(self.estimator, &quot;predict_proba&quot;)</span>
<span class="gi">+        ):</span>
<span class="gi">+            self.estimator_ = clone(self.estimator)</span>
<span class="gi">+            _set_random_states(self.estimator_, random_state)</span>
<span class="gi">+</span>
<span class="gi">+        elif self.estimator is None:</span>
<span class="gi">+            self.estimator_ = RandomForestClassifier(</span>
<span class="gi">+                n_estimators=100,</span>
<span class="gi">+                random_state=self.random_state,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        self._validate_estimator(random_state)</span>
<span class="gi">+</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        skf = StratifiedKFold(</span>
<span class="gi">+            n_splits=self.cv,</span>
<span class="gi">+            shuffle=True,</span>
<span class="gi">+            random_state=random_state,</span>
<span class="gi">+        )</span>
<span class="gi">+        probabilities = cross_val_predict(</span>
<span class="gi">+            self.estimator_,</span>
<span class="gi">+            X,</span>
<span class="gi">+            y,</span>
<span class="gi">+            cv=skf,</span>
<span class="gi">+            n_jobs=self.n_jobs,</span>
<span class="gi">+            method=&quot;predict_proba&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        probabilities = probabilities[range(len(y)), y]</span>
<span class="gi">+</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                threshold = np.percentile(</span>
<span class="gi">+                    probabilities[y == target_class],</span>
<span class="gi">+                    (1.0 - (n_samples / target_stats[target_class])) * 100.0,</span>
<span class="gi">+                )</span>
<span class="gi">+                index_target_class = np.flatnonzero(</span>
<span class="gi">+                    probabilities[y == target_class] &gt;= threshold</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                index_target_class = slice(None)</span>
<span class="gi">+</span>
<span class="gi">+            idx_under = np.concatenate(</span>
<span class="gi">+                (</span>
<span class="gi">+                    idx_under,</span>
<span class="gi">+                    np.flatnonzero(y == target_class)[index_target_class],</span>
<span class="gi">+                ),</span>
<span class="gi">+                axis=0,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_nearmiss.py b/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gh">index f64b76a..70f647f 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_nearmiss.py</span>
<span class="gu">@@ -1,17 +1,26 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling based on nearmiss methods.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution, check_neighbors_object
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval
<span class="w"> </span>from ..base import BaseUnderSampler


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class NearMiss(BaseUnderSampler):
<span class="w"> </span>    &quot;&quot;&quot;Class to perform under-sampling based on NearMiss methods.

<span class="gu">@@ -102,24 +111,39 @@ class NearMiss(BaseUnderSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 100, 1: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseUnderSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;version&#39;: [Interval(numbers.Integral, 1, 3,</span>
<span class="gd">-        closed=&#39;both&#39;)], &#39;n_neighbors&#39;: [Interval(numbers.Integral, 1, None,</span>
<span class="gd">-        closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;, &#39;kneighbors_graph&#39;])],</span>
<span class="gd">-        &#39;n_neighbors_ver3&#39;: [Interval(numbers.Integral, 1, None, closed=</span>
<span class="gd">-        &#39;left&#39;), HasMethods([&#39;kneighbors&#39;, &#39;kneighbors_graph&#39;])], &#39;n_jobs&#39;:</span>
<span class="gd">-        [numbers.Integral, None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, version=1, n_neighbors=</span>
<span class="gd">-        3, n_neighbors_ver3=3, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseUnderSampler._parameter_constraints,</span>
<span class="gi">+        &quot;version&quot;: [Interval(numbers.Integral, 1, 3, closed=&quot;both&quot;)],</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_neighbors_ver3&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        version=1,</span>
<span class="gi">+        n_neighbors=3,</span>
<span class="gi">+        n_neighbors_ver3=3,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.version = version
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.n_neighbors_ver3 = n_neighbors_ver3
<span class="w"> </span>        self.n_jobs = n_jobs

<span class="gd">-    def _selection_dist_based(self, X, y, dist_vec, num_samples, key,</span>
<span class="gd">-        sel_strategy=&#39;nearest&#39;):</span>
<span class="gi">+    def _selection_dist_based(</span>
<span class="gi">+        self, X, y, dist_vec, num_samples, key, sel_strategy=&quot;nearest&quot;</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        &quot;&quot;&quot;Select the appropriate samples depending of the strategy selected.

<span class="w"> </span>        Parameters
<span class="gu">@@ -148,8 +172,143 @@ class NearMiss(BaseUnderSampler):</span>
<span class="w"> </span>            The list of the indices of the selected samples.

<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        # Compute the distance considering the farthest neighbour</span>
<span class="gi">+        dist_avg_vec = np.sum(dist_vec[:, -self.nn_.n_neighbors :], axis=1)</span>
<span class="gi">+</span>
<span class="gi">+        target_class_indices = np.flatnonzero(y == key)</span>
<span class="gi">+        if dist_vec.shape[0] != _safe_indexing(X, target_class_indices).shape[0]:</span>
<span class="gi">+            raise RuntimeError(</span>
<span class="gi">+                &quot;The samples to be selected do not correspond&quot;</span>
<span class="gi">+                &quot; to the distance matrix given. Ensure that&quot;</span>
<span class="gi">+                &quot; both `X[y == key]` and `dist_vec` are&quot;</span>
<span class="gi">+                &quot; related.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Sort the list of distance and get the index</span>
<span class="gi">+        if sel_strategy == &quot;nearest&quot;:</span>
<span class="gi">+            sort_way = False</span>
<span class="gi">+        else:  # sel_strategy == &quot;farthest&quot;:</span>
<span class="gi">+            sort_way = True</span>
<span class="gi">+</span>
<span class="gi">+        sorted_idx = sorted(</span>
<span class="gi">+            range(len(dist_avg_vec)),</span>
<span class="gi">+            key=dist_avg_vec.__getitem__,</span>
<span class="gi">+            reverse=sort_way,</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        # Throw a warning to tell the user that we did not have enough samples</span>
<span class="gi">+        # to select and that we just select everything</span>
<span class="gi">+        if len(sorted_idx) &lt; num_samples:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;The number of the samples to be selected is larger&quot;</span>
<span class="gi">+                &quot; than the number of samples available. The&quot;</span>
<span class="gi">+                &quot; balancing ratio cannot be ensure and all samples&quot;</span>
<span class="gi">+                &quot; will be returned.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        # Select the desired number of samples</span>
<span class="gi">+        return sorted_idx[:num_samples]</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        self.nn_ = check_neighbors_object(&quot;n_neighbors&quot;, self.n_neighbors)</span>
<span class="gi">+        self.nn_.set_params(**{&quot;n_jobs&quot;: self.n_jobs})</span>
<span class="gi">+</span>
<span class="gi">+        if self.version == 3:</span>
<span class="gi">+            self.nn_ver3_ = check_neighbors_object(</span>
<span class="gi">+                &quot;n_neighbors_ver3&quot;, self.n_neighbors_ver3</span>
<span class="gi">+            )</span>
<span class="gi">+            self.nn_ver3_.set_params(**{&quot;n_jobs&quot;: self.n_jobs})</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        minority_class_indices = np.flatnonzero(y == class_minority)</span>
<span class="gi">+</span>
<span class="gi">+        self.nn_.fit(_safe_indexing(X, minority_class_indices))</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                target_class_indices = np.flatnonzero(y == target_class)</span>
<span class="gi">+                X_class = _safe_indexing(X, target_class_indices)</span>
<span class="gi">+                y_class = _safe_indexing(y, target_class_indices)</span>
<span class="gi">+</span>
<span class="gi">+                if self.version == 1:</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_.kneighbors(</span>
<span class="gi">+                        X_class, n_neighbors=self.nn_.n_neighbors</span>
<span class="gi">+                    )</span>
<span class="gi">+                    index_target_class = self._selection_dist_based(</span>
<span class="gi">+                        X,</span>
<span class="gi">+                        y,</span>
<span class="gi">+                        dist_vec,</span>
<span class="gi">+                        n_samples,</span>
<span class="gi">+                        target_class,</span>
<span class="gi">+                        sel_strategy=&quot;nearest&quot;,</span>
<span class="gi">+                    )</span>
<span class="gi">+                elif self.version == 2:</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_.kneighbors(</span>
<span class="gi">+                        X_class, n_neighbors=target_stats[class_minority]</span>
<span class="gi">+                    )</span>
<span class="gi">+                    index_target_class = self._selection_dist_based(</span>
<span class="gi">+                        X,</span>
<span class="gi">+                        y,</span>
<span class="gi">+                        dist_vec,</span>
<span class="gi">+                        n_samples,</span>
<span class="gi">+                        target_class,</span>
<span class="gi">+                        sel_strategy=&quot;nearest&quot;,</span>
<span class="gi">+                    )</span>
<span class="gi">+                elif self.version == 3:</span>
<span class="gi">+                    self.nn_ver3_.fit(X_class)</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_ver3_.kneighbors(</span>
<span class="gi">+                        _safe_indexing(X, minority_class_indices)</span>
<span class="gi">+                    )</span>
<span class="gi">+                    idx_vec_farthest = np.unique(idx_vec.reshape(-1))</span>
<span class="gi">+                    X_class_selected = _safe_indexing(X_class, idx_vec_farthest)</span>
<span class="gi">+                    y_class_selected = _safe_indexing(y_class, idx_vec_farthest)</span>
<span class="gi">+</span>
<span class="gi">+                    dist_vec, idx_vec = self.nn_.kneighbors(</span>
<span class="gi">+                        X_class_selected, n_neighbors=self.nn_.n_neighbors</span>
<span class="gi">+                    )</span>
<span class="gi">+                    index_target_class = self._selection_dist_based(</span>
<span class="gi">+                        X_class_selected,</span>
<span class="gi">+                        y_class_selected,</span>
<span class="gi">+                        dist_vec,</span>
<span class="gi">+                        n_samples,</span>
<span class="gi">+                        target_class,</span>
<span class="gi">+                        sel_strategy=&quot;farthest&quot;,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    # idx_tmp is relative to the feature selected in the</span>
<span class="gi">+                    # previous step and we need to find the indirection</span>
<span class="gi">+                    index_target_class = idx_vec_farthest[index_target_class]</span>
<span class="gi">+            else:</span>
<span class="gi">+                index_target_class = slice(None)</span>
<span class="gi">+</span>
<span class="gi">+            idx_under = np.concatenate(</span>
<span class="gi">+                (</span>
<span class="gi">+                    idx_under,</span>
<span class="gi">+                    np.flatnonzero(y == target_class)[index_target_class],</span>
<span class="gi">+                ),</span>
<span class="gi">+                axis=0,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+    # fmt: off</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;sample_indices&quot;: True,</span>
<span class="gi">+            &quot;_xfail_checks&quot;: {</span>
<span class="gi">+                &quot;check_samplers_fit_resample&quot;:</span>
<span class="gi">+                &quot;Fails for NearMiss-3 with less samples than expected&quot;</span>
<span class="gi">+            }</span>
<span class="gi">+        }</span>
<span class="gi">+    # fmt: on</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py b/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gh">index e0a2f31..188ba32 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_neighbourhood_cleaning_rule.py</span>
<span class="gu">@@ -1,21 +1,31 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class performing under-sampling based on the neighbourhood cleaning rule.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Hidden, Interval, StrOptions
<span class="w"> </span>from ..base import BaseCleaningSampler
<span class="w"> </span>from ._edited_nearest_neighbours import EditedNearestNeighbours
<span class="gd">-SEL_KIND = &#39;all&#39;, &#39;mode&#39;</span>
<span class="gi">+</span>
<span class="gi">+SEL_KIND = (&quot;all&quot;, &quot;mode&quot;)</span>


<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class NeighbourhoodCleaningRule(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Undersample based on the neighbourhood cleaning rule.

<span class="gu">@@ -129,18 +139,32 @@ class NeighbourhoodCleaningRule(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 888, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;edited_nearest_neighbours&#39;: [HasMethods([</span>
<span class="gd">-        &#39;fit_resample&#39;]), None], &#39;n_neighbors&#39;: [Interval(numbers.Integral,</span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;])], &#39;kind_sel&#39;: [StrOptions({&#39;all&#39;, &#39;mode&#39;}),</span>
<span class="gd">-        Hidden(StrOptions({&#39;deprecated&#39;}))], &#39;threshold_cleaning&#39;: [</span>
<span class="gd">-        Interval(numbers.Real, 0, None, closed=&#39;neither&#39;)], &#39;n_jobs&#39;: [</span>
<span class="gd">-        numbers.Integral, None]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;,</span>
<span class="gd">-        edited_nearest_neighbours=None, n_neighbors=3, kind_sel=</span>
<span class="gd">-        &#39;deprecated&#39;, threshold_cleaning=0.5, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;edited_nearest_neighbours&quot;: [</span>
<span class="gi">+            HasMethods([&quot;fit_resample&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;kind_sel&quot;: [StrOptions({&quot;all&quot;, &quot;mode&quot;}), Hidden(StrOptions({&quot;deprecated&quot;}))],</span>
<span class="gi">+        &quot;threshold_cleaning&quot;: [Interval(numbers.Real, 0, None, closed=&quot;neither&quot;)],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        edited_nearest_neighbours=None,</span>
<span class="gi">+        n_neighbors=3,</span>
<span class="gi">+        kind_sel=&quot;deprecated&quot;,</span>
<span class="gi">+        threshold_cleaning=0.5,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.edited_nearest_neighbours = edited_nearest_neighbours
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="gu">@@ -150,4 +174,85 @@ class NeighbourhoodCleaningRule(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Create the objects required by NCR.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(self.n_neighbors, numbers.Integral):</span>
<span class="gi">+            self.nn_ = KNeighborsClassifier(</span>
<span class="gi">+                n_neighbors=self.n_neighbors, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        elif isinstance(self.n_neighbors, NearestNeighbors):</span>
<span class="gi">+            # backward compatibility when passing a NearestNeighbors object</span>
<span class="gi">+            self.nn_ = KNeighborsClassifier(</span>
<span class="gi">+                n_neighbors=self.n_neighbors.n_neighbors - 1, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.nn_ = clone(self.n_neighbors)</span>
<span class="gi">+</span>
<span class="gi">+        if self.edited_nearest_neighbours is None:</span>
<span class="gi">+            self.edited_nearest_neighbours_ = EditedNearestNeighbours(</span>
<span class="gi">+                sampling_strategy=self.sampling_strategy,</span>
<span class="gi">+                n_neighbors=self.n_neighbors,</span>
<span class="gi">+                kind_sel=&quot;mode&quot;,</span>
<span class="gi">+                n_jobs=self.n_jobs,</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            self.edited_nearest_neighbours_ = clone(self.edited_nearest_neighbours)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        if self.kind_sel != &quot;deprecated&quot;:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                &quot;`kind_sel` is deprecated in 0.12 and will be removed in 0.14. &quot;</span>
<span class="gi">+                &quot;It already has not effect and corresponds to the `&#39;all&#39;` option.&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+        self._validate_estimator()</span>
<span class="gi">+        self.edited_nearest_neighbours_.fit_resample(X, y)</span>
<span class="gi">+        index_not_a1 = self.edited_nearest_neighbours_.sample_indices_</span>
<span class="gi">+        index_a1 = np.ones(y.shape, dtype=bool)</span>
<span class="gi">+        index_a1[index_not_a1] = False</span>
<span class="gi">+        index_a1 = np.flatnonzero(index_a1)</span>
<span class="gi">+</span>
<span class="gi">+        # clean the neighborhood</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        # compute which classes to consider for cleaning for the A2 group</span>
<span class="gi">+        self.classes_to_clean_ = [</span>
<span class="gi">+            c</span>
<span class="gi">+            for c, n_samples in target_stats.items()</span>
<span class="gi">+            if (</span>
<span class="gi">+                c in self.sampling_strategy_.keys()</span>
<span class="gi">+                and (n_samples &gt; target_stats[class_minority] * self.threshold_cleaning)</span>
<span class="gi">+            )</span>
<span class="gi">+        ]</span>
<span class="gi">+        self.nn_.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        class_minority_indices = np.flatnonzero(y == class_minority)</span>
<span class="gi">+        X_minority = _safe_indexing(X, class_minority_indices)</span>
<span class="gi">+        y_minority = _safe_indexing(y, class_minority_indices)</span>
<span class="gi">+</span>
<span class="gi">+        y_pred_minority = self.nn_.predict(X_minority)</span>
<span class="gi">+        # add an additional sample since the query points contains the original dataset</span>
<span class="gi">+        neighbors_to_minority_indices = self.nn_.kneighbors(</span>
<span class="gi">+            X_minority, n_neighbors=self.nn_.n_neighbors + 1, return_distance=False</span>
<span class="gi">+        )[:, 1:]</span>
<span class="gi">+</span>
<span class="gi">+        mask_misclassified_minority = y_pred_minority != y_minority</span>
<span class="gi">+        index_a2 = np.ravel(neighbors_to_minority_indices[mask_misclassified_minority])</span>
<span class="gi">+        index_a2 = np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                index</span>
<span class="gi">+                for index in np.unique(index_a2)</span>
<span class="gi">+                if y[index] in self.classes_to_clean_</span>
<span class="gi">+            ]</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        union_a1_a2 = np.union1d(index_a1, index_a2).astype(int)</span>
<span class="gi">+        selected_samples = np.ones(y.shape, dtype=bool)</span>
<span class="gi">+        selected_samples[union_a1_a2] = False</span>
<span class="gi">+        self.sample_indices_ = np.flatnonzero(selected_samples)</span>
<span class="gi">+</span>
<span class="gi">+        return (</span>
<span class="gi">+            _safe_indexing(X, self.sample_indices_),</span>
<span class="gi">+            _safe_indexing(y, self.sample_indices_),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py b/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gh">index 6c0b322..e0e5b41 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_one_sided_selection.py</span>
<span class="gu">@@ -1,11 +1,18 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling based on one-sided selection method.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.base import clone
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring, _random_state_docstring
<span class="w"> </span>from ...utils._param_validation import HasMethods, Interval
<span class="gu">@@ -13,9 +20,11 @@ from ..base import BaseCleaningSampler</span>
<span class="w"> </span>from ._tomek_links import TomekLinks


<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring, random_state=</span>
<span class="gd">-    _random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class OneSidedSelection(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Class to perform under-sampling based on one-sided selection method.

<span class="gu">@@ -109,15 +118,28 @@ class OneSidedSelection(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 496, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_neighbors&#39;: [Interval(numbers.Integral, </span>
<span class="gd">-        1, None, closed=&#39;left&#39;), HasMethods([&#39;kneighbors&#39;,</span>
<span class="gd">-        &#39;kneighbors_graph&#39;]), None], &#39;n_seeds_S&#39;: [Interval(numbers.</span>
<span class="gd">-        Integral, 1, None, closed=&#39;left&#39;)], &#39;n_jobs&#39;: [numbers.Integral,</span>
<span class="gd">-        None], &#39;random_state&#39;: [&#39;random_state&#39;]}</span>
<span class="gd">-</span>
<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        n_neighbors=None, n_seeds_S=1, n_jobs=None):</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_neighbors&quot;: [</span>
<span class="gi">+            Interval(numbers.Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            HasMethods([&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]),</span>
<span class="gi">+            None,</span>
<span class="gi">+        ],</span>
<span class="gi">+        &quot;n_seeds_S&quot;: [Interval(numbers.Integral, 1, None, closed=&quot;left&quot;)],</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self,</span>
<span class="gi">+        *,</span>
<span class="gi">+        sampling_strategy=&quot;auto&quot;,</span>
<span class="gi">+        random_state=None,</span>
<span class="gi">+        n_neighbors=None,</span>
<span class="gi">+        n_seeds_S=1,</span>
<span class="gi">+        n_jobs=None,</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="gu">@@ -126,9 +148,80 @@ class OneSidedSelection(BaseCleaningSampler):</span>

<span class="w"> </span>    def _validate_estimator(self):
<span class="w"> </span>        &quot;&quot;&quot;Private function to create the NN estimator&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if self.n_neighbors is None:</span>
<span class="gi">+            estimator = KNeighborsClassifier(n_neighbors=1, n_jobs=self.n_jobs)</span>
<span class="gi">+        elif isinstance(self.n_neighbors, int):</span>
<span class="gi">+            estimator = KNeighborsClassifier(</span>
<span class="gi">+                n_neighbors=self.n_neighbors, n_jobs=self.n_jobs</span>
<span class="gi">+            )</span>
<span class="gi">+        elif isinstance(self.n_neighbors, KNeighborsClassifier):</span>
<span class="gi">+            estimator = clone(self.n_neighbors)</span>
<span class="gi">+</span>
<span class="gi">+        return estimator</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        estimator = self._validate_estimator()</span>
<span class="gi">+</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        self.estimators_ = []</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                # select a sample from the current class</span>
<span class="gi">+                idx_maj = np.flatnonzero(y == target_class)</span>
<span class="gi">+                sel_idx_maj = random_state.randint(</span>
<span class="gi">+                    low=0, high=target_stats[target_class], size=self.n_seeds_S</span>
<span class="gi">+                )</span>
<span class="gi">+                idx_maj_sample = idx_maj[sel_idx_maj]</span>
<span class="gi">+</span>
<span class="gi">+                minority_class_indices = np.flatnonzero(y == class_minority)</span>
<span class="gi">+                C_indices = np.append(minority_class_indices, idx_maj_sample)</span>
<span class="gi">+</span>
<span class="gi">+                # create the set composed of all minority samples and one</span>
<span class="gi">+                # sample from the current class.</span>
<span class="gi">+                C_x = _safe_indexing(X, C_indices)</span>
<span class="gi">+                C_y = _safe_indexing(y, C_indices)</span>
<span class="gi">+</span>
<span class="gi">+                # create the set S with removing the seed from S</span>
<span class="gi">+                # since that it will be added anyway</span>
<span class="gi">+                idx_maj_extracted = np.delete(idx_maj, sel_idx_maj, axis=0)</span>
<span class="gi">+                S_x = _safe_indexing(X, idx_maj_extracted)</span>
<span class="gi">+                S_y = _safe_indexing(y, idx_maj_extracted)</span>
<span class="gi">+                self.estimators_.append(clone(estimator).fit(C_x, C_y))</span>
<span class="gi">+                pred_S_y = self.estimators_[-1].predict(S_x)</span>
<span class="gi">+</span>
<span class="gi">+                S_misclassified_indices = np.flatnonzero(pred_S_y != S_y)</span>
<span class="gi">+                idx_tmp = idx_maj_extracted[S_misclassified_indices]</span>
<span class="gi">+                idx_under = np.concatenate((idx_under, idx_maj_sample, idx_tmp), axis=0)</span>
<span class="gi">+            else:</span>
<span class="gi">+                idx_under = np.concatenate(</span>
<span class="gi">+                    (idx_under, np.flatnonzero(y == target_class)), axis=0</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        X_resampled = _safe_indexing(X, idx_under)</span>
<span class="gi">+        y_resampled = _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+        # apply Tomek cleaning</span>
<span class="gi">+        tl = TomekLinks(sampling_strategy=list(self.sampling_strategy_.keys()))</span>
<span class="gi">+        X_cleaned, y_cleaned = tl.fit_resample(X_resampled, y_resampled)</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = _safe_indexing(idx_under, tl.sample_indices_)</span>
<span class="gi">+</span>
<span class="gi">+        return X_cleaned, y_cleaned</span>

<span class="w"> </span>    @property
<span class="w"> </span>    def estimator_(self):
<span class="w"> </span>        &quot;&quot;&quot;Last fitted k-NN estimator.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            &quot;`estimator_` attribute has been deprecated in 0.12 and will be &quot;</span>
<span class="gi">+            &quot;removed in 0.14. Use `estimators_` instead.&quot;,</span>
<span class="gi">+            FutureWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return self.estimators_[-1]</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_random_under_sampler.py b/imblearn/under_sampling/_prototype_selection/_random_under_sampler.py</span>
<span class="gh">index 8e943b8..876195a 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_random_under_sampler.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_random_under_sampler.py</span>
<span class="gu">@@ -1,14 +1,22 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform random under-sampling.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.utils import _safe_indexing, check_random_state
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution, check_target_type
<span class="w"> </span>from ...utils._docstring import _random_state_docstring
<span class="w"> </span>from ...utils._validation import _check_X
<span class="w"> </span>from ..base import BaseUnderSampler


<span class="gd">-@Substitution(sampling_strategy=BaseUnderSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, random_state=_random_state_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,</span>
<span class="gi">+    random_state=_random_state_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class RandomUnderSampler(BaseUnderSampler):
<span class="w"> </span>    &quot;&quot;&quot;Class to perform random under-sampling.

<span class="gu">@@ -74,12 +82,61 @@ class RandomUnderSampler(BaseUnderSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{0: 100, 1: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseUnderSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;replacement&#39;: [&#39;boolean&#39;], &#39;random_state&#39;:</span>
<span class="gd">-        [&#39;random_state&#39;]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, random_state=None,</span>
<span class="gd">-        replacement=False):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseUnderSampler._parameter_constraints,</span>
<span class="gi">+        &quot;replacement&quot;: [&quot;boolean&quot;],</span>
<span class="gi">+        &quot;random_state&quot;: [&quot;random_state&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(</span>
<span class="gi">+        self, *, sampling_strategy=&quot;auto&quot;, random_state=None, replacement=False</span>
<span class="gi">+    ):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.random_state = random_state
<span class="w"> </span>        self.replacement = replacement
<span class="gi">+</span>
<span class="gi">+    def _check_X_y(self, X, y):</span>
<span class="gi">+        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X = _check_X(X)</span>
<span class="gi">+        self._check_n_features(X, reset=True)</span>
<span class="gi">+        self._check_feature_names(X, reset=True)</span>
<span class="gi">+        return X, y, binarize_y</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        random_state = check_random_state(self.random_state)</span>
<span class="gi">+</span>
<span class="gi">+        idx_under = np.empty((0,), dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+        for target_class in np.unique(y):</span>
<span class="gi">+            if target_class in self.sampling_strategy_.keys():</span>
<span class="gi">+                n_samples = self.sampling_strategy_[target_class]</span>
<span class="gi">+                index_target_class = random_state.choice(</span>
<span class="gi">+                    range(np.count_nonzero(y == target_class)),</span>
<span class="gi">+                    size=n_samples,</span>
<span class="gi">+                    replace=self.replacement,</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                index_target_class = slice(None)</span>
<span class="gi">+</span>
<span class="gi">+            idx_under = np.concatenate(</span>
<span class="gi">+                (</span>
<span class="gi">+                    idx_under,</span>
<span class="gi">+                    np.flatnonzero(y == target_class)[index_target_class],</span>
<span class="gi">+                ),</span>
<span class="gi">+                axis=0,</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        self.sample_indices_ = idx_under</span>
<span class="gi">+</span>
<span class="gi">+        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {</span>
<span class="gi">+            &quot;X_types&quot;: [&quot;2darray&quot;, &quot;string&quot;, &quot;sparse&quot;, &quot;dataframe&quot;],</span>
<span class="gi">+            &quot;sample_indices&quot;: True,</span>
<span class="gi">+            &quot;allow_nan&quot;: True,</span>
<span class="gi">+            &quot;_xfail_checks&quot;: {</span>
<span class="gi">+                &quot;check_complex_data&quot;: &quot;Robust to this type of data.&quot;,</span>
<span class="gi">+            },</span>
<span class="gi">+        }</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/_tomek_links.py b/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gh">index a64dc2a..b0f9549 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/_tomek_links.py</span>
<span class="gu">@@ -1,15 +1,25 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Class to perform under-sampling by removing Tomek&#39;s links.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Fernando Nogueira</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils import _safe_indexing
<span class="gi">+</span>
<span class="w"> </span>from ...utils import Substitution
<span class="w"> </span>from ...utils._docstring import _n_jobs_docstring
<span class="w"> </span>from ..base import BaseCleaningSampler


<span class="gd">-@Substitution(sampling_strategy=BaseCleaningSampler.</span>
<span class="gd">-    _sampling_strategy_docstring, n_jobs=_n_jobs_docstring)</span>
<span class="gi">+@Substitution(</span>
<span class="gi">+    sampling_strategy=BaseCleaningSampler._sampling_strategy_docstring,</span>
<span class="gi">+    n_jobs=_n_jobs_docstring,</span>
<span class="gi">+)</span>
<span class="w"> </span>class TomekLinks(BaseCleaningSampler):
<span class="w"> </span>    &quot;&quot;&quot;Under-sampling by removing Tomek&#39;s links.

<span class="gu">@@ -79,10 +89,13 @@ class TomekLinks(BaseCleaningSampler):</span>
<span class="w"> </span>    &gt;&gt;&gt; print(&#39;Resampled dataset shape %s&#39; % Counter(y_res))
<span class="w"> </span>    Resampled dataset shape Counter({{1: 897, 0: 100}})
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {**BaseCleaningSampler.</span>
<span class="gd">-        _parameter_constraints, &#39;n_jobs&#39;: [numbers.Integral, None]}</span>

<span class="gd">-    def __init__(self, *, sampling_strategy=&#39;auto&#39;, n_jobs=None):</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        **BaseCleaningSampler._parameter_constraints,</span>
<span class="gi">+        &quot;n_jobs&quot;: [numbers.Integral, None],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    def __init__(self, *, sampling_strategy=&quot;auto&quot;, n_jobs=None):</span>
<span class="w"> </span>        super().__init__(sampling_strategy=sampling_strategy)
<span class="w"> </span>        self.n_jobs = n_jobs

<span class="gu">@@ -112,4 +125,36 @@ class TomekLinks(BaseCleaningSampler):</span>
<span class="w"> </span>            Boolean vector on len( # samples ), with True for majority samples
<span class="w"> </span>            that are Tomek links.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        links = np.zeros(len(y), dtype=bool)</span>
<span class="gi">+</span>
<span class="gi">+        # find which class to not consider</span>
<span class="gi">+        class_excluded = [c for c in np.unique(y) if c not in class_type]</span>
<span class="gi">+</span>
<span class="gi">+        # there is a Tomek link between two samples if they are both nearest</span>
<span class="gi">+        # neighbors of each others.</span>
<span class="gi">+        for index_sample, target_sample in enumerate(y):</span>
<span class="gi">+            if target_sample in class_excluded:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            if y[nn_index[index_sample]] != target_sample:</span>
<span class="gi">+                if nn_index[nn_index[index_sample]] == index_sample:</span>
<span class="gi">+                    links[index_sample] = True</span>
<span class="gi">+</span>
<span class="gi">+        return links</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        # Find the nearest neighbour of every point</span>
<span class="gi">+        nn = NearestNeighbors(n_neighbors=2, n_jobs=self.n_jobs)</span>
<span class="gi">+        nn.fit(X)</span>
<span class="gi">+        nns = nn.kneighbors(X, return_distance=False)[:, 1]</span>
<span class="gi">+</span>
<span class="gi">+        links = self.is_tomek(y, nns, self.sampling_strategy_)</span>
<span class="gi">+        self.sample_indices_ = np.flatnonzero(np.logical_not(links))</span>
<span class="gi">+</span>
<span class="gi">+        return (</span>
<span class="gi">+            _safe_indexing(X, self.sample_indices_),</span>
<span class="gi">+            _safe_indexing(y, self.sample_indices_),</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    def _more_tags(self):</span>
<span class="gi">+        return {&quot;sample_indices&quot;: True}</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_allknn.py b/imblearn/under_sampling/_prototype_selection/tests/test_allknn.py</span>
<span class="gh">index c2868a7..131f959 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_allknn.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_allknn.py</span>
<span class="gu">@@ -1,27 +1,357 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module repeated edited nearest neighbour.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_allclose, assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import AllKNN
<span class="gd">-X = np.array([[-0.12840393, 0.66446571], [1.32319756, -0.13181616], [</span>
<span class="gd">-    0.04296502, -0.37981873], [0.83631853, 0.18569783], [1.02956816, </span>
<span class="gd">-    0.36061601], [1.12202806, 0.33811558], [-0.53171468, -0.53735182], [</span>
<span class="gd">-    1.3381556, 0.35956356], [-0.35946678, 0.72510189], [1.32326943, </span>
<span class="gd">-    0.28393874], [2.94290565, -0.13986434], [0.28294738, -1.00125525], [</span>
<span class="gd">-    0.34218094, -0.58781961], [-0.88864036, -0.33782387], [-1.10146139, </span>
<span class="gd">-    0.91782682], [-0.7969716, -0.50493969], [0.73489726, 0.43915195], [</span>
<span class="gd">-    0.2096964, -0.61814058], [-0.28479268, 0.70459548], [1.84864913, </span>
<span class="gd">-    0.14729596], [1.59068979, -0.96622933], [0.73418199, -0.02222847], [</span>
<span class="gd">-    0.50307437, 0.498805], [0.84929742, 0.41042894], [0.62649535, </span>
<span class="gd">-    0.46600596], [0.79270821, -0.41386668], [1.16606871, -0.25641059], [</span>
<span class="gd">-    1.57356906, 0.30390519], [1.0304995, -0.16955962], [1.67314371, </span>
<span class="gd">-    0.19231498], [0.98382284, 0.37184502], [0.48921682, -1.38504507], [-</span>
<span class="gd">-    0.46226554, -0.50481004], [-0.03918551, -0.68540745], [0.24991051, -</span>
<span class="gd">-    1.00864997], [0.80541964, -0.34465185], [0.1732627, -1.61323172], [</span>
<span class="gd">-    0.69804044, 0.44810796], [-0.5506368, -0.42072426], [-0.34474418, </span>
<span class="gd">-    0.21969797]])</span>
<span class="gd">-Y = np.array([1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2, 2, 0, 1, 2, 1, 2, 1, 1, 2,</span>
<span class="gd">-    2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 0])</span>
<span class="gd">-R_TOL = 0.0001</span>
<span class="gi">+</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [-0.12840393, 0.66446571],</span>
<span class="gi">+        [1.32319756, -0.13181616],</span>
<span class="gi">+        [0.04296502, -0.37981873],</span>
<span class="gi">+        [0.83631853, 0.18569783],</span>
<span class="gi">+        [1.02956816, 0.36061601],</span>
<span class="gi">+        [1.12202806, 0.33811558],</span>
<span class="gi">+        [-0.53171468, -0.53735182],</span>
<span class="gi">+        [1.3381556, 0.35956356],</span>
<span class="gi">+        [-0.35946678, 0.72510189],</span>
<span class="gi">+        [1.32326943, 0.28393874],</span>
<span class="gi">+        [2.94290565, -0.13986434],</span>
<span class="gi">+        [0.28294738, -1.00125525],</span>
<span class="gi">+        [0.34218094, -0.58781961],</span>
<span class="gi">+        [-0.88864036, -0.33782387],</span>
<span class="gi">+        [-1.10146139, 0.91782682],</span>
<span class="gi">+        [-0.7969716, -0.50493969],</span>
<span class="gi">+        [0.73489726, 0.43915195],</span>
<span class="gi">+        [0.2096964, -0.61814058],</span>
<span class="gi">+        [-0.28479268, 0.70459548],</span>
<span class="gi">+        [1.84864913, 0.14729596],</span>
<span class="gi">+        [1.59068979, -0.96622933],</span>
<span class="gi">+        [0.73418199, -0.02222847],</span>
<span class="gi">+        [0.50307437, 0.498805],</span>
<span class="gi">+        [0.84929742, 0.41042894],</span>
<span class="gi">+        [0.62649535, 0.46600596],</span>
<span class="gi">+        [0.79270821, -0.41386668],</span>
<span class="gi">+        [1.16606871, -0.25641059],</span>
<span class="gi">+        [1.57356906, 0.30390519],</span>
<span class="gi">+        [1.0304995, -0.16955962],</span>
<span class="gi">+        [1.67314371, 0.19231498],</span>
<span class="gi">+        [0.98382284, 0.37184502],</span>
<span class="gi">+        [0.48921682, -1.38504507],</span>
<span class="gi">+        [-0.46226554, -0.50481004],</span>
<span class="gi">+        [-0.03918551, -0.68540745],</span>
<span class="gi">+        [0.24991051, -1.00864997],</span>
<span class="gi">+        [0.80541964, -0.34465185],</span>
<span class="gi">+        [0.1732627, -1.61323172],</span>
<span class="gi">+        [0.69804044, 0.44810796],</span>
<span class="gi">+        [-0.5506368, -0.42072426],</span>
<span class="gi">+        [-0.34474418, 0.21969797],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="gi">+Y = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        0,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="gi">+R_TOL = 1e-4</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_allknn_fit_resample():</span>
<span class="gi">+    allknn = AllKNN()</span>
<span class="gi">+    X_resampled, y_resampled = allknn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [-1.10146139, 0.91782682],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_allclose(X_resampled, X_gt, rtol=R_TOL)</span>
<span class="gi">+    assert_allclose(y_resampled, y_gt, rtol=R_TOL)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_all_knn_allow_minority():</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=10000,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        weights=[0.2, 0.3, 0.5],</span>
<span class="gi">+        class_sep=0.4,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    allknn = AllKNN(allow_minority=True)</span>
<span class="gi">+    X_res_1, y_res_1 = allknn.fit_resample(X, y)</span>
<span class="gi">+    allknn = AllKNN()</span>
<span class="gi">+    X_res_2, y_res_2 = allknn.fit_resample(X, y)</span>
<span class="gi">+    assert len(y_res_1) &lt; len(y_res_2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_allknn_fit_resample_mode():</span>
<span class="gi">+    allknn = AllKNN(kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = allknn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [-0.12840393, 0.66446571],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [-0.35946678, 0.72510189],</span>
<span class="gi">+            [-1.10146139, 0.91782682],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [-0.28479268, 0.70459548],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [1.32319756, -0.13181616],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_allknn_fit_resample_with_nn_object():</span>
<span class="gi">+    nn = NearestNeighbors(n_neighbors=4)</span>
<span class="gi">+    allknn = AllKNN(n_neighbors=nn, kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = allknn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [-0.12840393, 0.66446571],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [-0.35946678, 0.72510189],</span>
<span class="gi">+            [-1.10146139, 0.91782682],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [-0.28479268, 0.70459548],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [1.32319756, -0.13181616],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_alknn_not_good_object():</span>
<span class="gi">+    nn = &quot;rnd&quot;</span>
<span class="gi">+    allknn = AllKNN(n_neighbors=nn, kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        allknn.fit_resample(X, Y)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py b/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gh">index b14a1ef..5cc8f41 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_condensed_nearest_neighbour.py</span>
<span class="gu">@@ -1,28 +1,129 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module condensed nearest neighbour.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import CondensedNearestNeighbour
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[2.59928271, 0.93323465], [0.25738379, 0.95564169], [</span>
<span class="gd">-    1.42772181, 0.526027], [1.92365863, 0.82718767], [-0.10903849, -</span>
<span class="gd">-    0.12085181], [-0.284881, -0.62730973], [0.57062627, 1.19528323], [</span>
<span class="gd">-    0.03394306, 0.03986753], [0.78318102, 2.59153329], [0.35831463, </span>
<span class="gd">-    1.33483198], [-0.14313184, -1.0412815], [0.01936241, 0.17799828], [-</span>
<span class="gd">-    1.25020462, -0.40402054], [-0.09816301, -0.74662486], [-0.01252787, </span>
<span class="gd">-    0.34102657], [0.52726792, -0.38735648], [0.2821046, -0.07862747], [</span>
<span class="gd">-    0.05230552, 0.09043907], [0.15198585, 0.12512646], [0.70524765, </span>
<span class="gd">-    0.39816382]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [2.59928271, 0.93323465],</span>
<span class="gi">+        [0.25738379, 0.95564169],</span>
<span class="gi">+        [1.42772181, 0.526027],</span>
<span class="gi">+        [1.92365863, 0.82718767],</span>
<span class="gi">+        [-0.10903849, -0.12085181],</span>
<span class="gi">+        [-0.284881, -0.62730973],</span>
<span class="gi">+        [0.57062627, 1.19528323],</span>
<span class="gi">+        [0.03394306, 0.03986753],</span>
<span class="gi">+        [0.78318102, 2.59153329],</span>
<span class="gi">+        [0.35831463, 1.33483198],</span>
<span class="gi">+        [-0.14313184, -1.0412815],</span>
<span class="gi">+        [0.01936241, 0.17799828],</span>
<span class="gi">+        [-1.25020462, -0.40402054],</span>
<span class="gi">+        [-0.09816301, -0.74662486],</span>
<span class="gi">+        [-0.01252787, 0.34102657],</span>
<span class="gi">+        [0.52726792, -0.38735648],</span>
<span class="gi">+        [0.2821046, -0.07862747],</span>
<span class="gi">+        [0.05230552, 0.09043907],</span>
<span class="gi">+        [0.15198585, 0.12512646],</span>
<span class="gi">+        [0.70524765, 0.39816382],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1])


<span class="gi">+def test_cnn_init():</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+</span>
<span class="gi">+    assert cnn.n_seeds_S == 1</span>
<span class="gi">+    assert cnn.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_cnn_fit_resample():</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = cnn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.10903849, -0.12085181],</span>
<span class="gi">+            [0.01936241, 0.17799828],</span>
<span class="gi">+            [0.05230552, 0.09043907],</span>
<span class="gi">+            [-1.25020462, -0.40402054],</span>
<span class="gi">+            [0.70524765, 0.39816382],</span>
<span class="gi">+            [0.35831463, 1.33483198],</span>
<span class="gi">+            [-0.284881, -0.62730973],</span>
<span class="gi">+            [0.03394306, 0.03986753],</span>
<span class="gi">+            [-0.01252787, 0.34102657],</span>
<span class="gi">+            [0.15198585, 0.12512646],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 2])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;n_neighbors&quot;, [1, KNeighborsClassifier(n_neighbors=1)])</span>
<span class="gi">+def test_cnn_fit_resample_with_object(n_neighbors):</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED, n_neighbors=n_neighbors)</span>
<span class="gi">+    X_resampled, y_resampled = cnn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.10903849, -0.12085181],</span>
<span class="gi">+            [0.01936241, 0.17799828],</span>
<span class="gi">+            [0.05230552, 0.09043907],</span>
<span class="gi">+            [-1.25020462, -0.40402054],</span>
<span class="gi">+            [0.70524765, 0.39816382],</span>
<span class="gi">+            [0.35831463, 1.33483198],</span>
<span class="gi">+            [-0.284881, -0.62730973],</span>
<span class="gi">+            [0.03394306, 0.03986753],</span>
<span class="gi">+            [-0.01252787, 0.34102657],</span>
<span class="gi">+            [0.15198585, 0.12512646],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 2])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED, n_neighbors=1)</span>
<span class="gi">+    X_resampled, y_resampled = cnn.fit_resample(X, Y)</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_condensed_nearest_neighbour_multiclass():
<span class="w"> </span>    &quot;&quot;&quot;Check the validity of the fitted attributes `estimators_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1_000,</span>
<span class="gi">+        n_classes=4,</span>
<span class="gi">+        weights=[0.1, 0.2, 0.2, 0.5],</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+    cnn.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert len(cnn.estimators_) == len(cnn.sampling_strategy_)</span>
<span class="gi">+    other_classes = []</span>
<span class="gi">+    for est in cnn.estimators_:</span>
<span class="gi">+        assert est.classes_[0] == 0  # minority class</span>
<span class="gi">+        assert est.classes_[1] in {1, 2, 3}  # other classes</span>
<span class="gi">+        other_classes.append(est.classes_[1])</span>
<span class="gi">+    assert len(set(other_classes)) == len(other_classes)</span>


<span class="gi">+# TODO: remove in 0.14</span>
<span class="w"> </span>def test_condensed_nearest_neighbors_deprecation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing the parameter `estimator_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    cnn = CondensedNearestNeighbour(random_state=RND_SEED)</span>
<span class="gi">+    cnn.fit_resample(X, Y)</span>
<span class="gi">+    warn_msg = &quot;`estimator_` attribute has been deprecated&quot;</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=warn_msg):</span>
<span class="gi">+        cnn.estimator_</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py b/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gh">index 9333224..00a0ce5 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_edited_nearest_neighbours.py</span>
<span class="gu">@@ -1,22 +1,140 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module edited nearest neighbour.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import EditedNearestNeighbours
<span class="gd">-X = np.array([[2.59928271, 0.93323465], [0.25738379, 0.95564169], [</span>
<span class="gd">-    1.42772181, 0.526027], [1.92365863, 0.82718767], [-0.10903849, -</span>
<span class="gd">-    0.12085181], [-0.284881, -0.62730973], [0.57062627, 1.19528323], [</span>
<span class="gd">-    0.03394306, 0.03986753], [0.78318102, 2.59153329], [0.35831463, </span>
<span class="gd">-    1.33483198], [-0.14313184, -1.0412815], [0.01936241, 0.17799828], [-</span>
<span class="gd">-    1.25020462, -0.40402054], [-0.09816301, -0.74662486], [-0.01252787, </span>
<span class="gd">-    0.34102657], [0.52726792, -0.38735648], [0.2821046, -0.07862747], [</span>
<span class="gd">-    0.05230552, 0.09043907], [0.15198585, 0.12512646], [0.70524765, </span>
<span class="gd">-    0.39816382]])</span>
<span class="gi">+</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [2.59928271, 0.93323465],</span>
<span class="gi">+        [0.25738379, 0.95564169],</span>
<span class="gi">+        [1.42772181, 0.526027],</span>
<span class="gi">+        [1.92365863, 0.82718767],</span>
<span class="gi">+        [-0.10903849, -0.12085181],</span>
<span class="gi">+        [-0.284881, -0.62730973],</span>
<span class="gi">+        [0.57062627, 1.19528323],</span>
<span class="gi">+        [0.03394306, 0.03986753],</span>
<span class="gi">+        [0.78318102, 2.59153329],</span>
<span class="gi">+        [0.35831463, 1.33483198],</span>
<span class="gi">+        [-0.14313184, -1.0412815],</span>
<span class="gi">+        [0.01936241, 0.17799828],</span>
<span class="gi">+        [-1.25020462, -0.40402054],</span>
<span class="gi">+        [-0.09816301, -0.74662486],</span>
<span class="gi">+        [-0.01252787, 0.34102657],</span>
<span class="gi">+        [0.52726792, -0.38735648],</span>
<span class="gi">+        [0.2821046, -0.07862747],</span>
<span class="gi">+        [0.05230552, 0.09043907],</span>
<span class="gi">+        [0.15198585, 0.12512646],</span>
<span class="gi">+        [0.70524765, 0.39816382],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1])


<span class="gi">+def test_enn_init():</span>
<span class="gi">+    enn = EditedNearestNeighbours()</span>
<span class="gi">+</span>
<span class="gi">+    assert enn.n_neighbors == 3</span>
<span class="gi">+    assert enn.kind_sel == &quot;all&quot;</span>
<span class="gi">+    assert enn.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_enn_fit_resample():</span>
<span class="gi">+    enn = EditedNearestNeighbours()</span>
<span class="gi">+    X_resampled, y_resampled = enn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.10903849, -0.12085181],</span>
<span class="gi">+            [0.01936241, 0.17799828],</span>
<span class="gi">+            [2.59928271, 0.93323465],</span>
<span class="gi">+            [1.92365863, 0.82718767],</span>
<span class="gi">+            [0.25738379, 0.95564169],</span>
<span class="gi">+            [0.78318102, 2.59153329],</span>
<span class="gi">+            [0.52726792, -0.38735648],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 1, 1, 2, 2, 2])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_enn_fit_resample_mode():</span>
<span class="gi">+    enn = EditedNearestNeighbours(kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = enn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.10903849, -0.12085181],</span>
<span class="gi">+            [0.01936241, 0.17799828],</span>
<span class="gi">+            [2.59928271, 0.93323465],</span>
<span class="gi">+            [1.42772181, 0.526027],</span>
<span class="gi">+            [1.92365863, 0.82718767],</span>
<span class="gi">+            [0.25738379, 0.95564169],</span>
<span class="gi">+            [-0.284881, -0.62730973],</span>
<span class="gi">+            [0.57062627, 1.19528323],</span>
<span class="gi">+            [0.78318102, 2.59153329],</span>
<span class="gi">+            [0.35831463, 1.33483198],</span>
<span class="gi">+            [-0.14313184, -1.0412815],</span>
<span class="gi">+            [-0.09816301, -0.74662486],</span>
<span class="gi">+            [0.52726792, -0.38735648],</span>
<span class="gi">+            [0.2821046, -0.07862747],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_enn_fit_resample_with_nn_object():</span>
<span class="gi">+    nn = NearestNeighbors(n_neighbors=4)</span>
<span class="gi">+    enn = EditedNearestNeighbours(n_neighbors=nn, kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = enn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.10903849, -0.12085181],</span>
<span class="gi">+            [0.01936241, 0.17799828],</span>
<span class="gi">+            [2.59928271, 0.93323465],</span>
<span class="gi">+            [1.42772181, 0.526027],</span>
<span class="gi">+            [1.92365863, 0.82718767],</span>
<span class="gi">+            [0.25738379, 0.95564169],</span>
<span class="gi">+            [-0.284881, -0.62730973],</span>
<span class="gi">+            [0.57062627, 1.19528323],</span>
<span class="gi">+            [0.78318102, 2.59153329],</span>
<span class="gi">+            [0.35831463, 1.33483198],</span>
<span class="gi">+            [-0.14313184, -1.0412815],</span>
<span class="gi">+            [-0.09816301, -0.74662486],</span>
<span class="gi">+            [0.52726792, -0.38735648],</span>
<span class="gi">+            [0.2821046, -0.07862747],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_enn_check_kind_selection():
<span class="w"> </span>    &quot;&quot;&quot;Check that `check_sel=&quot;all&quot;` is more conservative than
<span class="w"> </span>    `check_sel=&quot;mode&quot;`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000,</span>
<span class="gi">+        n_classes=2,</span>
<span class="gi">+        weights=[0.3, 0.7],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    enn_all = EditedNearestNeighbours(kind_sel=&quot;all&quot;)</span>
<span class="gi">+    enn_mode = EditedNearestNeighbours(kind_sel=&quot;mode&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    enn_all.fit_resample(X, y)</span>
<span class="gi">+    enn_mode.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert enn_all.sample_indices_.size &lt; enn_mode.sample_indices_.size</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py b/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gh">index bdb3a01..a63bb45 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_instance_hardness_threshold.py</span>
<span class="gu">@@ -1,22 +1,101 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module .&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
<span class="w"> </span>from sklearn.naive_bayes import GaussianNB as NB
<span class="w"> </span>from sklearn.pipeline import make_pipeline
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import InstanceHardnessThreshold
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[-0.3879569, 0.6894251], [-0.09322739, 1.28177189], [-</span>
<span class="gd">-    0.77740357, 0.74097941], [0.91542919, -0.65453327], [-0.03852113, </span>
<span class="gd">-    0.40910479], [-0.43877303, 1.07366684], [-0.85795321, 0.82980738], [-</span>
<span class="gd">-    0.18430329, 0.52328473], [-0.30126957, -0.66268378], [-0.65571327, </span>
<span class="gd">-    0.42412021], [-0.28305528, 0.30284991], [0.20246714, -0.34727125], [</span>
<span class="gd">-    1.06446472, -1.09279772], [0.30543283, -0.02589502], [-0.00717161, </span>
<span class="gd">-    0.00318087]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [-0.3879569, 0.6894251],</span>
<span class="gi">+        [-0.09322739, 1.28177189],</span>
<span class="gi">+        [-0.77740357, 0.74097941],</span>
<span class="gi">+        [0.91542919, -0.65453327],</span>
<span class="gi">+        [-0.03852113, 0.40910479],</span>
<span class="gi">+        [-0.43877303, 1.07366684],</span>
<span class="gi">+        [-0.85795321, 0.82980738],</span>
<span class="gi">+        [-0.18430329, 0.52328473],</span>
<span class="gi">+        [-0.30126957, -0.66268378],</span>
<span class="gi">+        [-0.65571327, 0.42412021],</span>
<span class="gi">+        [-0.28305528, 0.30284991],</span>
<span class="gi">+        [0.20246714, -0.34727125],</span>
<span class="gi">+        [1.06446472, -1.09279772],</span>
<span class="gi">+        [0.30543283, -0.02589502],</span>
<span class="gi">+        [-0.00717161, 0.00318087],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])
<span class="w"> </span>ESTIMATOR = GradientBoostingClassifier(random_state=RND_SEED)


<span class="gi">+def test_iht_init():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    iht = InstanceHardnessThreshold(</span>
<span class="gi">+        estimator=ESTIMATOR,</span>
<span class="gi">+        sampling_strategy=sampling_strategy,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    assert iht.sampling_strategy == sampling_strategy</span>
<span class="gi">+    assert iht.random_state == RND_SEED</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iht_fit_resample():</span>
<span class="gi">+    iht = InstanceHardnessThreshold(estimator=ESTIMATOR, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (12, 2)</span>
<span class="gi">+    assert y_resampled.shape == (12,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iht_fit_resample_half():</span>
<span class="gi">+    sampling_strategy = {0: 3, 1: 3}</span>
<span class="gi">+    iht = InstanceHardnessThreshold(</span>
<span class="gi">+        estimator=NB(),</span>
<span class="gi">+        sampling_strategy=sampling_strategy,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (6, 2)</span>
<span class="gi">+    assert y_resampled.shape == (6,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iht_fit_resample_class_obj():</span>
<span class="gi">+    est = GradientBoostingClassifier(random_state=RND_SEED)</span>
<span class="gi">+    iht = InstanceHardnessThreshold(estimator=est, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (12, 2)</span>
<span class="gi">+    assert y_resampled.shape == (12,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iht_reproducibility():</span>
<span class="gi">+    from sklearn.datasets import load_digits</span>
<span class="gi">+</span>
<span class="gi">+    X_digits, y_digits = load_digits(return_X_y=True)</span>
<span class="gi">+    idx_sampled = []</span>
<span class="gi">+    for seed in range(5):</span>
<span class="gi">+        est = RandomForestClassifier(n_estimators=10, random_state=seed)</span>
<span class="gi">+        iht = InstanceHardnessThreshold(estimator=est, random_state=RND_SEED)</span>
<span class="gi">+        iht.fit_resample(X_digits, y_digits)</span>
<span class="gi">+        idx_sampled.append(iht.sample_indices_.copy())</span>
<span class="gi">+    for idx_1, idx_2 in zip(idx_sampled, idx_sampled[1:]):</span>
<span class="gi">+        assert_array_equal(idx_1, idx_2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_iht_fit_resample_default_estimator():</span>
<span class="gi">+    iht = InstanceHardnessThreshold(estimator=None, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    assert isinstance(iht.estimator_, RandomForestClassifier)</span>
<span class="gi">+    assert X_resampled.shape == (12, 2)</span>
<span class="gi">+    assert y_resampled.shape == (12,)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_iht_estimator_pipeline():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can pass a pipeline containing a classifier.

<span class="gu">@@ -26,4 +105,8 @@ def test_iht_estimator_pipeline():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/pull/1049
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    model = make_pipeline(GradientBoostingClassifier(random_state=RND_SEED))</span>
<span class="gi">+    iht = InstanceHardnessThreshold(estimator=model, random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = iht.fit_resample(X, Y)</span>
<span class="gi">+    assert X_resampled.shape == (12, 2)</span>
<span class="gi">+    assert y_resampled.shape == (12,)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_nearmiss.py b/imblearn/under_sampling/_prototype_selection/tests/test_nearmiss.py</span>
<span class="gh">index a80ec95..9ab0da4 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_nearmiss.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_nearmiss.py</span>
<span class="gu">@@ -1,14 +1,210 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module nearmiss.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import NearMiss
<span class="gd">-X = np.array([[1.17737838, -0.2002118], [0.4960075, 0.86130762], [-</span>
<span class="gd">-    0.05903827, 0.10947647], [0.91464286, 1.61369212], [-0.54619583, </span>
<span class="gd">-    1.73009918], [-0.60413357, 0.24628718], [0.45713638, 1.31069295], [-</span>
<span class="gd">-    0.04032409, 3.01186964], [0.03142011, 0.12323596], [0.50701028, -</span>
<span class="gd">-    0.17636928], [-0.80809175, -1.09917302], [-0.20497017, -0.26630228], [</span>
<span class="gd">-    0.99272351, -0.11631728], [-1.95581933, 0.69609604], [1.15157493, -</span>
<span class="gd">-    1.2981518]])</span>
<span class="gi">+</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [1.17737838, -0.2002118],</span>
<span class="gi">+        [0.4960075, 0.86130762],</span>
<span class="gi">+        [-0.05903827, 0.10947647],</span>
<span class="gi">+        [0.91464286, 1.61369212],</span>
<span class="gi">+        [-0.54619583, 1.73009918],</span>
<span class="gi">+        [-0.60413357, 0.24628718],</span>
<span class="gi">+        [0.45713638, 1.31069295],</span>
<span class="gi">+        [-0.04032409, 3.01186964],</span>
<span class="gi">+        [0.03142011, 0.12323596],</span>
<span class="gi">+        [0.50701028, -0.17636928],</span>
<span class="gi">+        [-0.80809175, -1.09917302],</span>
<span class="gi">+        [-0.20497017, -0.26630228],</span>
<span class="gi">+        [0.99272351, -0.11631728],</span>
<span class="gi">+        [-1.95581933, 0.69609604],</span>
<span class="gi">+        [1.15157493, -1.2981518],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 0, 2, 1, 2])
<span class="gd">-VERSION_NEARMISS = 1, 2, 3</span>
<span class="gi">+</span>
<span class="gi">+VERSION_NEARMISS = (1, 2, 3)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_nm_fit_resample_auto():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    X_gt = [</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [1.17737838, -0.2002118],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [1.15157493, -1.2981518],</span>
<span class="gi">+                [-0.54619583, 1.73009918],</span>
<span class="gi">+                [0.99272351, -0.11631728],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+    ]</span>
<span class="gi">+    y_gt = [</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+    ]</span>
<span class="gi">+    for version_idx, version in enumerate(VERSION_NEARMISS):</span>
<span class="gi">+        nm = NearMiss(sampling_strategy=sampling_strategy, version=version)</span>
<span class="gi">+        X_resampled, y_resampled = nm.fit_resample(X, Y)</span>
<span class="gi">+        assert_array_equal(X_resampled, X_gt[version_idx])</span>
<span class="gi">+        assert_array_equal(y_resampled, y_gt[version_idx])</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_nm_fit_resample_float_sampling_strategy():</span>
<span class="gi">+    sampling_strategy = {0: 3, 1: 4, 2: 4}</span>
<span class="gi">+    X_gt = [</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [1.17737838, -0.2002118],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+                [0.99272351, -0.11631728],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [1.17737838, -0.2002118],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+                [0.99272351, -0.11631728],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [1.17737838, -0.2002118],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [1.15157493, -1.2981518],</span>
<span class="gi">+                [-0.54619583, 1.73009918],</span>
<span class="gi">+                [0.99272351, -0.11631728],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+    ]</span>
<span class="gi">+    y_gt = [</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]),</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    for version_idx, version in enumerate(VERSION_NEARMISS):</span>
<span class="gi">+        nm = NearMiss(sampling_strategy=sampling_strategy, version=version)</span>
<span class="gi">+        X_resampled, y_resampled = nm.fit_resample(X, Y)</span>
<span class="gi">+        assert_array_equal(X_resampled, X_gt[version_idx])</span>
<span class="gi">+        assert_array_equal(y_resampled, y_gt[version_idx])</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_nm_fit_resample_nn_obj():</span>
<span class="gi">+    sampling_strategy = &quot;auto&quot;</span>
<span class="gi">+    nn = NearestNeighbors(n_neighbors=3)</span>
<span class="gi">+    X_gt = [</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [-0.05903827, 0.10947647],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.50701028, -0.17636928],</span>
<span class="gi">+                [0.4960075, 0.86130762],</span>
<span class="gi">+                [0.45713638, 1.31069295],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+        np.array(</span>
<span class="gi">+            [</span>
<span class="gi">+                [0.91464286, 1.61369212],</span>
<span class="gi">+                [-0.80809175, -1.09917302],</span>
<span class="gi">+                [-0.20497017, -0.26630228],</span>
<span class="gi">+                [1.17737838, -0.2002118],</span>
<span class="gi">+                [-0.60413357, 0.24628718],</span>
<span class="gi">+                [0.03142011, 0.12323596],</span>
<span class="gi">+                [1.15157493, -1.2981518],</span>
<span class="gi">+                [-0.54619583, 1.73009918],</span>
<span class="gi">+                [0.99272351, -0.11631728],</span>
<span class="gi">+            ]</span>
<span class="gi">+        ),</span>
<span class="gi">+    ]</span>
<span class="gi">+    y_gt = [</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+        np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),</span>
<span class="gi">+    ]</span>
<span class="gi">+    for version_idx, version in enumerate(VERSION_NEARMISS):</span>
<span class="gi">+        nm = NearMiss(</span>
<span class="gi">+            sampling_strategy=sampling_strategy,</span>
<span class="gi">+            version=version,</span>
<span class="gi">+            n_neighbors=nn,</span>
<span class="gi">+        )</span>
<span class="gi">+        X_resampled, y_resampled = nm.fit_resample(X, Y)</span>
<span class="gi">+        assert_array_equal(X_resampled, X_gt[version_idx])</span>
<span class="gi">+        assert_array_equal(y_resampled, y_gt[version_idx])</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py b/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gh">index 97c8fd5..97a1d02 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_neighbourhood_cleaning_rule.py</span>
<span class="gu">@@ -1,17 +1,84 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module neighbourhood cleaning rule.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import EditedNearestNeighbours, NeighbourhoodCleaningRule


<span class="gi">+@pytest.fixture(scope=&quot;module&quot;)</span>
<span class="gi">+def data():</span>
<span class="gi">+    return make_classification(</span>
<span class="gi">+        n_samples=200,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        n_informative=2,</span>
<span class="gi">+        n_redundant=0,</span>
<span class="gi">+        n_repeated=0,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.3, 0.6],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_ncr_threshold_cleaning(data):
<span class="w"> </span>    &quot;&quot;&quot;Test the effect of the `threshold_cleaning` parameter.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+    # with a large `threshold_cleaning`, the algorithm is equivalent to ENN</span>
<span class="gi">+    enn = EditedNearestNeighbours()</span>
<span class="gi">+    ncr = NeighbourhoodCleaningRule(</span>
<span class="gi">+        edited_nearest_neighbours=enn, n_neighbors=10, threshold_cleaning=10</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    enn.fit_resample(X, y)</span>
<span class="gi">+    ncr.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_equal(np.sort(enn.sample_indices_), np.sort(ncr.sample_indices_))</span>
<span class="gi">+    assert ncr.classes_to_clean_ == []</span>
<span class="gi">+</span>
<span class="gi">+    # set a threshold that we should consider only the class #2</span>
<span class="gi">+    counter = Counter(y)</span>
<span class="gi">+    threshold = counter[1] / counter[0]</span>
<span class="gi">+    ncr.set_params(threshold_cleaning=threshold)</span>
<span class="gi">+    ncr.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert set(ncr.classes_to_clean_) == {2}</span>
<span class="gi">+</span>
<span class="gi">+    # making the threshold slightly smaller to take into account class #1</span>
<span class="gi">+    ncr.set_params(threshold_cleaning=threshold - np.finfo(np.float32).eps)</span>
<span class="gi">+    ncr.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert set(ncr.classes_to_clean_) == {1, 2}</span>


<span class="w"> </span>def test_ncr_n_neighbors(data):
<span class="w"> </span>    &quot;&quot;&quot;Check the effect of the NN on the cleaning of the second phase.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    enn = EditedNearestNeighbours()</span>
<span class="gi">+    ncr = NeighbourhoodCleaningRule(edited_nearest_neighbours=enn, n_neighbors=3)</span>
<span class="gi">+</span>
<span class="gi">+    ncr.fit_resample(X, y)</span>
<span class="gi">+    sample_indices_3_nn = ncr.sample_indices_</span>
<span class="gi">+</span>
<span class="gi">+    ncr.set_params(n_neighbors=10).fit_resample(X, y)</span>
<span class="gi">+    sample_indices_10_nn = ncr.sample_indices_</span>
<span class="gi">+</span>
<span class="gi">+    # we should have a more aggressive cleaning with n_neighbors is larger</span>
<span class="gi">+    assert len(sample_indices_3_nn) &gt; len(sample_indices_10_nn)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# TODO: remove in 0.14</span>
<span class="gi">+@pytest.mark.parametrize(&quot;kind_sel&quot;, [&quot;all&quot;, &quot;mode&quot;])</span>
<span class="gi">+def test_ncr_deprecate_kind_sel(data, kind_sel):</span>
<span class="gi">+    X, y = data</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;`kind_sel` is deprecated&quot;):</span>
<span class="gi">+        NeighbourhoodCleaningRule(kind_sel=kind_sel).fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py b/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gh">index e861896..3fb5458 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_one_sided_selection.py</span>
<span class="gu">@@ -1,26 +1,129 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module one-sided selection.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.neighbors import KNeighborsClassifier
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import OneSidedSelection
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[-0.3879569, 0.6894251], [-0.09322739, 1.28177189], [-</span>
<span class="gd">-    0.77740357, 0.74097941], [0.91542919, -0.65453327], [-0.03852113, </span>
<span class="gd">-    0.40910479], [-0.43877303, 1.07366684], [-0.85795321, 0.82980738], [-</span>
<span class="gd">-    0.18430329, 0.52328473], [-0.30126957, -0.66268378], [-0.65571327, </span>
<span class="gd">-    0.42412021], [-0.28305528, 0.30284991], [0.20246714, -0.34727125], [</span>
<span class="gd">-    1.06446472, -1.09279772], [0.30543283, -0.02589502], [-0.00717161, </span>
<span class="gd">-    0.00318087]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [-0.3879569, 0.6894251],</span>
<span class="gi">+        [-0.09322739, 1.28177189],</span>
<span class="gi">+        [-0.77740357, 0.74097941],</span>
<span class="gi">+        [0.91542919, -0.65453327],</span>
<span class="gi">+        [-0.03852113, 0.40910479],</span>
<span class="gi">+        [-0.43877303, 1.07366684],</span>
<span class="gi">+        [-0.85795321, 0.82980738],</span>
<span class="gi">+        [-0.18430329, 0.52328473],</span>
<span class="gi">+        [-0.30126957, -0.66268378],</span>
<span class="gi">+        [-0.65571327, 0.42412021],</span>
<span class="gi">+        [-0.28305528, 0.30284991],</span>
<span class="gi">+        [0.20246714, -0.34727125],</span>
<span class="gi">+        [1.06446472, -1.09279772],</span>
<span class="gi">+        [0.30543283, -0.02589502],</span>
<span class="gi">+        [-0.00717161, 0.00318087],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])


<span class="gi">+def test_oss_init():</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED)</span>
<span class="gi">+</span>
<span class="gi">+    assert oss.n_seeds_S == 1</span>
<span class="gi">+    assert oss.n_jobs is None</span>
<span class="gi">+    assert oss.random_state == RND_SEED</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_oss_fit_resample():</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = oss.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.3879569, 0.6894251],</span>
<span class="gi">+            [0.91542919, -0.65453327],</span>
<span class="gi">+            [-0.65571327, 0.42412021],</span>
<span class="gi">+            [1.06446472, -1.09279772],</span>
<span class="gi">+            [0.30543283, -0.02589502],</span>
<span class="gi">+            [-0.00717161, 0.00318087],</span>
<span class="gi">+            [-0.09322739, 1.28177189],</span>
<span class="gi">+            [-0.77740357, 0.74097941],</span>
<span class="gi">+            [-0.43877303, 1.07366684],</span>
<span class="gi">+            [-0.85795321, 0.82980738],</span>
<span class="gi">+            [-0.30126957, -0.66268378],</span>
<span class="gi">+            [0.20246714, -0.34727125],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;n_neighbors&quot;, [1, KNeighborsClassifier(n_neighbors=1)])</span>
<span class="gi">+def test_oss_with_object(n_neighbors):</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED, n_neighbors=n_neighbors)</span>
<span class="gi">+    X_resampled, y_resampled = oss.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.3879569, 0.6894251],</span>
<span class="gi">+            [0.91542919, -0.65453327],</span>
<span class="gi">+            [-0.65571327, 0.42412021],</span>
<span class="gi">+            [1.06446472, -1.09279772],</span>
<span class="gi">+            [0.30543283, -0.02589502],</span>
<span class="gi">+            [-0.00717161, 0.00318087],</span>
<span class="gi">+            [-0.09322739, 1.28177189],</span>
<span class="gi">+            [-0.77740357, 0.74097941],</span>
<span class="gi">+            [-0.43877303, 1.07366684],</span>
<span class="gi">+            [-0.85795321, 0.82980738],</span>
<span class="gi">+            [-0.30126957, -0.66268378],</span>
<span class="gi">+            [0.20246714, -0.34727125],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+    knn = 1</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED, n_neighbors=knn)</span>
<span class="gi">+    X_resampled, y_resampled = oss.fit_resample(X, Y)</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_one_sided_selection_multiclass():
<span class="w"> </span>    &quot;&quot;&quot;Check the validity of the fitted attributes `estimators_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1_000,</span>
<span class="gi">+        n_classes=4,</span>
<span class="gi">+        weights=[0.1, 0.2, 0.2, 0.5],</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED)</span>
<span class="gi">+    oss.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert len(oss.estimators_) == len(oss.sampling_strategy_)</span>
<span class="gi">+    other_classes = []</span>
<span class="gi">+    for est in oss.estimators_:</span>
<span class="gi">+        assert est.classes_[0] == 0  # minority class</span>
<span class="gi">+        assert est.classes_[1] in {1, 2, 3}  # other classes</span>
<span class="gi">+        other_classes.append(est.classes_[1])</span>
<span class="gi">+    assert len(set(other_classes)) == len(other_classes)</span>


<span class="gi">+# TODO: remove in 0.14</span>
<span class="w"> </span>def test_one_sided_selection_deprecation():
<span class="w"> </span>    &quot;&quot;&quot;Check that we raise a FutureWarning when accessing the parameter `estimator_`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    oss = OneSidedSelection(random_state=RND_SEED)</span>
<span class="gi">+    oss.fit_resample(X, Y)</span>
<span class="gi">+    warn_msg = &quot;`estimator_` attribute has been deprecated&quot;</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=warn_msg):</span>
<span class="gi">+        oss.estimator_</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py b/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gh">index 96745c6..f4e9279 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_random_under_sampler.py</span>
<span class="gu">@@ -1,30 +1,167 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module random under sampler.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter
<span class="w"> </span>from datetime import datetime
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import RandomUnderSampler
<span class="gi">+</span>
<span class="w"> </span>RND_SEED = 0
<span class="gd">-X = np.array([[0.04352327, -0.20515826], [0.92923648, 0.76103773], [</span>
<span class="gd">-    0.20792588, 1.49407907], [0.47104475, 0.44386323], [0.22950086, </span>
<span class="gd">-    0.33367433], [0.15490546, 0.3130677], [0.09125309, -0.85409574], [</span>
<span class="gd">-    0.12372842, 0.6536186], [0.13347175, 0.12167502], [0.094035, -2.55298982]])</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.04352327, -0.20515826],</span>
<span class="gi">+        [0.92923648, 0.76103773],</span>
<span class="gi">+        [0.20792588, 1.49407907],</span>
<span class="gi">+        [0.47104475, 0.44386323],</span>
<span class="gi">+        [0.22950086, 0.33367433],</span>
<span class="gi">+        [0.15490546, 0.3130677],</span>
<span class="gi">+        [0.09125309, -0.85409574],</span>
<span class="gi">+        [0.12372842, 0.6536186],</span>
<span class="gi">+        [0.13347175, 0.12167502],</span>
<span class="gi">+        [0.094035, -2.55298982],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 0, 1, 0, 1, 1, 1, 1, 0, 1])


<span class="gd">-@pytest.mark.parametrize(&#39;sampling_strategy&#39;, [&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-    &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;as_frame&quot;, [True, False], ids=[&quot;dataframe&quot;, &quot;array&quot;])</span>
<span class="gi">+def test_rus_fit_resample(as_frame):</span>
<span class="gi">+    if as_frame:</span>
<span class="gi">+        pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+        X_ = pd.DataFrame(X)</span>
<span class="gi">+    else:</span>
<span class="gi">+        X_ = X</span>
<span class="gi">+    rus = RandomUnderSampler(random_state=RND_SEED, replacement=True)</span>
<span class="gi">+    X_resampled, y_resampled = rus.fit_resample(X_, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.13347175, 0.12167502],</span>
<span class="gi">+            [0.09125309, -0.85409574],</span>
<span class="gi">+            [0.12372842, 0.6536186],</span>
<span class="gi">+            [0.04352327, -0.20515826],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 1, 1, 1])</span>
<span class="gi">+</span>
<span class="gi">+    if as_frame:</span>
<span class="gi">+        assert hasattr(X_resampled, &quot;loc&quot;)</span>
<span class="gi">+        # FIXME: we should use to_numpy with pandas &gt;= 0.25</span>
<span class="gi">+        X_resampled = X_resampled.values</span>
<span class="gi">+</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_rus_fit_resample_half():</span>
<span class="gi">+    sampling_strategy = {0: 3, 1: 6}</span>
<span class="gi">+    rus = RandomUnderSampler(</span>
<span class="gi">+        sampling_strategy=sampling_strategy,</span>
<span class="gi">+        random_state=RND_SEED,</span>
<span class="gi">+        replacement=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    X_resampled, y_resampled = rus.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.47104475, 0.44386323],</span>
<span class="gi">+            [0.92923648, 0.76103773],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.20792588, 1.49407907],</span>
<span class="gi">+            [0.15490546, 0.3130677],</span>
<span class="gi">+            [0.12372842, 0.6536186],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_multiclass_fit_resample():</span>
<span class="gi">+    y = Y.copy()</span>
<span class="gi">+    y[5] = 2</span>
<span class="gi">+    y[6] = 2</span>
<span class="gi">+    rus = RandomUnderSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_resampled, y_resampled = rus.fit_resample(X, y)</span>
<span class="gi">+    count_y_res = Counter(y_resampled)</span>
<span class="gi">+    assert count_y_res[0] == 2</span>
<span class="gi">+    assert count_y_res[1] == 2</span>
<span class="gi">+    assert count_y_res[2] == 2</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_under_sampling_heterogeneous_data():</span>
<span class="gi">+    X_hetero = np.array(</span>
<span class="gi">+        [[&quot;xxx&quot;, 1, 1.0], [&quot;yyy&quot;, 2, 2.0], [&quot;zzz&quot;, 3, 3.0]], dtype=object</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 0, 1])</span>
<span class="gi">+    rus = RandomUnderSampler(random_state=RND_SEED)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X_hetero, y)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res.shape[0] == 2</span>
<span class="gi">+    assert y_res.shape[0] == 2</span>
<span class="gi">+    assert X_res.dtype == object</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_random_under_sampling_nan_inf():</span>
<span class="gi">+    # check that we can undersample even with missing or infinite data</span>
<span class="gi">+    # regression tests for #605</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    n_not_finite = X.shape[0] // 3</span>
<span class="gi">+    row_indices = rng.choice(np.arange(X.shape[0]), size=n_not_finite)</span>
<span class="gi">+    col_indices = rng.randint(0, X.shape[1], size=n_not_finite)</span>
<span class="gi">+    not_finite_values = rng.choice([np.nan, np.inf], size=n_not_finite)</span>
<span class="gi">+</span>
<span class="gi">+    X_ = X.copy()</span>
<span class="gi">+    X_[row_indices, col_indices] = not_finite_values</span>
<span class="gi">+</span>
<span class="gi">+    rus = RandomUnderSampler(random_state=0)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X_, Y)</span>
<span class="gi">+</span>
<span class="gi">+    assert y_res.shape == (6,)</span>
<span class="gi">+    assert X_res.shape == (6, 2)</span>
<span class="gi">+    assert np.any(~np.isfinite(X_res))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy&quot;, [&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;]</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_random_under_sampler_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseUnderSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=100,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.3, 0.6],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    RandomUnderSampler(sampling_strategy=sampling_strategy).fit_resample(X, y)</span>


<span class="w"> </span>def test_random_under_sampling_datetime():
<span class="w"> </span>    &quot;&quot;&quot;Check that we don&#39;t convert input data and only sample from it.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    X = pd.DataFrame({&quot;label&quot;: [0, 0, 0, 1], &quot;td&quot;: [datetime.now()] * 4})</span>
<span class="gi">+    y = X[&quot;label&quot;]</span>
<span class="gi">+    rus = RandomUnderSampler(random_state=0)</span>
<span class="gi">+    X_res, y_res = rus.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    pd.testing.assert_series_equal(X_res.dtypes, X.dtypes)</span>
<span class="gi">+    pd.testing.assert_index_equal(X_res.index, y_res.index)</span>
<span class="gi">+    assert_array_equal(y_res.to_numpy(), np.array([0, 1]))</span>


<span class="w"> </span>def test_random_under_sampler_full_nat():
<span class="gu">@@ -33,4 +170,18 @@ def test_random_under_sampler_full_nat():</span>
<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = pd.DataFrame(</span>
<span class="gi">+        {</span>
<span class="gi">+            &quot;col_str&quot;: [&quot;abc&quot;, &quot;def&quot;, &quot;xyz&quot;],</span>
<span class="gi">+            &quot;col_timedelta&quot;: pd.to_timedelta([np.nan, np.nan, np.nan]),</span>
<span class="gi">+        }</span>
<span class="gi">+    )</span>
<span class="gi">+    y = np.array([0, 0, 1])</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = RandomUnderSampler().fit_resample(X, y)</span>
<span class="gi">+    assert X_res.shape == (2, 2)</span>
<span class="gi">+    assert y_res.shape == (2,)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_res[&quot;col_timedelta&quot;].dtype == &quot;timedelta64[ns]&quot;</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_repeated_edited_nearest_neighbours.py b/imblearn/under_sampling/_prototype_selection/tests/test_repeated_edited_nearest_neighbours.py</span>
<span class="gh">index 92df66a..edd3a91 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_repeated_edited_nearest_neighbours.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_repeated_edited_nearest_neighbours.py</span>
<span class="gu">@@ -1,25 +1,338 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module repeated edited nearest neighbour.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import RepeatedEditedNearestNeighbours
<span class="gd">-X = np.array([[-0.12840393, 0.66446571], [1.32319756, -0.13181616], [</span>
<span class="gd">-    0.04296502, -0.37981873], [0.83631853, 0.18569783], [1.02956816, </span>
<span class="gd">-    0.36061601], [1.12202806, 0.33811558], [-0.53171468, -0.53735182], [</span>
<span class="gd">-    1.3381556, 0.35956356], [-0.35946678, 0.72510189], [1.32326943, </span>
<span class="gd">-    0.28393874], [2.94290565, -0.13986434], [0.28294738, -1.00125525], [</span>
<span class="gd">-    0.34218094, -0.58781961], [-0.88864036, -0.33782387], [-1.10146139, </span>
<span class="gd">-    0.91782682], [-0.7969716, -0.50493969], [0.73489726, 0.43915195], [</span>
<span class="gd">-    0.2096964, -0.61814058], [-0.28479268, 0.70459548], [1.84864913, </span>
<span class="gd">-    0.14729596], [1.59068979, -0.96622933], [0.73418199, -0.02222847], [</span>
<span class="gd">-    0.50307437, 0.498805], [0.84929742, 0.41042894], [0.62649535, </span>
<span class="gd">-    0.46600596], [0.79270821, -0.41386668], [1.16606871, -0.25641059], [</span>
<span class="gd">-    1.57356906, 0.30390519], [1.0304995, -0.16955962], [1.67314371, </span>
<span class="gd">-    0.19231498], [0.98382284, 0.37184502], [0.48921682, -1.38504507], [-</span>
<span class="gd">-    0.46226554, -0.50481004], [-0.03918551, -0.68540745], [0.24991051, -</span>
<span class="gd">-    1.00864997], [0.80541964, -0.34465185], [0.1732627, -1.61323172], [</span>
<span class="gd">-    0.69804044, 0.44810796], [-0.5506368, -0.42072426], [-0.34474418, </span>
<span class="gd">-    0.21969797]])</span>
<span class="gd">-Y = np.array([1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2, 2, 0, 1, 2, 1, 2, 1, 1, 2,</span>
<span class="gd">-    2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 0])</span>
<span class="gi">+</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [-0.12840393, 0.66446571],</span>
<span class="gi">+        [1.32319756, -0.13181616],</span>
<span class="gi">+        [0.04296502, -0.37981873],</span>
<span class="gi">+        [0.83631853, 0.18569783],</span>
<span class="gi">+        [1.02956816, 0.36061601],</span>
<span class="gi">+        [1.12202806, 0.33811558],</span>
<span class="gi">+        [-0.53171468, -0.53735182],</span>
<span class="gi">+        [1.3381556, 0.35956356],</span>
<span class="gi">+        [-0.35946678, 0.72510189],</span>
<span class="gi">+        [1.32326943, 0.28393874],</span>
<span class="gi">+        [2.94290565, -0.13986434],</span>
<span class="gi">+        [0.28294738, -1.00125525],</span>
<span class="gi">+        [0.34218094, -0.58781961],</span>
<span class="gi">+        [-0.88864036, -0.33782387],</span>
<span class="gi">+        [-1.10146139, 0.91782682],</span>
<span class="gi">+        [-0.7969716, -0.50493969],</span>
<span class="gi">+        [0.73489726, 0.43915195],</span>
<span class="gi">+        [0.2096964, -0.61814058],</span>
<span class="gi">+        [-0.28479268, 0.70459548],</span>
<span class="gi">+        [1.84864913, 0.14729596],</span>
<span class="gi">+        [1.59068979, -0.96622933],</span>
<span class="gi">+        [0.73418199, -0.02222847],</span>
<span class="gi">+        [0.50307437, 0.498805],</span>
<span class="gi">+        [0.84929742, 0.41042894],</span>
<span class="gi">+        [0.62649535, 0.46600596],</span>
<span class="gi">+        [0.79270821, -0.41386668],</span>
<span class="gi">+        [1.16606871, -0.25641059],</span>
<span class="gi">+        [1.57356906, 0.30390519],</span>
<span class="gi">+        [1.0304995, -0.16955962],</span>
<span class="gi">+        [1.67314371, 0.19231498],</span>
<span class="gi">+        [0.98382284, 0.37184502],</span>
<span class="gi">+        [0.48921682, -1.38504507],</span>
<span class="gi">+        [-0.46226554, -0.50481004],</span>
<span class="gi">+        [-0.03918551, -0.68540745],</span>
<span class="gi">+        [0.24991051, -1.00864997],</span>
<span class="gi">+        [0.80541964, -0.34465185],</span>
<span class="gi">+        [0.1732627, -1.61323172],</span>
<span class="gi">+        [0.69804044, 0.44810796],</span>
<span class="gi">+        [-0.5506368, -0.42072426],</span>
<span class="gi">+        [-0.34474418, 0.21969797],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="gi">+Y = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        0,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        2,</span>
<span class="gi">+        1,</span>
<span class="gi">+        2,</span>
<span class="gi">+        0,</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_renn_init():</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours()</span>
<span class="gi">+</span>
<span class="gi">+    assert renn.n_neighbors == 3</span>
<span class="gi">+    assert renn.kind_sel == &quot;all&quot;</span>
<span class="gi">+    assert renn.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_renn_iter_wrong():</span>
<span class="gi">+    max_iter = -1</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours(max_iter=max_iter)</span>
<span class="gi">+    with pytest.raises(ValueError):</span>
<span class="gi">+        renn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_renn_fit_resample():</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours()</span>
<span class="gi">+    X_resampled, y_resampled = renn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+    assert 0 &lt; renn.n_iter_ &lt;= renn.max_iter</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_renn_fit_resample_mode_object():</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours(kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = renn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [-0.12840393, 0.66446571],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [-0.35946678, 0.72510189],</span>
<span class="gi">+            [2.94290565, -0.13986434],</span>
<span class="gi">+            [-1.10146139, 0.91782682],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [-0.28479268, 0.70459548],</span>
<span class="gi">+            [1.84864913, 0.14729596],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [1.67314371, 0.19231498],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [1.32319756, -0.13181616],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+    assert 0 &lt; renn.n_iter_ &lt;= renn.max_iter</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_renn_fit_resample_mode():</span>
<span class="gi">+    nn = NearestNeighbors(n_neighbors=4)</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours(n_neighbors=nn, kind_sel=&quot;mode&quot;)</span>
<span class="gi">+    X_resampled, y_resampled = renn.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [-0.53171468, -0.53735182],</span>
<span class="gi">+            [-0.88864036, -0.33782387],</span>
<span class="gi">+            [-0.46226554, -0.50481004],</span>
<span class="gi">+            [-0.34474418, 0.21969797],</span>
<span class="gi">+            [-0.12840393, 0.66446571],</span>
<span class="gi">+            [1.02956816, 0.36061601],</span>
<span class="gi">+            [1.12202806, 0.33811558],</span>
<span class="gi">+            [-0.35946678, 0.72510189],</span>
<span class="gi">+            [2.94290565, -0.13986434],</span>
<span class="gi">+            [-1.10146139, 0.91782682],</span>
<span class="gi">+            [0.73489726, 0.43915195],</span>
<span class="gi">+            [-0.28479268, 0.70459548],</span>
<span class="gi">+            [1.84864913, 0.14729596],</span>
<span class="gi">+            [0.50307437, 0.498805],</span>
<span class="gi">+            [0.84929742, 0.41042894],</span>
<span class="gi">+            [0.62649535, 0.46600596],</span>
<span class="gi">+            [1.67314371, 0.19231498],</span>
<span class="gi">+            [0.98382284, 0.37184502],</span>
<span class="gi">+            [0.69804044, 0.44810796],</span>
<span class="gi">+            [1.32319756, -0.13181616],</span>
<span class="gi">+            [0.04296502, -0.37981873],</span>
<span class="gi">+            [0.28294738, -1.00125525],</span>
<span class="gi">+            [0.34218094, -0.58781961],</span>
<span class="gi">+            [0.2096964, -0.61814058],</span>
<span class="gi">+            [1.59068979, -0.96622933],</span>
<span class="gi">+            [0.73418199, -0.02222847],</span>
<span class="gi">+            [0.79270821, -0.41386668],</span>
<span class="gi">+            [1.16606871, -0.25641059],</span>
<span class="gi">+            [1.0304995, -0.16955962],</span>
<span class="gi">+            [0.48921682, -1.38504507],</span>
<span class="gi">+            [-0.03918551, -0.68540745],</span>
<span class="gi">+            [0.24991051, -1.00864997],</span>
<span class="gi">+            [0.80541964, -0.34465185],</span>
<span class="gi">+            [0.1732627, -1.61323172],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            0,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            1,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+            2,</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+    assert 0 &lt; renn.n_iter_ &lt;= renn.max_iter</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;max_iter, n_iter&quot;,</span>
<span class="gi">+    [(2, 2), (5, 3)],</span>
<span class="gi">+)</span>
<span class="gi">+def test_renn_iter_attribute(max_iter, n_iter):</span>
<span class="gi">+    renn = RepeatedEditedNearestNeighbours(max_iter=max_iter)</span>
<span class="gi">+    renn.fit_resample(X, Y)</span>
<span class="gi">+    assert renn.n_iter_ == n_iter</span>
<span class="gh">diff --git a/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py b/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gh">index c1fd8e5..5fd8378 100644</span>
<span class="gd">--- a/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gi">+++ b/imblearn/under_sampling/_prototype_selection/tests/test_tomek_links.py</span>
<span class="gu">@@ -1,24 +1,89 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test the module Tomek&#39;s links.&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.datasets import make_classification
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gi">+</span>
<span class="w"> </span>from imblearn.under_sampling import TomekLinks
<span class="gd">-X = np.array([[0.31230513, 0.1216318], [0.68481731, 0.51935141], [</span>
<span class="gd">-    1.34192108, -0.13367336], [0.62366841, -0.21312976], [1.61091956, -</span>
<span class="gd">-    0.40283504], [-0.37162401, -2.19400981], [0.74680821, 1.63827342], [</span>
<span class="gd">-    0.2184254, 0.24299982], [0.61472253, -0.82309052], [0.19893132, -</span>
<span class="gd">-    0.47761769], [1.06514042, -0.0770537], [0.97407872, 0.44454207], [</span>
<span class="gd">-    1.40301027, -0.83648734], [-1.20515198, -1.02689695], [-0.27410027, -</span>
<span class="gd">-    0.54194484], [0.8381014, 0.44085498], [-0.23374509, 0.18370049], [-</span>
<span class="gd">-    0.32635887, -0.29299653], [-0.00288378, 0.84259929], [1.79580611, -</span>
<span class="gd">-    0.02219234]])</span>
<span class="gi">+</span>
<span class="gi">+X = np.array(</span>
<span class="gi">+    [</span>
<span class="gi">+        [0.31230513, 0.1216318],</span>
<span class="gi">+        [0.68481731, 0.51935141],</span>
<span class="gi">+        [1.34192108, -0.13367336],</span>
<span class="gi">+        [0.62366841, -0.21312976],</span>
<span class="gi">+        [1.61091956, -0.40283504],</span>
<span class="gi">+        [-0.37162401, -2.19400981],</span>
<span class="gi">+        [0.74680821, 1.63827342],</span>
<span class="gi">+        [0.2184254, 0.24299982],</span>
<span class="gi">+        [0.61472253, -0.82309052],</span>
<span class="gi">+        [0.19893132, -0.47761769],</span>
<span class="gi">+        [1.06514042, -0.0770537],</span>
<span class="gi">+        [0.97407872, 0.44454207],</span>
<span class="gi">+        [1.40301027, -0.83648734],</span>
<span class="gi">+        [-1.20515198, -1.02689695],</span>
<span class="gi">+        [-0.27410027, -0.54194484],</span>
<span class="gi">+        [0.8381014, 0.44085498],</span>
<span class="gi">+        [-0.23374509, 0.18370049],</span>
<span class="gi">+        [-0.32635887, -0.29299653],</span>
<span class="gi">+        [-0.00288378, 0.84259929],</span>
<span class="gi">+        [1.79580611, -0.02219234],</span>
<span class="gi">+    ]</span>
<span class="gi">+)</span>
<span class="w"> </span>Y = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0])


<span class="gd">-@pytest.mark.parametrize(&#39;sampling_strategy&#39;, [&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-    &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;])</span>
<span class="gi">+def test_tl_init():</span>
<span class="gi">+    tl = TomekLinks()</span>
<span class="gi">+    assert tl.n_jobs is None</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_tl_fit_resample():</span>
<span class="gi">+    tl = TomekLinks()</span>
<span class="gi">+    X_resampled, y_resampled = tl.fit_resample(X, Y)</span>
<span class="gi">+</span>
<span class="gi">+    X_gt = np.array(</span>
<span class="gi">+        [</span>
<span class="gi">+            [0.31230513, 0.1216318],</span>
<span class="gi">+            [0.68481731, 0.51935141],</span>
<span class="gi">+            [1.34192108, -0.13367336],</span>
<span class="gi">+            [0.62366841, -0.21312976],</span>
<span class="gi">+            [1.61091956, -0.40283504],</span>
<span class="gi">+            [-0.37162401, -2.19400981],</span>
<span class="gi">+            [0.74680821, 1.63827342],</span>
<span class="gi">+            [0.2184254, 0.24299982],</span>
<span class="gi">+            [0.61472253, -0.82309052],</span>
<span class="gi">+            [0.19893132, -0.47761769],</span>
<span class="gi">+            [0.97407872, 0.44454207],</span>
<span class="gi">+            [1.40301027, -0.83648734],</span>
<span class="gi">+            [-1.20515198, -1.02689695],</span>
<span class="gi">+            [-0.23374509, 0.18370049],</span>
<span class="gi">+            [-0.32635887, -0.29299653],</span>
<span class="gi">+            [-0.00288378, 0.84259929],</span>
<span class="gi">+            [1.79580611, -0.02219234],</span>
<span class="gi">+        ]</span>
<span class="gi">+    )</span>
<span class="gi">+    y_gt = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0])</span>
<span class="gi">+    assert_array_equal(X_resampled, X_gt)</span>
<span class="gi">+    assert_array_equal(y_resampled, y_gt)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy&quot;, [&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;]</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_tomek_links_strings(sampling_strategy):
<span class="w"> </span>    &quot;&quot;&quot;Check that we support all supposed strings as `sampling_strategy` in
<span class="w"> </span>    a sampler inheriting from `BaseCleaningSampler`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=100,</span>
<span class="gi">+        n_clusters_per_class=1,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        weights=[0.1, 0.3, 0.6],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    TomekLinks(sampling_strategy=sampling_strategy).fit_resample(X, y)</span>
<span class="gh">diff --git a/imblearn/under_sampling/base.py b/imblearn/under_sampling/base.py</span>
<span class="gh">index f502ba5..92da457 100644</span>
<span class="gd">--- a/imblearn/under_sampling/base.py</span>
<span class="gi">+++ b/imblearn/under_sampling/base.py</span>
<span class="gu">@@ -1,8 +1,12 @@</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>Base class for the under-sampling method.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numbers
<span class="w"> </span>from collections.abc import Mapping
<span class="gi">+</span>
<span class="w"> </span>from ..base import BaseSampler
<span class="w"> </span>from ..utils._param_validation import Interval, StrOptions

<span class="gu">@@ -13,9 +17,10 @@ class BaseUnderSampler(BaseSampler):</span>
<span class="w"> </span>    Warning: This class should not be used directly. Use the derive classes
<span class="w"> </span>    instead.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;under-sampling&#39;</span>
<span class="gd">-    _sampling_strategy_docstring = (</span>
<span class="gd">-        &quot;&quot;&quot;sampling_strategy : float, str, dict, callable, default=&#39;auto&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_type = &quot;under-sampling&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_strategy_docstring = &quot;&quot;&quot;sampling_strategy : float, str, dict, callable, default=&#39;auto&#39;</span>
<span class="w"> </span>        Sampling information to sample the data set.

<span class="w"> </span>        - When ``float``, it corresponds to the desired ratio of the number of
<span class="gu">@@ -51,11 +56,16 @@ class BaseUnderSampler(BaseSampler):</span>
<span class="w"> </span>        - When callable, function taking ``y`` and returns a ``dict``. The keys
<span class="w"> </span>          correspond to the targeted classes. The values correspond to the
<span class="w"> </span>          desired number of samples for each class.
<span class="gd">-        &quot;&quot;&quot;</span>
<span class="gd">-        .rstrip())</span>
<span class="gd">-    _parameter_constraints: dict = {&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), Mapping, callable]}</span>
<span class="gi">+        &quot;&quot;&quot;.rstrip()  # noqa: E501</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;sampling_strategy&quot;: [</span>
<span class="gi">+            Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+            StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+            Mapping,</span>
<span class="gi">+            callable,</span>
<span class="gi">+        ],</span>
<span class="gi">+    }</span>


<span class="w"> </span>class BaseCleaningSampler(BaseSampler):
<span class="gu">@@ -64,9 +74,10 @@ class BaseCleaningSampler(BaseSampler):</span>
<span class="w"> </span>    Warning: This class should not be used directly. Use the derive classes
<span class="w"> </span>    instead.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;clean-sampling&#39;</span>
<span class="gd">-    _sampling_strategy_docstring = (</span>
<span class="gd">-        &quot;&quot;&quot;sampling_strategy : str, list or callable</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_type = &quot;clean-sampling&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_strategy_docstring = &quot;&quot;&quot;sampling_strategy : str, list or callable</span>
<span class="w"> </span>        Sampling information to sample the data set.

<span class="w"> </span>        - When ``str``, specify the class targeted by the resampling. Note the
<span class="gu">@@ -89,8 +100,13 @@ class BaseCleaningSampler(BaseSampler):</span>
<span class="w"> </span>        - When callable, function taking ``y`` and returns a ``dict``. The keys
<span class="w"> </span>          correspond to the targeted classes. The values correspond to the
<span class="w"> </span>          desired number of samples for each class.
<span class="gd">-        &quot;&quot;&quot;</span>
<span class="gd">-        .rstrip())</span>
<span class="gd">-    _parameter_constraints: dict = {&#39;sampling_strategy&#39;: [Interval(numbers.</span>
<span class="gd">-        Real, 0, 1, closed=&#39;right&#39;), StrOptions({&#39;auto&#39;, &#39;majority&#39;,</span>
<span class="gd">-        &#39;not minority&#39;, &#39;not majority&#39;, &#39;all&#39;}), list, callable]}</span>
<span class="gi">+        &quot;&quot;&quot;.rstrip()</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {</span>
<span class="gi">+        &quot;sampling_strategy&quot;: [</span>
<span class="gi">+            Interval(numbers.Real, 0, 1, closed=&quot;right&quot;),</span>
<span class="gi">+            StrOptions({&quot;auto&quot;, &quot;majority&quot;, &quot;not minority&quot;, &quot;not majority&quot;, &quot;all&quot;}),</span>
<span class="gi">+            list,</span>
<span class="gi">+            callable,</span>
<span class="gi">+        ],</span>
<span class="gi">+    }</span>
<span class="gh">diff --git a/imblearn/utils/_available_if.py b/imblearn/utils/_available_if.py</span>
<span class="gh">index 51d5fc6..bca75e7 100644</span>
<span class="gd">--- a/imblearn/utils/_available_if.py</span>
<span class="gi">+++ b/imblearn/utils/_available_if.py</span>
<span class="gu">@@ -1,13 +1,17 @@</span>
<span class="w"> </span>&quot;&quot;&quot;This is a copy of sklearn/utils/_available_if.py. It can be removed when
<span class="w"> </span>we support scikit-learn &gt;= 1.1.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# mypy: ignore-errors</span>
<span class="gi">+</span>
<span class="w"> </span>from functools import update_wrapper, wraps
<span class="w"> </span>from types import MethodType
<span class="gi">+</span>
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gd">-if sklearn_version &lt; parse_version(&#39;1.1&#39;):</span>

<span class="gi">+if sklearn_version &lt; parse_version(&quot;1.1&quot;):</span>

<span class="w"> </span>    class _AvailableIfDescriptor:
<span class="w"> </span>        &quot;&quot;&quot;Implements a conditional property using the descriptor protocol.
<span class="gu">@@ -24,23 +28,30 @@ if sklearn_version &lt; parse_version(&#39;1.1&#39;):</span>
<span class="w"> </span>            self.fn = fn
<span class="w"> </span>            self.check = check
<span class="w"> </span>            self.attribute_name = attribute_name
<span class="gi">+</span>
<span class="gi">+            # update the docstring of the descriptor</span>
<span class="w"> </span>            update_wrapper(self, fn)

<span class="w"> </span>        def __get__(self, obj, owner=None):
<span class="w"> </span>            attr_err = AttributeError(
<span class="gd">-                f&#39;This {owner.__name__!r} has no attribute {self.attribute_name!r}&#39;</span>
<span class="gd">-                )</span>
<span class="gi">+                f&quot;This {owner.__name__!r} has no attribute {self.attribute_name!r}&quot;</span>
<span class="gi">+            )</span>
<span class="w"> </span>            if obj is not None:
<span class="gi">+                # delegate only on instances, not the classes.</span>
<span class="gi">+                # this is to allow access to the docstrings.</span>
<span class="w"> </span>                if not self.check(obj):
<span class="w"> </span>                    raise attr_err
<span class="w"> </span>                out = MethodType(self.fn, obj)
<span class="gd">-            else:</span>

<span class="gi">+            else:</span>
<span class="gi">+                # This makes it possible to use the decorated method as an</span>
<span class="gi">+                # unbound method, for instance when monkeypatching.</span>
<span class="w"> </span>                @wraps(self.fn)
<span class="w"> </span>                def out(*args, **kwargs):
<span class="w"> </span>                    if not self.check(args[0]):
<span class="w"> </span>                        raise attr_err
<span class="w"> </span>                    return self.fn(*args, **kwargs)
<span class="gi">+</span>
<span class="w"> </span>            return out

<span class="w"> </span>    def available_if(check):
<span class="gu">@@ -82,6 +93,7 @@ if sklearn_version &lt; parse_version(&#39;1.1&#39;):</span>
<span class="w"> </span>        &gt;&gt;&gt; obj.say_hello()
<span class="w"> </span>        Hello
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return lambda fn: _AvailableIfDescriptor(fn, check, attribute_name=fn.__name__)</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="gd">-    from sklearn.utils.metaestimators import available_if</span>
<span class="gi">+    from sklearn.utils.metaestimators import available_if  # noqa</span>
<span class="gh">diff --git a/imblearn/utils/_docstring.py b/imblearn/utils/_docstring.py</span>
<span class="gh">index b678c3d..61921c3 100644</span>
<span class="gd">--- a/imblearn/utils/_docstring.py</span>
<span class="gi">+++ b/imblearn/utils/_docstring.py</span>
<span class="gu">@@ -1,5 +1,8 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utilities for docstring in imbalanced-learn.&quot;&quot;&quot;

<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>

<span class="w"> </span>class Substitution:
<span class="w"> </span>    &quot;&quot;&quot;Decorate a function&#39;s or a class&#39; docstring to perform string
<span class="gu">@@ -11,7 +14,8 @@ class Substitution:</span>

<span class="w"> </span>    def __init__(self, *args, **kwargs):
<span class="w"> </span>        if args and kwargs:
<span class="gd">-            raise AssertionError(&#39;Only positional or keyword args are allowed&#39;)</span>
<span class="gi">+            raise AssertionError(&quot;Only positional or keyword args are allowed&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>        self.params = args or kwargs

<span class="w"> </span>    def __call__(self, obj):
<span class="gu">@@ -20,8 +24,7 @@ class Substitution:</span>
<span class="w"> </span>        return obj


<span class="gd">-_random_state_docstring = (</span>
<span class="gd">-    &quot;&quot;&quot;random_state : int, RandomState instance, default=None</span>
<span class="gi">+_random_state_docstring = &quot;&quot;&quot;random_state : int, RandomState instance, default=None</span>
<span class="w"> </span>        Control the randomization of the algorithm.

<span class="w"> </span>        - If int, ``random_state`` is the seed used by the random number
<span class="gu">@@ -30,14 +33,12 @@ _random_state_docstring = (</span>
<span class="w"> </span>          generator;
<span class="w"> </span>        - If ``None``, the random number generator is the ``RandomState``
<span class="w"> </span>          instance used by ``np.random``.
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gd">-    .rstrip())</span>
<span class="gd">-_n_jobs_docstring = (</span>
<span class="gd">-    &quot;&quot;&quot;n_jobs : int, default=None</span>
<span class="gi">+    &quot;&quot;&quot;.rstrip()</span>
<span class="gi">+</span>
<span class="gi">+_n_jobs_docstring = &quot;&quot;&quot;n_jobs : int, default=None</span>
<span class="w"> </span>        Number of CPU cores used during the cross-validation loop.
<span class="w"> </span>        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
<span class="w"> </span>        ``-1`` means using all processors. See
<span class="w"> </span>        `Glossary &lt;https://scikit-learn.org/stable/glossary.html#term-n-jobs&gt;`_
<span class="w"> </span>        for more details.
<span class="gd">-    &quot;&quot;&quot;</span>
<span class="gd">-    .rstrip())</span>
<span class="gi">+    &quot;&quot;&quot;.rstrip()</span>
<span class="gh">diff --git a/imblearn/utils/_metadata_requests.py b/imblearn/utils/_metadata_requests.py</span>
<span class="gh">index aa6e024..c81aa4f 100644</span>
<span class="gd">--- a/imblearn/utils/_metadata_requests.py</span>
<span class="gi">+++ b/imblearn/utils/_metadata_requests.py</span>
<span class="gu">@@ -76,21 +76,49 @@ of the ``RequestMethod`` descriptor to classes, which is done in the</span>
<span class="w"> </span>This mixin also implements the ``get_metadata_routing``, which meta-estimators
<span class="w"> </span>need to override, but it works for simple consumers as is.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Adrin Jalali &lt;adrin.jalali@gmail.com&gt;</span>
<span class="gi">+# License: BSD 3 clause</span>
<span class="gi">+</span>
<span class="w"> </span>import inspect
<span class="w"> </span>from collections import namedtuple
<span class="w"> </span>from copy import deepcopy
<span class="w"> </span>from typing import TYPE_CHECKING, Optional, Union
<span class="w"> </span>from warnings import warn
<span class="gi">+</span>
<span class="w"> </span>from sklearn import __version__, get_config
<span class="w"> </span>from sklearn.utils import Bunch
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(__version__)
<span class="gd">-if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="gd">-    SIMPLE_METHODS = [&#39;fit&#39;, &#39;partial_fit&#39;, &#39;predict&#39;, &#39;predict_proba&#39;,</span>
<span class="gd">-        &#39;predict_log_proba&#39;, &#39;decision_function&#39;, &#39;score&#39;, &#39;split&#39;,</span>
<span class="gd">-        &#39;transform&#39;, &#39;inverse_transform&#39;]</span>
<span class="gd">-    COMPOSITE_METHODS = {&#39;fit_transform&#39;: [&#39;fit&#39;, &#39;transform&#39;],</span>
<span class="gd">-        &#39;fit_predict&#39;: [&#39;fit&#39;, &#39;predict&#39;]}</span>
<span class="gi">+</span>
<span class="gi">+if parse_version(sklearn_version.base_version) &lt; parse_version(&quot;1.4&quot;):</span>
<span class="gi">+    # Only the following methods are supported in the routing mechanism. Adding new</span>
<span class="gi">+    # methods at the moment involves monkeypatching this list.</span>
<span class="gi">+    # Note that if this list is changed or monkeypatched, the corresponding method</span>
<span class="gi">+    # needs to be added under a TYPE_CHECKING condition like the one done here in</span>
<span class="gi">+    # _MetadataRequester</span>
<span class="gi">+    SIMPLE_METHODS = [</span>
<span class="gi">+        &quot;fit&quot;,</span>
<span class="gi">+        &quot;partial_fit&quot;,</span>
<span class="gi">+        &quot;predict&quot;,</span>
<span class="gi">+        &quot;predict_proba&quot;,</span>
<span class="gi">+        &quot;predict_log_proba&quot;,</span>
<span class="gi">+        &quot;decision_function&quot;,</span>
<span class="gi">+        &quot;score&quot;,</span>
<span class="gi">+        &quot;split&quot;,</span>
<span class="gi">+        &quot;transform&quot;,</span>
<span class="gi">+        &quot;inverse_transform&quot;,</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    # These methods are a composite of other methods and one cannot set their</span>
<span class="gi">+    # requests directly. Instead they should be set by setting the requests of the</span>
<span class="gi">+    # simple methods which make the composite ones.</span>
<span class="gi">+    COMPOSITE_METHODS = {</span>
<span class="gi">+        &quot;fit_transform&quot;: [&quot;fit&quot;, &quot;transform&quot;],</span>
<span class="gi">+        &quot;fit_predict&quot;: [&quot;fit&quot;, &quot;predict&quot;],</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="w"> </span>    METHODS = SIMPLE_METHODS + list(COMPOSITE_METHODS.keys())

<span class="w"> </span>    def _routing_enabled():
<span class="gu">@@ -104,7 +132,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            Whether metadata routing is enabled. If the config is not set, it
<span class="w"> </span>            defaults to False.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return get_config().get(&quot;enable_metadata_routing&quot;, False)</span>

<span class="w"> </span>    def _raise_for_params(params, owner, method):
<span class="w"> </span>        &quot;&quot;&quot;Raise an error if metadata routing is not enabled and params are passed.
<span class="gu">@@ -127,7 +155,19 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        ValueError
<span class="w"> </span>            If metadata routing is not enabled and params are passed.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        caller = (</span>
<span class="gi">+            f&quot;{owner.__class__.__name__}.{method}&quot;</span>
<span class="gi">+            if method</span>
<span class="gi">+            else owner.__class__.__name__</span>
<span class="gi">+        )</span>
<span class="gi">+        if not _routing_enabled() and params:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;Passing extra keyword arguments to {caller} is only supported if&quot;</span>
<span class="gi">+                &quot; enable_metadata_routing=True, which you can set using&quot;</span>
<span class="gi">+                &quot; `sklearn.set_config`. See the User Guide&quot;</span>
<span class="gi">+                &quot; &lt;https://scikit-learn.org/stable/metadata_routing.html&gt; for more&quot;</span>
<span class="gi">+                f&quot; details. Extra parameters passed are: {set(params)}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    def _raise_for_unsupported_routing(obj, method, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Raise when metadata routing is enabled and metadata is passed.
<span class="gu">@@ -149,8 +189,14 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        **kwargs : dict
<span class="w"> </span>            The metadata passed to the method.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        kwargs = {key: value for key, value in kwargs.items() if value is not None}</span>
<span class="gi">+        if _routing_enabled() and kwargs:</span>
<span class="gi">+            cls_name = obj.__class__.__name__</span>
<span class="gi">+            raise NotImplementedError(</span>
<span class="gi">+                f&quot;{cls_name}.{method} cannot accept given metadata &quot;</span>
<span class="gi">+                f&quot;({set(kwargs.keys())}) since metadata routing is not yet implemented &quot;</span>
<span class="gi">+                f&quot;for {cls_name}.&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class _RoutingNotSupportedMixin:
<span class="w"> </span>        &quot;&quot;&quot;A mixin to be used to remove the default `get_metadata_routing`.
<span class="gu">@@ -166,10 +212,29 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            &quot;&quot;&quot;Raise `NotImplementedError`.

<span class="w"> </span>            This estimator does not support metadata routing yet.&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gd">-    UNUSED = &#39;$UNUSED$&#39;</span>
<span class="gd">-    WARN = &#39;$WARN$&#39;</span>
<span class="gd">-    UNCHANGED = &#39;$UNCHANGED$&#39;</span>
<span class="gi">+            raise NotImplementedError(</span>
<span class="gi">+                f&quot;{self.__class__.__name__} has not implemented metadata routing yet.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    # Request values</span>
<span class="gi">+    # ==============</span>
<span class="gi">+    # Each request value needs to be one of the following values, or an alias.</span>
<span class="gi">+</span>
<span class="gi">+    # this is used in `__metadata_request__*` attributes to indicate that a</span>
<span class="gi">+    # metadata is not present even though it may be present in the</span>
<span class="gi">+    # corresponding method&#39;s signature.</span>
<span class="gi">+    UNUSED = &quot;$UNUSED$&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # this is used whenever a default value is changed, and therefore the user</span>
<span class="gi">+    # should explicitly set the value, otherwise a warning is shown. An example</span>
<span class="gi">+    # is when a meta-estimator is only a router, but then becomes also a</span>
<span class="gi">+    # consumer in a new release.</span>
<span class="gi">+    WARN = &quot;$WARN$&quot;</span>
<span class="gi">+</span>
<span class="gi">+    # this is the default used in `set_{method}_request` methods to indicate no</span>
<span class="gi">+    # change requested by the user.</span>
<span class="gi">+    UNCHANGED = &quot;$UNCHANGED$&quot;</span>
<span class="gi">+</span>
<span class="w"> </span>    VALID_REQUEST_VALUES = [False, True, None, UNUSED, WARN]

<span class="w"> </span>    def request_is_alias(item):
<span class="gu">@@ -188,7 +253,11 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        result : bool
<span class="w"> </span>            Whether the given item is a valid alias.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if item in VALID_REQUEST_VALUES:</span>
<span class="gi">+            return False</span>
<span class="gi">+</span>
<span class="gi">+        # item is only an alias if it&#39;s a valid identifier</span>
<span class="gi">+        return isinstance(item, str) and item.isidentifier()</span>

<span class="w"> </span>    def request_is_valid(item):
<span class="w"> </span>        &quot;&quot;&quot;Check if an item is a valid request value (and not an alias).
<span class="gu">@@ -203,8 +272,12 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        result : bool
<span class="w"> </span>            Whether the given item is valid.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        return item in VALID_REQUEST_VALUES</span>

<span class="gi">+    # Metadata Request for Simple Consumers</span>
<span class="gi">+    # =====================================</span>
<span class="gi">+    # This section includes MethodMetadataRequest and MetadataRequest which are</span>
<span class="gi">+    # used in simple consumers.</span>

<span class="w"> </span>    class MethodMetadataRequest:
<span class="w"> </span>        &quot;&quot;&quot;A prescription of how metadata is to be passed to a single method.
<span class="gu">@@ -233,9 +306,14 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        @property
<span class="w"> </span>        def requests(self):
<span class="w"> </span>            &quot;&quot;&quot;Dictionary of the form: ``{key: alias}``.&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gd">-</span>
<span class="gd">-        def add_request(self, *, param, alias):</span>
<span class="gi">+            return self._requests</span>
<span class="gi">+</span>
<span class="gi">+        def add_request(</span>
<span class="gi">+            self,</span>
<span class="gi">+            *,</span>
<span class="gi">+            param,</span>
<span class="gi">+            alias,</span>
<span class="gi">+        ):</span>
<span class="w"> </span>            &quot;&quot;&quot;Add request info for a metadata.

<span class="w"> </span>            Parameters
<span class="gu">@@ -255,7 +333,28 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>                - None: error if passed
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if not request_is_alias(alias) and not request_is_valid(alias):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;The alias you&#39;re setting for `{param}` should be either a &quot;</span>
<span class="gi">+                    &quot;valid identifier or one of {None, True, False}, but given &quot;</span>
<span class="gi">+                    f&quot;value is: `{alias}`&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if alias == param:</span>
<span class="gi">+                alias = True</span>
<span class="gi">+</span>
<span class="gi">+            if alias == UNUSED:</span>
<span class="gi">+                if param in self._requests:</span>
<span class="gi">+                    del self._requests[param]</span>
<span class="gi">+                else:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;Trying to remove parameter {param} with UNUSED which doesn&#39;t&quot;</span>
<span class="gi">+                        &quot; exist.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+            else:</span>
<span class="gi">+                self._requests[param] = alias</span>
<span class="gi">+</span>
<span class="gi">+            return self</span>

<span class="w"> </span>        def _get_param_names(self, return_alias):
<span class="w"> </span>            &quot;&quot;&quot;Get names of all metadata that can be consumed or routed by this method.
<span class="gu">@@ -274,7 +373,11 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            names : set of str
<span class="w"> </span>                A set of strings with the names of all parameters.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return set(</span>
<span class="gi">+                alias if return_alias and not request_is_valid(alias) else prop</span>
<span class="gi">+                for prop, alias in self._requests.items()</span>
<span class="gi">+                if not request_is_valid(alias) or alias is not False</span>
<span class="gi">+            )</span>

<span class="w"> </span>        def _check_warnings(self, *, params):
<span class="w"> </span>            &quot;&quot;&quot;Check whether metadata is passed which is marked as WARN.
<span class="gu">@@ -286,7 +389,19 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            params : dict
<span class="w"> </span>                The metadata passed to a method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            params = {} if params is None else params</span>
<span class="gi">+            warn_params = {</span>
<span class="gi">+                prop</span>
<span class="gi">+                for prop, alias in self._requests.items()</span>
<span class="gi">+                if alias == WARN and prop in params</span>
<span class="gi">+            }</span>
<span class="gi">+            for param in warn_params:</span>
<span class="gi">+                warn(</span>
<span class="gi">+                    f&quot;Support for {param} has recently been added to this class. &quot;</span>
<span class="gi">+                    &quot;To maintain backward compatibility, it is ignored now. &quot;</span>
<span class="gi">+                    &quot;You can set the request value to False to silence this &quot;</span>
<span class="gi">+                    &quot;warning, or to True to consume and use the metadata.&quot;</span>
<span class="gi">+                )</span>

<span class="w"> </span>        def _route_params(self, params):
<span class="w"> </span>            &quot;&quot;&quot;Prepare the given parameters to be passed to the method.
<span class="gu">@@ -305,7 +420,30 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to
<span class="w"> </span>                the corresponding method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            self._check_warnings(params=params)</span>
<span class="gi">+            unrequested = dict()</span>
<span class="gi">+            args = {arg: value for arg, value in params.items() if value is not None}</span>
<span class="gi">+            res = Bunch()</span>
<span class="gi">+            for prop, alias in self._requests.items():</span>
<span class="gi">+                if alias is False or alias == WARN:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                elif alias is True and prop in args:</span>
<span class="gi">+                    res[prop] = args[prop]</span>
<span class="gi">+                elif alias is None and prop in args:</span>
<span class="gi">+                    unrequested[prop] = args[prop]</span>
<span class="gi">+                elif alias in args:</span>
<span class="gi">+                    res[prop] = args[alias]</span>
<span class="gi">+            if unrequested:</span>
<span class="gi">+                raise UnsetMetadataPassedError(</span>
<span class="gi">+                    message=(</span>
<span class="gi">+                        f&quot;[{&#39;, &#39;.join([key for key in unrequested])}] are passed but &quot;</span>
<span class="gi">+                        &quot;are not explicitly set as requested or not for&quot;</span>
<span class="gi">+                        f&quot; {self.owner}.{self.method}&quot;</span>
<span class="gi">+                    ),</span>
<span class="gi">+                    unrequested_params=unrequested,</span>
<span class="gi">+                    routed_params=res,</span>
<span class="gi">+                )</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def _consumes(self, params):
<span class="w"> </span>            &quot;&quot;&quot;Check whether the given parameters are consumed by this method.
<span class="gu">@@ -320,7 +458,14 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            consumed : set of str
<span class="w"> </span>                A set of parameters which are consumed by this method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            params = set(params)</span>
<span class="gi">+            res = set()</span>
<span class="gi">+            for prop, alias in self._requests.items():</span>
<span class="gi">+                if alias is True and prop in params:</span>
<span class="gi">+                    res.add(prop)</span>
<span class="gi">+                elif isinstance(alias, str) and alias in params:</span>
<span class="gi">+                    res.add(alias)</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def _serialize(self):
<span class="w"> </span>            &quot;&quot;&quot;Serialize the object.
<span class="gu">@@ -330,7 +475,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            obj : dict
<span class="w"> </span>                A serialized version of the instance in the form of a dictionary.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return self._requests</span>

<span class="w"> </span>        def __repr__(self):
<span class="w"> </span>            return str(self._serialize())
<span class="gu">@@ -338,7 +483,6 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return str(repr(self))

<span class="gd">-</span>
<span class="w"> </span>    class MetadataRequest:
<span class="w"> </span>        &quot;&quot;&quot;Contains the metadata request info of a consumer.

<span class="gu">@@ -355,13 +499,20 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        owner : str
<span class="w"> </span>            The name of the object to which these requests belong.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        _type = &#39;metadata_request&#39;</span>
<span class="gi">+</span>
<span class="gi">+        # this is here for us to use this attribute&#39;s value instead of doing</span>
<span class="gi">+        # `isinstance` in our checks, so that we avoid issues when people vendor</span>
<span class="gi">+        # this file instead of using it directly from scikit-learn.</span>
<span class="gi">+        _type = &quot;metadata_request&quot;</span>

<span class="w"> </span>        def __init__(self, owner):
<span class="w"> </span>            self.owner = owner
<span class="w"> </span>            for method in SIMPLE_METHODS:
<span class="gd">-                setattr(self, method, MethodMetadataRequest(owner=owner,</span>
<span class="gd">-                    method=method))</span>
<span class="gi">+                setattr(</span>
<span class="gi">+                    self,</span>
<span class="gi">+                    method,</span>
<span class="gi">+                    MethodMetadataRequest(owner=owner, method=method),</span>
<span class="gi">+                )</span>

<span class="w"> </span>        def consumes(self, method, params):
<span class="w"> </span>            &quot;&quot;&quot;Check whether the given parameters are consumed by the given method.
<span class="gu">@@ -381,32 +532,45 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            consumed : set of str
<span class="w"> </span>                A set of parameters which are consumed by the given method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return getattr(self, method)._consumes(params=params)</span>

<span class="w"> </span>        def __getattr__(self, name):
<span class="gi">+            # Called when the default attribute access fails with an AttributeError</span>
<span class="gi">+            # (either __getattribute__() raises an AttributeError because name is</span>
<span class="gi">+            # not an instance attribute or an attribute in the class tree for self;</span>
<span class="gi">+            # or __get__() of a name property raises AttributeError). This method</span>
<span class="gi">+            # should either return the (computed) attribute value or raise an</span>
<span class="gi">+            # AttributeError exception.</span>
<span class="gi">+            # https://docs.python.org/3/reference/datamodel.html#object.__getattr__</span>
<span class="w"> </span>            if name not in COMPOSITE_METHODS:
<span class="w"> </span>                raise AttributeError(
<span class="w"> </span>                    f&quot;&#39;{self.__class__.__name__}&#39; object has no attribute &#39;{name}&#39;&quot;
<span class="gd">-                    )</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="w"> </span>            requests = {}
<span class="w"> </span>            for method in COMPOSITE_METHODS[name]:
<span class="w"> </span>                mmr = getattr(self, method)
<span class="w"> </span>                existing = set(requests.keys())
<span class="w"> </span>                upcoming = set(mmr.requests.keys())
<span class="w"> </span>                common = existing &amp; upcoming
<span class="gd">-                conflicts = [key for key in common if requests[key] != mmr.</span>
<span class="gd">-                    _requests[key]]</span>
<span class="gi">+                conflicts = [</span>
<span class="gi">+                    key for key in common if requests[key] != mmr._requests[key]</span>
<span class="gi">+                ]</span>
<span class="w"> </span>                if conflicts:
<span class="w"> </span>                    raise ValueError(
<span class="gd">-                        f&quot;Conflicting metadata requests for {&#39;, &#39;.join(conflicts)} while composing the requests for {name}. Metadata with the same name for methods {&#39;, &#39;.join(COMPOSITE_METHODS[name])} should have the same request value.&quot;</span>
<span class="gd">-                        )</span>
<span class="gi">+                        f&quot;Conflicting metadata requests for {&#39;, &#39;.join(conflicts)} &quot;</span>
<span class="gi">+                        f&quot;while composing the requests for {name}. Metadata with the &quot;</span>
<span class="gi">+                        f&quot;same name for methods {&#39;, &#39;.join(COMPOSITE_METHODS[name])} &quot;</span>
<span class="gi">+                        &quot;should have the same request value.&quot;</span>
<span class="gi">+                    )</span>
<span class="w"> </span>                requests.update(mmr._requests)
<span class="gd">-            return MethodMetadataRequest(owner=self.owner, method=name,</span>
<span class="gd">-                requests=requests)</span>
<span class="gi">+            return MethodMetadataRequest(</span>
<span class="gi">+                owner=self.owner, method=name, requests=requests</span>
<span class="gi">+            )</span>

<span class="gd">-        def _get_param_names(self, method, return_alias,</span>
<span class="gd">-            ignore_self_request=None):</span>
<span class="gd">-            &quot;&quot;&quot;Get names of all metadata that can be consumed or routed by specified                 method.</span>
<span class="gi">+        def _get_param_names(self, method, return_alias, ignore_self_request=None):</span>
<span class="gi">+            &quot;&quot;&quot;Get names of all metadata that can be consumed or routed by specified \</span>
<span class="gi">+                method.</span>

<span class="w"> </span>            This method returns the names of all metadata, even the ``False``
<span class="w"> </span>            ones.
<span class="gu">@@ -428,7 +592,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            names : set of str
<span class="w"> </span>                A set of strings with the names of all parameters.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return getattr(self, method)._get_param_names(return_alias=return_alias)</span>

<span class="w"> </span>        def _route_params(self, *, method, params):
<span class="w"> </span>            &quot;&quot;&quot;Prepare the given parameters to be passed to the method.
<span class="gu">@@ -451,7 +615,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to
<span class="w"> </span>                the corresponding method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return getattr(self, method)._route_params(params=params)</span>

<span class="w"> </span>        def _check_warnings(self, *, method, params):
<span class="w"> </span>            &quot;&quot;&quot;Check whether metadata is passed which is marked as WARN.
<span class="gu">@@ -466,7 +630,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            params : dict
<span class="w"> </span>                The metadata passed to a method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            getattr(self, method)._check_warnings(params=params)</span>

<span class="w"> </span>        def _serialize(self):
<span class="w"> </span>            &quot;&quot;&quot;Serialize the object.
<span class="gu">@@ -476,16 +640,32 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            obj : dict
<span class="w"> </span>                A serialized version of the instance in the form of a dictionary.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            output = dict()</span>
<span class="gi">+            for method in SIMPLE_METHODS:</span>
<span class="gi">+                mmr = getattr(self, method)</span>
<span class="gi">+                if len(mmr.requests):</span>
<span class="gi">+                    output[method] = mmr._serialize()</span>
<span class="gi">+            return output</span>

<span class="w"> </span>        def __repr__(self):
<span class="w"> </span>            return str(self._serialize())

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return str(repr(self))
<span class="gd">-    RouterMappingPair = namedtuple(&#39;RouterMappingPair&#39;, [&#39;mapping&#39;, &#39;router&#39;])</span>
<span class="gd">-    MethodPair = namedtuple(&#39;MethodPair&#39;, [&#39;callee&#39;, &#39;caller&#39;])</span>

<span class="gi">+    # Metadata Request for Routers</span>
<span class="gi">+    # ============================</span>
<span class="gi">+    # This section includes all objects required for MetadataRouter which is used</span>
<span class="gi">+    # in routers, returned by their ``get_metadata_routing``.</span>
<span class="gi">+</span>
<span class="gi">+    # This namedtuple is used to store a (mapping, routing) pair. Mapping is a</span>
<span class="gi">+    # MethodMapping object, and routing is the output of `get_metadata_routing`.</span>
<span class="gi">+    # MetadataRouter stores a collection of these namedtuples.</span>
<span class="gi">+    RouterMappingPair = namedtuple(&quot;RouterMappingPair&quot;, [&quot;mapping&quot;, &quot;router&quot;])</span>
<span class="gi">+</span>
<span class="gi">+    # A namedtuple storing a single method route. A collection of these namedtuples</span>
<span class="gi">+    # is stored in a MetadataRouter.</span>
<span class="gi">+    MethodPair = namedtuple(&quot;MethodPair&quot;, [&quot;callee&quot;, &quot;caller&quot;])</span>

<span class="w"> </span>    class MethodMapping:
<span class="w"> </span>        &quot;&quot;&quot;Stores the mapping between callee and caller methods for a router.
<span class="gu">@@ -523,7 +703,18 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self : MethodMapping
<span class="w"> </span>                Returns self.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if callee not in METHODS:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;Given callee:{callee} is not a valid method. Valid methods are:&quot;</span>
<span class="gi">+                    f&quot; {METHODS}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            if caller not in METHODS:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;Given caller:{caller} is not a valid method. Valid methods are:&quot;</span>
<span class="gi">+                    f&quot; {METHODS}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            self._routes.append(MethodPair(callee=callee, caller=caller))</span>
<span class="gi">+            return self</span>

<span class="w"> </span>        def _serialize(self):
<span class="w"> </span>            &quot;&quot;&quot;Serialize the object.
<span class="gu">@@ -533,7 +724,10 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            obj : list
<span class="w"> </span>                A serialized version of the instance in the form of a list.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            result = list()</span>
<span class="gi">+            for route in self._routes:</span>
<span class="gi">+                result.append({&quot;callee&quot;: route.callee, &quot;caller&quot;: route.caller})</span>
<span class="gi">+            return result</span>

<span class="w"> </span>        @classmethod
<span class="w"> </span>        def from_str(cls, route):
<span class="gu">@@ -554,7 +748,15 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                A :class:`~sklearn.utils.metadata_routing.MethodMapping` instance
<span class="w"> </span>                constructed from the given string.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            routing = cls()</span>
<span class="gi">+            if route == &quot;one-to-one&quot;:</span>
<span class="gi">+                for method in METHODS:</span>
<span class="gi">+                    routing.add(callee=method, caller=method)</span>
<span class="gi">+            elif route in METHODS:</span>
<span class="gi">+                routing.add(callee=route, caller=route)</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(&quot;route should be &#39;one-to-one&#39; or a single method!&quot;)</span>
<span class="gi">+            return routing</span>

<span class="w"> </span>        def __repr__(self):
<span class="w"> </span>            return str(self._serialize())
<span class="gu">@@ -562,7 +764,6 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return str(repr(self))

<span class="gd">-</span>
<span class="w"> </span>    class MetadataRouter:
<span class="w"> </span>        &quot;&quot;&quot;Stores and handles metadata routing for a router object.

<span class="gu">@@ -581,10 +782,18 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        owner : str
<span class="w"> </span>            The name of the object to which these requests belong.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        _type = &#39;metadata_router&#39;</span>
<span class="gi">+</span>
<span class="gi">+        # this is here for us to use this attribute&#39;s value instead of doing</span>
<span class="gi">+        # `isinstance`` in our checks, so that we avoid issues when people vendor</span>
<span class="gi">+        # this file instead of using it directly from scikit-learn.</span>
<span class="gi">+        _type = &quot;metadata_router&quot;</span>

<span class="w"> </span>        def __init__(self, owner):
<span class="w"> </span>            self._route_mappings = dict()
<span class="gi">+            # `_self_request` is used if the router is also a consumer.</span>
<span class="gi">+            # _self_request, (added using `add_self_request()`) is treated</span>
<span class="gi">+            # differently from the other objects which are stored in</span>
<span class="gi">+            # _route_mappings.</span>
<span class="w"> </span>            self._self_request = None
<span class="w"> </span>            self.owner = owner

<span class="gu">@@ -612,7 +821,17 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self : MetadataRouter
<span class="w"> </span>                Returns `self`.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if getattr(obj, &quot;_type&quot;, None) == &quot;metadata_request&quot;:</span>
<span class="gi">+                self._self_request = deepcopy(obj)</span>
<span class="gi">+            elif hasattr(obj, &quot;_get_metadata_request&quot;):</span>
<span class="gi">+                self._self_request = deepcopy(obj._get_metadata_request())</span>
<span class="gi">+            else:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;Given `obj` is neither a `MetadataRequest` nor does it implement &quot;</span>
<span class="gi">+                    &quot;the required API. Inheriting from `BaseEstimator` implements the &quot;</span>
<span class="gi">+                    &quot;required API.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return self</span>

<span class="w"> </span>        def add(self, *, method_mapping, **objs):
<span class="w"> </span>            &quot;&quot;&quot;Add named objects with their corresponding method mapping.
<span class="gu">@@ -633,7 +852,16 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self : MetadataRouter
<span class="w"> </span>                Returns `self`.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if isinstance(method_mapping, str):</span>
<span class="gi">+                method_mapping = MethodMapping.from_str(method_mapping)</span>
<span class="gi">+            else:</span>
<span class="gi">+                method_mapping = deepcopy(method_mapping)</span>
<span class="gi">+</span>
<span class="gi">+            for name, obj in objs.items():</span>
<span class="gi">+                self._route_mappings[name] = RouterMappingPair(</span>
<span class="gi">+                    mapping=method_mapping, router=get_routing_for_object(obj)</span>
<span class="gi">+                )</span>
<span class="gi">+            return self</span>

<span class="w"> </span>        def consumes(self, method, params):
<span class="w"> </span>            &quot;&quot;&quot;Check whether the given parameters are consumed by the given method.
<span class="gu">@@ -653,11 +881,22 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            consumed : set of str
<span class="w"> </span>                A set of parameters which are consumed by the given method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            res = set()</span>
<span class="gi">+            if self._self_request:</span>
<span class="gi">+                res = res | self._self_request.consumes(method=method, params=params)</span>
<span class="gi">+</span>
<span class="gi">+            for _, route_mapping in self._route_mappings.items():</span>
<span class="gi">+                for callee, caller in route_mapping.mapping:</span>
<span class="gi">+                    if caller == method:</span>
<span class="gi">+                        res = res | route_mapping.router.consumes(</span>
<span class="gi">+                            method=callee, params=params</span>
<span class="gi">+                        )</span>

<span class="gd">-        def _get_param_names(self, *, method, return_alias, ignore_self_request</span>
<span class="gd">-            ):</span>
<span class="gd">-            &quot;&quot;&quot;Get names of all metadata that can be consumed or routed by specified                 method.</span>
<span class="gi">+            return res</span>
<span class="gi">+</span>
<span class="gi">+        def _get_param_names(self, *, method, return_alias, ignore_self_request):</span>
<span class="gi">+            &quot;&quot;&quot;Get names of all metadata that can be consumed or routed by specified \</span>
<span class="gi">+                method.</span>

<span class="w"> </span>            This method returns the names of all metadata, even the ``False``
<span class="w"> </span>            ones.
<span class="gu">@@ -681,7 +920,25 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            names : set of str
<span class="w"> </span>                A set of strings with the names of all parameters.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            res = set()</span>
<span class="gi">+            if self._self_request and not ignore_self_request:</span>
<span class="gi">+                res = res.union(</span>
<span class="gi">+                    self._self_request._get_param_names(</span>
<span class="gi">+                        method=method, return_alias=return_alias</span>
<span class="gi">+                    )</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            for name, route_mapping in self._route_mappings.items():</span>
<span class="gi">+                for callee, caller in route_mapping.mapping:</span>
<span class="gi">+                    if caller == method:</span>
<span class="gi">+                        res = res.union(</span>
<span class="gi">+                            route_mapping.router._get_param_names(</span>
<span class="gi">+                                method=callee,</span>
<span class="gi">+                                return_alias=True,</span>
<span class="gi">+                                ignore_self_request=False,</span>
<span class="gi">+                            )</span>
<span class="gi">+                        )</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def _route_params(self, *, params, method):
<span class="w"> </span>            &quot;&quot;&quot;Prepare the given parameters to be passed to the method.
<span class="gu">@@ -708,7 +965,31 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to
<span class="w"> </span>                the corresponding method.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            res = Bunch()</span>
<span class="gi">+            if self._self_request:</span>
<span class="gi">+                res.update(</span>
<span class="gi">+                    self._self_request._route_params(params=params, method=method)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            param_names = self._get_param_names(</span>
<span class="gi">+                method=method, return_alias=True, ignore_self_request=True</span>
<span class="gi">+            )</span>
<span class="gi">+            child_params = {</span>
<span class="gi">+                key: value for key, value in params.items() if key in param_names</span>
<span class="gi">+            }</span>
<span class="gi">+            for key in set(res.keys()).intersection(child_params.keys()):</span>
<span class="gi">+                # conflicts are okay if the passed objects are the same, but it&#39;s</span>
<span class="gi">+                # an issue if they&#39;re different objects.</span>
<span class="gi">+                if child_params[key] is not res[key]:</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;In {self.owner}, there is a conflict on {key} between what is&quot;</span>
<span class="gi">+                        &quot; requested for this estimator and what is requested by its&quot;</span>
<span class="gi">+                        &quot; children. You can resolve this conflict by using an alias for&quot;</span>
<span class="gi">+                        &quot; the child estimator(s) requested metadata.&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+            res.update(child_params)</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def route_params(self, *, caller, params):
<span class="w"> </span>            &quot;&quot;&quot;Return the input parameters requested by child objects.
<span class="gu">@@ -738,7 +1019,20 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                used to pass the required metadata to corresponding methods or
<span class="w"> </span>                corresponding child objects.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if self._self_request:</span>
<span class="gi">+                self._self_request._check_warnings(params=params, method=caller)</span>
<span class="gi">+</span>
<span class="gi">+            res = Bunch()</span>
<span class="gi">+            for name, route_mapping in self._route_mappings.items():</span>
<span class="gi">+                router, mapping = route_mapping.router, route_mapping.mapping</span>
<span class="gi">+</span>
<span class="gi">+                res[name] = Bunch()</span>
<span class="gi">+                for _callee, _caller in mapping:</span>
<span class="gi">+                    if _caller == caller:</span>
<span class="gi">+                        res[name][_callee] = router._route_params(</span>
<span class="gi">+                            params=params, method=_callee</span>
<span class="gi">+                        )</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def validate_metadata(self, *, method, params):
<span class="w"> </span>            &quot;&quot;&quot;Validate given metadata for a method.
<span class="gu">@@ -756,7 +1050,21 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            params : dict
<span class="w"> </span>                A dictionary of provided metadata.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            param_names = self._get_param_names(</span>
<span class="gi">+                method=method, return_alias=False, ignore_self_request=False</span>
<span class="gi">+            )</span>
<span class="gi">+            if self._self_request:</span>
<span class="gi">+                self_params = self._self_request._get_param_names(</span>
<span class="gi">+                    method=method, return_alias=False</span>
<span class="gi">+                )</span>
<span class="gi">+            else:</span>
<span class="gi">+                self_params = set()</span>
<span class="gi">+            extra_keys = set(params.keys()) - param_names - self_params</span>
<span class="gi">+            if extra_keys:</span>
<span class="gi">+                raise TypeError(</span>
<span class="gi">+                    f&quot;{self.owner}.{method} got unexpected argument(s) {extra_keys}, &quot;</span>
<span class="gi">+                    &quot;which are not requested metadata in any object.&quot;</span>
<span class="gi">+                )</span>

<span class="w"> </span>        def _serialize(self):
<span class="w"> </span>            &quot;&quot;&quot;Serialize the object.
<span class="gu">@@ -766,15 +1074,27 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            obj : dict
<span class="w"> </span>                A serialized version of the instance in the form of a dictionary.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            res = dict()</span>
<span class="gi">+            if self._self_request:</span>
<span class="gi">+                res[&quot;$self_request&quot;] = self._self_request._serialize()</span>
<span class="gi">+            for name, route_mapping in self._route_mappings.items():</span>
<span class="gi">+                res[name] = dict()</span>
<span class="gi">+                res[name][&quot;mapping&quot;] = route_mapping.mapping._serialize()</span>
<span class="gi">+                res[name][&quot;router&quot;] = route_mapping.router._serialize()</span>
<span class="gi">+</span>
<span class="gi">+            return res</span>

<span class="w"> </span>        def __iter__(self):
<span class="w"> </span>            if self._self_request:
<span class="gd">-                yield &#39;$self_request&#39;, RouterMappingPair(mapping=</span>
<span class="gd">-                    MethodMapping.from_str(&#39;one-to-one&#39;), router=self.</span>
<span class="gd">-                    _self_request)</span>
<span class="gi">+                yield (</span>
<span class="gi">+                    &quot;$self_request&quot;,</span>
<span class="gi">+                    RouterMappingPair(</span>
<span class="gi">+                        mapping=MethodMapping.from_str(&quot;one-to-one&quot;),</span>
<span class="gi">+                        router=self._self_request,</span>
<span class="gi">+                    ),</span>
<span class="gi">+                )</span>
<span class="w"> </span>            for name, route_mapping in self._route_mappings.items():
<span class="gd">-                yield name, route_mapping</span>
<span class="gi">+                yield (name, route_mapping)</span>

<span class="w"> </span>        def __repr__(self):
<span class="w"> </span>            return str(self._serialize())
<span class="gu">@@ -813,7 +1133,23 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            A ``MetadataRequest`` or a ``MetadataRouting`` taken or created from
<span class="w"> </span>            the given object.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        # doing this instead of a try/except since an AttributeError could be raised</span>
<span class="gi">+        # for other reasons.</span>
<span class="gi">+        if hasattr(obj, &quot;get_metadata_routing&quot;):</span>
<span class="gi">+            return deepcopy(obj.get_metadata_routing())</span>
<span class="gi">+</span>
<span class="gi">+        elif getattr(obj, &quot;_type&quot;, None) in [&quot;metadata_request&quot;, &quot;metadata_router&quot;]:</span>
<span class="gi">+            return deepcopy(obj)</span>
<span class="gi">+</span>
<span class="gi">+        return MetadataRequest(owner=None)</span>
<span class="gi">+</span>
<span class="gi">+    # Request method</span>
<span class="gi">+    # ==============</span>
<span class="gi">+    # This section includes what&#39;s needed for the request method descriptor and</span>
<span class="gi">+    # their dynamic generation in a meta class.</span>
<span class="gi">+</span>
<span class="gi">+    # These strings are used to dynamically generate the docstrings for</span>
<span class="gi">+    # set_{method}_request methods.</span>
<span class="w"> </span>    REQUESTER_DOC = &quot;&quot;&quot;        Request metadata passed to the ``{method}`` method.

<span class="w"> </span>            Note that this method is only relevant if
<span class="gu">@@ -823,13 +1159,18 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>            The options for each parameter are:

<span class="gd">-            - ``True``: metadata is requested, and     passed to ``{method}`` if provided. The request is ignored if     metadata is not provided.</span>
<span class="gi">+            - ``True``: metadata is requested, and \</span>
<span class="gi">+    passed to ``{method}`` if provided. The request is ignored if \</span>
<span class="gi">+    metadata is not provided.</span>

<span class="gd">-            - ``False``: metadata is not requested and the meta-estimator     will not pass it to ``{method}``.</span>
<span class="gi">+            - ``False``: metadata is not requested and the meta-estimator \</span>
<span class="gi">+    will not pass it to ``{method}``.</span>

<span class="gd">-            - ``None``: metadata is not requested, and the meta-estimator     will raise an error if the user provides it.</span>
<span class="gi">+            - ``None``: metadata is not requested, and the meta-estimator \</span>
<span class="gi">+    will raise an error if the user provides it.</span>

<span class="gd">-            - ``str``: metadata should be passed to the meta-estimator with     this given alias instead of the original name.</span>
<span class="gi">+            - ``str``: metadata should be passed to the meta-estimator with \</span>
<span class="gi">+    this given alias instead of the original name.</span>

<span class="w"> </span>            The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
<span class="w"> </span>            existing request. This allows you to change the request for some
<span class="gu">@@ -845,7 +1186,8 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            Parameters
<span class="w"> </span>            ----------
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    REQUESTER_DOC_PARAM = &quot;&quot;&quot;        {metadata} : str, True, False, or None,                         default=sklearn.utils.metadata_routing.UNCHANGED</span>
<span class="gi">+    REQUESTER_DOC_PARAM = &quot;&quot;&quot;        {metadata} : str, True, False, or None, \</span>
<span class="gi">+                        default=sklearn.utils.metadata_routing.UNCHANGED</span>
<span class="w"> </span>                Metadata routing for ``{metadata}`` parameter in ``{method}``.

<span class="w"> </span>    &quot;&quot;&quot;
<span class="gu">@@ -855,7 +1197,6 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                The updated object.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-</span>
<span class="w"> </span>    class RequestMethod:
<span class="w"> </span>        &quot;&quot;&quot;
<span class="w"> </span>        A descriptor for request methods.
<span class="gu">@@ -895,7 +1236,7 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self.validate_keys = validate_keys

<span class="w"> </span>        def __get__(self, instance, owner):
<span class="gd">-</span>
<span class="gi">+            # we would want to have a method which accepts only the expected args</span>
<span class="w"> </span>            def func(*args, **kw):
<span class="w"> </span>                &quot;&quot;&quot;Updates the request for provided parameters

<span class="gu">@@ -904,46 +1245,76 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                &quot;&quot;&quot;
<span class="w"> </span>                if not _routing_enabled():
<span class="w"> </span>                    raise RuntimeError(
<span class="gd">-                        &#39;This method is only available when metadata routing is enabled. You can enable it using sklearn.set_config(enable_metadata_routing=True).&#39;</span>
<span class="gd">-                        )</span>
<span class="gd">-                if self.validate_keys and set(kw) - set(self.keys):</span>
<span class="gi">+                        &quot;This method is only available when metadata routing is &quot;</span>
<span class="gi">+                        &quot;enabled. You can enable it using&quot;</span>
<span class="gi">+                        &quot; sklearn.set_config(enable_metadata_routing=True).&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+                if self.validate_keys and (set(kw) - set(self.keys)):</span>
<span class="w"> </span>                    raise TypeError(
<span class="gd">-                        f&#39;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments are: {set(self.keys)}&#39;</span>
<span class="gd">-                        )</span>
<span class="gi">+                        f&quot;Unexpected args: {set(kw) - set(self.keys)}. Accepted &quot;</span>
<span class="gi">+                        f&quot;arguments are: {set(self.keys)}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+                # This makes it possible to use the decorated method as an unbound</span>
<span class="gi">+                # method, for instance when monkeypatching.</span>
<span class="gi">+                # https://github.com/scikit-learn/scikit-learn/issues/28632</span>
<span class="w"> </span>                if instance is None:
<span class="w"> </span>                    _instance = args[0]
<span class="w"> </span>                    args = args[1:]
<span class="w"> </span>                else:
<span class="w"> </span>                    _instance = instance
<span class="gi">+</span>
<span class="gi">+                # Replicating python&#39;s behavior when positional args are given other</span>
<span class="gi">+                # than `self`, and `self` is only allowed if this method is unbound.</span>
<span class="w"> </span>                if args:
<span class="w"> </span>                    raise TypeError(
<span class="gd">-                        f&#39;set_{self.name}_request() takes 0 positional argument but {len(args)} were given&#39;</span>
<span class="gd">-                        )</span>
<span class="gi">+                        f&quot;set_{self.name}_request() takes 0 positional argument but&quot;</span>
<span class="gi">+                        f&quot; {len(args)} were given&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="w"> </span>                requests = _instance._get_metadata_request()
<span class="w"> </span>                method_metadata_request = getattr(requests, self.name)
<span class="gi">+</span>
<span class="w"> </span>                for prop, alias in kw.items():
<span class="w"> </span>                    if alias is not UNCHANGED:
<span class="gd">-                        method_metadata_request.add_request(param=prop,</span>
<span class="gd">-                            alias=alias)</span>
<span class="gi">+                        method_metadata_request.add_request(param=prop, alias=alias)</span>
<span class="w"> </span>                _instance._metadata_request = requests
<span class="gi">+</span>
<span class="w"> </span>                return _instance
<span class="gd">-            func.__name__ = f&#39;set_{self.name}_request&#39;</span>
<span class="gd">-            params = [inspect.Parameter(name=&#39;self&#39;, kind=inspect.Parameter</span>
<span class="gd">-                .POSITIONAL_OR_KEYWORD, annotation=owner)]</span>
<span class="gd">-            params.extend([inspect.Parameter(k, inspect.Parameter.</span>
<span class="gd">-                KEYWORD_ONLY, default=UNCHANGED, annotation=Optional[Union[</span>
<span class="gd">-                bool, None, str]]) for k in self.keys])</span>
<span class="gd">-            func.__signature__ = inspect.Signature(params,</span>
<span class="gd">-                return_annotation=owner)</span>
<span class="gi">+</span>
<span class="gi">+            # Now we set the relevant attributes of the function so that it seems</span>
<span class="gi">+            # like a normal method to the end user, with known expected arguments.</span>
<span class="gi">+            func.__name__ = f&quot;set_{self.name}_request&quot;</span>
<span class="gi">+            params = [</span>
<span class="gi">+                inspect.Parameter(</span>
<span class="gi">+                    name=&quot;self&quot;,</span>
<span class="gi">+                    kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,</span>
<span class="gi">+                    annotation=owner,</span>
<span class="gi">+                )</span>
<span class="gi">+            ]</span>
<span class="gi">+            params.extend(</span>
<span class="gi">+                [</span>
<span class="gi">+                    inspect.Parameter(</span>
<span class="gi">+                        k,</span>
<span class="gi">+                        inspect.Parameter.KEYWORD_ONLY,</span>
<span class="gi">+                        default=UNCHANGED,</span>
<span class="gi">+                        annotation=Optional[Union[bool, None, str]],</span>
<span class="gi">+                    )</span>
<span class="gi">+                    for k in self.keys</span>
<span class="gi">+                ]</span>
<span class="gi">+            )</span>
<span class="gi">+            func.__signature__ = inspect.Signature(</span>
<span class="gi">+                params,</span>
<span class="gi">+                return_annotation=owner,</span>
<span class="gi">+            )</span>
<span class="w"> </span>            doc = REQUESTER_DOC.format(method=self.name)
<span class="w"> </span>            for metadata in self.keys:
<span class="gd">-                doc += REQUESTER_DOC_PARAM.format(metadata=metadata, method</span>
<span class="gd">-                    =self.name)</span>
<span class="gi">+                doc += REQUESTER_DOC_PARAM.format(metadata=metadata, method=self.name)</span>
<span class="w"> </span>            doc += REQUESTER_DOC_RETURN
<span class="w"> </span>            func.__doc__ = doc
<span class="w"> </span>            return func

<span class="gd">-</span>
<span class="w"> </span>    class _MetadataRequester:
<span class="w"> </span>        &quot;&quot;&quot;Mixin class for adding metadata request functionality.

<span class="gu">@@ -951,7 +1322,27 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        .. versionadded:: 1.3
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        if TYPE_CHECKING:</span>
<span class="gi">+</span>
<span class="gi">+        if TYPE_CHECKING:  # pragma: no cover</span>
<span class="gi">+            # This code is never run in runtime, but it&#39;s here for type checking.</span>
<span class="gi">+            # Type checkers fail to understand that the `set_{method}_request`</span>
<span class="gi">+            # methods are dynamically generated, and they complain that they are</span>
<span class="gi">+            # not defined. We define them here to make type checkers happy.</span>
<span class="gi">+            # During type checking analyzers assume this to be True.</span>
<span class="gi">+            # The following list of defined methods mirrors the list of methods</span>
<span class="gi">+            # in SIMPLE_METHODS.</span>
<span class="gi">+            # fmt: off</span>
<span class="gi">+            def set_fit_request(self, **kwargs): pass</span>
<span class="gi">+            def set_partial_fit_request(self, **kwargs): pass</span>
<span class="gi">+            def set_predict_request(self, **kwargs): pass</span>
<span class="gi">+            def set_predict_proba_request(self, **kwargs): pass</span>
<span class="gi">+            def set_predict_log_proba_request(self, **kwargs): pass</span>
<span class="gi">+            def set_decision_function_request(self, **kwargs): pass</span>
<span class="gi">+            def set_score_request(self, **kwargs): pass</span>
<span class="gi">+            def set_split_request(self, **kwargs): pass</span>
<span class="gi">+            def set_transform_request(self, **kwargs): pass</span>
<span class="gi">+            def set_inverse_transform_request(self, **kwargs): pass</span>
<span class="gi">+            # fmt: on</span>

<span class="w"> </span>        def __init_subclass__(cls, **kwargs):
<span class="w"> </span>            &quot;&quot;&quot;Set the ``set_{method}_request`` methods.
<span class="gu">@@ -973,14 +1364,22 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            try:
<span class="w"> </span>                requests = cls._get_default_requests()
<span class="w"> </span>            except Exception:
<span class="gi">+                # if there are any issues in the default values, it will be raised</span>
<span class="gi">+                # when ``get_metadata_routing`` is called. Here we are going to</span>
<span class="gi">+                # ignore all the issues such as bad defaults etc.</span>
<span class="w"> </span>                super().__init_subclass__(**kwargs)
<span class="w"> </span>                return
<span class="gi">+</span>
<span class="w"> </span>            for method in SIMPLE_METHODS:
<span class="w"> </span>                mmr = getattr(requests, method)
<span class="gi">+                # set ``set_{method}_request``` methods</span>
<span class="w"> </span>                if not len(mmr.requests):
<span class="w"> </span>                    continue
<span class="gd">-                setattr(cls, f&#39;set_{method}_request&#39;, RequestMethod(method,</span>
<span class="gd">-                    sorted(mmr.requests.keys())))</span>
<span class="gi">+                setattr(</span>
<span class="gi">+                    cls,</span>
<span class="gi">+                    f&quot;set_{method}_request&quot;,</span>
<span class="gi">+                    RequestMethod(method, sorted(mmr.requests.keys())),</span>
<span class="gi">+                )</span>
<span class="w"> </span>            super().__init_subclass__(**kwargs)

<span class="w"> </span>        @classmethod
<span class="gu">@@ -1003,7 +1402,25 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            method_request : MethodMetadataRequest
<span class="w"> </span>                The prepared request using the method&#39;s signature.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            mmr = MethodMetadataRequest(owner=cls.__name__, method=method)</span>
<span class="gi">+            # Here we use `isfunction` instead of `ismethod` because calling `getattr`</span>
<span class="gi">+            # on a class instead of an instance returns an unbound function.</span>
<span class="gi">+            if not hasattr(cls, method) or not inspect.isfunction(getattr(cls, method)):</span>
<span class="gi">+                return mmr</span>
<span class="gi">+            # ignore the first parameter of the method, which is usually &quot;self&quot;</span>
<span class="gi">+            params = list(inspect.signature(getattr(cls, method)).parameters.items())[</span>
<span class="gi">+                1:</span>
<span class="gi">+            ]</span>
<span class="gi">+            for pname, param in params:</span>
<span class="gi">+                if pname in {&quot;X&quot;, &quot;y&quot;, &quot;Y&quot;, &quot;Xt&quot;, &quot;yt&quot;}:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                if param.kind in {param.VAR_POSITIONAL, param.VAR_KEYWORD}:</span>
<span class="gi">+                    continue</span>
<span class="gi">+                mmr.add_request(</span>
<span class="gi">+                    param=pname,</span>
<span class="gi">+                    alias=None,</span>
<span class="gi">+                )</span>
<span class="gi">+            return mmr</span>

<span class="w"> </span>        @classmethod
<span class="w"> </span>        def _get_default_requests(cls):
<span class="gu">@@ -1013,7 +1430,43 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            class attributes, as well as determining request keys from method
<span class="w"> </span>            signatures.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            requests = MetadataRequest(owner=cls.__name__)</span>
<span class="gi">+</span>
<span class="gi">+            for method in SIMPLE_METHODS:</span>
<span class="gi">+                setattr(</span>
<span class="gi">+                    requests,</span>
<span class="gi">+                    method,</span>
<span class="gi">+                    cls._build_request_for_signature(router=requests, method=method),</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            # Then overwrite those defaults with the ones provided in</span>
<span class="gi">+            # __metadata_request__* attributes. Defaults set in</span>
<span class="gi">+            # __metadata_request__* attributes take precedence over signature</span>
<span class="gi">+            # sniffing.</span>
<span class="gi">+</span>
<span class="gi">+            # need to go through the MRO since this is a class attribute and</span>
<span class="gi">+            # ``vars`` doesn&#39;t report the parent class attributes. We go through</span>
<span class="gi">+            # the reverse of the MRO so that child classes have precedence over</span>
<span class="gi">+            # their parents.</span>
<span class="gi">+            defaults = dict()</span>
<span class="gi">+            for base_class in reversed(inspect.getmro(cls)):</span>
<span class="gi">+                base_defaults = {</span>
<span class="gi">+                    attr: value</span>
<span class="gi">+                    for attr, value in vars(base_class).items()</span>
<span class="gi">+                    if &quot;__metadata_request__&quot; in attr</span>
<span class="gi">+                }</span>
<span class="gi">+                defaults.update(base_defaults)</span>
<span class="gi">+            defaults = dict(sorted(defaults.items()))</span>
<span class="gi">+</span>
<span class="gi">+            for attr, value in defaults.items():</span>
<span class="gi">+                # we don&#39;t check for attr.startswith() since python prefixes attrs</span>
<span class="gi">+                # starting with __ with the `_ClassName`.</span>
<span class="gi">+                substr = &quot;__metadata_request__&quot;</span>
<span class="gi">+                method = attr[attr.index(substr) + len(substr) :]</span>
<span class="gi">+                for prop, alias in value.items():</span>
<span class="gi">+                    getattr(requests, method).add_request(param=prop, alias=alias)</span>
<span class="gi">+</span>
<span class="gi">+            return requests</span>

<span class="w"> </span>        def _get_metadata_request(self):
<span class="w"> </span>            &quot;&quot;&quot;Get requested data properties.
<span class="gu">@@ -1026,7 +1479,12 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            request : MetadataRequest
<span class="w"> </span>                A :class:`~sklearn.utils.metadata_routing.MetadataRequest` instance.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            if hasattr(self, &quot;_metadata_request&quot;):</span>
<span class="gi">+                requests = get_routing_for_object(self._metadata_request)</span>
<span class="gi">+            else:</span>
<span class="gi">+                requests = self._get_default_requests()</span>
<span class="gi">+</span>
<span class="gi">+            return requests</span>

<span class="w"> </span>        def get_metadata_routing(self):
<span class="w"> </span>            &quot;&quot;&quot;Get metadata routing of this object.
<span class="gu">@@ -1040,8 +1498,17 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>                A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
<span class="w"> </span>                routing information.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            return self._get_metadata_request()</span>

<span class="gi">+    # Process Routing in Routers</span>
<span class="gi">+    # ==========================</span>
<span class="gi">+    # This is almost always the only method used in routers to process and route</span>
<span class="gi">+    # given metadata. This is to minimize the boilerplate required in routers.</span>
<span class="gi">+</span>
<span class="gi">+    # Here the first two arguments are positional only which makes everything</span>
<span class="gi">+    # passed as keyword argument a metadata. The first two args also have an `_`</span>
<span class="gi">+    # prefix to reduce the chances of name collisions with the passed metadata, and</span>
<span class="gi">+    # since they&#39;re positional only, users will never type those underscores.</span>
<span class="w"> </span>    def process_routing(_obj, _method, /, **kwargs):
<span class="w"> </span>        &quot;&quot;&quot;Validate and route input parameters.

<span class="gu">@@ -1078,7 +1545,59 @@ if parse_version(sklearn_version.base_version) &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            metadata to corresponding methods or corresponding child objects. The object
<span class="w"> </span>            names are those defined in `obj.get_metadata_routing()`.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if not kwargs:</span>
<span class="gi">+            # If routing is not enabled and kwargs are empty, then we don&#39;t have to</span>
<span class="gi">+            # try doing any routing, we can simply return a structure which returns</span>
<span class="gi">+            # an empty dict on routed_params.ANYTHING.ANY_METHOD.</span>
<span class="gi">+            class EmptyRequest:</span>
<span class="gi">+                def get(self, name, default=None):</span>
<span class="gi">+                    return Bunch(**{method: dict() for method in METHODS})</span>
<span class="gi">+</span>
<span class="gi">+                def __getitem__(self, name):</span>
<span class="gi">+                    return Bunch(**{method: dict() for method in METHODS})</span>
<span class="gi">+</span>
<span class="gi">+                def __getattr__(self, name):</span>
<span class="gi">+                    return Bunch(**{method: dict() for method in METHODS})</span>
<span class="gi">+</span>
<span class="gi">+            return EmptyRequest()</span>
<span class="gi">+</span>
<span class="gi">+        if not (</span>
<span class="gi">+            hasattr(_obj, &quot;get_metadata_routing&quot;) or isinstance(_obj, MetadataRouter)</span>
<span class="gi">+        ):</span>
<span class="gi">+            raise AttributeError(</span>
<span class="gi">+                f&quot;The given object ({repr(_obj.__class__.__name__)}) needs to either&quot;</span>
<span class="gi">+                &quot; implement the routing method `get_metadata_routing` or be a&quot;</span>
<span class="gi">+                &quot; `MetadataRouter` instance.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        if _method not in METHODS:</span>
<span class="gi">+            raise TypeError(</span>
<span class="gi">+                f&quot;Can only route and process input on these methods: {METHODS}, &quot;</span>
<span class="gi">+                f&quot;while the passed method is: {_method}.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+        request_routing = get_routing_for_object(_obj)</span>
<span class="gi">+        request_routing.validate_metadata(params=kwargs, method=_method)</span>
<span class="gi">+        routed_params = request_routing.route_params(params=kwargs, caller=_method)</span>
<span class="gi">+</span>
<span class="gi">+        return routed_params</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.exceptions import UnsetMetadataPassedError
<span class="gd">-    from sklearn.utils._metadata_requests import COMPOSITE_METHODS, METHODS, SIMPLE_METHODS, UNCHANGED, UNUSED, WARN, MetadataRequest, MetadataRouter, MethodMapping, _MetadataRequester, _raise_for_params, _raise_for_unsupported_routing, _routing_enabled, _RoutingNotSupportedMixin, get_routing_for_object, process_routing</span>
<span class="gi">+    from sklearn.utils._metadata_requests import (  # type: ignore[no-redef]</span>
<span class="gi">+        COMPOSITE_METHODS,  # noqa</span>
<span class="gi">+        METHODS,  # noqa</span>
<span class="gi">+        SIMPLE_METHODS,  # noqa</span>
<span class="gi">+        UNCHANGED,</span>
<span class="gi">+        UNUSED,</span>
<span class="gi">+        WARN,</span>
<span class="gi">+        MetadataRequest,</span>
<span class="gi">+        MetadataRouter,</span>
<span class="gi">+        MethodMapping,</span>
<span class="gi">+        _MetadataRequester,  # noqa</span>
<span class="gi">+        _raise_for_params,  # noqa</span>
<span class="gi">+        _raise_for_unsupported_routing,  # noqa</span>
<span class="gi">+        _routing_enabled,</span>
<span class="gi">+        _RoutingNotSupportedMixin,  # noqa</span>
<span class="gi">+        get_routing_for_object,</span>
<span class="gi">+        process_routing,  # noqa</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/utils/_param_validation.py b/imblearn/utils/_param_validation.py</span>
<span class="gh">index 47542c0..3ccabf2 100644</span>
<span class="gd">--- a/imblearn/utils/_param_validation.py</span>
<span class="gi">+++ b/imblearn/utils/_param_validation.py</span>
<span class="gu">@@ -1,6 +1,7 @@</span>
<span class="w"> </span>&quot;&quot;&quot;This is a copy of sklearn/utils/_param_validation.py. It can be removed when
<span class="w"> </span>we support scikit-learn &gt;= 1.2.
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+# mypy: ignore-errors</span>
<span class="w"> </span>import functools
<span class="w"> </span>import math
<span class="w"> </span>import operator
<span class="gu">@@ -9,23 +10,27 @@ from abc import ABC, abstractmethod</span>
<span class="w"> </span>from collections.abc import Iterable
<span class="w"> </span>from inspect import signature
<span class="w"> </span>from numbers import Integral, Real
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import sklearn
<span class="w"> </span>from scipy.sparse import csr_matrix, issparse
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from .._config import config_context, get_config
<span class="w"> </span>from ..utils.fixes import _is_arraylike_not_scalar
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gd">-if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="gi">+if sklearn_version &lt; parse_version(&quot;1.4&quot;):</span>

<span class="w"> </span>    class InvalidParameterError(ValueError, TypeError):
<span class="w"> </span>        &quot;&quot;&quot;Custom exception to be raised when the parameter of a class/method/function
<span class="w"> </span>        does not have a valid type or value.
<span class="w"> </span>        &quot;&quot;&quot;

<span class="gd">-    def validate_parameter_constraints(parameter_constraints, params,</span>
<span class="gd">-        caller_name):</span>
<span class="gi">+        # Inherits from ValueError and TypeError to keep backward compatibility.</span>
<span class="gi">+</span>
<span class="gi">+    def validate_parameter_constraints(parameter_constraints, params, caller_name):</span>
<span class="w"> </span>        &quot;&quot;&quot;Validate types and values of given parameters.

<span class="w"> </span>        Parameters
<span class="gu">@@ -61,7 +66,46 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        caller_name : str
<span class="w"> </span>            The name of the estimator or function or method that called this function.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        for param_name, param_val in params.items():</span>
<span class="gi">+            # We allow parameters to not have a constraint so that third party</span>
<span class="gi">+            # estimators can inherit from sklearn estimators without having to</span>
<span class="gi">+            # necessarily use the validation tools.</span>
<span class="gi">+            if param_name not in parameter_constraints:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            constraints = parameter_constraints[param_name]</span>
<span class="gi">+</span>
<span class="gi">+            if constraints == &quot;no_validation&quot;:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            constraints = [make_constraint(constraint) for constraint in constraints]</span>
<span class="gi">+</span>
<span class="gi">+            for constraint in constraints:</span>
<span class="gi">+                if constraint.is_satisfied_by(param_val):</span>
<span class="gi">+                    # this constraint is satisfied, no need to check further.</span>
<span class="gi">+                    break</span>
<span class="gi">+            else:</span>
<span class="gi">+                # No constraint is satisfied, raise with an informative message.</span>
<span class="gi">+</span>
<span class="gi">+                # Ignore constraints that we don&#39;t want to expose in the error</span>
<span class="gi">+                # message, i.e. options that are for internal purpose or not</span>
<span class="gi">+                # officially supported.</span>
<span class="gi">+                constraints = [</span>
<span class="gi">+                    constraint for constraint in constraints if not constraint.hidden</span>
<span class="gi">+                ]</span>
<span class="gi">+</span>
<span class="gi">+                if len(constraints) == 1:</span>
<span class="gi">+                    constraints_str = f&quot;{constraints[0]}&quot;</span>
<span class="gi">+                else:</span>
<span class="gi">+                    constraints_str = (</span>
<span class="gi">+                        f&quot;{&#39;, &#39;.join([str(c) for c in constraints[:-1]])} or&quot;</span>
<span class="gi">+                        f&quot; {constraints[-1]}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+</span>
<span class="gi">+                raise InvalidParameterError(</span>
<span class="gi">+                    f&quot;The {param_name!r} parameter of {caller_name} must be&quot;</span>
<span class="gi">+                    f&quot; {constraints_str}. Got {param_val!r} instead.&quot;</span>
<span class="gi">+                )</span>

<span class="w"> </span>    def make_constraint(constraint):
<span class="w"> </span>        &quot;&quot;&quot;Convert the constraint into the appropriate Constraint object.
<span class="gu">@@ -76,10 +120,37 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        constraint : instance of _Constraint
<span class="w"> </span>            The converted constraint.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gd">-    def validate_params(parameter_constraints, *, prefer_skip_nested_validation</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;array-like&quot;:</span>
<span class="gi">+            return _ArrayLikes()</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;sparse matrix&quot;:</span>
<span class="gi">+            return _SparseMatrices()</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;random_state&quot;:</span>
<span class="gi">+            return _RandomStates()</span>
<span class="gi">+        if constraint is callable:</span>
<span class="gi">+            return _Callables()</span>
<span class="gi">+        if constraint is None:</span>
<span class="gi">+            return _NoneConstraint()</span>
<span class="gi">+        if isinstance(constraint, type):</span>
<span class="gi">+            return _InstancesOf(constraint)</span>
<span class="gi">+        if isinstance(</span>
<span class="gi">+            constraint, (Interval, StrOptions, Options, HasMethods, MissingValues)</span>
<span class="w"> </span>        ):
<span class="gi">+            return constraint</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;boolean&quot;:</span>
<span class="gi">+            return _Booleans()</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;verbose&quot;:</span>
<span class="gi">+            return _VerboseHelper()</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;cv_object&quot;:</span>
<span class="gi">+            return _CVObjects()</span>
<span class="gi">+        if isinstance(constraint, Hidden):</span>
<span class="gi">+            constraint = make_constraint(constraint.constraint)</span>
<span class="gi">+            constraint.hidden = True</span>
<span class="gi">+            return constraint</span>
<span class="gi">+        if isinstance(constraint, str) and constraint == &quot;nan&quot;:</span>
<span class="gi">+            return _NanConstraint()</span>
<span class="gi">+        raise ValueError(f&quot;Unknown constraint type: {constraint}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    def validate_params(parameter_constraints, *, prefer_skip_nested_validation):</span>
<span class="w"> </span>        &quot;&quot;&quot;Decorator to validate types and values of functions and methods.

<span class="w"> </span>        Parameters
<span class="gu">@@ -110,8 +181,62 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        decorated_function : function or method
<span class="w"> </span>            The decorated function.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="gi">+        def decorator(func):</span>
<span class="gi">+            # The dict of parameter constraints is set as an attribute of the function</span>
<span class="gi">+            # to make it possible to dynamically introspect the constraints for</span>
<span class="gi">+            # automatic testing.</span>
<span class="gi">+            setattr(func, &quot;_skl_parameter_constraints&quot;, parameter_constraints)</span>
<span class="gi">+</span>
<span class="gi">+            @functools.wraps(func)</span>
<span class="gi">+            def wrapper(*args, **kwargs):</span>
<span class="gi">+                global_skip_validation = get_config()[&quot;skip_parameter_validation&quot;]</span>
<span class="gi">+                if global_skip_validation:</span>
<span class="gi">+                    return func(*args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+                func_sig = signature(func)</span>
<span class="gi">+</span>
<span class="gi">+                # Map *args/**kwargs to the function signature</span>
<span class="gi">+                params = func_sig.bind(*args, **kwargs)</span>
<span class="gi">+                params.apply_defaults()</span>
<span class="gi">+</span>
<span class="gi">+                # ignore self/cls and positional/keyword markers</span>
<span class="gi">+                to_ignore = [</span>
<span class="gi">+                    p.name</span>
<span class="gi">+                    for p in func_sig.parameters.values()</span>
<span class="gi">+                    if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)</span>
<span class="gi">+                ]</span>
<span class="gi">+                to_ignore += [&quot;self&quot;, &quot;cls&quot;]</span>
<span class="gi">+                params = {</span>
<span class="gi">+                    k: v for k, v in params.arguments.items() if k not in to_ignore</span>
<span class="gi">+                }</span>
<span class="gi">+</span>
<span class="gi">+                validate_parameter_constraints(</span>
<span class="gi">+                    parameter_constraints, params, caller_name=func.__qualname__</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                try:</span>
<span class="gi">+                    with config_context(</span>
<span class="gi">+                        skip_parameter_validation=(</span>
<span class="gi">+                            prefer_skip_nested_validation or global_skip_validation</span>
<span class="gi">+                        )</span>
<span class="gi">+                    ):</span>
<span class="gi">+                        return func(*args, **kwargs)</span>
<span class="gi">+                except InvalidParameterError as e:</span>
<span class="gi">+                    # When the function is just a wrapper around an estimator, we allow</span>
<span class="gi">+                    # the function to delegate validation to the estimator, but we</span>
<span class="gi">+                    # replace the name of the estimator by the name of the function in</span>
<span class="gi">+                    # the error message to avoid confusion.</span>
<span class="gi">+                    msg = re.sub(</span>
<span class="gi">+                        r&quot;parameter of \w+ must be&quot;,</span>
<span class="gi">+                        f&quot;parameter of {func.__qualname__} must be&quot;,</span>
<span class="gi">+                        str(e),</span>
<span class="gi">+                    )</span>
<span class="gi">+                    raise InvalidParameterError(msg) from e</span>
<span class="gi">+</span>
<span class="gi">+            return wrapper</span>
<span class="gi">+</span>
<span class="gi">+        return decorator</span>

<span class="w"> </span>    class RealNotInt(Real):
<span class="w"> </span>        &quot;&quot;&quot;A type that represents reals that are not instances of int.
<span class="gu">@@ -120,12 +245,20 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        isintance(1, RealNotInt) -&gt; False
<span class="w"> </span>        isinstance(1.0, RealNotInt) -&gt; True
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gi">+</span>
<span class="w"> </span>    RealNotInt.register(float)

<span class="w"> </span>    def _type_name(t):
<span class="w"> </span>        &quot;&quot;&quot;Convert type into human readable string.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-</span>
<span class="gi">+        module = t.__module__</span>
<span class="gi">+        qualname = t.__qualname__</span>
<span class="gi">+        if module == &quot;builtins&quot;:</span>
<span class="gi">+            return qualname</span>
<span class="gi">+        elif t == Real:</span>
<span class="gi">+            return &quot;float&quot;</span>
<span class="gi">+        elif t == Integral:</span>
<span class="gi">+            return &quot;int&quot;</span>
<span class="gi">+        return f&quot;{module}.{qualname}&quot;</span>

<span class="w"> </span>    class _Constraint(ABC):
<span class="w"> </span>        &quot;&quot;&quot;Base class for the constraint objects.&quot;&quot;&quot;
<span class="gu">@@ -147,13 +280,11 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            is_satisfied : bool
<span class="w"> </span>                Whether or not the constraint is satisfied by this value.
<span class="w"> </span>            &quot;&quot;&quot;
<span class="gd">-            pass</span>

<span class="w"> </span>        @abstractmethod
<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            &quot;&quot;&quot;A human readable representational string of the constraint.&quot;&quot;&quot;

<span class="gd">-</span>
<span class="w"> </span>    class _InstancesOf(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing instances of a given type.

<span class="gu">@@ -167,30 +298,47 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            super().__init__()
<span class="w"> </span>            self.type = type

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return f&#39;an instance of {_type_name(self.type)!r}&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return isinstance(val, self.type)</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return f&quot;an instance of {_type_name(self.type)!r}&quot;</span>

<span class="w"> </span>    class _NoneConstraint(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing the None singleton.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;None&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return val is None</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;None&quot;</span>

<span class="w"> </span>    class _NanConstraint(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing the indicator `np.nan`.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;numpy.nan&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return (</span>
<span class="gi">+                not isinstance(val, Integral)</span>
<span class="gi">+                and isinstance(val, Real)</span>
<span class="gi">+                and math.isnan(val)</span>
<span class="gi">+            )</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;numpy.nan&quot;</span>

<span class="w"> </span>    class _PandasNAConstraint(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing the indicator `pd.NA`.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;pandas.NA&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            try:</span>
<span class="gi">+                import pandas as pd</span>

<span class="gi">+                return isinstance(val, type(pd.NA)) and pd.isna(val)</span>
<span class="gi">+            except ImportError:</span>
<span class="gi">+                return False</span>
<span class="gi">+</span>
<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;pandas.NA&quot;</span>

<span class="w"> </span>    class Options(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing a finite set of instances of a given type.
<span class="gu">@@ -212,20 +360,27 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self.type = type
<span class="w"> </span>            self.options = options
<span class="w"> </span>            self.deprecated = deprecated or set()
<span class="gi">+</span>
<span class="w"> </span>            if self.deprecated - self.options:
<span class="w"> </span>                raise ValueError(
<span class="gd">-                    &#39;The deprecated options must be a subset of the options.&#39;)</span>
<span class="gi">+                    &quot;The deprecated options must be a subset of the options.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return isinstance(val, self.type) and val in self.options</span>

<span class="w"> </span>        def _mark_if_deprecated(self, option):
<span class="w"> </span>            &quot;&quot;&quot;Add a deprecated mark to an option if needed.&quot;&quot;&quot;
<span class="gd">-            pass</span>
<span class="gi">+            option_str = f&quot;{option!r}&quot;</span>
<span class="gi">+            if option in self.deprecated:</span>
<span class="gi">+                option_str = f&quot;{option_str} (deprecated)&quot;</span>
<span class="gi">+            return option_str</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            options_str = (
<span class="w"> </span>                f&quot;{&#39;, &#39;.join([self._mark_if_deprecated(o) for o in self.options])}&quot;
<span class="gd">-                )</span>
<span class="gd">-            return f&#39;a {_type_name(self.type)} among {{{options_str}}}&#39;</span>
<span class="gd">-</span>
<span class="gi">+            )</span>
<span class="gi">+            return f&quot;a {_type_name(self.type)} among {{{options_str}}}&quot;</span>

<span class="w"> </span>    class StrOptions(Options):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing a finite set of strings.
<span class="gu">@@ -243,7 +398,6 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        def __init__(self, options, *, deprecated=None):
<span class="w"> </span>            super().__init__(type=str, options=options, deprecated=deprecated)

<span class="gd">-</span>
<span class="w"> </span>    class Interval(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing a typed interval.

<span class="gu">@@ -286,58 +440,118 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            self.left = left
<span class="w"> </span>            self.right = right
<span class="w"> </span>            self.closed = closed
<span class="gi">+</span>
<span class="w"> </span>            self._check_params()

<span class="gi">+        def _check_params(self):</span>
<span class="gi">+            if self.type not in (Integral, Real, RealNotInt):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;type must be either numbers.Integral, numbers.Real or RealNotInt.&quot;</span>
<span class="gi">+                    f&quot; Got {self.type} instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if self.closed not in (&quot;left&quot;, &quot;right&quot;, &quot;both&quot;, &quot;neither&quot;):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    &quot;closed must be either &#39;left&#39;, &#39;right&#39;, &#39;both&#39; or &#39;neither&#39;. &quot;</span>
<span class="gi">+                    f&quot;Got {self.closed} instead.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+            if self.type is Integral:</span>
<span class="gi">+                suffix = &quot;for an interval over the integers.&quot;</span>
<span class="gi">+                if self.left is not None and not isinstance(self.left, Integral):</span>
<span class="gi">+                    raise TypeError(f&quot;Expecting left to be an int {suffix}&quot;)</span>
<span class="gi">+                if self.right is not None and not isinstance(self.right, Integral):</span>
<span class="gi">+                    raise TypeError(f&quot;Expecting right to be an int {suffix}&quot;)</span>
<span class="gi">+                if self.left is None and self.closed in (&quot;left&quot;, &quot;both&quot;):</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;left can&#39;t be None when closed == {self.closed} {suffix}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+                if self.right is None and self.closed in (&quot;right&quot;, &quot;both&quot;):</span>
<span class="gi">+                    raise ValueError(</span>
<span class="gi">+                        f&quot;right can&#39;t be None when closed == {self.closed} {suffix}&quot;</span>
<span class="gi">+                    )</span>
<span class="gi">+            else:</span>
<span class="gi">+                if self.left is not None and not isinstance(self.left, Real):</span>
<span class="gi">+                    raise TypeError(&quot;Expecting left to be a real number.&quot;)</span>
<span class="gi">+                if self.right is not None and not isinstance(self.right, Real):</span>
<span class="gi">+                    raise TypeError(&quot;Expecting right to be a real number.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+            if (</span>
<span class="gi">+                self.right is not None</span>
<span class="gi">+                and self.left is not None</span>
<span class="gi">+                and self.right &lt;= self.left</span>
<span class="gi">+            ):</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;right can&#39;t be less than left. Got left={self.left} and &quot;</span>
<span class="gi">+                    f&quot;right={self.right}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="w"> </span>        def __contains__(self, val):
<span class="w"> </span>            if not isinstance(val, Integral) and np.isnan(val):
<span class="w"> </span>                return False
<span class="gd">-            left_cmp = operator.lt if self.closed in (&#39;left&#39;, &#39;both&#39;</span>
<span class="gd">-                ) else operator.le</span>
<span class="gd">-            right_cmp = operator.gt if self.closed in (&#39;right&#39;, &#39;both&#39;</span>
<span class="gd">-                ) else operator.ge</span>
<span class="gi">+</span>
<span class="gi">+            left_cmp = operator.lt if self.closed in (&quot;left&quot;, &quot;both&quot;) else operator.le</span>
<span class="gi">+            right_cmp = operator.gt if self.closed in (&quot;right&quot;, &quot;both&quot;) else operator.ge</span>
<span class="gi">+</span>
<span class="w"> </span>            left = -np.inf if self.left is None else self.left
<span class="w"> </span>            right = np.inf if self.right is None else self.right
<span class="gi">+</span>
<span class="w"> </span>            if left_cmp(val, left):
<span class="w"> </span>                return False
<span class="w"> </span>            if right_cmp(val, right):
<span class="w"> </span>                return False
<span class="w"> </span>            return True

<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            if not isinstance(val, self.type):</span>
<span class="gi">+                return False</span>
<span class="gi">+</span>
<span class="gi">+            return val in self</span>
<span class="gi">+</span>
<span class="w"> </span>        def __str__(self):
<span class="gd">-            type_str = &#39;an int&#39; if self.type is Integral else &#39;a float&#39;</span>
<span class="gd">-            left_bracket = &#39;[&#39; if self.closed in (&#39;left&#39;, &#39;both&#39;) else &#39;(&#39;</span>
<span class="gd">-            left_bound = &#39;-inf&#39; if self.left is None else self.left</span>
<span class="gd">-            right_bound = &#39;inf&#39; if self.right is None else self.right</span>
<span class="gd">-            right_bracket = &#39;]&#39; if self.closed in (&#39;right&#39;, &#39;both&#39;) else &#39;)&#39;</span>
<span class="gi">+            type_str = &quot;an int&quot; if self.type is Integral else &quot;a float&quot;</span>
<span class="gi">+            left_bracket = &quot;[&quot; if self.closed in (&quot;left&quot;, &quot;both&quot;) else &quot;(&quot;</span>
<span class="gi">+            left_bound = &quot;-inf&quot; if self.left is None else self.left</span>
<span class="gi">+            right_bound = &quot;inf&quot; if self.right is None else self.right</span>
<span class="gi">+            right_bracket = &quot;]&quot; if self.closed in (&quot;right&quot;, &quot;both&quot;) else &quot;)&quot;</span>
<span class="gi">+</span>
<span class="gi">+            # better repr if the bounds were given as integers</span>
<span class="w"> </span>            if not self.type == Integral and isinstance(self.left, Real):
<span class="w"> </span>                left_bound = float(left_bound)
<span class="w"> </span>            if not self.type == Integral and isinstance(self.right, Real):
<span class="w"> </span>                right_bound = float(right_bound)
<span class="gd">-            return (</span>
<span class="gd">-                f&#39;{type_str} in the range {left_bracket}{left_bound}, {right_bound}{right_bracket}&#39;</span>
<span class="gd">-                )</span>

<span class="gi">+            return (</span>
<span class="gi">+                f&quot;{type_str} in the range &quot;</span>
<span class="gi">+                f&quot;{left_bracket}{left_bound}, {right_bound}{right_bracket}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class _ArrayLikes(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing array-likes&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;an array-like&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return _is_arraylike_not_scalar(val)</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;an array-like&quot;</span>

<span class="w"> </span>    class _SparseMatrices(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing sparse matrices.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;a sparse matrix&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return issparse(val)</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;a sparse matrix&quot;</span>

<span class="w"> </span>    class _Callables(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing callables.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;a callable&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return callable(val)</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;a callable&quot;</span>

<span class="w"> </span>    class _RandomStates(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing random states.
<span class="gu">@@ -348,15 +562,20 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        def __init__(self):
<span class="w"> </span>            super().__init__()
<span class="gd">-            self._constraints = [Interval(Integral, 0, 2 ** 32 - 1, closed=</span>
<span class="gd">-                &#39;both&#39;), _InstancesOf(np.random.RandomState), _NoneConstraint()</span>
<span class="gd">-                ]</span>
<span class="gi">+            self._constraints = [</span>
<span class="gi">+                Interval(Integral, 0, 2**32 - 1, closed=&quot;both&quot;),</span>
<span class="gi">+                _InstancesOf(np.random.RandomState),</span>
<span class="gi">+                _NoneConstraint(),</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return any(c.is_satisfied_by(val) for c in self._constraints)</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return (
<span class="gd">-                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}&quot;</span>
<span class="gd">-                )</span>
<span class="gd">-</span>
<span class="gi">+                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or&quot;</span>
<span class="gi">+                f&quot; {self._constraints[-1]}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class _Booleans(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing boolean likes.
<span class="gu">@@ -367,13 +586,19 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        def __init__(self):
<span class="w"> </span>            super().__init__()
<span class="gd">-            self._constraints = [_InstancesOf(bool), _InstancesOf(np.bool_)]</span>
<span class="gi">+            self._constraints = [</span>
<span class="gi">+                _InstancesOf(bool),</span>
<span class="gi">+                _InstancesOf(np.bool_),</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return any(c.is_satisfied_by(val) for c in self._constraints)</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return (
<span class="gd">-                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}&quot;</span>
<span class="gd">-                )</span>
<span class="gd">-</span>
<span class="gi">+                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or&quot;</span>
<span class="gi">+                f&quot; {self._constraints[-1]}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class _VerboseHelper(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Helper constraint for the verbose parameter.
<span class="gu">@@ -384,14 +609,20 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        def __init__(self):
<span class="w"> </span>            super().__init__()
<span class="gd">-            self._constraints = [Interval(Integral, 0, None, closed=&#39;left&#39;),</span>
<span class="gd">-                _InstancesOf(bool), _InstancesOf(np.bool_)]</span>
<span class="gi">+            self._constraints = [</span>
<span class="gi">+                Interval(Integral, 0, None, closed=&quot;left&quot;),</span>
<span class="gi">+                _InstancesOf(bool),</span>
<span class="gi">+                _InstancesOf(np.bool_),</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return any(c.is_satisfied_by(val) for c in self._constraints)</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return (
<span class="gd">-                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}&quot;</span>
<span class="gd">-                )</span>
<span class="gd">-</span>
<span class="gi">+                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or&quot;</span>
<span class="gi">+                f&quot; {self._constraints[-1]}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class MissingValues(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Helper constraint for the `missing_values` parameters.
<span class="gu">@@ -415,19 +646,28 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        def __init__(self, numeric_only=False):
<span class="w"> </span>            super().__init__()
<span class="gi">+</span>
<span class="w"> </span>            self.numeric_only = numeric_only
<span class="gd">-            self._constraints = [_InstancesOf(Integral), Interval(Real,</span>
<span class="gd">-                None, None, closed=&#39;both&#39;), _NanConstraint(),</span>
<span class="gd">-                _PandasNAConstraint()]</span>
<span class="gi">+</span>
<span class="gi">+            self._constraints = [</span>
<span class="gi">+                _InstancesOf(Integral),</span>
<span class="gi">+                # we use an interval of Real to ignore np.nan that has its own</span>
<span class="gi">+                # constraint</span>
<span class="gi">+                Interval(Real, None, None, closed=&quot;both&quot;),</span>
<span class="gi">+                _NanConstraint(),</span>
<span class="gi">+                _PandasNAConstraint(),</span>
<span class="gi">+            ]</span>
<span class="w"> </span>            if not self.numeric_only:
<span class="gd">-                self._constraints.extend([_InstancesOf(str), _NoneConstraint()]</span>
<span class="gd">-                    )</span>
<span class="gi">+                self._constraints.extend([_InstancesOf(str), _NoneConstraint()])</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return any(c.is_satisfied_by(val) for c in self._constraints)</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return (
<span class="gd">-                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}&quot;</span>
<span class="gd">-                )</span>
<span class="gd">-</span>
<span class="gi">+                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or&quot;</span>
<span class="gi">+                f&quot; {self._constraints[-1]}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class HasMethods(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing objects that expose specific methods.
<span class="gu">@@ -441,30 +681,37 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>            The method(s) that the object is expected to expose.
<span class="w"> </span>        &quot;&quot;&quot;

<span class="gd">-        @validate_params({&#39;methods&#39;: [str, list]},</span>
<span class="gd">-            prefer_skip_nested_validation=True)</span>
<span class="gi">+        @validate_params(</span>
<span class="gi">+            {&quot;methods&quot;: [str, list]},</span>
<span class="gi">+            prefer_skip_nested_validation=True,</span>
<span class="gi">+        )</span>
<span class="w"> </span>        def __init__(self, methods):
<span class="w"> </span>            super().__init__()
<span class="w"> </span>            if isinstance(methods, str):
<span class="w"> </span>                methods = [methods]
<span class="w"> </span>            self.methods = methods

<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return all(callable(getattr(val, method, None)) for method in self.methods)</span>
<span class="gi">+</span>
<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            if len(self.methods) == 1:
<span class="gd">-                methods = f&#39;{self.methods[0]!r}&#39;</span>
<span class="gi">+                methods = f&quot;{self.methods[0]!r}&quot;</span>
<span class="w"> </span>            else:
<span class="w"> </span>                methods = (
<span class="gd">-                    f&quot;{&#39;, &#39;.join([repr(m) for m in self.methods[:-1]])} and {self.methods[-1]!r}&quot;</span>
<span class="gd">-                    )</span>
<span class="gd">-            return f&#39;an object implementing {methods}&#39;</span>
<span class="gd">-</span>
<span class="gi">+                    f&quot;{&#39;, &#39;.join([repr(m) for m in self.methods[:-1]])} and&quot;</span>
<span class="gi">+                    f&quot; {self.methods[-1]!r}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            return f&quot;an object implementing {methods}&quot;</span>

<span class="w"> </span>    class _IterablesNotString(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing iterables that are not strings.&quot;&quot;&quot;

<span class="gd">-        def __str__(self):</span>
<span class="gd">-            return &#39;an iterable&#39;</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return isinstance(val, Iterable) and not isinstance(val, str)</span>

<span class="gi">+        def __str__(self):</span>
<span class="gi">+            return &quot;an iterable&quot;</span>

<span class="w"> </span>    class _CVObjects(_Constraint):
<span class="w"> </span>        &quot;&quot;&quot;Constraint representing cv objects.
<span class="gu">@@ -480,15 +727,21 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>

<span class="w"> </span>        def __init__(self):
<span class="w"> </span>            super().__init__()
<span class="gd">-            self._constraints = [Interval(Integral, 2, None, closed=&#39;left&#39;),</span>
<span class="gd">-                HasMethods([&#39;split&#39;, &#39;get_n_splits&#39;]), _IterablesNotString(</span>
<span class="gd">-                ), _NoneConstraint()]</span>
<span class="gi">+            self._constraints = [</span>
<span class="gi">+                Interval(Integral, 2, None, closed=&quot;left&quot;),</span>
<span class="gi">+                HasMethods([&quot;split&quot;, &quot;get_n_splits&quot;]),</span>
<span class="gi">+                _IterablesNotString(),</span>
<span class="gi">+                _NoneConstraint(),</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+        def is_satisfied_by(self, val):</span>
<span class="gi">+            return any(c.is_satisfied_by(val) for c in self._constraints)</span>

<span class="w"> </span>        def __str__(self):
<span class="w"> </span>            return (
<span class="gd">-                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or {self._constraints[-1]}&quot;</span>
<span class="gd">-                )</span>
<span class="gd">-</span>
<span class="gi">+                f&quot;{&#39;, &#39;.join([str(c) for c in self._constraints[:-1]])} or&quot;</span>
<span class="gi">+                f&quot; {self._constraints[-1]}&quot;</span>
<span class="gi">+            )</span>

<span class="w"> </span>    class Hidden:
<span class="w"> </span>        &quot;&quot;&quot;Class encapsulating a constraint not meant to be exposed to the user.
<span class="gu">@@ -520,7 +773,49 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        val : object
<span class="w"> </span>            A value that does not satisfy the constraint.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(constraint, StrOptions):</span>
<span class="gi">+            return f&quot;not {&#39; or &#39;.join(constraint.options)}&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, MissingValues):</span>
<span class="gi">+            return np.array([1, 2, 3])</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _VerboseHelper):</span>
<span class="gi">+            return -1</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, HasMethods):</span>
<span class="gi">+            return type(&quot;HasNotMethods&quot;, (), {})()</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _IterablesNotString):</span>
<span class="gi">+            return &quot;a string&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _CVObjects):</span>
<span class="gi">+            return &quot;not a cv object&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, Interval) and constraint.type is Integral:</span>
<span class="gi">+            if constraint.left is not None:</span>
<span class="gi">+                return constraint.left - 1</span>
<span class="gi">+            if constraint.right is not None:</span>
<span class="gi">+                return constraint.right + 1</span>
<span class="gi">+</span>
<span class="gi">+            # There&#39;s no integer outside (-inf, +inf)</span>
<span class="gi">+            raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, Interval) and constraint.type in (Real, RealNotInt):</span>
<span class="gi">+            if constraint.left is not None:</span>
<span class="gi">+                return constraint.left - 1e-6</span>
<span class="gi">+            if constraint.right is not None:</span>
<span class="gi">+                return constraint.right + 1e-6</span>
<span class="gi">+</span>
<span class="gi">+            # bounds are -inf, +inf</span>
<span class="gi">+            if constraint.closed in (&quot;right&quot;, &quot;neither&quot;):</span>
<span class="gi">+                return -np.inf</span>
<span class="gi">+            if constraint.closed in (&quot;left&quot;, &quot;neither&quot;):</span>
<span class="gi">+                return np.inf</span>
<span class="gi">+</span>
<span class="gi">+            # interval is [-inf, +inf]</span>
<span class="gi">+            return np.nan</span>
<span class="gi">+</span>
<span class="gi">+        raise NotImplementedError</span>

<span class="w"> </span>    def generate_valid_param(constraint):
<span class="w"> </span>        &quot;&quot;&quot;Return a value that does satisfy a constraint.
<span class="gu">@@ -537,9 +832,103 @@ if sklearn_version &lt; parse_version(&#39;1.4&#39;):</span>
<span class="w"> </span>        val : object
<span class="w"> </span>            A value that does satisfy the constraint.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if isinstance(constraint, _ArrayLikes):</span>
<span class="gi">+            return np.array([1, 2, 3])</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _SparseMatrices):</span>
<span class="gi">+            return csr_matrix([[0, 1], [1, 0]])</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _RandomStates):</span>
<span class="gi">+            return np.random.RandomState(42)</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _Callables):</span>
<span class="gi">+            return lambda x: x</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _NoneConstraint):</span>
<span class="gi">+            return None</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _InstancesOf):</span>
<span class="gi">+            if constraint.type is np.ndarray:</span>
<span class="gi">+                # special case for ndarray since it can&#39;t be instantiated without</span>
<span class="gi">+                # arguments</span>
<span class="gi">+                return np.array([1, 2, 3])</span>
<span class="gi">+</span>
<span class="gi">+            if constraint.type in (Integral, Real):</span>
<span class="gi">+                # special case for Integral and Real since they are abstract classes</span>
<span class="gi">+                return 1</span>
<span class="gi">+</span>
<span class="gi">+            return constraint.type()</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _Booleans):</span>
<span class="gi">+            return True</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _VerboseHelper):</span>
<span class="gi">+            return 1</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, MissingValues) and constraint.numeric_only:</span>
<span class="gi">+            return np.nan</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, MissingValues) and not constraint.numeric_only:</span>
<span class="gi">+            return &quot;missing&quot;</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, HasMethods):</span>
<span class="gi">+            return type(</span>
<span class="gi">+                &quot;ValidHasMethods&quot;,</span>
<span class="gi">+                (),</span>
<span class="gi">+                {m: lambda self: None for m in constraint.methods},</span>
<span class="gi">+            )()</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _IterablesNotString):</span>
<span class="gi">+            return [1, 2, 3]</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, _CVObjects):</span>
<span class="gi">+            return 5</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, Options):  # includes StrOptions</span>
<span class="gi">+            for option in constraint.options:</span>
<span class="gi">+                return option</span>
<span class="gi">+</span>
<span class="gi">+        if isinstance(constraint, Interval):</span>
<span class="gi">+            interval = constraint</span>
<span class="gi">+            if interval.left is None and interval.right is None:</span>
<span class="gi">+                return 0</span>
<span class="gi">+            elif interval.left is None:</span>
<span class="gi">+                return interval.right - 1</span>
<span class="gi">+            elif interval.right is None:</span>
<span class="gi">+                return interval.left + 1</span>
<span class="gi">+            else:</span>
<span class="gi">+                if interval.type is Real:</span>
<span class="gi">+                    return (interval.left + interval.right) / 2</span>
<span class="gi">+                else:</span>
<span class="gi">+                    return interval.left + 1</span>
<span class="gi">+</span>
<span class="gi">+        raise ValueError(f&quot;Unknown constraint type: {constraint}&quot;)</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="gd">-    from sklearn.utils._param_validation import generate_invalid_param_val</span>
<span class="gd">-    from sklearn.utils._param_validation import generate_valid_param</span>
<span class="gd">-    from sklearn.utils._param_validation import validate_parameter_constraints</span>
<span class="gd">-    from sklearn.utils._param_validation import HasMethods, Hidden, Interval, InvalidParameterError, MissingValues, Options, RealNotInt, StrOptions, _ArrayLikes, _Booleans, _Callables, _CVObjects, _InstancesOf, _IterablesNotString, _NanConstraint, _NoneConstraint, _PandasNAConstraint, _RandomStates, _SparseMatrices, _VerboseHelper, make_constraint, validate_params</span>
<span class="gi">+    from sklearn.utils._param_validation import generate_invalid_param_val  # noqa</span>
<span class="gi">+    from sklearn.utils._param_validation import generate_valid_param  # noqa</span>
<span class="gi">+    from sklearn.utils._param_validation import validate_parameter_constraints  # noqa</span>
<span class="gi">+    from sklearn.utils._param_validation import (</span>
<span class="gi">+        HasMethods,</span>
<span class="gi">+        Hidden,</span>
<span class="gi">+        Interval,</span>
<span class="gi">+        InvalidParameterError,</span>
<span class="gi">+        MissingValues,</span>
<span class="gi">+        Options,</span>
<span class="gi">+        RealNotInt,</span>
<span class="gi">+        StrOptions,</span>
<span class="gi">+        _ArrayLikes,</span>
<span class="gi">+        _Booleans,</span>
<span class="gi">+        _Callables,</span>
<span class="gi">+        _CVObjects,</span>
<span class="gi">+        _InstancesOf,</span>
<span class="gi">+        _IterablesNotString,</span>
<span class="gi">+        _NanConstraint,</span>
<span class="gi">+        _NoneConstraint,</span>
<span class="gi">+        _PandasNAConstraint,</span>
<span class="gi">+        _RandomStates,</span>
<span class="gi">+        _SparseMatrices,</span>
<span class="gi">+        _VerboseHelper,</span>
<span class="gi">+        make_constraint,</span>
<span class="gi">+        validate_params,</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/utils/_show_versions.py b/imblearn/utils/_show_versions.py</span>
<span class="gh">index e6bd42b..4912e3a 100644</span>
<span class="gd">--- a/imblearn/utils/_show_versions.py</span>
<span class="gi">+++ b/imblearn/utils/_show_versions.py</span>
<span class="gu">@@ -4,6 +4,10 @@ and filing issues on GitHub.</span>
<span class="w"> </span>Adapted from :func:`sklearn.show_versions`,
<span class="w"> </span>which was adapted from :func:`pandas.show_versions`
<span class="w"> </span>&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Author: Alexander L. Hayes &lt;hayesall@iu.edu&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from .. import __version__


<span class="gu">@@ -14,7 +18,32 @@ def _get_deps_info():</span>
<span class="w"> </span>    deps_info: dict
<span class="w"> </span>        version information on relevant Python libraries
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    deps = [</span>
<span class="gi">+        &quot;imbalanced-learn&quot;,</span>
<span class="gi">+        &quot;pip&quot;,</span>
<span class="gi">+        &quot;setuptools&quot;,</span>
<span class="gi">+        &quot;numpy&quot;,</span>
<span class="gi">+        &quot;scipy&quot;,</span>
<span class="gi">+        &quot;scikit-learn&quot;,</span>
<span class="gi">+        &quot;Cython&quot;,</span>
<span class="gi">+        &quot;pandas&quot;,</span>
<span class="gi">+        &quot;keras&quot;,</span>
<span class="gi">+        &quot;tensorflow&quot;,</span>
<span class="gi">+        &quot;joblib&quot;,</span>
<span class="gi">+    ]</span>
<span class="gi">+</span>
<span class="gi">+    deps_info = {</span>
<span class="gi">+        &quot;imbalanced-learn&quot;: __version__,</span>
<span class="gi">+    }</span>
<span class="gi">+</span>
<span class="gi">+    from importlib.metadata import PackageNotFoundError, version</span>
<span class="gi">+</span>
<span class="gi">+    for modname in deps:</span>
<span class="gi">+        try:</span>
<span class="gi">+            deps_info[modname] = version(modname)</span>
<span class="gi">+        except PackageNotFoundError:</span>
<span class="gi">+            deps_info[modname] = None</span>
<span class="gi">+    return deps_info</span>


<span class="w"> </span>def show_versions(github=False):
<span class="gu">@@ -27,4 +56,37 @@ def show_versions(github=False):</span>
<span class="w"> </span>    github : bool,
<span class="w"> </span>        If true, wrap system info with GitHub markup.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    from sklearn.utils._show_versions import _get_sys_info</span>
<span class="gi">+</span>
<span class="gi">+    _sys_info = _get_sys_info()</span>
<span class="gi">+    _deps_info = _get_deps_info()</span>
<span class="gi">+    _github_markup = (</span>
<span class="gi">+        &quot;&lt;details&gt;&quot;</span>
<span class="gi">+        &quot;&lt;summary&gt;System, Dependency Information&lt;/summary&gt;\n\n&quot;</span>
<span class="gi">+        &quot;**System Information**\n\n&quot;</span>
<span class="gi">+        &quot;{0}\n&quot;</span>
<span class="gi">+        &quot;**Python Dependencies**\n\n&quot;</span>
<span class="gi">+        &quot;{1}\n&quot;</span>
<span class="gi">+        &quot;&lt;/details&gt;&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    if github:</span>
<span class="gi">+        _sys_markup = &quot;&quot;</span>
<span class="gi">+        _deps_markup = &quot;&quot;</span>
<span class="gi">+</span>
<span class="gi">+        for k, stat in _sys_info.items():</span>
<span class="gi">+            _sys_markup += f&quot;* {k:&lt;10}: `{stat}`\n&quot;</span>
<span class="gi">+        for k, stat in _deps_info.items():</span>
<span class="gi">+            _deps_markup += f&quot;* {k:&lt;10}: `{stat}`\n&quot;</span>
<span class="gi">+</span>
<span class="gi">+        print(_github_markup.format(_sys_markup, _deps_markup))</span>
<span class="gi">+</span>
<span class="gi">+    else:</span>
<span class="gi">+        print(&quot;\nSystem:&quot;)</span>
<span class="gi">+        for k, stat in _sys_info.items():</span>
<span class="gi">+            print(f&quot;{k:&gt;11}: {stat}&quot;)</span>
<span class="gi">+</span>
<span class="gi">+        print(&quot;\nPython dependencies:&quot;)</span>
<span class="gi">+        for k, stat in _deps_info.items():</span>
<span class="gi">+            print(f&quot;{k:&gt;11}: {stat}&quot;)</span>
<span class="gh">diff --git a/imblearn/utils/_validation.py b/imblearn/utils/_validation.py</span>
<span class="gh">index 38a7408..b21c157 100644</span>
<span class="gd">--- a/imblearn/utils/_validation.py</span>
<span class="gi">+++ b/imblearn/utils/_validation.py</span>
<span class="gu">@@ -1,9 +1,14 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utilities for input validation&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import OrderedDict
<span class="w"> </span>from functools import wraps
<span class="w"> </span>from inspect import Parameter, signature
<span class="w"> </span>from numbers import Integral, Real
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy.sparse import issparse
<span class="w"> </span>from sklearn.base import clone
<span class="gu">@@ -11,10 +16,17 @@ from sklearn.neighbors import NearestNeighbors</span>
<span class="w"> </span>from sklearn.utils import check_array, column_or_1d
<span class="w"> </span>from sklearn.utils.multiclass import type_of_target
<span class="w"> </span>from sklearn.utils.validation import _num_samples
<span class="gi">+</span>
<span class="w"> </span>from .fixes import _is_pandas_df
<span class="gd">-SAMPLING_KIND = (&#39;over-sampling&#39;, &#39;under-sampling&#39;, &#39;clean-sampling&#39;,</span>
<span class="gd">-    &#39;ensemble&#39;, &#39;bypass&#39;)</span>
<span class="gd">-TARGET_KIND = &#39;binary&#39;, &#39;multiclass&#39;, &#39;multilabel-indicator&#39;</span>
<span class="gi">+</span>
<span class="gi">+SAMPLING_KIND = (</span>
<span class="gi">+    &quot;over-sampling&quot;,</span>
<span class="gi">+    &quot;under-sampling&quot;,</span>
<span class="gi">+    &quot;clean-sampling&quot;,</span>
<span class="gi">+    &quot;ensemble&quot;,</span>
<span class="gi">+    &quot;bypass&quot;,</span>
<span class="gi">+)</span>
<span class="gi">+TARGET_KIND = (&quot;binary&quot;, &quot;multiclass&quot;, &quot;multilabel-indicator&quot;)</span>


<span class="w"> </span>class ArraysTransformer:
<span class="gu">@@ -24,6 +36,62 @@ class ArraysTransformer:</span>
<span class="w"> </span>        self.x_props = self._gets_props(X)
<span class="w"> </span>        self.y_props = self._gets_props(y)

<span class="gi">+    def transform(self, X, y):</span>
<span class="gi">+        X = self._transfrom_one(X, self.x_props)</span>
<span class="gi">+        y = self._transfrom_one(y, self.y_props)</span>
<span class="gi">+        if self.x_props[&quot;type&quot;].lower() == &quot;dataframe&quot; and self.y_props[</span>
<span class="gi">+            &quot;type&quot;</span>
<span class="gi">+        ].lower() in {&quot;series&quot;, &quot;dataframe&quot;}:</span>
<span class="gi">+            # We lost the y.index during resampling. We can safely use X.index to align</span>
<span class="gi">+            # them.</span>
<span class="gi">+            y.index = X.index</span>
<span class="gi">+        return X, y</span>
<span class="gi">+</span>
<span class="gi">+    def _gets_props(self, array):</span>
<span class="gi">+        props = {}</span>
<span class="gi">+        props[&quot;type&quot;] = array.__class__.__name__</span>
<span class="gi">+        props[&quot;columns&quot;] = getattr(array, &quot;columns&quot;, None)</span>
<span class="gi">+        props[&quot;name&quot;] = getattr(array, &quot;name&quot;, None)</span>
<span class="gi">+        props[&quot;dtypes&quot;] = getattr(array, &quot;dtypes&quot;, None)</span>
<span class="gi">+        return props</span>
<span class="gi">+</span>
<span class="gi">+    def _transfrom_one(self, array, props):</span>
<span class="gi">+        type_ = props[&quot;type&quot;].lower()</span>
<span class="gi">+        if type_ == &quot;list&quot;:</span>
<span class="gi">+            ret = array.tolist()</span>
<span class="gi">+        elif type_ == &quot;dataframe&quot;:</span>
<span class="gi">+            import pandas as pd</span>
<span class="gi">+</span>
<span class="gi">+            if issparse(array):</span>
<span class="gi">+                ret = pd.DataFrame.sparse.from_spmatrix(array, columns=props[&quot;columns&quot;])</span>
<span class="gi">+            else:</span>
<span class="gi">+                ret = pd.DataFrame(array, columns=props[&quot;columns&quot;])</span>
<span class="gi">+</span>
<span class="gi">+            try:</span>
<span class="gi">+                ret = ret.astype(props[&quot;dtypes&quot;])</span>
<span class="gi">+            except TypeError:</span>
<span class="gi">+                # We special case the following error:</span>
<span class="gi">+                # https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055</span>
<span class="gi">+                # There is no easy way to have a generic workaround. Here, we detect</span>
<span class="gi">+                # that we have a column with only null values that is datetime64</span>
<span class="gi">+                # (resulting from the np.vstack of the resampling).</span>
<span class="gi">+                for col in ret.columns:</span>
<span class="gi">+                    if (</span>
<span class="gi">+                        ret[col].isnull().all()</span>
<span class="gi">+                        and ret[col].dtype == &quot;datetime64[ns]&quot;</span>
<span class="gi">+                        and props[&quot;dtypes&quot;][col] == &quot;timedelta64[ns]&quot;</span>
<span class="gi">+                    ):</span>
<span class="gi">+                        ret[col] = pd.to_timedelta([&quot;NaT&quot;] * len(ret[col]))</span>
<span class="gi">+                # try again</span>
<span class="gi">+                ret = ret.astype(props[&quot;dtypes&quot;])</span>
<span class="gi">+        elif type_ == &quot;series&quot;:</span>
<span class="gi">+            import pandas as pd</span>
<span class="gi">+</span>
<span class="gi">+            ret = pd.Series(array, dtype=props[&quot;dtypes&quot;], name=props[&quot;name&quot;])</span>
<span class="gi">+        else:</span>
<span class="gi">+            ret = array</span>
<span class="gi">+        return ret</span>
<span class="gi">+</span>

<span class="w"> </span>def _is_neighbors_object(estimator):
<span class="w"> </span>    &quot;&quot;&quot;Check that the estimator exposes a KNeighborsMixin-like API.
<span class="gu">@@ -41,7 +109,8 @@ def _is_neighbors_object(estimator):</span>
<span class="w"> </span>    is_neighbors_object : bool
<span class="w"> </span>        True if the estimator exposes a KNeighborsMixin-like API.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    neighbors_attributes = [&quot;kneighbors&quot;, &quot;kneighbors_graph&quot;]</span>
<span class="gi">+    return all(hasattr(estimator, attr) for attr in neighbors_attributes)</span>


<span class="w"> </span>def check_neighbors_object(nn_name, nn_object, additional_neighbor=0):
<span class="gu">@@ -68,7 +137,15 @@ def check_neighbors_object(nn_name, nn_object, additional_neighbor=0):</span>
<span class="w"> </span>    nn_object : KNeighborsMixin
<span class="w"> </span>        The k-NN object.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if isinstance(nn_object, Integral):</span>
<span class="gi">+        return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)</span>
<span class="gi">+    # _is_neighbors_object(nn_object)</span>
<span class="gi">+    return clone(nn_object)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _count_class_sample(y):</span>
<span class="gi">+    unique, counts = np.unique(y, return_counts=True)</span>
<span class="gi">+    return dict(zip(unique, counts))</span>


<span class="w"> </span>def check_target_type(y, indicate_one_vs_all=False):
<span class="gu">@@ -94,58 +171,272 @@ def check_target_type(y, indicate_one_vs_all=False):</span>
<span class="w"> </span>        Indicate if the target was originally encoded in a one-vs-all fashion.
<span class="w"> </span>        Only returned if ``indicate_multilabel=True``.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    type_y = type_of_target(y)</span>
<span class="gi">+    if type_y == &quot;multilabel-indicator&quot;:</span>
<span class="gi">+        if np.any(y.sum(axis=1) &gt; 1):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Imbalanced-learn currently supports binary, multiclass and &quot;</span>
<span class="gi">+                &quot;binarized encoded multiclasss targets. Multilabel and &quot;</span>
<span class="gi">+                &quot;multioutput targets are not supported.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        y = y.argmax(axis=1)</span>
<span class="gi">+    else:</span>
<span class="gi">+        y = column_or_1d(y)</span>
<span class="gi">+</span>
<span class="gi">+    return (y, type_y == &quot;multilabel-indicator&quot;) if indicate_one_vs_all else y</span>


<span class="w"> </span>def _sampling_strategy_all(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        n_sample_majority = max(target_stats.values())</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_majority - value for (key, value) in target_stats.items()</span>
<span class="gi">+        }</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        n_sample_minority = min(target_stats.values())</span>
<span class="gi">+        sampling_strategy = {key: n_sample_minority for key in target_stats.keys()}</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy</span>


<span class="w"> </span>def _sampling_strategy_majority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting the majority class only.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;&#39;sampling_strategy&#39;=&#39;majority&#39; cannot be used with over-sampler.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        target_stats = _count_class_sample(y)</span>
<span class="gi">+        class_majority = max(target_stats, key=target_stats.get)</span>
<span class="gi">+        n_sample_minority = min(target_stats.values())</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_minority</span>
<span class="gi">+            for key in target_stats.keys()</span>
<span class="gi">+            if key == class_majority</span>
<span class="gi">+        }</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy</span>


<span class="w"> </span>def _sampling_strategy_not_majority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes but not the
<span class="w"> </span>    majority.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        n_sample_majority = max(target_stats.values())</span>
<span class="gi">+        class_majority = max(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_majority - value</span>
<span class="gi">+            for (key, value) in target_stats.items()</span>
<span class="gi">+            if key != class_majority</span>
<span class="gi">+        }</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        n_sample_minority = min(target_stats.values())</span>
<span class="gi">+        class_majority = max(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_minority</span>
<span class="gi">+            for key in target_stats.keys()</span>
<span class="gi">+            if key != class_majority</span>
<span class="gi">+        }</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy</span>


<span class="w"> </span>def _sampling_strategy_not_minority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting all classes but not the
<span class="w"> </span>    minority.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        n_sample_majority = max(target_stats.values())</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_majority - value</span>
<span class="gi">+            for (key, value) in target_stats.items()</span>
<span class="gi">+            if key != class_minority</span>
<span class="gi">+        }</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        n_sample_minority = min(target_stats.values())</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_minority</span>
<span class="gi">+            for key in target_stats.keys()</span>
<span class="gi">+            if key != class_minority</span>
<span class="gi">+        }</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy</span>


<span class="w"> </span>def _sampling_strategy_minority(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by targeting the minority class only.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        n_sample_majority = max(target_stats.values())</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy = {</span>
<span class="gi">+            key: n_sample_majority - value</span>
<span class="gi">+            for (key, value) in target_stats.items()</span>
<span class="gi">+            if key == class_minority</span>
<span class="gi">+        }</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;&#39;sampling_strategy&#39;=&#39;minority&#39; cannot be used with&quot;</span>
<span class="gi">+            &quot; under-sampler and clean-sampler.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy</span>


<span class="w"> </span>def _sampling_strategy_auto(y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target auto for over-sampling and not-minority for
<span class="w"> </span>    under-sampling.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        return _sampling_strategy_not_majority(y, sampling_type)</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot; or sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        return _sampling_strategy_not_minority(y, sampling_type)</span>


<span class="w"> </span>def _sampling_strategy_dict(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Returns sampling target by converting the dictionary depending of the
<span class="w"> </span>    sampling.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    # check that all keys in sampling_strategy are also in y</span>
<span class="gi">+    set_diff_sampling_strategy_target = set(sampling_strategy.keys()) - set(</span>
<span class="gi">+        target_stats.keys()</span>
<span class="gi">+    )</span>
<span class="gi">+    if len(set_diff_sampling_strategy_target) &gt; 0:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;The {set_diff_sampling_strategy_target} target class is/are not &quot;</span>
<span class="gi">+            f&quot;present in the data.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    # check that there is no negative number</span>
<span class="gi">+    if any(n_samples &lt; 0 for n_samples in sampling_strategy.values()):</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;The number of samples in a class cannot be negative.&quot;</span>
<span class="gi">+            f&quot;&#39;sampling_strategy&#39; contains some negative value: {sampling_strategy}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    sampling_strategy_ = {}</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        max(target_stats.values())</span>
<span class="gi">+        max(target_stats, key=target_stats.get)</span>
<span class="gi">+        for class_sample, n_samples in sampling_strategy.items():</span>
<span class="gi">+            if n_samples &lt; target_stats[class_sample]:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;With over-sampling methods, the number&quot;</span>
<span class="gi">+                    f&quot; of samples in a class should be greater&quot;</span>
<span class="gi">+                    f&quot; or equal to the original number of samples.&quot;</span>
<span class="gi">+                    f&quot; Originally, there is {target_stats[class_sample]} &quot;</span>
<span class="gi">+                    f&quot;samples and {n_samples} samples are asked.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            sampling_strategy_[class_sample] = n_samples - target_stats[class_sample]</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot;:</span>
<span class="gi">+        for class_sample, n_samples in sampling_strategy.items():</span>
<span class="gi">+            if n_samples &gt; target_stats[class_sample]:</span>
<span class="gi">+                raise ValueError(</span>
<span class="gi">+                    f&quot;With under-sampling methods, the number of&quot;</span>
<span class="gi">+                    f&quot; samples in a class should be less or equal&quot;</span>
<span class="gi">+                    f&quot; to the original number of samples.&quot;</span>
<span class="gi">+                    f&quot; Originally, there is {target_stats[class_sample]} &quot;</span>
<span class="gi">+                    f&quot;samples and {n_samples} samples are asked.&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+            sampling_strategy_[class_sample] = n_samples</span>
<span class="gi">+    elif sampling_type == &quot;clean-sampling&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;&#39;sampling_strategy&#39; as a dict for cleaning methods is &quot;</span>
<span class="gi">+            &quot;not supported. Please give a list of the classes to be &quot;</span>
<span class="gi">+            &quot;targeted by the sampling.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise NotImplementedError</span>
<span class="gi">+</span>
<span class="gi">+    return sampling_strategy_</span>


<span class="w"> </span>def _sampling_strategy_list(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;With cleaning methods, sampling_strategy can be a list to target the
<span class="w"> </span>    class of interest.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    if sampling_type != &quot;clean-sampling&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;&#39;sampling_strategy&#39; cannot be a list for samplers &quot;</span>
<span class="gi">+            &quot;which are not cleaning methods.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    # check that all keys in sampling_strategy are also in y</span>
<span class="gi">+    set_diff_sampling_strategy_target = set(sampling_strategy) - set(</span>
<span class="gi">+        target_stats.keys()</span>
<span class="gi">+    )</span>
<span class="gi">+    if len(set_diff_sampling_strategy_target) &gt; 0:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;The {set_diff_sampling_strategy_target} target class is/are not &quot;</span>
<span class="gi">+            f&quot;present in the data.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    return {</span>
<span class="gi">+        class_sample: min(target_stats.values()) for class_sample in sampling_strategy</span>
<span class="gi">+    }</span>


<span class="w"> </span>def _sampling_strategy_float(sampling_strategy, y, sampling_type):
<span class="w"> </span>    &quot;&quot;&quot;Take a proportion of the majority (over-sampling) or minority
<span class="w"> </span>    (under-sampling) class in binary classification.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    type_y = type_of_target(y)</span>
<span class="gi">+    if type_y != &quot;binary&quot;:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &#39;&quot;sampling_strategy&quot; can be a float only when the type &#39;</span>
<span class="gi">+            &quot;of target is binary. For multi-class, use a dict.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    target_stats = _count_class_sample(y)</span>
<span class="gi">+    if sampling_type == &quot;over-sampling&quot;:</span>
<span class="gi">+        n_sample_majority = max(target_stats.values())</span>
<span class="gi">+        class_majority = max(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy_ = {</span>
<span class="gi">+            key: int(n_sample_majority * sampling_strategy - value)</span>
<span class="gi">+            for (key, value) in target_stats.items()</span>
<span class="gi">+            if key != class_majority</span>
<span class="gi">+        }</span>
<span class="gi">+        if any([n_samples &lt;= 0 for n_samples in sampling_strategy_.values()]):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;The specified ratio required to remove samples &quot;</span>
<span class="gi">+                &quot;from the minority class while trying to &quot;</span>
<span class="gi">+                &quot;generate new samples. Please increase the &quot;</span>
<span class="gi">+                &quot;ratio.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+    elif sampling_type == &quot;under-sampling&quot;:</span>
<span class="gi">+        n_sample_minority = min(target_stats.values())</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        sampling_strategy_ = {</span>
<span class="gi">+            key: int(n_sample_minority / sampling_strategy)</span>
<span class="gi">+            for (key, value) in target_stats.items()</span>
<span class="gi">+            if key != class_minority</span>
<span class="gi">+        }</span>
<span class="gi">+        if any(</span>
<span class="gi">+            [</span>
<span class="gi">+                n_samples &gt; target_stats[target]</span>
<span class="gi">+                for target, n_samples in sampling_strategy_.items()</span>
<span class="gi">+            ]</span>
<span class="gi">+        ):</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;The specified ratio required to generate new &quot;</span>
<span class="gi">+                &quot;sample in the majority class while trying to &quot;</span>
<span class="gi">+                &quot;remove samples. Please increase the ratio.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+    else:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;&#39;clean-sampling&#39; methods do let the user specify the sampling ratio.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    return sampling_strategy_</span>


<span class="w"> </span>def check_sampling_strategy(sampling_strategy, y, sampling_type, **kwargs):
<span class="gu">@@ -236,14 +527,67 @@ def check_sampling_strategy(sampling_strategy, y, sampling_type, **kwargs):</span>
<span class="w"> </span>        the key being the class target and the value being the desired
<span class="w"> </span>        number of samples.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-SAMPLING_TARGET_KIND = {&#39;minority&#39;: _sampling_strategy_minority, &#39;majority&#39;:</span>
<span class="gd">-    _sampling_strategy_majority, &#39;not minority&#39;:</span>
<span class="gd">-    _sampling_strategy_not_minority, &#39;not majority&#39;:</span>
<span class="gd">-    _sampling_strategy_not_majority, &#39;all&#39;: _sampling_strategy_all, &#39;auto&#39;:</span>
<span class="gd">-    _sampling_strategy_auto}</span>
<span class="gi">+    if sampling_type not in SAMPLING_KIND:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;&#39;sampling_type&#39; should be one of {SAMPLING_KIND}. &quot;</span>
<span class="gi">+            f&quot;Got &#39;{sampling_type} instead.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if np.unique(y).size &lt;= 1:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;The target &#39;y&#39; needs to have more than 1 class. &quot;</span>
<span class="gi">+            f&quot;Got {np.unique(y).size} class instead&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    if sampling_type in (&quot;ensemble&quot;, &quot;bypass&quot;):</span>
<span class="gi">+        return sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+    if isinstance(sampling_strategy, str):</span>
<span class="gi">+        if sampling_strategy not in SAMPLING_TARGET_KIND.keys():</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;When &#39;sampling_strategy&#39; is a string, it needs&quot;</span>
<span class="gi">+                f&quot; to be one of {SAMPLING_TARGET_KIND}. Got &#39;{sampling_strategy}&#39; &quot;</span>
<span class="gi">+                f&quot;instead.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        return OrderedDict(</span>
<span class="gi">+            sorted(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type).items())</span>
<span class="gi">+        )</span>
<span class="gi">+    elif isinstance(sampling_strategy, dict):</span>
<span class="gi">+        return OrderedDict(</span>
<span class="gi">+            sorted(_sampling_strategy_dict(sampling_strategy, y, sampling_type).items())</span>
<span class="gi">+        )</span>
<span class="gi">+    elif isinstance(sampling_strategy, list):</span>
<span class="gi">+        return OrderedDict(</span>
<span class="gi">+            sorted(_sampling_strategy_list(sampling_strategy, y, sampling_type).items())</span>
<span class="gi">+        )</span>
<span class="gi">+    elif isinstance(sampling_strategy, Real):</span>
<span class="gi">+        if sampling_strategy &lt;= 0 or sampling_strategy &gt; 1:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                f&quot;When &#39;sampling_strategy&#39; is a float, it should be &quot;</span>
<span class="gi">+                f&quot;in the range (0, 1]. Got {sampling_strategy} instead.&quot;</span>
<span class="gi">+            )</span>
<span class="gi">+        return OrderedDict(</span>
<span class="gi">+            sorted(</span>
<span class="gi">+                _sampling_strategy_float(sampling_strategy, y, sampling_type).items()</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+    elif callable(sampling_strategy):</span>
<span class="gi">+        sampling_strategy_ = sampling_strategy(y, **kwargs)</span>
<span class="gi">+        return OrderedDict(</span>
<span class="gi">+            sorted(</span>
<span class="gi">+                _sampling_strategy_dict(sampling_strategy_, y, sampling_type).items()</span>
<span class="gi">+            )</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+SAMPLING_TARGET_KIND = {</span>
<span class="gi">+    &quot;minority&quot;: _sampling_strategy_minority,</span>
<span class="gi">+    &quot;majority&quot;: _sampling_strategy_majority,</span>
<span class="gi">+    &quot;not minority&quot;: _sampling_strategy_not_minority,</span>
<span class="gi">+    &quot;not majority&quot;: _sampling_strategy_not_majority,</span>
<span class="gi">+    &quot;all&quot;: _sampling_strategy_all,</span>
<span class="gi">+    &quot;auto&quot;: _sampling_strategy_auto,</span>
<span class="gi">+}</span>


<span class="w"> </span>def _deprecate_positional_args(f):
<span class="gu">@@ -257,9 +601,47 @@ def _deprecate_positional_args(f):</span>
<span class="w"> </span>    f : function
<span class="w"> </span>        function to check arguments on.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    sig = signature(f)</span>
<span class="gi">+    kwonly_args = []</span>
<span class="gi">+    all_args = []</span>
<span class="gi">+</span>
<span class="gi">+    for name, param in sig.parameters.items():</span>
<span class="gi">+        if param.kind == Parameter.POSITIONAL_OR_KEYWORD:</span>
<span class="gi">+            all_args.append(name)</span>
<span class="gi">+        elif param.kind == Parameter.KEYWORD_ONLY:</span>
<span class="gi">+            kwonly_args.append(name)</span>
<span class="gi">+</span>
<span class="gi">+    @wraps(f)</span>
<span class="gi">+    def inner_f(*args, **kwargs):</span>
<span class="gi">+        extra_args = len(args) - len(all_args)</span>
<span class="gi">+        if extra_args &gt; 0:</span>
<span class="gi">+            # ignore first &#39;self&#39; argument for instance methods</span>
<span class="gi">+            args_msg = [</span>
<span class="gi">+                f&quot;{name}={arg}&quot;</span>
<span class="gi">+                for name, arg in zip(kwonly_args[:extra_args], args[-extra_args:])</span>
<span class="gi">+            ]</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;Pass {&#39;, &#39;.join(args_msg)} as keyword args. From version 0.9 &quot;</span>
<span class="gi">+                f&quot;passing these as positional arguments will &quot;</span>
<span class="gi">+                f&quot;result in an error&quot;,</span>
<span class="gi">+                FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})</span>
<span class="gi">+        return f(**kwargs)</span>
<span class="gi">+</span>
<span class="gi">+    return inner_f</span>


<span class="w"> </span>def _check_X(X):
<span class="w"> </span>    &quot;&quot;&quot;Check X and do not check it if a dataframe.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    n_samples = _num_samples(X)</span>
<span class="gi">+    if n_samples &lt; 1:</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;Found array with {n_samples} sample(s) while a minimum of 1 is &quot;</span>
<span class="gi">+            &quot;required.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    if _is_pandas_df(X):</span>
<span class="gi">+        return X</span>
<span class="gi">+    return check_array(</span>
<span class="gi">+        X, dtype=None, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;], force_all_finite=False</span>
<span class="gi">+    )</span>
<span class="gh">diff --git a/imblearn/utils/deprecation.py b/imblearn/utils/deprecation.py</span>
<span class="gh">index a630c60..6d459b8 100644</span>
<span class="gd">--- a/imblearn/utils/deprecation.py</span>
<span class="gi">+++ b/imblearn/utils/deprecation.py</span>
<span class="gu">@@ -1,9 +1,12 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utilities for deprecation&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import warnings


<span class="gd">-def deprecate_parameter(sampler, version_deprecation, param_deprecated,</span>
<span class="gd">-    new_param=None):</span>
<span class="gi">+def deprecate_parameter(sampler, version_deprecation, param_deprecated, new_param=None):</span>
<span class="w"> </span>    &quot;&quot;&quot;Helper to deprecate a parameter by another one.

<span class="w"> </span>    Parameters
<span class="gu">@@ -22,4 +25,22 @@ def deprecate_parameter(sampler, version_deprecation, param_deprecated,</span>
<span class="w"> </span>        The parameter used instead of the deprecated parameter. By default, no
<span class="w"> </span>        parameter is expected.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    x, y = version_deprecation.split(&quot;.&quot;)</span>
<span class="gi">+    version_removed = x + &quot;.&quot; + str(int(y) + 2)</span>
<span class="gi">+    if new_param is None:</span>
<span class="gi">+        if getattr(sampler, param_deprecated) is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;&#39;{param_deprecated}&#39; is deprecated from {version_deprecation} and &quot;</span>
<span class="gi">+                f&quot; will be removed in {version_removed} for the estimator &quot;</span>
<span class="gi">+                f&quot;{sampler.__class__}.&quot;,</span>
<span class="gi">+                category=FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+    else:</span>
<span class="gi">+        if getattr(sampler, param_deprecated) is not None:</span>
<span class="gi">+            warnings.warn(</span>
<span class="gi">+                f&quot;&#39;{param_deprecated}&#39; is deprecated from {version_deprecation} and &quot;</span>
<span class="gi">+                f&quot;will be removed in {version_removed} for the estimator &quot;</span>
<span class="gi">+                f&quot;{sampler.__class__}. Use &#39;{new_param}&#39; instead.&quot;,</span>
<span class="gi">+                category=FutureWarning,</span>
<span class="gi">+            )</span>
<span class="gi">+            setattr(sampler, new_param, getattr(sampler, param_deprecated))</span>
<span class="gh">diff --git a/imblearn/utils/estimator_checks.py b/imblearn/utils/estimator_checks.py</span>
<span class="gh">index d3aea67..5704277 100644</span>
<span class="gd">--- a/imblearn/utils/estimator_checks.py</span>
<span class="gi">+++ b/imblearn/utils/estimator_checks.py</span>
<span class="gu">@@ -1,35 +1,153 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Utils to check the samplers and compatibility with scikit-learn&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Adapated from scikit-learn</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import re
<span class="w"> </span>import sys
<span class="w"> </span>import traceback
<span class="w"> </span>import warnings
<span class="w"> </span>from collections import Counter
<span class="w"> </span>from functools import partial
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>import sklearn
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import clone, is_classifier, is_regressor
<span class="w"> </span>from sklearn.cluster import KMeans
<span class="gd">-from sklearn.datasets import load_iris, make_blobs, make_classification, make_multilabel_classification</span>
<span class="gi">+from sklearn.datasets import (  # noqa</span>
<span class="gi">+    load_iris,</span>
<span class="gi">+    make_blobs,</span>
<span class="gi">+    make_classification,</span>
<span class="gi">+    make_multilabel_classification,</span>
<span class="gi">+)</span>
<span class="w"> </span>from sklearn.exceptions import SkipTestWarning
<span class="w"> </span>from sklearn.preprocessing import StandardScaler, label_binarize
<span class="w"> </span>from sklearn.utils._tags import _safe_tags
<span class="gd">-from sklearn.utils._testing import SkipTest, assert_allclose, assert_array_equal, assert_raises_regex, raises, set_random_state</span>
<span class="gd">-from sklearn.utils.estimator_checks import _enforce_estimator_tags_y, _get_check_estimator_ids, _maybe_mark_xfail</span>
<span class="gi">+from sklearn.utils._testing import (</span>
<span class="gi">+    SkipTest,</span>
<span class="gi">+    assert_allclose,</span>
<span class="gi">+    assert_array_equal,</span>
<span class="gi">+    assert_raises_regex,</span>
<span class="gi">+    raises,</span>
<span class="gi">+    set_random_state,</span>
<span class="gi">+)</span>
<span class="gi">+from sklearn.utils.estimator_checks import (</span>
<span class="gi">+    _enforce_estimator_tags_y,</span>
<span class="gi">+    _get_check_estimator_ids,</span>
<span class="gi">+    _maybe_mark_xfail,</span>
<span class="gi">+)</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    from sklearn.utils.estimator_checks import _enforce_estimator_tags_x
<span class="w"> </span>except ImportError:
<span class="gd">-    from sklearn.utils.estimator_checks import _enforce_estimator_tags_X as _enforce_estimator_tags_x</span>
<span class="gi">+    # scikit-learn &gt;= 1.2</span>
<span class="gi">+    from sklearn.utils.estimator_checks import (</span>
<span class="gi">+        _enforce_estimator_tags_X as _enforce_estimator_tags_x,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="w"> </span>from sklearn.utils.multiclass import type_of_target
<span class="gi">+</span>
<span class="w"> </span>from imblearn.datasets import make_imbalance
<span class="w"> </span>from imblearn.over_sampling.base import BaseOverSampler
<span class="w"> </span>from imblearn.under_sampling.base import BaseCleaningSampler, BaseUnderSampler
<span class="w"> </span>from imblearn.utils._param_validation import generate_invalid_param_val, make_constraint
<span class="gi">+</span>
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)


<span class="gi">+def sample_dataset_generator():</span>
<span class="gi">+    X, y = make_classification(</span>
<span class="gi">+        n_samples=1000,</span>
<span class="gi">+        n_classes=3,</span>
<span class="gi">+        n_informative=4,</span>
<span class="gi">+        weights=[0.2, 0.3, 0.5],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+    )</span>
<span class="gi">+    return X, y</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.fixture(name=&quot;sample_dataset_generator&quot;)</span>
<span class="gi">+def sample_dataset_generator_fixture():</span>
<span class="gi">+    return sample_dataset_generator()</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _set_checking_parameters(estimator):</span>
<span class="gi">+    params = estimator.get_params()</span>
<span class="gi">+    name = estimator.__class__.__name__</span>
<span class="gi">+    if &quot;n_estimators&quot; in params:</span>
<span class="gi">+        estimator.set_params(n_estimators=min(5, estimator.n_estimators))</span>
<span class="gi">+    if name == &quot;ClusterCentroids&quot;:</span>
<span class="gi">+        if sklearn_version &lt; parse_version(&quot;1.1&quot;):</span>
<span class="gi">+            algorithm = &quot;full&quot;</span>
<span class="gi">+        else:</span>
<span class="gi">+            algorithm = &quot;lloyd&quot;</span>
<span class="gi">+        estimator.set_params(</span>
<span class="gi">+            voting=&quot;soft&quot;,</span>
<span class="gi">+            estimator=KMeans(random_state=0, algorithm=algorithm, n_init=1),</span>
<span class="gi">+        )</span>
<span class="gi">+    if name == &quot;KMeansSMOTE&quot;:</span>
<span class="gi">+        estimator.set_params(kmeans_estimator=12)</span>
<span class="gi">+    if name == &quot;BalancedRandomForestClassifier&quot;:</span>
<span class="gi">+        # TODO: remove in 0.13</span>
<span class="gi">+        # future default in 0.13</span>
<span class="gi">+        estimator.set_params(replacement=True, sampling_strategy=&quot;all&quot;, bootstrap=False)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _yield_sampler_checks(sampler):</span>
<span class="gi">+    tags = sampler._get_tags()</span>
<span class="gi">+    yield check_target_type</span>
<span class="gi">+    yield check_samplers_one_label</span>
<span class="gi">+    yield check_samplers_fit</span>
<span class="gi">+    yield check_samplers_fit_resample</span>
<span class="gi">+    yield check_samplers_sampling_strategy_fit_resample</span>
<span class="gi">+    if &quot;sparse&quot; in tags[&quot;X_types&quot;]:</span>
<span class="gi">+        yield check_samplers_sparse</span>
<span class="gi">+    if &quot;dataframe&quot; in tags[&quot;X_types&quot;]:</span>
<span class="gi">+        yield check_samplers_pandas</span>
<span class="gi">+        yield check_samplers_pandas_sparse</span>
<span class="gi">+    if &quot;string&quot; in tags[&quot;X_types&quot;]:</span>
<span class="gi">+        yield check_samplers_string</span>
<span class="gi">+    if tags[&quot;allow_nan&quot;]:</span>
<span class="gi">+        yield check_samplers_nan</span>
<span class="gi">+    yield check_samplers_list</span>
<span class="gi">+    yield check_samplers_multiclass_ova</span>
<span class="gi">+    yield check_samplers_preserve_dtype</span>
<span class="gi">+    # we don&#39;t filter samplers based on their tag here because we want to make</span>
<span class="gi">+    # sure that the fitted attribute does not exist if the tag is not</span>
<span class="gi">+    # stipulated</span>
<span class="gi">+    yield check_samplers_sample_indices</span>
<span class="gi">+    yield check_samplers_2d_target</span>
<span class="gi">+    yield check_sampler_get_feature_names_out</span>
<span class="gi">+    yield check_sampler_get_feature_names_out_pandas</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _yield_classifier_checks(classifier):</span>
<span class="gi">+    yield check_classifier_on_multilabel_or_multioutput_targets</span>
<span class="gi">+    yield check_classifiers_with_encoded_labels</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _yield_all_checks(estimator):</span>
<span class="gi">+    name = estimator.__class__.__name__</span>
<span class="gi">+    tags = estimator._get_tags()</span>
<span class="gi">+    if tags[&quot;_skip_test&quot;]:</span>
<span class="gi">+        warnings.warn(</span>
<span class="gi">+            f&quot;Explicit SKIP via _skip_test tag for estimator {name}.&quot;,</span>
<span class="gi">+            SkipTestWarning,</span>
<span class="gi">+        )</span>
<span class="gi">+        return</span>
<span class="gi">+    # trigger our checks if this is a SamplerMixin</span>
<span class="gi">+    if hasattr(estimator, &quot;fit_resample&quot;):</span>
<span class="gi">+        for check in _yield_sampler_checks(estimator):</span>
<span class="gi">+            yield check</span>
<span class="gi">+    if hasattr(estimator, &quot;predict&quot;):</span>
<span class="gi">+        for check in _yield_classifier_checks(estimator):</span>
<span class="gi">+            yield check</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def parametrize_with_checks(estimators):
<span class="w"> </span>    &quot;&quot;&quot;Pytest specific decorator for parametrizing estimator checks.

<span class="gu">@@ -59,4 +177,652 @@ def parametrize_with_checks(estimators):</span>
<span class="w"> </span>    ... def test_sklearn_compatible_estimator(estimator, check):
<span class="w"> </span>    ...     check(estimator)
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    def checks_generator():</span>
<span class="gi">+        for estimator in estimators:</span>
<span class="gi">+            name = type(estimator).__name__</span>
<span class="gi">+            for check in _yield_all_checks(estimator):</span>
<span class="gi">+                check = partial(check, name)</span>
<span class="gi">+                yield _maybe_mark_xfail(estimator, check, pytest)</span>
<span class="gi">+</span>
<span class="gi">+    return pytest.mark.parametrize(</span>
<span class="gi">+        &quot;estimator, check&quot;, checks_generator(), ids=_get_check_estimator_ids</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_target_type(name, estimator_orig):</span>
<span class="gi">+    estimator = clone(estimator_orig)</span>
<span class="gi">+    # should raise warning if the target is continuous (we cannot raise error)</span>
<span class="gi">+    X = np.random.random((20, 2))</span>
<span class="gi">+    y = np.linspace(0, 1, 20)</span>
<span class="gi">+    msg = &quot;Unknown label type:&quot;</span>
<span class="gi">+    assert_raises_regex(</span>
<span class="gi">+        ValueError,</span>
<span class="gi">+        msg,</span>
<span class="gi">+        estimator.fit_resample,</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+    )</span>
<span class="gi">+    # if the target is multilabel then we should raise an error</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    y = rng.randint(2, size=(20, 3))</span>
<span class="gi">+    msg = &quot;Multilabel and multioutput targets are not supported.&quot;</span>
<span class="gi">+    assert_raises_regex(</span>
<span class="gi">+        ValueError,</span>
<span class="gi">+        msg,</span>
<span class="gi">+        estimator.fit_resample,</span>
<span class="gi">+        X,</span>
<span class="gi">+        y,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_one_label(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    error_string_fit = &quot;Sampler can&#39;t balance when only one class is present.&quot;</span>
<span class="gi">+    X = np.random.random((20, 2))</span>
<span class="gi">+    y = np.zeros(20)</span>
<span class="gi">+    try:</span>
<span class="gi">+        sampler.fit_resample(X, y)</span>
<span class="gi">+    except ValueError as e:</span>
<span class="gi">+        if &quot;class&quot; not in repr(e):</span>
<span class="gi">+            print(error_string_fit, sampler.__class__.__name__, e)</span>
<span class="gi">+            traceback.print_exc(file=sys.stdout)</span>
<span class="gi">+            raise e</span>
<span class="gi">+        else:</span>
<span class="gi">+            return</span>
<span class="gi">+    except Exception as exc:</span>
<span class="gi">+        print(error_string_fit, traceback, exc)</span>
<span class="gi">+        traceback.print_exc(file=sys.stdout)</span>
<span class="gi">+        raise exc</span>
<span class="gi">+    raise AssertionError(error_string_fit)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_fit(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    np.random.seed(42)  # Make this test reproducible</span>
<span class="gi">+    X = np.random.random((30, 2))</span>
<span class="gi">+    y = np.array([1] * 20 + [0] * 10)</span>
<span class="gi">+    sampler.fit_resample(X, y)</span>
<span class="gi">+    assert hasattr(</span>
<span class="gi">+        sampler, &quot;sampling_strategy_&quot;</span>
<span class="gi">+    ), &quot;No fitted attribute sampling_strategy_&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_fit_resample(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    target_stats = Counter(y)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    if isinstance(sampler, BaseOverSampler):</span>
<span class="gi">+        target_stats_res = Counter(y_res)</span>
<span class="gi">+        n_samples = max(target_stats.values())</span>
<span class="gi">+        assert all(value &gt;= n_samples for value in Counter(y_res).values())</span>
<span class="gi">+    elif isinstance(sampler, BaseUnderSampler):</span>
<span class="gi">+        n_samples = min(target_stats.values())</span>
<span class="gi">+        if name == &quot;InstanceHardnessThreshold&quot;:</span>
<span class="gi">+            # IHT does not enforce the number of samples but provide a number</span>
<span class="gi">+            # of samples the closest to the desired target.</span>
<span class="gi">+            assert all(</span>
<span class="gi">+                Counter(y_res)[k] &lt;= target_stats[k] for k in target_stats.keys()</span>
<span class="gi">+            )</span>
<span class="gi">+        else:</span>
<span class="gi">+            assert all(value == n_samples for value in Counter(y_res).values())</span>
<span class="gi">+    elif isinstance(sampler, BaseCleaningSampler):</span>
<span class="gi">+        target_stats_res = Counter(y_res)</span>
<span class="gi">+        class_minority = min(target_stats, key=target_stats.get)</span>
<span class="gi">+        assert all(</span>
<span class="gi">+            target_stats[class_sample] &gt; target_stats_res[class_sample]</span>
<span class="gi">+            for class_sample in target_stats.keys()</span>
<span class="gi">+            if class_sample != class_minority</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_sampling_strategy_fit_resample(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # in this test we will force all samplers to not change the class 1</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    expected_stat = Counter(y)[1]</span>
<span class="gi">+    if isinstance(sampler, BaseOverSampler):</span>
<span class="gi">+        sampling_strategy = {2: 498, 0: 498}</span>
<span class="gi">+        sampler.set_params(sampling_strategy=sampling_strategy)</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+        assert Counter(y_res)[1] == expected_stat</span>
<span class="gi">+    elif isinstance(sampler, BaseUnderSampler):</span>
<span class="gi">+        sampling_strategy = {2: 201, 0: 201}</span>
<span class="gi">+        sampler.set_params(sampling_strategy=sampling_strategy)</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+        assert Counter(y_res)[1] == expected_stat</span>
<span class="gi">+    elif isinstance(sampler, BaseCleaningSampler):</span>
<span class="gi">+        sampling_strategy = [2, 0]</span>
<span class="gi">+        sampler.set_params(sampling_strategy=sampling_strategy)</span>
<span class="gi">+        X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+        assert Counter(y_res)[1] == expected_stat</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_sparse(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # check that sparse matrices can be passed through the sampler leading to</span>
<span class="gi">+    # the same results than dense</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    X_sparse = sparse.csr_matrix(X)</span>
<span class="gi">+    X_res_sparse, y_res_sparse = sampler.fit_resample(X_sparse, y)</span>
<span class="gi">+    sampler = clone(sampler)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    assert sparse.issparse(X_res_sparse)</span>
<span class="gi">+    assert_allclose(X_res_sparse.A, X_res, rtol=1e-5)</span>
<span class="gi">+    assert_allclose(y_res_sparse, y_res)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_pandas_sparse(name, sampler_orig):</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # Check that the samplers handle pandas dataframe and pandas series</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    X_df = pd.DataFrame(</span>
<span class="gi">+        X, columns=[str(i) for i in range(X.shape[1])], dtype=pd.SparseDtype(float, 0)</span>
<span class="gi">+    )</span>
<span class="gi">+    y_s = pd.Series(y, name=&quot;class&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X_res_df, y_res_s = sampler.fit_resample(X_df, y_s)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    # check that we return the same type for dataframes or series types</span>
<span class="gi">+    assert isinstance(X_res_df, pd.DataFrame)</span>
<span class="gi">+    assert isinstance(y_res_s, pd.Series)</span>
<span class="gi">+</span>
<span class="gi">+    for column_dtype in X_res_df.dtypes:</span>
<span class="gi">+        assert isinstance(column_dtype, pd.SparseDtype)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_df.columns.tolist() == X_res_df.columns.tolist()</span>
<span class="gi">+    assert y_s.name == y_res_s.name</span>
<span class="gi">+</span>
<span class="gi">+    # FIXME: we should use to_numpy with pandas &gt;= 0.25</span>
<span class="gi">+    assert_allclose(X_res_df.values, X_res)</span>
<span class="gi">+    assert_allclose(y_res_s.values, y_res)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_pandas(name, sampler_orig):</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # Check that the samplers handle pandas dataframe and pandas series</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    X_df = pd.DataFrame(X, columns=[str(i) for i in range(X.shape[1])])</span>
<span class="gi">+    y_df = pd.DataFrame(y)</span>
<span class="gi">+    y_s = pd.Series(y, name=&quot;class&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X_res_df, y_res_s = sampler.fit_resample(X_df, y_s)</span>
<span class="gi">+    X_res_df, y_res_df = sampler.fit_resample(X_df, y_df)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    # check that we return the same type for dataframes or series types</span>
<span class="gi">+    assert isinstance(X_res_df, pd.DataFrame)</span>
<span class="gi">+    assert isinstance(y_res_df, pd.DataFrame)</span>
<span class="gi">+    assert isinstance(y_res_s, pd.Series)</span>
<span class="gi">+</span>
<span class="gi">+    assert X_df.columns.tolist() == X_res_df.columns.tolist()</span>
<span class="gi">+    assert y_df.columns.tolist() == y_res_df.columns.tolist()</span>
<span class="gi">+    assert y_s.name == y_res_s.name</span>
<span class="gi">+</span>
<span class="gi">+    # FIXME: we should use to_numpy with pandas &gt;= 0.25</span>
<span class="gi">+    assert_allclose(X_res_df.values, X_res)</span>
<span class="gi">+    assert_allclose(y_res_df.values.ravel(), y_res)</span>
<span class="gi">+    assert_allclose(y_res_s.values, y_res)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_list(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # Check that the can samplers handle simple lists</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    X_list = X.tolist()</span>
<span class="gi">+    y_list = y.tolist()</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    X_res_list, y_res_list = sampler.fit_resample(X_list, y_list)</span>
<span class="gi">+</span>
<span class="gi">+    assert isinstance(X_res_list, list)</span>
<span class="gi">+    assert isinstance(y_res_list, list)</span>
<span class="gi">+</span>
<span class="gi">+    assert_allclose(X_res, X_res_list)</span>
<span class="gi">+    assert_allclose(y_res, y_res_list)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_multiclass_ova(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    # Check that multiclass target lead to the same results than OVA encoding</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    y_ova = label_binarize(y, classes=np.unique(y))</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    X_res_ova, y_res_ova = sampler.fit_resample(X, y_ova)</span>
<span class="gi">+    assert_allclose(X_res, X_res_ova)</span>
<span class="gi">+    assert type_of_target(y_res_ova) == type_of_target(y_ova)</span>
<span class="gi">+    assert_allclose(y_res, y_res_ova.argmax(axis=1))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_2d_target(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+</span>
<span class="gi">+    y = y.reshape(-1, 1)  # Make the target 2d</span>
<span class="gi">+    sampler.fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_preserve_dtype(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    # Cast X and y to not default dtype</span>
<span class="gi">+    X = X.astype(np.float32)</span>
<span class="gi">+    y = y.astype(np.int32)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    assert X.dtype == X_res.dtype, &quot;X dtype is not preserved&quot;</span>
<span class="gi">+    assert y.dtype == y_res.dtype, &quot;y dtype is not preserved&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_sample_indices(name, sampler_orig):</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X, y = sample_dataset_generator()</span>
<span class="gi">+    sampler.fit_resample(X, y)</span>
<span class="gi">+    sample_indices = sampler._get_tags().get(&quot;sample_indices&quot;, None)</span>
<span class="gi">+    if sample_indices:</span>
<span class="gi">+        assert hasattr(sampler, &quot;sample_indices_&quot;) is sample_indices</span>
<span class="gi">+    else:</span>
<span class="gi">+        assert not hasattr(sampler, &quot;sample_indices_&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_string(name, sampler_orig):</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    categories = np.array([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], dtype=object)</span>
<span class="gi">+    n_samples = 30</span>
<span class="gi">+    X = rng.randint(low=0, high=3, size=n_samples).reshape(-1, 1)</span>
<span class="gi">+    X = categories[X]</span>
<span class="gi">+    y = rng.permutation([0] * 10 + [1] * 20)</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    assert X_res.dtype == object</span>
<span class="gi">+    assert X_res.shape[0] == y_res.shape[0]</span>
<span class="gi">+    assert_array_equal(np.unique(X_res.ravel()), categories)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_samplers_nan(name, sampler_orig):</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    categories = np.array([0, 1, np.nan], dtype=np.float64)</span>
<span class="gi">+    n_samples = 100</span>
<span class="gi">+    X = rng.randint(low=0, high=3, size=n_samples).reshape(-1, 1)</span>
<span class="gi">+    X = categories[X]</span>
<span class="gi">+    y = rng.permutation([0] * 40 + [1] * 60)</span>
<span class="gi">+</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y)</span>
<span class="gi">+    assert X_res.dtype == np.float64</span>
<span class="gi">+    assert X_res.shape[0] == y_res.shape[0]</span>
<span class="gi">+    assert np.any(np.isnan(X_res.ravel()))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_classifier_on_multilabel_or_multioutput_targets(name, estimator_orig):</span>
<span class="gi">+    estimator = clone(estimator_orig)</span>
<span class="gi">+    X, y = make_multilabel_classification(n_samples=30)</span>
<span class="gi">+    msg = &quot;Multilabel and multioutput targets are not supported.&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=msg):</span>
<span class="gi">+        estimator.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_classifiers_with_encoded_labels(name, classifier_orig):</span>
<span class="gi">+    # Non-regression test for #709</span>
<span class="gi">+    # https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+    classifier = clone(classifier_orig)</span>
<span class="gi">+    iris = load_iris(as_frame=True)</span>
<span class="gi">+    df, y = iris.data, iris.target</span>
<span class="gi">+    y = pd.Series(iris.target_names[iris.target], dtype=&quot;category&quot;)</span>
<span class="gi">+    df, y = make_imbalance(</span>
<span class="gi">+        df,</span>
<span class="gi">+        y,</span>
<span class="gi">+        sampling_strategy={</span>
<span class="gi">+            &quot;setosa&quot;: 30,</span>
<span class="gi">+            &quot;versicolor&quot;: 20,</span>
<span class="gi">+            &quot;virginica&quot;: 50,</span>
<span class="gi">+        },</span>
<span class="gi">+    )</span>
<span class="gi">+    classifier.set_params(sampling_strategy={&quot;setosa&quot;: 20, &quot;virginica&quot;: 20})</span>
<span class="gi">+    classifier.fit(df, y)</span>
<span class="gi">+    assert set(classifier.classes_) == set(y.cat.categories.tolist())</span>
<span class="gi">+    y_pred = classifier.predict(df)</span>
<span class="gi">+    assert set(y_pred) == set(y.cat.categories.tolist())</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_param_validation(name, estimator_orig):</span>
<span class="gi">+    # Check that an informative error is raised when the value of a constructor</span>
<span class="gi">+    # parameter does not have an appropriate type or value.</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+    X = rng.uniform(size=(20, 5))</span>
<span class="gi">+    y = rng.randint(0, 2, size=20)</span>
<span class="gi">+    y = _enforce_estimator_tags_y(estimator_orig, y)</span>
<span class="gi">+</span>
<span class="gi">+    estimator_params = estimator_orig.get_params(deep=False).keys()</span>
<span class="gi">+</span>
<span class="gi">+    # check that there is a constraint for each parameter</span>
<span class="gi">+    if estimator_params:</span>
<span class="gi">+        validation_params = estimator_orig._parameter_constraints.keys()</span>
<span class="gi">+        unexpected_params = set(validation_params) - set(estimator_params)</span>
<span class="gi">+        missing_params = set(estimator_params) - set(validation_params)</span>
<span class="gi">+        err_msg = (</span>
<span class="gi">+            f&quot;Mismatch between _parameter_constraints and the parameters of {name}.&quot;</span>
<span class="gi">+            f&quot;\nConsider the unexpected parameters {unexpected_params} and expected but&quot;</span>
<span class="gi">+            f&quot; missing parameters {missing_params}&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+        assert validation_params == estimator_params, err_msg</span>
<span class="gi">+</span>
<span class="gi">+    # this object does not have a valid type for sure for all params</span>
<span class="gi">+    param_with_bad_type = type(&quot;BadType&quot;, (), {})()</span>
<span class="gi">+</span>
<span class="gi">+    fit_methods = [&quot;fit&quot;, &quot;partial_fit&quot;, &quot;fit_transform&quot;, &quot;fit_predict&quot;, &quot;fit_resample&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    for param_name in estimator_params:</span>
<span class="gi">+        constraints = estimator_orig._parameter_constraints[param_name]</span>
<span class="gi">+</span>
<span class="gi">+        if constraints == &quot;no_validation&quot;:</span>
<span class="gi">+            # This parameter is not validated</span>
<span class="gi">+            continue  # pragma: no cover</span>
<span class="gi">+</span>
<span class="gi">+        match = rf&quot;The &#39;{param_name}&#39; parameter of {name} must be .* Got .* instead.&quot;</span>
<span class="gi">+        err_msg = (</span>
<span class="gi">+            f&quot;{name} does not raise an informative error message when the &quot;</span>
<span class="gi">+            f&quot;parameter {param_name} does not have a valid type or value.&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+        estimator = clone(estimator_orig)</span>
<span class="gi">+</span>
<span class="gi">+        # First, check that the error is raised if param doesn&#39;t match any valid type.</span>
<span class="gi">+        estimator.set_params(**{param_name: param_with_bad_type})</span>
<span class="gi">+</span>
<span class="gi">+        for method in fit_methods:</span>
<span class="gi">+            if not hasattr(estimator, method):</span>
<span class="gi">+                # the method is not accessible with the current set of parameters</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            with raises(ValueError, match=match, err_msg=err_msg):</span>
<span class="gi">+                if any(</span>
<span class="gi">+                    isinstance(X_type, str) and X_type.endswith(&quot;labels&quot;)</span>
<span class="gi">+                    for X_type in _safe_tags(estimator, key=&quot;X_types&quot;)</span>
<span class="gi">+                ):</span>
<span class="gi">+                    # The estimator is a label transformer and take only `y`</span>
<span class="gi">+                    getattr(estimator, method)(y)  # pragma: no cover</span>
<span class="gi">+                else:</span>
<span class="gi">+                    getattr(estimator, method)(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        # Then, for constraints that are more than a type constraint, check that the</span>
<span class="gi">+        # error is raised if param does match a valid type but does not match any valid</span>
<span class="gi">+        # value for this type.</span>
<span class="gi">+        constraints = [make_constraint(constraint) for constraint in constraints]</span>
<span class="gi">+</span>
<span class="gi">+        for constraint in constraints:</span>
<span class="gi">+            try:</span>
<span class="gi">+                bad_value = generate_invalid_param_val(constraint)</span>
<span class="gi">+            except NotImplementedError:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            estimator.set_params(**{param_name: bad_value})</span>
<span class="gi">+</span>
<span class="gi">+            for method in fit_methods:</span>
<span class="gi">+                if not hasattr(estimator, method):</span>
<span class="gi">+                    # the method is not accessible with the current set of parameters</span>
<span class="gi">+                    continue</span>
<span class="gi">+</span>
<span class="gi">+                with raises(ValueError, match=match, err_msg=err_msg):</span>
<span class="gi">+                    if any(</span>
<span class="gi">+                        X_type.endswith(&quot;labels&quot;)</span>
<span class="gi">+                        for X_type in _safe_tags(estimator, key=&quot;X_types&quot;)</span>
<span class="gi">+                    ):</span>
<span class="gi">+                        # The estimator is a label transformer and take only `y`</span>
<span class="gi">+                        getattr(estimator, method)(y)  # pragma: no cover</span>
<span class="gi">+                    else:</span>
<span class="gi">+                        getattr(estimator, method)(X, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_dataframe_column_names_consistency(name, estimator_orig):</span>
<span class="gi">+    try:</span>
<span class="gi">+        import pandas as pd</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        raise SkipTest(</span>
<span class="gi">+            &quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    tags = _safe_tags(estimator_orig)</span>
<span class="gi">+    is_supported_X_types = (</span>
<span class="gi">+        &quot;2darray&quot; in tags[&quot;X_types&quot;] or &quot;categorical&quot; in tags[&quot;X_types&quot;]</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    if not is_supported_X_types or tags[&quot;no_validation&quot;]:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    rng = np.random.RandomState(0)</span>
<span class="gi">+</span>
<span class="gi">+    estimator = clone(estimator_orig)</span>
<span class="gi">+    set_random_state(estimator)</span>
<span class="gi">+</span>
<span class="gi">+    X_orig = rng.normal(size=(150, 8))</span>
<span class="gi">+</span>
<span class="gi">+    X_orig = _enforce_estimator_tags_x(estimator, X_orig)</span>
<span class="gi">+    n_samples, n_features = X_orig.shape</span>
<span class="gi">+</span>
<span class="gi">+    names = np.array([f&quot;col_{i}&quot; for i in range(n_features)])</span>
<span class="gi">+    X = pd.DataFrame(X_orig, columns=names)</span>
<span class="gi">+</span>
<span class="gi">+    if is_regressor(estimator):</span>
<span class="gi">+        y = rng.normal(size=n_samples)</span>
<span class="gi">+    else:</span>
<span class="gi">+        y = rng.randint(low=0, high=2, size=n_samples)</span>
<span class="gi">+    y = _enforce_estimator_tags_y(estimator, y)</span>
<span class="gi">+</span>
<span class="gi">+    # Check that calling `fit` does not raise any warnings about feature names.</span>
<span class="gi">+    with warnings.catch_warnings():</span>
<span class="gi">+        warnings.filterwarnings(</span>
<span class="gi">+            &quot;error&quot;,</span>
<span class="gi">+            message=&quot;X does not have valid feature names&quot;,</span>
<span class="gi">+            category=UserWarning,</span>
<span class="gi">+            module=&quot;imblearn&quot;,</span>
<span class="gi">+        )</span>
<span class="gi">+        estimator.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    if not hasattr(estimator, &quot;feature_names_in_&quot;):</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            &quot;Estimator does not have a feature_names_in_ &quot;</span>
<span class="gi">+            &quot;attribute after fitting with a dataframe&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+    assert isinstance(estimator.feature_names_in_, np.ndarray)</span>
<span class="gi">+    assert estimator.feature_names_in_.dtype == object</span>
<span class="gi">+    assert_array_equal(estimator.feature_names_in_, names)</span>
<span class="gi">+</span>
<span class="gi">+    # Only check imblearn estimators for feature_names_in_ in docstring</span>
<span class="gi">+    module_name = estimator_orig.__module__</span>
<span class="gi">+    if (</span>
<span class="gi">+        module_name.startswith(&quot;imblearn.&quot;)</span>
<span class="gi">+        and not (&quot;test_&quot; in module_name or module_name.endswith(&quot;_testing&quot;))</span>
<span class="gi">+        and (&quot;feature_names_in_&quot; not in (estimator_orig.__doc__))</span>
<span class="gi">+    ):</span>
<span class="gi">+        raise ValueError(</span>
<span class="gi">+            f&quot;Estimator {name} does not document its feature_names_in_ attribute&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    check_methods = []</span>
<span class="gi">+    for method in (</span>
<span class="gi">+        &quot;predict&quot;,</span>
<span class="gi">+        &quot;transform&quot;,</span>
<span class="gi">+        &quot;decision_function&quot;,</span>
<span class="gi">+        &quot;predict_proba&quot;,</span>
<span class="gi">+        &quot;score&quot;,</span>
<span class="gi">+        &quot;score_samples&quot;,</span>
<span class="gi">+        &quot;predict_log_proba&quot;,</span>
<span class="gi">+    ):</span>
<span class="gi">+        if not hasattr(estimator, method):</span>
<span class="gi">+            continue</span>
<span class="gi">+</span>
<span class="gi">+        callable_method = getattr(estimator, method)</span>
<span class="gi">+        if method == &quot;score&quot;:</span>
<span class="gi">+            callable_method = partial(callable_method, y=y)</span>
<span class="gi">+        check_methods.append((method, callable_method))</span>
<span class="gi">+</span>
<span class="gi">+    for _, method in check_methods:</span>
<span class="gi">+        with warnings.catch_warnings():</span>
<span class="gi">+            warnings.filterwarnings(</span>
<span class="gi">+                &quot;error&quot;,</span>
<span class="gi">+                message=&quot;X does not have valid feature names&quot;,</span>
<span class="gi">+                category=UserWarning,</span>
<span class="gi">+                module=&quot;sklearn&quot;,</span>
<span class="gi">+            )</span>
<span class="gi">+            method(X)  # works without UserWarning for valid features</span>
<span class="gi">+</span>
<span class="gi">+    invalid_names = [</span>
<span class="gi">+        (names[::-1], &quot;Feature names must be in the same order as they were in fit.&quot;),</span>
<span class="gi">+        (</span>
<span class="gi">+            [f&quot;another_prefix_{i}&quot; for i in range(n_features)],</span>
<span class="gi">+            &quot;Feature names unseen at fit time:\n- another_prefix_0\n-&quot;</span>
<span class="gi">+            &quot; another_prefix_1\n&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            names[:3],</span>
<span class="gi">+            f&quot;Feature names seen at fit time, yet now missing:\n- {min(names[3:])}\n&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+    ]</span>
<span class="gi">+    params = {</span>
<span class="gi">+        key: value</span>
<span class="gi">+        for key, value in estimator.get_params().items()</span>
<span class="gi">+        if &quot;early_stopping&quot; in key</span>
<span class="gi">+    }</span>
<span class="gi">+    early_stopping_enabled = any(value is True for value in params.values())</span>
<span class="gi">+</span>
<span class="gi">+    for invalid_name, additional_message in invalid_names:</span>
<span class="gi">+        X_bad = pd.DataFrame(X, columns=invalid_name)</span>
<span class="gi">+</span>
<span class="gi">+        for name, method in check_methods:</span>
<span class="gi">+            if sklearn_version &gt;= parse_version(&quot;1.2&quot;):</span>
<span class="gi">+                expected_msg = re.escape(</span>
<span class="gi">+                    &quot;The feature names should match those that were passed during fit.&quot;</span>
<span class="gi">+                    f&quot;\n{additional_message}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                with raises(</span>
<span class="gi">+                    ValueError, match=expected_msg, err_msg=f&quot;{name} did not raise&quot;</span>
<span class="gi">+                ):</span>
<span class="gi">+                    method(X_bad)</span>
<span class="gi">+            else:</span>
<span class="gi">+                expected_msg = re.escape(</span>
<span class="gi">+                    &quot;The feature names should match those that were passed &quot;</span>
<span class="gi">+                    &quot;during fit. Starting version 1.2, an error will be raised.\n&quot;</span>
<span class="gi">+                    f&quot;{additional_message}&quot;</span>
<span class="gi">+                )</span>
<span class="gi">+                with warnings.catch_warnings():</span>
<span class="gi">+                    warnings.filterwarnings(</span>
<span class="gi">+                        &quot;error&quot;,</span>
<span class="gi">+                        category=FutureWarning,</span>
<span class="gi">+                        module=&quot;sklearn&quot;,</span>
<span class="gi">+                    )</span>
<span class="gi">+                    with raises(</span>
<span class="gi">+                        FutureWarning,</span>
<span class="gi">+                        match=expected_msg,</span>
<span class="gi">+                        err_msg=f&quot;{name} did not raise&quot;,</span>
<span class="gi">+                    ):</span>
<span class="gi">+                        method(X_bad)</span>
<span class="gi">+</span>
<span class="gi">+        # partial_fit checks on second call</span>
<span class="gi">+        # Do not call partial fit if early_stopping is on</span>
<span class="gi">+        if not hasattr(estimator, &quot;partial_fit&quot;) or early_stopping_enabled:</span>
<span class="gi">+            continue</span>
<span class="gi">+</span>
<span class="gi">+        estimator = clone(estimator_orig)</span>
<span class="gi">+        if is_classifier(estimator):</span>
<span class="gi">+            classes = np.unique(y)</span>
<span class="gi">+            estimator.partial_fit(X, y, classes=classes)</span>
<span class="gi">+        else:</span>
<span class="gi">+            estimator.partial_fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+        with raises(ValueError, match=expected_msg):</span>
<span class="gi">+            estimator.partial_fit(X_bad, y)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_sampler_get_feature_names_out(name, sampler_orig):</span>
<span class="gi">+    tags = sampler_orig._get_tags()</span>
<span class="gi">+    if &quot;2darray&quot; not in tags[&quot;X_types&quot;] or tags[&quot;no_validation&quot;]:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_blobs(</span>
<span class="gi">+        n_samples=30,</span>
<span class="gi">+        centers=[[0, 0, 0], [1, 1, 1]],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        cluster_std=0.1,</span>
<span class="gi">+    )</span>
<span class="gi">+    X = StandardScaler().fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X = _enforce_estimator_tags_x(sampler, X)</span>
<span class="gi">+</span>
<span class="gi">+    n_features = X.shape[1]</span>
<span class="gi">+    set_random_state(sampler)</span>
<span class="gi">+</span>
<span class="gi">+    y_ = y</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(X, y=y_)</span>
<span class="gi">+    input_features = [f&quot;feature{i}&quot; for i in range(n_features)]</span>
<span class="gi">+</span>
<span class="gi">+    # input_features names is not the same length as n_features_in_</span>
<span class="gi">+    with raises(ValueError, match=&quot;input_features should have length equal&quot;):</span>
<span class="gi">+        sampler.get_feature_names_out(input_features[::2])</span>
<span class="gi">+</span>
<span class="gi">+    feature_names_out = sampler.get_feature_names_out(input_features)</span>
<span class="gi">+    assert feature_names_out is not None</span>
<span class="gi">+    assert isinstance(feature_names_out, np.ndarray)</span>
<span class="gi">+    assert feature_names_out.dtype == object</span>
<span class="gi">+    assert all(isinstance(name, str) for name in feature_names_out)</span>
<span class="gi">+</span>
<span class="gi">+    n_features_out = X_res.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+    assert (</span>
<span class="gi">+        len(feature_names_out) == n_features_out</span>
<span class="gi">+    ), f&quot;Expected {n_features_out} feature names, got {len(feature_names_out)}&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def check_sampler_get_feature_names_out_pandas(name, sampler_orig):</span>
<span class="gi">+    try:</span>
<span class="gi">+        import pandas as pd</span>
<span class="gi">+    except ImportError:</span>
<span class="gi">+        raise SkipTest(</span>
<span class="gi">+            &quot;pandas is not installed: not checking column name consistency for pandas&quot;</span>
<span class="gi">+        )</span>
<span class="gi">+</span>
<span class="gi">+    tags = sampler_orig._get_tags()</span>
<span class="gi">+    if &quot;2darray&quot; not in tags[&quot;X_types&quot;] or tags[&quot;no_validation&quot;]:</span>
<span class="gi">+        return</span>
<span class="gi">+</span>
<span class="gi">+    X, y = make_blobs(</span>
<span class="gi">+        n_samples=30,</span>
<span class="gi">+        centers=[[0, 0, 0], [1, 1, 1]],</span>
<span class="gi">+        random_state=0,</span>
<span class="gi">+        n_features=2,</span>
<span class="gi">+        cluster_std=0.1,</span>
<span class="gi">+    )</span>
<span class="gi">+    X = StandardScaler().fit_transform(X)</span>
<span class="gi">+</span>
<span class="gi">+    sampler = clone(sampler_orig)</span>
<span class="gi">+    X = _enforce_estimator_tags_x(sampler, X)</span>
<span class="gi">+</span>
<span class="gi">+    n_features = X.shape[1]</span>
<span class="gi">+    set_random_state(sampler)</span>
<span class="gi">+</span>
<span class="gi">+    y_ = y</span>
<span class="gi">+    feature_names_in = [f&quot;col{i}&quot; for i in range(n_features)]</span>
<span class="gi">+    df = pd.DataFrame(X, columns=feature_names_in)</span>
<span class="gi">+    X_res, y_res = sampler.fit_resample(df, y=y_)</span>
<span class="gi">+</span>
<span class="gi">+    # error is raised when `input_features` do not match feature_names_in</span>
<span class="gi">+    invalid_feature_names = [f&quot;bad{i}&quot; for i in range(n_features)]</span>
<span class="gi">+    with raises(ValueError, match=&quot;input_features is not equal to feature_names_in_&quot;):</span>
<span class="gi">+        sampler.get_feature_names_out(invalid_feature_names)</span>
<span class="gi">+</span>
<span class="gi">+    feature_names_out_default = sampler.get_feature_names_out()</span>
<span class="gi">+    feature_names_in_explicit_names = sampler.get_feature_names_out(feature_names_in)</span>
<span class="gi">+    assert_array_equal(feature_names_out_default, feature_names_in_explicit_names)</span>
<span class="gi">+</span>
<span class="gi">+    n_features_out = X_res.shape[1]</span>
<span class="gi">+</span>
<span class="gi">+    assert (</span>
<span class="gi">+        len(feature_names_out_default) == n_features_out</span>
<span class="gi">+    ), f&quot;Expected {n_features_out} feature names, got {len(feature_names_out_default)}&quot;</span>
<span class="gh">diff --git a/imblearn/utils/fixes.py b/imblearn/utils/fixes.py</span>
<span class="gh">index 801067f..023d8a1 100644</span>
<span class="gd">--- a/imblearn/utils/fixes.py</span>
<span class="gi">+++ b/imblearn/utils/fixes.py</span>
<span class="gu">@@ -6,23 +6,39 @@ which the fix is no longer needed.</span>
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>import functools
<span class="w"> </span>import sys
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import scipy
<span class="w"> </span>import scipy.stats
<span class="w"> </span>import sklearn
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>from .._config import config_context, get_config
<span class="gi">+</span>
<span class="w"> </span>sp_version = parse_version(scipy.__version__)
<span class="w"> </span>sklearn_version = parse_version(sklearn.__version__)
<span class="gd">-if sklearn_version &gt;= parse_version(&#39;1.1&#39;):</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# TODO: Remove when SciPy 1.9 is the minimum supported version</span>
<span class="gi">+def _mode(a, axis=0):</span>
<span class="gi">+    if sp_version &gt;= parse_version(&quot;1.9.0&quot;):</span>
<span class="gi">+        return scipy.stats.mode(a, axis=axis, keepdims=True)</span>
<span class="gi">+    return scipy.stats.mode(a, axis=axis)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# TODO: Remove when scikit-learn 1.1 is the minimum supported version</span>
<span class="gi">+if sklearn_version &gt;= parse_version(&quot;1.1&quot;):</span>
<span class="w"> </span>    from sklearn.utils.validation import _is_arraylike_not_scalar
<span class="w"> </span>else:
<span class="w"> </span>    from sklearn.utils.validation import _is_arraylike

<span class="w"> </span>    def _is_arraylike_not_scalar(array):
<span class="w"> </span>        &quot;&quot;&quot;Return True if array is array-like and not a scalar&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gd">-if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="gi">+        return _is_arraylike(array) and not np.isscalar(array)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+# TODO: remove when scikit-learn minimum version is 1.3</span>
<span class="gi">+if sklearn_version &lt; parse_version(&quot;1.3&quot;):</span>

<span class="w"> </span>    def _fit_context(*, prefer_skip_nested_validation):
<span class="w"> </span>        &quot;&quot;&quot;Decorator to run the fit methods of estimators within context managers.
<span class="gu">@@ -47,10 +63,36 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        decorated_fit : method
<span class="w"> </span>            The decorated fit method.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+</span>
<span class="gi">+        def decorator(fit_method):</span>
<span class="gi">+            @functools.wraps(fit_method)</span>
<span class="gi">+            def wrapper(estimator, *args, **kwargs):</span>
<span class="gi">+                global_skip_validation = get_config()[&quot;skip_parameter_validation&quot;]</span>
<span class="gi">+</span>
<span class="gi">+                # we don&#39;t want to validate again for each call to partial_fit</span>
<span class="gi">+                partial_fit_and_fitted = (</span>
<span class="gi">+                    fit_method.__name__ == &quot;partial_fit&quot; and _is_fitted(estimator)</span>
<span class="gi">+                )</span>
<span class="gi">+</span>
<span class="gi">+                if not global_skip_validation and not partial_fit_and_fitted:</span>
<span class="gi">+                    estimator._validate_params()</span>
<span class="gi">+</span>
<span class="gi">+                with config_context(</span>
<span class="gi">+                    skip_parameter_validation=(</span>
<span class="gi">+                        prefer_skip_nested_validation or global_skip_validation</span>
<span class="gi">+                    )</span>
<span class="gi">+                ):</span>
<span class="gi">+                    return fit_method(estimator, *args, **kwargs)</span>
<span class="gi">+</span>
<span class="gi">+            return wrapper</span>
<span class="gi">+</span>
<span class="gi">+        return decorator</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="gd">-    from sklearn.base import _fit_context</span>
<span class="gd">-if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="gi">+    from sklearn.base import _fit_context  # type: ignore[no-redef] # noqa</span>
<span class="gi">+</span>
<span class="gi">+# TODO: remove when scikit-learn minimum version is 1.3</span>
<span class="gi">+if sklearn_version &lt; parse_version(&quot;1.3&quot;):</span>

<span class="w"> </span>    def _is_fitted(estimator, attributes=None, all_or_any=all):
<span class="w"> </span>        &quot;&quot;&quot;Determine if an estimator is fitted
<span class="gu">@@ -76,13 +118,33 @@ if sklearn_version &lt; parse_version(&#39;1.3&#39;):</span>
<span class="w"> </span>        fitted : bool
<span class="w"> </span>            Whether the estimator is fitted.
<span class="w"> </span>        &quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if attributes is not None:</span>
<span class="gi">+            if not isinstance(attributes, (list, tuple)):</span>
<span class="gi">+                attributes = [attributes]</span>
<span class="gi">+            return all_or_any([hasattr(estimator, attr) for attr in attributes])</span>
<span class="gi">+</span>
<span class="gi">+        if hasattr(estimator, &quot;__sklearn_is_fitted__&quot;):</span>
<span class="gi">+            return estimator.__sklearn_is_fitted__()</span>
<span class="gi">+</span>
<span class="gi">+        fitted_attrs = [</span>
<span class="gi">+            v for v in vars(estimator) if v.endswith(&quot;_&quot;) and not v.startswith(&quot;__&quot;)</span>
<span class="gi">+        ]</span>
<span class="gi">+        return len(fitted_attrs) &gt; 0</span>
<span class="gi">+</span>
<span class="w"> </span>else:
<span class="gd">-    from sklearn.utils.validation import _is_fitted</span>
<span class="gi">+    from sklearn.utils.validation import _is_fitted  # type: ignore[no-redef]</span>
<span class="gi">+</span>
<span class="w"> </span>try:
<span class="w"> </span>    from sklearn.utils.validation import _is_pandas_df
<span class="w"> </span>except ImportError:

<span class="w"> </span>    def _is_pandas_df(X):
<span class="w"> </span>        &quot;&quot;&quot;Return True if the X is a pandas dataframe.&quot;&quot;&quot;
<span class="gd">-        pass</span>
<span class="gi">+        if hasattr(X, &quot;columns&quot;) and hasattr(X, &quot;iloc&quot;):</span>
<span class="gi">+            # Likely a pandas DataFrame, we explicitly check the type to confirm.</span>
<span class="gi">+            try:</span>
<span class="gi">+                pd = sys.modules[&quot;pandas&quot;]</span>
<span class="gi">+            except KeyError:</span>
<span class="gi">+                return False</span>
<span class="gi">+            return isinstance(X, pd.DataFrame)</span>
<span class="gi">+        return False</span>
<span class="gh">diff --git a/imblearn/utils/testing.py b/imblearn/utils/testing.py</span>
<span class="gh">index aa344b1..8c19d61 100644</span>
<span class="gd">--- a/imblearn/utils/testing.py</span>
<span class="gi">+++ b/imblearn/utils/testing.py</span>
<span class="gu">@@ -1,9 +1,15 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test utilities.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Adapted from scikit-learn</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import inspect
<span class="w"> </span>import pkgutil
<span class="w"> </span>from importlib import import_module
<span class="w"> </span>from operator import itemgetter
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>from scipy import sparse
<span class="w"> </span>from sklearn.base import BaseEstimator
<span class="gu">@@ -11,7 +17,9 @@ from sklearn.neighbors import KDTree</span>
<span class="w"> </span>from sklearn.utils._testing import ignore_warnings


<span class="gd">-def all_estimators(type_filter=None):</span>
<span class="gi">+def all_estimators(</span>
<span class="gi">+    type_filter=None,</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Get a list of all estimators from imblearn.

<span class="w"> </span>    This function crawls the module and gets all classes that inherit
<span class="gu">@@ -35,7 +43,73 @@ def all_estimators(type_filter=None):</span>
<span class="w"> </span>        List of (name, class), where ``name`` is the class name as string
<span class="w"> </span>        and ``class`` is the actual type of the class.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    from ..base import SamplerMixin</span>
<span class="gi">+</span>
<span class="gi">+    def is_abstract(c):</span>
<span class="gi">+        if not (hasattr(c, &quot;__abstractmethods__&quot;)):</span>
<span class="gi">+            return False</span>
<span class="gi">+        if not len(c.__abstractmethods__):</span>
<span class="gi">+            return False</span>
<span class="gi">+        return True</span>
<span class="gi">+</span>
<span class="gi">+    all_classes = []</span>
<span class="gi">+    modules_to_ignore = {&quot;tests&quot;}</span>
<span class="gi">+    root = str(Path(__file__).parent.parent)</span>
<span class="gi">+    # Ignore deprecation warnings triggered at import time and from walking</span>
<span class="gi">+    # packages</span>
<span class="gi">+    with ignore_warnings(category=FutureWarning):</span>
<span class="gi">+        for importer, modname, ispkg in pkgutil.walk_packages(</span>
<span class="gi">+            path=[root], prefix=&quot;imblearn.&quot;</span>
<span class="gi">+        ):</span>
<span class="gi">+            mod_parts = modname.split(&quot;.&quot;)</span>
<span class="gi">+            if any(part in modules_to_ignore for part in mod_parts) or &quot;._&quot; in modname:</span>
<span class="gi">+                continue</span>
<span class="gi">+            module = import_module(modname)</span>
<span class="gi">+            classes = inspect.getmembers(module, inspect.isclass)</span>
<span class="gi">+            classes = [</span>
<span class="gi">+                (name, est_cls) for name, est_cls in classes if not name.startswith(&quot;_&quot;)</span>
<span class="gi">+            ]</span>
<span class="gi">+</span>
<span class="gi">+            all_classes.extend(classes)</span>
<span class="gi">+</span>
<span class="gi">+    all_classes = set(all_classes)</span>
<span class="gi">+</span>
<span class="gi">+    estimators = [</span>
<span class="gi">+        c</span>
<span class="gi">+        for c in all_classes</span>
<span class="gi">+        if (issubclass(c[1], BaseEstimator) and c[0] != &quot;BaseEstimator&quot;)</span>
<span class="gi">+    ]</span>
<span class="gi">+    # get rid of abstract base classes</span>
<span class="gi">+    estimators = [c for c in estimators if not is_abstract(c[1])]</span>
<span class="gi">+</span>
<span class="gi">+    # get rid of sklearn estimators which have been imported in some classes</span>
<span class="gi">+    estimators = [c for c in estimators if &quot;sklearn&quot; not in c[1].__module__]</span>
<span class="gi">+</span>
<span class="gi">+    if type_filter is not None:</span>
<span class="gi">+        if not isinstance(type_filter, list):</span>
<span class="gi">+            type_filter = [type_filter]</span>
<span class="gi">+        else:</span>
<span class="gi">+            type_filter = list(type_filter)  # copy</span>
<span class="gi">+        filtered_estimators = []</span>
<span class="gi">+        filters = {&quot;sampler&quot;: SamplerMixin}</span>
<span class="gi">+        for name, mixin in filters.items():</span>
<span class="gi">+            if name in type_filter:</span>
<span class="gi">+                type_filter.remove(name)</span>
<span class="gi">+                filtered_estimators.extend(</span>
<span class="gi">+                    [est for est in estimators if issubclass(est[1], mixin)]</span>
<span class="gi">+                )</span>
<span class="gi">+        estimators = filtered_estimators</span>
<span class="gi">+        if type_filter:</span>
<span class="gi">+            raise ValueError(</span>
<span class="gi">+                &quot;Parameter type_filter must be &#39;sampler&#39; or &quot;</span>
<span class="gi">+                &quot;None, got&quot;</span>
<span class="gi">+                &quot; %s.&quot; % repr(type_filter)</span>
<span class="gi">+            )</span>
<span class="gi">+</span>
<span class="gi">+    # drop duplicates, sort for reproducibility</span>
<span class="gi">+    # itemgetter is used to ensure the sort does not extend to the 2nd item of</span>
<span class="gi">+    # the tuple</span>
<span class="gi">+    return sorted(set(estimators), key=itemgetter(0))</span>


<span class="w"> </span>class _CustomNearestNeighbors(BaseEstimator):
<span class="gu">@@ -44,11 +118,24 @@ class _CustomNearestNeighbors(BaseEstimator):</span>
<span class="w"> </span>    `kneighbors_graph` is ignored and `metric` does not have any impact.
<span class="w"> </span>    &quot;&quot;&quot;

<span class="gd">-    def __init__(self, n_neighbors=1, metric=&#39;euclidean&#39;):</span>
<span class="gi">+    def __init__(self, n_neighbors=1, metric=&quot;euclidean&quot;):</span>
<span class="w"> </span>        self.n_neighbors = n_neighbors
<span class="w"> </span>        self.metric = metric

<span class="gd">-    def kneighbors_graph(X=None, n_neighbors=None, mode=&#39;connectivity&#39;):</span>
<span class="gi">+    def fit(self, X, y=None):</span>
<span class="gi">+        X = X.toarray() if sparse.issparse(X) else X</span>
<span class="gi">+        self._kd_tree = KDTree(X)</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def kneighbors(self, X, n_neighbors=None, return_distance=True):</span>
<span class="gi">+        n_neighbors = n_neighbors if n_neighbors is not None else self.n_neighbors</span>
<span class="gi">+        X = X.toarray() if sparse.issparse(X) else X</span>
<span class="gi">+        distances, indices = self._kd_tree.query(X, k=n_neighbors)</span>
<span class="gi">+        if return_distance:</span>
<span class="gi">+            return distances, indices</span>
<span class="gi">+        return indices</span>
<span class="gi">+</span>
<span class="gi">+    def kneighbors_graph(X=None, n_neighbors=None, mode=&quot;connectivity&quot;):</span>
<span class="w"> </span>        &quot;&quot;&quot;This method is not used within imblearn but it is required for
<span class="w"> </span>        duck-typing.&quot;&quot;&quot;
<span class="w"> </span>        pass
<span class="gu">@@ -60,3 +147,11 @@ class _CustomClusterer(BaseEstimator):</span>
<span class="w"> </span>    def __init__(self, n_clusters=1, expose_cluster_centers=True):
<span class="w"> </span>        self.n_clusters = n_clusters
<span class="w"> </span>        self.expose_cluster_centers = expose_cluster_centers
<span class="gi">+</span>
<span class="gi">+    def fit(self, X, y=None):</span>
<span class="gi">+        if self.expose_cluster_centers:</span>
<span class="gi">+            self.cluster_centers_ = np.random.randn(self.n_clusters, X.shape[1])</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def predict(self, X):</span>
<span class="gi">+        return np.zeros(len(X), dtype=int)</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_deprecation.py b/imblearn/utils/tests/test_deprecation.py</span>
<span class="gh">index 2411624..f7e084d 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_deprecation.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_deprecation.py</span>
<span class="gu">@@ -1,10 +1,21 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the deprecation helper&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="gi">+</span>
<span class="w"> </span>from imblearn.utils.deprecation import deprecate_parameter


<span class="w"> </span>class Sampler:
<span class="gd">-</span>
<span class="w"> </span>    def __init__(self):
<span class="gd">-        self.a = &#39;something&#39;</span>
<span class="gd">-        self.b = &#39;something&#39;</span>
<span class="gi">+        self.a = &quot;something&quot;</span>
<span class="gi">+        self.b = &quot;something&quot;</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_deprecate_parameter():</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;is deprecated from&quot;):</span>
<span class="gi">+        deprecate_parameter(Sampler(), &quot;0.2&quot;, &quot;a&quot;)</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;Use &#39;b&#39; instead.&quot;):</span>
<span class="gi">+        deprecate_parameter(Sampler(), &quot;0.2&quot;, &quot;a&quot;, &quot;b&quot;)</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_docstring.py b/imblearn/utils/tests/test_docstring.py</span>
<span class="gh">index f377d75..4a07536 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_docstring.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_docstring.py</span>
<span class="gu">@@ -1,7 +1,13 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test utilities for docstring.&quot;&quot;&quot;
<span class="gi">+</span>
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import sys
<span class="w"> </span>import textwrap
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="gi">+</span>
<span class="w"> </span>from imblearn.utils import Substitution
<span class="w"> </span>from imblearn.utils._docstring import _n_jobs_docstring, _random_state_docstring

<span class="gu">@@ -11,7 +17,7 @@ def _dedent_docstring(docstring):</span>

<span class="w"> </span>    xref: https://github.com/python/cpython/issues/81283
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return &quot;\n&quot;.join([textwrap.dedent(line) for line in docstring.split(&quot;\n&quot;)])</span>


<span class="w"> </span>func_docstring = &quot;&quot;&quot;A function.
<span class="gu">@@ -33,7 +39,7 @@ def func(param_1, param_2):</span>

<span class="w"> </span>    {param_2}
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    return param_1, param_2</span>


<span class="w"> </span>cls_docstring = &quot;&quot;&quot;A class.
<span class="gu">@@ -66,10 +72,28 @@ if sys.version_info &gt;= (3, 13):</span>
<span class="w"> </span>    cls_docstring = _dedent_docstring(cls_docstring)


<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;obj, obj_docstring&quot;, [(func, func_docstring), (cls, cls_docstring)]</span>
<span class="gi">+)</span>
<span class="gi">+def test_docstring_inject(obj, obj_docstring):</span>
<span class="gi">+    obj_injected_docstring = Substitution(param_1=&quot;xxx&quot;, param_2=&quot;yyy&quot;)(obj)</span>
<span class="gi">+    assert obj_injected_docstring.__doc__ == obj_docstring</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_docstring_template():</span>
<span class="gi">+    assert &quot;random_state&quot; in _random_state_docstring</span>
<span class="gi">+    assert &quot;n_jobs&quot; in _n_jobs_docstring</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_docstring_with_python_OO():
<span class="w"> </span>    &quot;&quot;&quot;Check that we don&#39;t raise a warning if the code is executed with -OO.

<span class="w"> </span>    Non-regression test for:
<span class="w"> </span>    https://github.com/scikit-learn-contrib/imbalanced-learn/issues/945
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    instance = cls(param_1=&quot;xxx&quot;, param_2=&quot;yyy&quot;)</span>
<span class="gi">+    instance.__doc__ = None  # simulate -OO</span>
<span class="gi">+</span>
<span class="gi">+    instance = Substitution(param_1=&quot;xxx&quot;, param_2=&quot;yyy&quot;)(instance)</span>
<span class="gi">+</span>
<span class="gi">+    assert instance.__doc__ is None</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_estimator_checks.py b/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gh">index e93b1c3..ca704f2 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_estimator_checks.py</span>
<span class="gu">@@ -2,42 +2,121 @@ import numpy as np</span>
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.base import BaseEstimator
<span class="w"> </span>from sklearn.utils.multiclass import check_classification_targets
<span class="gi">+</span>
<span class="w"> </span>from imblearn.base import BaseSampler
<span class="w"> </span>from imblearn.over_sampling.base import BaseOverSampler
<span class="w"> </span>from imblearn.utils import check_target_type as target_check
<span class="gd">-from imblearn.utils.estimator_checks import check_samplers_fit, check_samplers_nan, check_samplers_one_label, check_samplers_preserve_dtype, check_samplers_sparse, check_samplers_string, check_target_type</span>
<span class="gi">+from imblearn.utils.estimator_checks import (</span>
<span class="gi">+    check_samplers_fit,</span>
<span class="gi">+    check_samplers_nan,</span>
<span class="gi">+    check_samplers_one_label,</span>
<span class="gi">+    check_samplers_preserve_dtype,</span>
<span class="gi">+    check_samplers_sparse,</span>
<span class="gi">+    check_samplers_string,</span>
<span class="gi">+    check_target_type,</span>
<span class="gi">+)</span>


<span class="w"> </span>class BaseBadSampler(BaseEstimator):
<span class="w"> </span>    &quot;&quot;&quot;Sampler without inputs checking.&quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;bypass&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_type = &quot;bypass&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def fit(self, X, y):</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>
<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        check_classification_targets(y)</span>
<span class="gi">+        self.fit(X, y)</span>
<span class="gi">+        return X, y</span>


<span class="w"> </span>class SamplerSingleClass(BaseSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler that would sample even with a single class.&quot;&quot;&quot;
<span class="gd">-    _sampling_type = &#39;bypass&#39;</span>
<span class="gi">+</span>
<span class="gi">+    _sampling_type = &quot;bypass&quot;</span>
<span class="gi">+</span>
<span class="gi">+    def fit_resample(self, X, y):</span>
<span class="gi">+        return self._fit_resample(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        return X, y</span>


<span class="w"> </span>class NotFittedSampler(BaseBadSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler without target checking.&quot;&quot;&quot;

<span class="gi">+    def fit(self, X, y):</span>
<span class="gi">+        X, y = self._validate_data(X, y)</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>

<span class="w"> </span>class NoAcceptingSparseSampler(BaseBadSampler):
<span class="w"> </span>    &quot;&quot;&quot;Sampler which does not accept sparse matrix.&quot;&quot;&quot;

<span class="gi">+    def fit(self, X, y):</span>
<span class="gi">+        X, y = self._validate_data(X, y)</span>
<span class="gi">+        self.sampling_strategy_ = &quot;sampling_strategy_&quot;</span>
<span class="gi">+        return self</span>
<span class="gi">+</span>

<span class="w"> </span>class NotPreservingDtypeSampler(BaseSampler):
<span class="gd">-    _sampling_type = &#39;bypass&#39;</span>
<span class="gd">-    _parameter_constraints: dict = {&#39;sampling_strategy&#39;: &#39;no_validation&#39;}</span>
<span class="gi">+    _sampling_type = &quot;bypass&quot;</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {&quot;sampling_strategy&quot;: &quot;no_validation&quot;}</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        return X.astype(np.float64), y.astype(np.int64)</span>


<span class="w"> </span>class IndicesSampler(BaseOverSampler):
<span class="gd">-    pass</span>
<span class="gi">+    def _check_X_y(self, X, y):</span>
<span class="gi">+        y, binarize_y = target_check(y, indicate_one_vs_all=True)</span>
<span class="gi">+        X, y = self._validate_data(</span>
<span class="gi">+            X,</span>
<span class="gi">+            y,</span>
<span class="gi">+            reset=True,</span>
<span class="gi">+            dtype=None,</span>
<span class="gi">+            force_all_finite=False,</span>
<span class="gi">+        )</span>
<span class="gi">+        return X, y, binarize_y</span>
<span class="gi">+</span>
<span class="gi">+    def _fit_resample(self, X, y):</span>
<span class="gi">+        n_max_count_class = np.bincount(y).max()</span>
<span class="gi">+        indices = np.random.choice(np.arange(X.shape[0]), size=n_max_count_class * 2)</span>
<span class="gi">+        return X[indices], y[indices]</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_check_samplers_string():</span>
<span class="gi">+    sampler = IndicesSampler()</span>
<span class="gi">+    check_samplers_string(sampler.__class__.__name__, sampler)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_check_samplers_nan():</span>
<span class="gi">+    sampler = IndicesSampler()</span>
<span class="gi">+    check_samplers_nan(sampler.__class__.__name__, sampler)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+mapping_estimator_error = {</span>
<span class="gi">+    &quot;BaseBadSampler&quot;: (AssertionError, &quot;ValueError not raised by fit&quot;),</span>
<span class="gi">+    &quot;SamplerSingleClass&quot;: (AssertionError, &quot;Sampler can&#39;t balance when only&quot;),</span>
<span class="gi">+    &quot;NotFittedSampler&quot;: (AssertionError, &quot;No fitted attribute&quot;),</span>
<span class="gi">+    &quot;NoAcceptingSparseSampler&quot;: (TypeError, &quot;dense data is required&quot;),</span>
<span class="gi">+    &quot;NotPreservingDtypeSampler&quot;: (AssertionError, &quot;X dtype is not preserved&quot;),</span>
<span class="gi">+}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _test_single_check(Estimator, check):</span>
<span class="gi">+    estimator = Estimator()</span>
<span class="gi">+    name = estimator.__class__.__name__</span>
<span class="gi">+    err_type, err_msg = mapping_estimator_error[name]</span>
<span class="gi">+    with pytest.raises(err_type, match=err_msg):</span>
<span class="gi">+        check(name, estimator)</span>


<span class="gd">-mapping_estimator_error = {&#39;BaseBadSampler&#39;: (AssertionError,</span>
<span class="gd">-    &#39;ValueError not raised by fit&#39;), &#39;SamplerSingleClass&#39;: (AssertionError,</span>
<span class="gd">-    &quot;Sampler can&#39;t balance when only&quot;), &#39;NotFittedSampler&#39;: (AssertionError,</span>
<span class="gd">-    &#39;No fitted attribute&#39;), &#39;NoAcceptingSparseSampler&#39;: (TypeError,</span>
<span class="gd">-    &#39;dense data is required&#39;), &#39;NotPreservingDtypeSampler&#39;: (AssertionError,</span>
<span class="gd">-    &#39;X dtype is not preserved&#39;)}</span>
<span class="gi">+def test_all_checks():</span>
<span class="gi">+    _test_single_check(BaseBadSampler, check_target_type)</span>
<span class="gi">+    _test_single_check(SamplerSingleClass, check_samplers_one_label)</span>
<span class="gi">+    _test_single_check(NotFittedSampler, check_samplers_fit)</span>
<span class="gi">+    _test_single_check(NoAcceptingSparseSampler, check_samplers_sparse)</span>
<span class="gi">+    _test_single_check(NotPreservingDtypeSampler, check_samplers_preserve_dtype)</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_min_dependencies.py b/imblearn/utils/tests/test_min_dependencies.py</span>
<span class="gh">index d25700b..cd53703 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_min_dependencies.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_min_dependencies.py</span>
<span class="gu">@@ -3,7 +3,49 @@ import os</span>
<span class="w"> </span>import platform
<span class="w"> </span>import re
<span class="w"> </span>from pathlib import Path
<span class="gi">+</span>
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.utils.fixes import parse_version
<span class="gi">+</span>
<span class="w"> </span>import imblearn
<span class="w"> </span>from imblearn._min_dependencies import dependent_packages
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.skipif(</span>
<span class="gi">+    platform.system() == &quot;Windows&quot;, reason=&quot;This test is enough on unix system&quot;</span>
<span class="gi">+)</span>
<span class="gi">+def test_min_dependencies_readme():</span>
<span class="gi">+    # Test that the minimum dependencies in the README.rst file are</span>
<span class="gi">+    # consistent with the minimum dependencies defined at the file:</span>
<span class="gi">+    # imblearn/_min_dependencies.py</span>
<span class="gi">+</span>
<span class="gi">+    pattern = re.compile(</span>
<span class="gi">+        r&quot;(\.\. \|)&quot;</span>
<span class="gi">+        + r&quot;(([A-Za-z]+\-?)+)&quot;</span>
<span class="gi">+        + r&quot;(MinVersion\| replace::)&quot;</span>
<span class="gi">+        + r&quot;( [0-9]+\.[0-9]+(\.[0-9]+)?)&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    readme_path = Path(imblearn.__path__[0]).parents[0]</span>
<span class="gi">+    readme_file = readme_path / &quot;README.rst&quot;</span>
<span class="gi">+</span>
<span class="gi">+    if not os.path.exists(readme_file):</span>
<span class="gi">+        # Skip the test if the README.rst file is not available.</span>
<span class="gi">+        # For instance, when installing scikit-learn from wheels</span>
<span class="gi">+        pytest.skip(&quot;The README.rst file is not available.&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    with readme_file.open(&quot;r&quot;) as f:</span>
<span class="gi">+        for line in f:</span>
<span class="gi">+            matched = pattern.match(line)</span>
<span class="gi">+</span>
<span class="gi">+            if not matched:</span>
<span class="gi">+                continue</span>
<span class="gi">+</span>
<span class="gi">+            package, version = matched.group(2), matched.group(5)</span>
<span class="gi">+            package = package.lower()</span>
<span class="gi">+</span>
<span class="gi">+            if package in dependent_packages:</span>
<span class="gi">+                version = parse_version(version)</span>
<span class="gi">+                min_version = parse_version(dependent_packages[package][0])</span>
<span class="gi">+</span>
<span class="gi">+                assert version == min_version, f&quot;{package} has a mismatched version&quot;</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_param_validation.py b/imblearn/utils/tests/test_param_validation.py</span>
<span class="gh">index 8b0709d..38af664 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_param_validation.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_param_validation.py</span>
<span class="gu">@@ -2,61 +2,119 @@</span>
<span class="w"> </span>removed when we support scikit-learn &gt;= 1.2.
<span class="w"> </span>&quot;&quot;&quot;
<span class="w"> </span>from numbers import Integral, Real
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from scipy.sparse import csr_matrix
<span class="w"> </span>from sklearn.base import BaseEstimator
<span class="w"> </span>from sklearn.model_selection import LeaveOneOut
<span class="w"> </span>from sklearn.utils import deprecated
<span class="gi">+</span>
<span class="w"> </span>from imblearn._config import config_context, get_config
<span class="w"> </span>from imblearn.base import _ParamsValidationMixin
<span class="gd">-from imblearn.utils._param_validation import HasMethods, Hidden, Interval, InvalidParameterError, MissingValues, Options, RealNotInt, StrOptions, _ArrayLikes, _Booleans, _Callables, _CVObjects, _InstancesOf, _IterablesNotString, _NanConstraint, _NoneConstraint, _PandasNAConstraint, _RandomStates, _SparseMatrices, _VerboseHelper, generate_invalid_param_val, generate_valid_param, make_constraint, validate_params</span>
<span class="gi">+from imblearn.utils._param_validation import (</span>
<span class="gi">+    HasMethods,</span>
<span class="gi">+    Hidden,</span>
<span class="gi">+    Interval,</span>
<span class="gi">+    InvalidParameterError,</span>
<span class="gi">+    MissingValues,</span>
<span class="gi">+    Options,</span>
<span class="gi">+    RealNotInt,</span>
<span class="gi">+    StrOptions,</span>
<span class="gi">+    _ArrayLikes,</span>
<span class="gi">+    _Booleans,</span>
<span class="gi">+    _Callables,</span>
<span class="gi">+    _CVObjects,</span>
<span class="gi">+    _InstancesOf,</span>
<span class="gi">+    _IterablesNotString,</span>
<span class="gi">+    _NanConstraint,</span>
<span class="gi">+    _NoneConstraint,</span>
<span class="gi">+    _PandasNAConstraint,</span>
<span class="gi">+    _RandomStates,</span>
<span class="gi">+    _SparseMatrices,</span>
<span class="gi">+    _VerboseHelper,</span>
<span class="gi">+    generate_invalid_param_val,</span>
<span class="gi">+    generate_valid_param,</span>
<span class="gi">+    make_constraint,</span>
<span class="gi">+    validate_params,</span>
<span class="gi">+)</span>
<span class="w"> </span>from imblearn.utils.fixes import _fit_context


<span class="gd">-@validate_params({&#39;a&#39;: [Real], &#39;b&#39;: [Real], &#39;c&#39;: [Real], &#39;d&#39;: [Real]},</span>
<span class="gd">-    prefer_skip_nested_validation=True)</span>
<span class="gi">+# Some helpers for the tests</span>
<span class="gi">+@validate_params(</span>
<span class="gi">+    {&quot;a&quot;: [Real], &quot;b&quot;: [Real], &quot;c&quot;: [Real], &quot;d&quot;: [Real]},</span>
<span class="gi">+    prefer_skip_nested_validation=True,</span>
<span class="gi">+)</span>
<span class="w"> </span>def _func(a, b=0, *args, c, d=0, **kwargs):
<span class="w"> </span>    &quot;&quot;&quot;A function to test the validation of functions.&quot;&quot;&quot;
<span class="gd">-    pass</span>


<span class="w"> </span>class _Class:
<span class="w"> </span>    &quot;&quot;&quot;A class to test the _InstancesOf constraint and the validation of methods.&quot;&quot;&quot;

<span class="gd">-    @validate_params({&#39;a&#39;: [Real]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    @validate_params({&quot;a&quot;: [Real]}, prefer_skip_nested_validation=True)</span>
<span class="w"> </span>    def _method(self, a):
<span class="w"> </span>        &quot;&quot;&quot;A validated method&quot;&quot;&quot;
<span class="gd">-        pass</span>

<span class="w"> </span>    @deprecated()
<span class="gd">-    @validate_params({&#39;a&#39;: [Real]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    @validate_params({&quot;a&quot;: [Real]}, prefer_skip_nested_validation=True)</span>
<span class="w"> </span>    def _deprecated_method(self, a):
<span class="w"> </span>        &quot;&quot;&quot;A deprecated validated method&quot;&quot;&quot;
<span class="gd">-        pass</span>


<span class="w"> </span>class _Estimator(_ParamsValidationMixin, BaseEstimator):
<span class="w"> </span>    &quot;&quot;&quot;An estimator to test the validation of estimator parameters.&quot;&quot;&quot;
<span class="gd">-    _parameter_constraints: dict = {&#39;a&#39;: [Real]}</span>
<span class="gi">+</span>
<span class="gi">+    _parameter_constraints: dict = {&quot;a&quot;: [Real]}</span>

<span class="w"> </span>    def __init__(self, a):
<span class="w"> </span>        self.a = a

<span class="gi">+    @_fit_context(prefer_skip_nested_validation=True)</span>
<span class="gi">+    def fit(self, X=None, y=None):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>

<span class="gd">-@pytest.mark.parametrize(&#39;interval_type&#39;, [Integral, Real])</span>
<span class="gi">+@pytest.mark.parametrize(&quot;interval_type&quot;, [Integral, Real])</span>
<span class="w"> </span>def test_interval_range(interval_type):
<span class="w"> </span>    &quot;&quot;&quot;Check the range of values depending on closed.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    interval = Interval(interval_type, -2, 2, closed=&quot;left&quot;)</span>
<span class="gi">+    assert -2 in interval</span>
<span class="gi">+    assert 2 not in interval</span>
<span class="gi">+</span>
<span class="gi">+    interval = Interval(interval_type, -2, 2, closed=&quot;right&quot;)</span>
<span class="gi">+    assert -2 not in interval</span>
<span class="gi">+    assert 2 in interval</span>
<span class="gi">+</span>
<span class="gi">+    interval = Interval(interval_type, -2, 2, closed=&quot;both&quot;)</span>
<span class="gi">+    assert -2 in interval</span>
<span class="gi">+    assert 2 in interval</span>

<span class="gi">+    interval = Interval(interval_type, -2, 2, closed=&quot;neither&quot;)</span>
<span class="gi">+    assert -2 not in interval</span>
<span class="gi">+    assert 2 not in interval</span>

<span class="gd">-@pytest.mark.parametrize(&#39;interval_type&#39;, [Integral, Real])</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;interval_type&quot;, [Integral, Real])</span>
<span class="w"> </span>def test_interval_large_integers(interval_type):
<span class="w"> </span>    &quot;&quot;&quot;Check that Interval constraint work with large integers.

<span class="w"> </span>    non-regression test for #26648.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    interval = Interval(interval_type, 0, 2, closed=&quot;neither&quot;)</span>
<span class="gi">+    assert 2**65 not in interval</span>
<span class="gi">+    assert 2**128 not in interval</span>
<span class="gi">+    assert float(2**65) not in interval</span>
<span class="gi">+    assert float(2**128) not in interval</span>
<span class="gi">+</span>
<span class="gi">+    interval = Interval(interval_type, 0, 2**128, closed=&quot;neither&quot;)</span>
<span class="gi">+    assert 2**65 in interval</span>
<span class="gi">+    assert 2**128 not in interval</span>
<span class="gi">+    assert float(2**65) in interval</span>
<span class="gi">+    assert float(2**128) not in interval</span>
<span class="gi">+</span>
<span class="gi">+    assert 2**1024 not in interval</span>


<span class="w"> </span>def test_interval_inf_in_bounds():
<span class="gu">@@ -64,235 +122,565 @@ def test_interval_inf_in_bounds():</span>

<span class="w"> </span>    Only valid for real intervals.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    interval = Interval(Real, 0, None, closed=&quot;right&quot;)</span>
<span class="gi">+    assert np.inf in interval</span>

<span class="gi">+    interval = Interval(Real, None, 0, closed=&quot;left&quot;)</span>
<span class="gi">+    assert -np.inf in interval</span>

<span class="gd">-@pytest.mark.parametrize(&#39;interval&#39;, [Interval(Real, 0, 1, closed=&#39;left&#39;),</span>
<span class="gd">-    Interval(Real, None, None, closed=&#39;both&#39;)])</span>
<span class="gi">+    interval = Interval(Real, None, None, closed=&quot;neither&quot;)</span>
<span class="gi">+    assert np.inf not in interval</span>
<span class="gi">+    assert -np.inf not in interval</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;interval&quot;,</span>
<span class="gi">+    [Interval(Real, 0, 1, closed=&quot;left&quot;), Interval(Real, None, None, closed=&quot;both&quot;)],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_nan_not_in_interval(interval):
<span class="w"> </span>    &quot;&quot;&quot;Check that np.nan is not in any interval.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@pytest.mark.parametrize(&#39;params, error, match&#39;, [({&#39;type&#39;: Integral,</span>
<span class="gd">-    &#39;left&#39;: 1.0, &#39;right&#39;: 2, &#39;closed&#39;: &#39;both&#39;}, TypeError,</span>
<span class="gd">-    &#39;Expecting left to be an int for an interval over the integers&#39;), ({</span>
<span class="gd">-    &#39;type&#39;: Integral, &#39;left&#39;: 1, &#39;right&#39;: 2.0, &#39;closed&#39;: &#39;neither&#39;},</span>
<span class="gd">-    TypeError,</span>
<span class="gd">-    &#39;Expecting right to be an int for an interval over the integers&#39;), ({</span>
<span class="gd">-    &#39;type&#39;: Integral, &#39;left&#39;: None, &#39;right&#39;: 0, &#39;closed&#39;: &#39;left&#39;},</span>
<span class="gd">-    ValueError, &quot;left can&#39;t be None when closed == left&quot;), ({&#39;type&#39;:</span>
<span class="gd">-    Integral, &#39;left&#39;: 0, &#39;right&#39;: None, &#39;closed&#39;: &#39;right&#39;}, ValueError,</span>
<span class="gd">-    &quot;right can&#39;t be None when closed == right&quot;), ({&#39;type&#39;: Integral, &#39;left&#39;:</span>
<span class="gd">-    1, &#39;right&#39;: -1, &#39;closed&#39;: &#39;both&#39;}, ValueError,</span>
<span class="gd">-    &quot;right can&#39;t be less than left&quot;)])</span>
<span class="gi">+    assert np.nan not in interval</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;params, error, match&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;type&quot;: Integral, &quot;left&quot;: 1.0, &quot;right&quot;: 2, &quot;closed&quot;: &quot;both&quot;},</span>
<span class="gi">+            TypeError,</span>
<span class="gi">+            r&quot;Expecting left to be an int for an interval over the integers&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;type&quot;: Integral, &quot;left&quot;: 1, &quot;right&quot;: 2.0, &quot;closed&quot;: &quot;neither&quot;},</span>
<span class="gi">+            TypeError,</span>
<span class="gi">+            &quot;Expecting right to be an int for an interval over the integers&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;type&quot;: Integral, &quot;left&quot;: None, &quot;right&quot;: 0, &quot;closed&quot;: &quot;left&quot;},</span>
<span class="gi">+            ValueError,</span>
<span class="gi">+            r&quot;left can&#39;t be None when closed == left&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;type&quot;: Integral, &quot;left&quot;: 0, &quot;right&quot;: None, &quot;closed&quot;: &quot;right&quot;},</span>
<span class="gi">+            ValueError,</span>
<span class="gi">+            r&quot;right can&#39;t be None when closed == right&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {&quot;type&quot;: Integral, &quot;left&quot;: 1, &quot;right&quot;: -1, &quot;closed&quot;: &quot;both&quot;},</span>
<span class="gi">+            ValueError,</span>
<span class="gi">+            r&quot;right can&#39;t be less than left&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_interval_errors(params, error, match):
<span class="w"> </span>    &quot;&quot;&quot;Check that informative errors are raised for invalid combination of parameters&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with pytest.raises(error, match=match):</span>
<span class="gi">+        Interval(**params)</span>


<span class="w"> </span>def test_stroptions():
<span class="w"> </span>    &quot;&quot;&quot;Sanity check for the StrOptions constraint&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    options = StrOptions({&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}, deprecated={&quot;c&quot;})</span>
<span class="gi">+    assert options.is_satisfied_by(&quot;a&quot;)</span>
<span class="gi">+    assert options.is_satisfied_by(&quot;c&quot;)</span>
<span class="gi">+    assert not options.is_satisfied_by(&quot;d&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    assert &quot;&#39;c&#39; (deprecated)&quot; in str(options)</span>


<span class="w"> </span>def test_options():
<span class="w"> </span>    &quot;&quot;&quot;Sanity check for the Options constraint&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    options = Options(Real, {-0.5, 0.5, np.inf}, deprecated={-0.5})</span>
<span class="gi">+    assert options.is_satisfied_by(-0.5)</span>
<span class="gi">+    assert options.is_satisfied_by(np.inf)</span>
<span class="gi">+    assert not options.is_satisfied_by(1.23)</span>
<span class="gi">+</span>
<span class="gi">+    assert &quot;-0.5 (deprecated)&quot; in str(options)</span>


<span class="gd">-@pytest.mark.parametrize(&#39;type, expected_type_name&#39;, [(int, &#39;int&#39;), (</span>
<span class="gd">-    Integral, &#39;int&#39;), (Real, &#39;float&#39;), (np.ndarray, &#39;numpy.ndarray&#39;)])</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;type, expected_type_name&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (int, &quot;int&quot;),</span>
<span class="gi">+        (Integral, &quot;int&quot;),</span>
<span class="gi">+        (Real, &quot;float&quot;),</span>
<span class="gi">+        (np.ndarray, &quot;numpy.ndarray&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_instances_of_type_human_readable(type, expected_type_name):
<span class="w"> </span>    &quot;&quot;&quot;Check the string representation of the _InstancesOf constraint.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = _InstancesOf(type)</span>
<span class="gi">+    assert str(constraint) == f&quot;an instance of &#39;{expected_type_name}&#39;&quot;</span>


<span class="w"> </span>def test_hasmethods():
<span class="w"> </span>    &quot;&quot;&quot;Check the HasMethods constraint.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = HasMethods([&quot;a&quot;, &quot;b&quot;])</span>
<span class="gi">+</span>
<span class="gi">+    class _Good:</span>
<span class="gi">+        def a(self):</span>
<span class="gi">+            pass  # pragma: no cover</span>
<span class="gi">+</span>
<span class="gi">+        def b(self):</span>
<span class="gi">+            pass  # pragma: no cover</span>
<span class="gi">+</span>
<span class="gi">+    class _Bad:</span>
<span class="gi">+        def a(self):</span>
<span class="gi">+            pass  # pragma: no cover</span>

<span class="gi">+    assert constraint.is_satisfied_by(_Good())</span>
<span class="gi">+    assert not constraint.is_satisfied_by(_Bad())</span>
<span class="gi">+    assert str(constraint) == &quot;an object implementing &#39;a&#39; and &#39;b&#39;&quot;</span>

<span class="gd">-@pytest.mark.parametrize(&#39;constraint&#39;, [Interval(Real, None, 0, closed=</span>
<span class="gd">-    &#39;left&#39;), Interval(Real, 0, None, closed=&#39;left&#39;), Interval(Real, None,</span>
<span class="gd">-    None, closed=&#39;neither&#39;), StrOptions({&#39;a&#39;, &#39;b&#39;, &#39;c&#39;}), MissingValues(),</span>
<span class="gd">-    MissingValues(numeric_only=True), _VerboseHelper(), HasMethods(&#39;fit&#39;),</span>
<span class="gd">-    _IterablesNotString(), _CVObjects()])</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;constraint&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        Interval(Real, None, 0, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(Real, 0, None, closed=&quot;left&quot;),</span>
<span class="gi">+        Interval(Real, None, None, closed=&quot;neither&quot;),</span>
<span class="gi">+        StrOptions({&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}),</span>
<span class="gi">+        MissingValues(),</span>
<span class="gi">+        MissingValues(numeric_only=True),</span>
<span class="gi">+        _VerboseHelper(),</span>
<span class="gi">+        HasMethods(&quot;fit&quot;),</span>
<span class="gi">+        _IterablesNotString(),</span>
<span class="gi">+        _CVObjects(),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_generate_invalid_param_val(constraint):
<span class="w"> </span>    &quot;&quot;&quot;Check that the value generated does not satisfy the constraint&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@pytest.mark.parametrize(&#39;integer_interval, real_interval&#39;, [(Interval(</span>
<span class="gd">-    Integral, None, 3, closed=&#39;right&#39;), Interval(RealNotInt, -5, 5, closed=</span>
<span class="gd">-    &#39;both&#39;)), (Interval(Integral, None, 3, closed=&#39;right&#39;), Interval(</span>
<span class="gd">-    RealNotInt, -5, 5, closed=&#39;neither&#39;)), (Interval(Integral, None, 3,</span>
<span class="gd">-    closed=&#39;right&#39;), Interval(RealNotInt, 4, 5, closed=&#39;both&#39;)), (Interval(</span>
<span class="gd">-    Integral, None, 3, closed=&#39;right&#39;), Interval(RealNotInt, 5, None,</span>
<span class="gd">-    closed=&#39;left&#39;)), (Interval(Integral, None, 3, closed=&#39;right&#39;), Interval</span>
<span class="gd">-    (RealNotInt, 4, None, closed=&#39;neither&#39;)), (Interval(Integral, 3, None,</span>
<span class="gd">-    closed=&#39;left&#39;), Interval(RealNotInt, -5, 5, closed=&#39;both&#39;)), (Interval(</span>
<span class="gd">-    Integral, 3, None, closed=&#39;left&#39;), Interval(RealNotInt, -5, 5, closed=</span>
<span class="gd">-    &#39;neither&#39;)), (Interval(Integral, 3, None, closed=&#39;left&#39;), Interval(</span>
<span class="gd">-    RealNotInt, 1, 2, closed=&#39;both&#39;)), (Interval(Integral, 3, None, closed=</span>
<span class="gd">-    &#39;left&#39;), Interval(RealNotInt, None, -5, closed=&#39;left&#39;)), (Interval(</span>
<span class="gd">-    Integral, 3, None, closed=&#39;left&#39;), Interval(RealNotInt, None, -4,</span>
<span class="gd">-    closed=&#39;neither&#39;)), (Interval(Integral, -5, 5, closed=&#39;both&#39;), Interval</span>
<span class="gd">-    (RealNotInt, None, 1, closed=&#39;right&#39;)), (Interval(Integral, -5, 5,</span>
<span class="gd">-    closed=&#39;both&#39;), Interval(RealNotInt, 1, None, closed=&#39;left&#39;)), (</span>
<span class="gd">-    Interval(Integral, -5, 5, closed=&#39;both&#39;), Interval(RealNotInt, -10, -4,</span>
<span class="gd">-    closed=&#39;neither&#39;)), (Interval(Integral, -5, 5, closed=&#39;both&#39;), Interval</span>
<span class="gd">-    (RealNotInt, -10, -4, closed=&#39;right&#39;)), (Interval(Integral, -5, 5,</span>
<span class="gd">-    closed=&#39;neither&#39;), Interval(RealNotInt, 6, 10, closed=&#39;neither&#39;)), (</span>
<span class="gd">-    Interval(Integral, -5, 5, closed=&#39;neither&#39;), Interval(RealNotInt, 6, 10,</span>
<span class="gd">-    closed=&#39;left&#39;)), (Interval(Integral, 2, None, closed=&#39;left&#39;), Interval(</span>
<span class="gd">-    RealNotInt, 0, 1, closed=&#39;both&#39;)), (Interval(Integral, 1, None, closed=</span>
<span class="gd">-    &#39;left&#39;), Interval(RealNotInt, 0, 1, closed=&#39;both&#39;))])</span>
<span class="gd">-def test_generate_invalid_param_val_2_intervals(integer_interval, real_interval</span>
<span class="gd">-    ):</span>
<span class="gi">+    bad_value = generate_invalid_param_val(constraint)</span>
<span class="gi">+    assert not constraint.is_satisfied_by(bad_value)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;integer_interval, real_interval&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, None, 3, closed=&quot;right&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, None, 3, closed=&quot;right&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -5, 5, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, None, 3, closed=&quot;right&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 4, 5, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, None, 3, closed=&quot;right&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 5, None, closed=&quot;left&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, None, 3, closed=&quot;right&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 4, None, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 3, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 3, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -5, 5, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 3, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 1, 2, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 3, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, None, -5, closed=&quot;left&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 3, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, None, -4, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+            Interval(RealNotInt, None, 1, closed=&quot;right&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -10, -4, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;both&quot;),</span>
<span class="gi">+            Interval(RealNotInt, -10, -4, closed=&quot;right&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;neither&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 6, 10, closed=&quot;neither&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, -5, 5, closed=&quot;neither&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 6, 10, closed=&quot;left&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 2, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 0, 1, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            Interval(Integral, 1, None, closed=&quot;left&quot;),</span>
<span class="gi">+            Interval(RealNotInt, 0, 1, closed=&quot;both&quot;),</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_generate_invalid_param_val_2_intervals(integer_interval, real_interval):</span>
<span class="w"> </span>    &quot;&quot;&quot;Check that the value generated for an interval constraint does not satisfy any of
<span class="w"> </span>    the interval constraints.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    bad_value = generate_invalid_param_val(constraint=real_interval)</span>
<span class="gi">+    assert not real_interval.is_satisfied_by(bad_value)</span>
<span class="gi">+    assert not integer_interval.is_satisfied_by(bad_value)</span>

<span class="gi">+    bad_value = generate_invalid_param_val(constraint=integer_interval)</span>
<span class="gi">+    assert not real_interval.is_satisfied_by(bad_value)</span>
<span class="gi">+    assert not integer_interval.is_satisfied_by(bad_value)</span>

<span class="gd">-@pytest.mark.parametrize(&#39;constraint&#39;, [_ArrayLikes(), _InstancesOf(list),</span>
<span class="gd">-    _Callables(), _NoneConstraint(), _RandomStates(), _SparseMatrices(),</span>
<span class="gd">-    _Booleans(), Interval(Integral, None, None, closed=&#39;neither&#39;)])</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;constraint&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        _ArrayLikes(),</span>
<span class="gi">+        _InstancesOf(list),</span>
<span class="gi">+        _Callables(),</span>
<span class="gi">+        _NoneConstraint(),</span>
<span class="gi">+        _RandomStates(),</span>
<span class="gi">+        _SparseMatrices(),</span>
<span class="gi">+        _Booleans(),</span>
<span class="gi">+        Interval(Integral, None, None, closed=&quot;neither&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_generate_invalid_param_val_all_valid(constraint):
<span class="w"> </span>    &quot;&quot;&quot;Check that the function raises NotImplementedError when there&#39;s no invalid value
<span class="w"> </span>    for the constraint.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@pytest.mark.parametrize(&#39;constraint&#39;, [_ArrayLikes(), _Callables(),</span>
<span class="gd">-    _InstancesOf(list), _NoneConstraint(), _RandomStates(), _SparseMatrices</span>
<span class="gd">-    (), _Booleans(), _VerboseHelper(), MissingValues(), MissingValues(</span>
<span class="gd">-    numeric_only=True), StrOptions({&#39;a&#39;, &#39;b&#39;, &#39;c&#39;}), Options(Integral, {1, </span>
<span class="gd">-    2, 3}), Interval(Integral, None, None, closed=&#39;neither&#39;), Interval(</span>
<span class="gd">-    Integral, 0, 10, closed=&#39;neither&#39;), Interval(Integral, 0, None, closed=</span>
<span class="gd">-    &#39;neither&#39;), Interval(Integral, None, 0, closed=&#39;neither&#39;), Interval(</span>
<span class="gd">-    Real, 0, 1, closed=&#39;neither&#39;), Interval(Real, 0, None, closed=&#39;both&#39;),</span>
<span class="gd">-    Interval(Real, None, 0, closed=&#39;right&#39;), HasMethods(&#39;fit&#39;),</span>
<span class="gd">-    _IterablesNotString(), _CVObjects()])</span>
<span class="gi">+    with pytest.raises(NotImplementedError):</span>
<span class="gi">+        generate_invalid_param_val(constraint)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;constraint&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        _ArrayLikes(),</span>
<span class="gi">+        _Callables(),</span>
<span class="gi">+        _InstancesOf(list),</span>
<span class="gi">+        _NoneConstraint(),</span>
<span class="gi">+        _RandomStates(),</span>
<span class="gi">+        _SparseMatrices(),</span>
<span class="gi">+        _Booleans(),</span>
<span class="gi">+        _VerboseHelper(),</span>
<span class="gi">+        MissingValues(),</span>
<span class="gi">+        MissingValues(numeric_only=True),</span>
<span class="gi">+        StrOptions({&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}),</span>
<span class="gi">+        Options(Integral, {1, 2, 3}),</span>
<span class="gi">+        Interval(Integral, None, None, closed=&quot;neither&quot;),</span>
<span class="gi">+        Interval(Integral, 0, 10, closed=&quot;neither&quot;),</span>
<span class="gi">+        Interval(Integral, 0, None, closed=&quot;neither&quot;),</span>
<span class="gi">+        Interval(Integral, None, 0, closed=&quot;neither&quot;),</span>
<span class="gi">+        Interval(Real, 0, 1, closed=&quot;neither&quot;),</span>
<span class="gi">+        Interval(Real, 0, None, closed=&quot;both&quot;),</span>
<span class="gi">+        Interval(Real, None, 0, closed=&quot;right&quot;),</span>
<span class="gi">+        HasMethods(&quot;fit&quot;),</span>
<span class="gi">+        _IterablesNotString(),</span>
<span class="gi">+        _CVObjects(),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_generate_valid_param(constraint):
<span class="w"> </span>    &quot;&quot;&quot;Check that the value generated does satisfy the constraint.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@pytest.mark.parametrize(&#39;constraint_declaration, value&#39;, [(Interval(Real, </span>
<span class="gd">-    0, 1, closed=&#39;both&#39;), 0.42), (Interval(Integral, 0, None, closed=</span>
<span class="gd">-    &#39;neither&#39;), 42), (StrOptions({&#39;a&#39;, &#39;b&#39;, &#39;c&#39;}), &#39;b&#39;), (Options(type, {np</span>
<span class="gd">-    .float32, np.float64}), np.float64), (callable, lambda x: x + 1), (None,</span>
<span class="gd">-    None), (&#39;array-like&#39;, [[1, 2], [3, 4]]), (&#39;array-like&#39;, np.array([[1, 2</span>
<span class="gd">-    ], [3, 4]])), (&#39;sparse matrix&#39;, csr_matrix([[1, 2], [3, 4]])), (</span>
<span class="gd">-    &#39;random_state&#39;, 0), (&#39;random_state&#39;, np.random.RandomState(0)), (</span>
<span class="gd">-    &#39;random_state&#39;, None), (_Class, _Class()), (int, 1), (Real, 0.5), (</span>
<span class="gd">-    &#39;boolean&#39;, False), (&#39;verbose&#39;, 1), (&#39;nan&#39;, np.nan), (MissingValues(), -</span>
<span class="gd">-    1), (MissingValues(), -1.0), (MissingValues(), 2 ** 1028), (</span>
<span class="gd">-    MissingValues(), None), (MissingValues(), float(&#39;nan&#39;)), (MissingValues</span>
<span class="gd">-    (), np.nan), (MissingValues(), &#39;missing&#39;), (HasMethods(&#39;fit&#39;),</span>
<span class="gd">-    _Estimator(a=0)), (&#39;cv_object&#39;, 5)])</span>
<span class="gi">+    value = generate_valid_param(constraint)</span>
<span class="gi">+    assert constraint.is_satisfied_by(value)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;constraint_declaration, value&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (Interval(Real, 0, 1, closed=&quot;both&quot;), 0.42),</span>
<span class="gi">+        (Interval(Integral, 0, None, closed=&quot;neither&quot;), 42),</span>
<span class="gi">+        (StrOptions({&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}), &quot;b&quot;),</span>
<span class="gi">+        (Options(type, {np.float32, np.float64}), np.float64),</span>
<span class="gi">+        (callable, lambda x: x + 1),</span>
<span class="gi">+        (None, None),</span>
<span class="gi">+        (&quot;array-like&quot;, [[1, 2], [3, 4]]),</span>
<span class="gi">+        (&quot;array-like&quot;, np.array([[1, 2], [3, 4]])),</span>
<span class="gi">+        (&quot;sparse matrix&quot;, csr_matrix([[1, 2], [3, 4]])),</span>
<span class="gi">+        (&quot;random_state&quot;, 0),</span>
<span class="gi">+        (&quot;random_state&quot;, np.random.RandomState(0)),</span>
<span class="gi">+        (&quot;random_state&quot;, None),</span>
<span class="gi">+        (_Class, _Class()),</span>
<span class="gi">+        (int, 1),</span>
<span class="gi">+        (Real, 0.5),</span>
<span class="gi">+        (&quot;boolean&quot;, False),</span>
<span class="gi">+        (&quot;verbose&quot;, 1),</span>
<span class="gi">+        (&quot;nan&quot;, np.nan),</span>
<span class="gi">+        (MissingValues(), -1),</span>
<span class="gi">+        (MissingValues(), -1.0),</span>
<span class="gi">+        (MissingValues(), 2**1028),</span>
<span class="gi">+        (MissingValues(), None),</span>
<span class="gi">+        (MissingValues(), float(&quot;nan&quot;)),</span>
<span class="gi">+        (MissingValues(), np.nan),</span>
<span class="gi">+        (MissingValues(), &quot;missing&quot;),</span>
<span class="gi">+        (HasMethods(&quot;fit&quot;), _Estimator(a=0)),</span>
<span class="gi">+        (&quot;cv_object&quot;, 5),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_is_satisfied_by(constraint_declaration, value):
<span class="w"> </span>    &quot;&quot;&quot;Sanity check for the is_satisfied_by method&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gd">-</span>
<span class="gd">-</span>
<span class="gd">-@pytest.mark.parametrize(&#39;constraint_declaration, expected_constraint_class&#39;,</span>
<span class="gd">-    [(Interval(Real, 0, 1, closed=&#39;both&#39;), Interval), (StrOptions({</span>
<span class="gd">-    &#39;option1&#39;, &#39;option2&#39;}), StrOptions), (Options(Real, {0.42, 1.23}),</span>
<span class="gd">-    Options), (&#39;array-like&#39;, _ArrayLikes), (&#39;sparse matrix&#39;,</span>
<span class="gd">-    _SparseMatrices), (&#39;random_state&#39;, _RandomStates), (None,</span>
<span class="gd">-    _NoneConstraint), (callable, _Callables), (int, _InstancesOf), (</span>
<span class="gd">-    &#39;boolean&#39;, _Booleans), (&#39;verbose&#39;, _VerboseHelper), (MissingValues(</span>
<span class="gd">-    numeric_only=True), MissingValues), (HasMethods(&#39;fit&#39;), HasMethods), (</span>
<span class="gd">-    &#39;cv_object&#39;, _CVObjects), (&#39;nan&#39;, _NanConstraint)])</span>
<span class="gi">+    constraint = make_constraint(constraint_declaration)</span>
<span class="gi">+    assert constraint.is_satisfied_by(value)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;constraint_declaration, expected_constraint_class&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (Interval(Real, 0, 1, closed=&quot;both&quot;), Interval),</span>
<span class="gi">+        (StrOptions({&quot;option1&quot;, &quot;option2&quot;}), StrOptions),</span>
<span class="gi">+        (Options(Real, {0.42, 1.23}), Options),</span>
<span class="gi">+        (&quot;array-like&quot;, _ArrayLikes),</span>
<span class="gi">+        (&quot;sparse matrix&quot;, _SparseMatrices),</span>
<span class="gi">+        (&quot;random_state&quot;, _RandomStates),</span>
<span class="gi">+        (None, _NoneConstraint),</span>
<span class="gi">+        (callable, _Callables),</span>
<span class="gi">+        (int, _InstancesOf),</span>
<span class="gi">+        (&quot;boolean&quot;, _Booleans),</span>
<span class="gi">+        (&quot;verbose&quot;, _VerboseHelper),</span>
<span class="gi">+        (MissingValues(numeric_only=True), MissingValues),</span>
<span class="gi">+        (HasMethods(&quot;fit&quot;), HasMethods),</span>
<span class="gi">+        (&quot;cv_object&quot;, _CVObjects),</span>
<span class="gi">+        (&quot;nan&quot;, _NanConstraint),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="w"> </span>def test_make_constraint(constraint_declaration, expected_constraint_class):
<span class="w"> </span>    &quot;&quot;&quot;Check that make_constraint dispatches to the appropriate constraint class&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = make_constraint(constraint_declaration)</span>
<span class="gi">+    assert constraint.__class__ is expected_constraint_class</span>


<span class="w"> </span>def test_make_constraint_unknown():
<span class="w"> </span>    &quot;&quot;&quot;Check that an informative error is raised when an unknown constraint is passed&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;Unknown constraint&quot;):</span>
<span class="gi">+        make_constraint(&quot;not a valid constraint&quot;)</span>


<span class="w"> </span>def test_validate_params():
<span class="w"> </span>    &quot;&quot;&quot;Check that validate_params works no matter how the arguments are passed&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;a&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(&quot;wrong&quot;, c=1)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;b&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(*[1, &quot;wrong&quot;], c=1)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;c&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(1, **{&quot;c&quot;: &quot;wrong&quot;})</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;d&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(1, c=1, d=&quot;wrong&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # check in the presence of extra positional and keyword args</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;b&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(0, *[&quot;wrong&quot;, 2, 3], c=4, **{&quot;e&quot;: 5})</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;c&#39; parameter of _func must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _func(0, *[1, 2, 3], c=&quot;four&quot;, **{&quot;e&quot;: 5})</span>


<span class="w"> </span>def test_validate_params_missing_params():
<span class="w"> </span>    &quot;&quot;&quot;Check that no error is raised when there are parameters without
<span class="w"> </span>    constraints
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params({&quot;a&quot;: [int]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    def func(a, b):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    func(1, 2)</span>


<span class="w"> </span>def test_decorate_validated_function():
<span class="w"> </span>    &quot;&quot;&quot;Check that validate_params functions can be decorated&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    decorated_function = deprecated()(_func)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;Function _func is deprecated&quot;):</span>
<span class="gi">+        decorated_function(1, 2, c=3)</span>
<span class="gi">+</span>
<span class="gi">+    # outer decorator does not interfere with validation</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;Function _func is deprecated&quot;):</span>
<span class="gi">+        with pytest.raises(</span>
<span class="gi">+            InvalidParameterError, match=r&quot;The &#39;c&#39; parameter of _func must be&quot;</span>
<span class="gi">+        ):</span>
<span class="gi">+            decorated_function(1, 2, c=&quot;wrong&quot;)</span>


<span class="w"> </span>def test_validate_params_method():
<span class="w"> </span>    &quot;&quot;&quot;Check that validate_params works with methods&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;a&#39; parameter of _Class._method must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        _Class()._method(&quot;wrong&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # validated method can be decorated</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=&quot;Function _deprecated_method is deprecated&quot;):</span>
<span class="gi">+        with pytest.raises(</span>
<span class="gi">+            InvalidParameterError,</span>
<span class="gi">+            match=&quot;The &#39;a&#39; parameter of _Class._deprecated_method must be&quot;,</span>
<span class="gi">+        ):</span>
<span class="gi">+            _Class()._deprecated_method(&quot;wrong&quot;)</span>


<span class="w"> </span>def test_validate_params_estimator():
<span class="w"> </span>    &quot;&quot;&quot;Check that validate_params works with Estimator instances&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    # no validation in init</span>
<span class="gi">+    est = _Estimator(&quot;wrong&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;a&#39; parameter of _Estimator must be&quot;</span>
<span class="gi">+    ):</span>
<span class="gi">+        est.fit()</span>


<span class="w"> </span>def test_stroptions_deprecated_subset():
<span class="w"> </span>    &quot;&quot;&quot;Check that the deprecated parameter must be a subset of options.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;deprecated options must be a subset&quot;):</span>
<span class="gi">+        StrOptions({&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}, deprecated={&quot;a&quot;, &quot;d&quot;})</span>


<span class="w"> </span>def test_hidden_constraint():
<span class="w"> </span>    &quot;&quot;&quot;Check that internal constraints are not exposed in the error message.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params(</span>
<span class="gi">+        {&quot;param&quot;: [Hidden(list), dict]}, prefer_skip_nested_validation=True</span>
<span class="gi">+    )</span>
<span class="gi">+    def f(param):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    # list and dict are valid params</span>
<span class="gi">+    f({&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3})</span>
<span class="gi">+    f([1, 2, 3])</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;param&#39; parameter&quot;</span>
<span class="gi">+    ) as exc_info:</span>
<span class="gi">+        f(param=&quot;bad&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # the list option is not exposed in the error message</span>
<span class="gi">+    err_msg = str(exc_info.value)</span>
<span class="gi">+    assert &quot;an instance of &#39;dict&#39;&quot; in err_msg</span>
<span class="gi">+    assert &quot;an instance of &#39;list&#39;&quot; not in err_msg</span>


<span class="w"> </span>def test_hidden_stroptions():
<span class="w"> </span>    &quot;&quot;&quot;Check that we can have 2 StrOptions constraints, one being hidden.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params(</span>
<span class="gi">+        {&quot;param&quot;: [StrOptions({&quot;auto&quot;}), Hidden(StrOptions({&quot;warn&quot;}))]},</span>
<span class="gi">+        prefer_skip_nested_validation=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    def f(param):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    # &quot;auto&quot; and &quot;warn&quot; are valid params</span>
<span class="gi">+    f(&quot;auto&quot;)</span>
<span class="gi">+    f(&quot;warn&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        InvalidParameterError, match=&quot;The &#39;param&#39; parameter&quot;</span>
<span class="gi">+    ) as exc_info:</span>
<span class="gi">+        f(param=&quot;bad&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # the &quot;warn&quot; option is not exposed in the error message</span>
<span class="gi">+    err_msg = str(exc_info.value)</span>
<span class="gi">+    assert &quot;auto&quot; in err_msg</span>
<span class="gi">+    assert &quot;warn&quot; not in err_msg</span>


<span class="w"> </span>def test_validate_params_set_param_constraints_attribute():
<span class="w"> </span>    &quot;&quot;&quot;Check that the validate_params decorator properly sets the parameter constraints
<span class="w"> </span>    as attribute of the decorated function/method.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    assert hasattr(_func, &quot;_skl_parameter_constraints&quot;)</span>
<span class="gi">+    assert hasattr(_Class()._method, &quot;_skl_parameter_constraints&quot;)</span>


<span class="w"> </span>def test_boolean_constraint_deprecated_int():
<span class="w"> </span>    &quot;&quot;&quot;Check that validate_params raise a deprecation message but still passes
<span class="w"> </span>    validation when using an int for a parameter accepting a boolean.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params({&quot;param&quot;: [&quot;boolean&quot;]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    def f(param):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    # True/False and np.bool_(True/False) are valid params</span>
<span class="gi">+    f(True)</span>
<span class="gi">+    f(np.bool_(False))</span>


<span class="w"> </span>def test_no_validation():
<span class="w"> </span>    &quot;&quot;&quot;Check that validation can be skipped for a parameter.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params(</span>
<span class="gi">+        {&quot;param1&quot;: [int, None], &quot;param2&quot;: &quot;no_validation&quot;},</span>
<span class="gi">+        prefer_skip_nested_validation=True,</span>
<span class="gi">+    )</span>
<span class="gi">+    def f(param1=None, param2=None):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    # param1 is validated</span>
<span class="gi">+    with pytest.raises(InvalidParameterError, match=&quot;The &#39;param1&#39; parameter&quot;):</span>
<span class="gi">+        f(param1=&quot;wrong&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # param2 is not validated: any type is valid.</span>
<span class="gi">+    class SomeType:</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    f(param2=SomeType)</span>
<span class="gi">+    f(param2=SomeType())</span>


<span class="w"> </span>def test_pandas_na_constraint_with_pd_na():
<span class="w"> </span>    &quot;&quot;&quot;Add a specific test for checking support for `pandas.NA`.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    na_constraint = _PandasNAConstraint()</span>
<span class="gi">+    assert na_constraint.is_satisfied_by(pd.NA)</span>
<span class="gi">+    assert not na_constraint.is_satisfied_by(np.array([1, 2, 3]))</span>


<span class="w"> </span>def test_iterable_not_string():
<span class="w"> </span>    &quot;&quot;&quot;Check that a string does not satisfy the _IterableNotString constraint.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = _IterablesNotString()</span>
<span class="gi">+    assert constraint.is_satisfied_by([1, 2, 3])</span>
<span class="gi">+    assert constraint.is_satisfied_by(range(10))</span>
<span class="gi">+    assert not constraint.is_satisfied_by(&quot;some string&quot;)</span>


<span class="w"> </span>def test_cv_objects():
<span class="w"> </span>    &quot;&quot;&quot;Check that the _CVObjects constraint accepts all current ways
<span class="w"> </span>    to pass cv objects.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = _CVObjects()</span>
<span class="gi">+    assert constraint.is_satisfied_by(5)</span>
<span class="gi">+    assert constraint.is_satisfied_by(LeaveOneOut())</span>
<span class="gi">+    assert constraint.is_satisfied_by([([1, 2], [3, 4]), ([3, 4], [1, 2])])</span>
<span class="gi">+    assert constraint.is_satisfied_by(None)</span>
<span class="gi">+    assert not constraint.is_satisfied_by(&quot;not a CV object&quot;)</span>


<span class="w"> </span>def test_third_party_estimator():
<span class="gu">@@ -300,35 +688,98 @@ def test_third_party_estimator():</span>
<span class="w"> </span>    party estimator does not impose a match between the dict of constraints and the
<span class="w"> </span>    parameters of the estimator.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    class ThirdPartyEstimator(_Estimator):</span>
<span class="gi">+        def __init__(self, b):</span>
<span class="gi">+            self.b = b</span>
<span class="gi">+            super().__init__(a=0)</span>
<span class="gi">+</span>
<span class="gi">+        def fit(self, X=None, y=None):</span>
<span class="gi">+            super().fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    # does not raise, even though &quot;b&quot; is not in the constraints dict and &quot;a&quot; is not</span>
<span class="gi">+    # a parameter of the estimator.</span>
<span class="gi">+    ThirdPartyEstimator(b=0).fit()</span>


<span class="w"> </span>def test_interval_real_not_int():
<span class="w"> </span>    &quot;&quot;&quot;Check for the type RealNotInt in the Interval constraint.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    constraint = Interval(RealNotInt, 0, 1, closed=&quot;both&quot;)</span>
<span class="gi">+    assert constraint.is_satisfied_by(1.0)</span>
<span class="gi">+    assert not constraint.is_satisfied_by(1)</span>


<span class="w"> </span>def test_real_not_int():
<span class="w"> </span>    &quot;&quot;&quot;Check for the RealNotInt type.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+    assert isinstance(1.0, RealNotInt)</span>
<span class="gi">+    assert not isinstance(1, RealNotInt)</span>
<span class="gi">+    assert isinstance(np.float64(1), RealNotInt)</span>
<span class="gi">+    assert not isinstance(np.int64(1), RealNotInt)</span>


<span class="w"> </span>def test_skip_param_validation():
<span class="w"> </span>    &quot;&quot;&quot;Check that param validation can be skipped using config_context.&quot;&quot;&quot;
<span class="gd">-    pass</span>

<span class="gi">+    @validate_params({&quot;a&quot;: [int]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    def f(a):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.raises(InvalidParameterError, match=&quot;The &#39;a&#39; parameter&quot;):</span>
<span class="gi">+        f(a=&quot;1&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # does not raise</span>
<span class="gi">+    with config_context(skip_parameter_validation=True):</span>
<span class="gi">+        f(a=&quot;1&quot;)</span>

<span class="gd">-@pytest.mark.parametrize(&#39;prefer_skip_nested_validation&#39;, [True, False])</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;prefer_skip_nested_validation&quot;, [True, False])</span>
<span class="w"> </span>def test_skip_nested_validation(prefer_skip_nested_validation):
<span class="w"> </span>    &quot;&quot;&quot;Check that nested validation can be skipped.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params({&quot;a&quot;: [int]}, prefer_skip_nested_validation=True)</span>
<span class="gi">+    def f(a):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params(</span>
<span class="gi">+        {&quot;b&quot;: [int]},</span>
<span class="gi">+        prefer_skip_nested_validation=prefer_skip_nested_validation,</span>
<span class="gi">+    )</span>
<span class="gi">+    def g(b):</span>
<span class="gi">+        # calls f with a bad parameter type</span>
<span class="gi">+        return f(a=&quot;invalid_param_value&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    # Validation for g is never skipped.</span>
<span class="gi">+    with pytest.raises(InvalidParameterError, match=&quot;The &#39;b&#39; parameter&quot;):</span>
<span class="gi">+        g(b=&quot;invalid_param_value&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    if prefer_skip_nested_validation:</span>
<span class="gi">+        g(b=1)  # does not raise because inner f is not validated</span>
<span class="gi">+    else:</span>
<span class="gi">+        with pytest.raises(InvalidParameterError, match=&quot;The &#39;a&#39; parameter&quot;):</span>
<span class="gi">+            g(b=1)</span>


<span class="w"> </span>@pytest.mark.parametrize(
<span class="gd">-    &#39;skip_parameter_validation, prefer_skip_nested_validation, expected_skipped&#39;</span>
<span class="gd">-    , [(True, True, True), (True, False, True), (False, True, True), (False,</span>
<span class="gd">-    False, False)])</span>
<span class="gd">-def test_skip_nested_validation_and_config_context(skip_parameter_validation,</span>
<span class="gd">-    prefer_skip_nested_validation, expected_skipped):</span>
<span class="gi">+    &quot;skip_parameter_validation, prefer_skip_nested_validation, expected_skipped&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (True, True, True),</span>
<span class="gi">+        (True, False, True),</span>
<span class="gi">+        (False, True, True),</span>
<span class="gi">+        (False, False, False),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_skip_nested_validation_and_config_context(</span>
<span class="gi">+    skip_parameter_validation, prefer_skip_nested_validation, expected_skipped</span>
<span class="gi">+):</span>
<span class="w"> </span>    &quot;&quot;&quot;Check interaction between global skip and local skip.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    @validate_params(</span>
<span class="gi">+        {&quot;a&quot;: [int]}, prefer_skip_nested_validation=prefer_skip_nested_validation</span>
<span class="gi">+    )</span>
<span class="gi">+    def g(a):</span>
<span class="gi">+        return get_config()[&quot;skip_parameter_validation&quot;]</span>
<span class="gi">+</span>
<span class="gi">+    with config_context(skip_parameter_validation=skip_parameter_validation):</span>
<span class="gi">+        actual_skipped = g(1)</span>
<span class="gi">+</span>
<span class="gi">+    assert actual_skipped == expected_skipped</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_show_versions.py b/imblearn/utils/tests/test_show_versions.py</span>
<span class="gh">index ca6a29e..1b43053 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_show_versions.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_show_versions.py</span>
<span class="gu">@@ -1,2 +1,60 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the show_versions helper. Based on the sklearn tests.&quot;&quot;&quot;
<span class="gi">+# Author: Alexander L. Hayes &lt;hayesall@iu.edu&gt;</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from imblearn.utils._show_versions import _get_deps_info, show_versions
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_get_deps_info():</span>
<span class="gi">+    _deps_info = _get_deps_info()</span>
<span class="gi">+    assert &quot;pip&quot; in _deps_info</span>
<span class="gi">+    assert &quot;setuptools&quot; in _deps_info</span>
<span class="gi">+    assert &quot;imbalanced-learn&quot; in _deps_info</span>
<span class="gi">+    assert &quot;scikit-learn&quot; in _deps_info</span>
<span class="gi">+    assert &quot;numpy&quot; in _deps_info</span>
<span class="gi">+    assert &quot;scipy&quot; in _deps_info</span>
<span class="gi">+    assert &quot;Cython&quot; in _deps_info</span>
<span class="gi">+    assert &quot;pandas&quot; in _deps_info</span>
<span class="gi">+    assert &quot;joblib&quot; in _deps_info</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_show_versions_default(capsys):</span>
<span class="gi">+    show_versions()</span>
<span class="gi">+    out, err = capsys.readouterr()</span>
<span class="gi">+    assert &quot;python&quot; in out</span>
<span class="gi">+    assert &quot;executable&quot; in out</span>
<span class="gi">+    assert &quot;machine&quot; in out</span>
<span class="gi">+    assert &quot;pip&quot; in out</span>
<span class="gi">+    assert &quot;setuptools&quot; in out</span>
<span class="gi">+    assert &quot;imbalanced-learn&quot; in out</span>
<span class="gi">+    assert &quot;scikit-learn&quot; in out</span>
<span class="gi">+    assert &quot;numpy&quot; in out</span>
<span class="gi">+    assert &quot;scipy&quot; in out</span>
<span class="gi">+    assert &quot;Cython&quot; in out</span>
<span class="gi">+    assert &quot;pandas&quot; in out</span>
<span class="gi">+    assert &quot;keras&quot; in out</span>
<span class="gi">+    assert &quot;tensorflow&quot; in out</span>
<span class="gi">+    assert &quot;joblib&quot; in out</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_show_versions_github(capsys):</span>
<span class="gi">+    show_versions(github=True)</span>
<span class="gi">+    out, err = capsys.readouterr()</span>
<span class="gi">+    assert &quot;&lt;details&gt;&lt;summary&gt;System, Dependency Information&lt;/summary&gt;&quot; in out</span>
<span class="gi">+    assert &quot;**System Information**&quot; in out</span>
<span class="gi">+    assert &quot;* python&quot; in out</span>
<span class="gi">+    assert &quot;* executable&quot; in out</span>
<span class="gi">+    assert &quot;* machine&quot; in out</span>
<span class="gi">+    assert &quot;**Python Dependencies**&quot; in out</span>
<span class="gi">+    assert &quot;* pip&quot; in out</span>
<span class="gi">+    assert &quot;* setuptools&quot; in out</span>
<span class="gi">+    assert &quot;* imbalanced-learn&quot; in out</span>
<span class="gi">+    assert &quot;* scikit-learn&quot; in out</span>
<span class="gi">+    assert &quot;* numpy&quot; in out</span>
<span class="gi">+    assert &quot;* scipy&quot; in out</span>
<span class="gi">+    assert &quot;* Cython&quot; in out</span>
<span class="gi">+    assert &quot;* pandas&quot; in out</span>
<span class="gi">+    assert &quot;* keras&quot; in out</span>
<span class="gi">+    assert &quot;* tensorflow&quot; in out</span>
<span class="gi">+    assert &quot;* joblib&quot; in out</span>
<span class="gi">+    assert &quot;&lt;/details&gt;&quot; in out</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_testing.py b/imblearn/utils/tests/test_testing.py</span>
<span class="gh">index 421be2b..1b37978 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_testing.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_testing.py</span>
<span class="gu">@@ -1,12 +1,49 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the testing module&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.neighbors._base import KNeighborsMixin
<span class="gi">+</span>
<span class="w"> </span>from imblearn.base import SamplerMixin
<span class="w"> </span>from imblearn.utils.testing import _CustomNearestNeighbors, all_estimators


<span class="gi">+def test_all_estimators():</span>
<span class="gi">+    # check if the filtering is working with a list or a single string</span>
<span class="gi">+    type_filter = &quot;sampler&quot;</span>
<span class="gi">+    all_estimators(type_filter=type_filter)</span>
<span class="gi">+    type_filter = [&quot;sampler&quot;]</span>
<span class="gi">+    estimators = all_estimators(type_filter=type_filter)</span>
<span class="gi">+    for estimator in estimators:</span>
<span class="gi">+        # check that all estimators are sampler</span>
<span class="gi">+        assert issubclass(estimator[1], SamplerMixin)</span>
<span class="gi">+</span>
<span class="gi">+    # check that an error is raised when the type is unknown</span>
<span class="gi">+    type_filter = &quot;rnd&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;Parameter type_filter must be &#39;sampler&#39;&quot;):</span>
<span class="gi">+        all_estimators(type_filter=type_filter)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="w"> </span>def test_custom_nearest_neighbors():
<span class="w"> </span>    &quot;&quot;&quot;Check that our custom nearest neighbors can be used for our internal
<span class="w"> </span>    duck-typing.&quot;&quot;&quot;
<span class="gd">-    pass</span>
<span class="gi">+</span>
<span class="gi">+    neareat_neighbors = _CustomNearestNeighbors(n_neighbors=3)</span>
<span class="gi">+</span>
<span class="gi">+    assert not isinstance(neareat_neighbors, KNeighborsMixin)</span>
<span class="gi">+    assert hasattr(neareat_neighbors, &quot;kneighbors&quot;)</span>
<span class="gi">+    assert hasattr(neareat_neighbors, &quot;kneighbors_graph&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    rng = np.random.RandomState(42)</span>
<span class="gi">+    X = rng.randn(150, 3)</span>
<span class="gi">+    y = rng.randint(0, 2, 150)</span>
<span class="gi">+    neareat_neighbors.fit(X, y)</span>
<span class="gi">+</span>
<span class="gi">+    distances, indices = neareat_neighbors.kneighbors(X)</span>
<span class="gi">+    assert distances.shape == (150, 3)</span>
<span class="gi">+    assert indices.shape == (150, 3)</span>
<span class="gi">+    np.testing.assert_allclose(distances[:, 0], 0.0)</span>
<span class="gi">+    np.testing.assert_allclose(indices[:, 0], np.arange(150))</span>
<span class="gh">diff --git a/imblearn/utils/tests/test_validation.py b/imblearn/utils/tests/test_validation.py</span>
<span class="gh">index b7e2a02..4394f04 100644</span>
<span class="gd">--- a/imblearn/utils/tests/test_validation.py</span>
<span class="gi">+++ b/imblearn/utils/tests/test_validation.py</span>
<span class="gu">@@ -1,13 +1,382 @@</span>
<span class="w"> </span>&quot;&quot;&quot;Test for the validation helper&quot;&quot;&quot;
<span class="gi">+# Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="gi">+#          Christos Aridas</span>
<span class="gi">+# License: MIT</span>
<span class="gi">+</span>
<span class="w"> </span>from collections import Counter, OrderedDict
<span class="gi">+</span>
<span class="w"> </span>import numpy as np
<span class="w"> </span>import pytest
<span class="w"> </span>from sklearn.cluster import KMeans
<span class="w"> </span>from sklearn.neighbors import NearestNeighbors
<span class="w"> </span>from sklearn.neighbors._base import KNeighborsMixin
<span class="w"> </span>from sklearn.utils._testing import assert_array_equal
<span class="gd">-from imblearn.utils import check_neighbors_object, check_sampling_strategy, check_target_type</span>
<span class="gd">-from imblearn.utils._validation import ArraysTransformer, _deprecate_positional_args, _is_neighbors_object</span>
<span class="gi">+</span>
<span class="gi">+from imblearn.utils import (</span>
<span class="gi">+    check_neighbors_object,</span>
<span class="gi">+    check_sampling_strategy,</span>
<span class="gi">+    check_target_type,</span>
<span class="gi">+)</span>
<span class="gi">+from imblearn.utils._validation import (</span>
<span class="gi">+    ArraysTransformer,</span>
<span class="gi">+    _deprecate_positional_args,</span>
<span class="gi">+    _is_neighbors_object,</span>
<span class="gi">+)</span>
<span class="w"> </span>from imblearn.utils.testing import _CustomNearestNeighbors
<span class="gi">+</span>
<span class="w"> </span>multiclass_target = np.array([1] * 50 + [2] * 100 + [3] * 25)
<span class="w"> </span>binary_target = np.array([1] * 25 + [0] * 100)
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_check_neighbors_object():</span>
<span class="gi">+    name = &quot;n_neighbors&quot;</span>
<span class="gi">+    n_neighbors = 1</span>
<span class="gi">+    estimator = check_neighbors_object(name, n_neighbors)</span>
<span class="gi">+    assert issubclass(type(estimator), KNeighborsMixin)</span>
<span class="gi">+    assert estimator.n_neighbors == 1</span>
<span class="gi">+    estimator = check_neighbors_object(name, n_neighbors, 1)</span>
<span class="gi">+    assert issubclass(type(estimator), KNeighborsMixin)</span>
<span class="gi">+    assert estimator.n_neighbors == 2</span>
<span class="gi">+    estimator = NearestNeighbors(n_neighbors=n_neighbors)</span>
<span class="gi">+    estimator_cloned = check_neighbors_object(name, estimator)</span>
<span class="gi">+    assert estimator.n_neighbors == estimator_cloned.n_neighbors</span>
<span class="gi">+    estimator = _CustomNearestNeighbors()</span>
<span class="gi">+    estimator_cloned = check_neighbors_object(name, estimator)</span>
<span class="gi">+    assert isinstance(estimator_cloned, _CustomNearestNeighbors)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;target, output_target&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (np.array([0, 1, 1]), np.array([0, 1, 1])),</span>
<span class="gi">+        (np.array([0, 1, 2]), np.array([0, 1, 2])),</span>
<span class="gi">+        (np.array([[0, 1], [1, 0]]), np.array([1, 0])),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_check_target_type(target, output_target):</span>
<span class="gi">+    converted_target = check_target_type(target.astype(int))</span>
<span class="gi">+    assert_array_equal(converted_target, output_target.astype(int))</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;target, output_target, is_ova&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (np.array([0, 1, 1]), np.array([0, 1, 1]), False),</span>
<span class="gi">+        (np.array([0, 1, 2]), np.array([0, 1, 2]), False),</span>
<span class="gi">+        (np.array([[0, 1], [1, 0]]), np.array([1, 0]), True),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_check_target_type_ova(target, output_target, is_ova):</span>
<span class="gi">+    converted_target, binarize_target = check_target_type(</span>
<span class="gi">+        target.astype(int), indicate_one_vs_all=True</span>
<span class="gi">+    )</span>
<span class="gi">+    assert_array_equal(converted_target, output_target.astype(int))</span>
<span class="gi">+    assert binarize_target == is_ova</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_check_sampling_strategy_warning():</span>
<span class="gi">+    msg = &quot;dict for cleaning methods is not supported&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=msg):</span>
<span class="gi">+        check_sampling_strategy({1: 0, 2: 0, 3: 0}, multiclass_target, &quot;clean-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;ratio, y, type, err_msg&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (</span>
<span class="gi">+            0.5,</span>
<span class="gi">+            binary_target,</span>
<span class="gi">+            &quot;clean-sampling&quot;,</span>
<span class="gi">+            &quot;&#39;clean-sampling&#39; methods do let the user specify the sampling ratio&quot;,  # noqa</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            0.1,</span>
<span class="gi">+            np.array([0] * 10 + [1] * 20),</span>
<span class="gi">+            &quot;over-sampling&quot;,</span>
<span class="gi">+            &quot;remove samples from the minority class while trying to generate new&quot;,  # noqa</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            0.1,</span>
<span class="gi">+            np.array([0] * 10 + [1] * 20),</span>
<span class="gi">+            &quot;under-sampling&quot;,</span>
<span class="gi">+            &quot;generate new sample in the majority class while trying to remove&quot;,</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_check_sampling_strategy_float_error(ratio, y, type, err_msg):</span>
<span class="gi">+    with pytest.raises(ValueError, match=err_msg):</span>
<span class="gi">+        check_sampling_strategy(ratio, y, type)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_check_sampling_strategy_error():</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;&#39;sampling_type&#39; should be one of&quot;):</span>
<span class="gi">+        check_sampling_strategy(&quot;auto&quot;, np.array([1, 2, 3]), &quot;rnd&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    error_regex = &quot;The target &#39;y&#39; needs to have more than 1 class.&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=error_regex):</span>
<span class="gi">+        check_sampling_strategy(&quot;auto&quot;, np.ones((10,)), &quot;over-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    error_regex = &quot;When &#39;sampling_strategy&#39; is a string, it needs to be one of&quot;</span>
<span class="gi">+    with pytest.raises(ValueError, match=error_regex):</span>
<span class="gi">+        check_sampling_strategy(&quot;rnd&quot;, np.array([1, 2, 3]), &quot;over-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, sampling_type, err_msg&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (&quot;majority&quot;, &quot;over-sampling&quot;, &quot;over-sampler&quot;),</span>
<span class="gi">+        (&quot;minority&quot;, &quot;under-sampling&quot;, &quot;under-sampler&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_check_sampling_strategy_error_wrong_string(</span>
<span class="gi">+    sampling_strategy, sampling_type, err_msg</span>
<span class="gi">+):</span>
<span class="gi">+    with pytest.raises(</span>
<span class="gi">+        ValueError,</span>
<span class="gi">+        match=(&quot;&#39;{}&#39; cannot be used with {}&quot;.format(sampling_strategy, err_msg)),</span>
<span class="gi">+    ):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, np.array([1, 2, 3]), sampling_type)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, sampling_method&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        ({10: 10}, &quot;under-sampling&quot;),</span>
<span class="gi">+        ({10: 10}, &quot;over-sampling&quot;),</span>
<span class="gi">+        ([10], &quot;clean-sampling&quot;),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_sampling_strategy_class_target_unknown(sampling_strategy, sampling_method):</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;are not present in the data.&quot;):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, sampling_method)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sampling_strategy_dict_error():</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    sampling_strategy = {1: -100, 2: 50, 3: 25}</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;in a class cannot be negative.&quot;):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, &quot;under-sampling&quot;)</span>
<span class="gi">+    sampling_strategy = {1: 45, 2: 100, 3: 70}</span>
<span class="gi">+    error_regex = (</span>
<span class="gi">+        &quot;With over-sampling methods, the number of samples in a&quot;</span>
<span class="gi">+        &quot; class should be greater or equal to the original number&quot;</span>
<span class="gi">+        &quot; of samples. Originally, there is 50 samples and 45&quot;</span>
<span class="gi">+        &quot; samples are asked.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.raises(ValueError, match=error_regex):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, &quot;over-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    error_regex = (</span>
<span class="gi">+        &quot;With under-sampling methods, the number of samples in a&quot;</span>
<span class="gi">+        &quot; class should be less or equal to the original number of&quot;</span>
<span class="gi">+        &quot; samples. Originally, there is 25 samples and 70 samples&quot;</span>
<span class="gi">+        &quot; are asked.&quot;</span>
<span class="gi">+    )</span>
<span class="gi">+    with pytest.raises(ValueError, match=error_regex):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, &quot;under-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;sampling_strategy&quot;, [-10, 10])</span>
<span class="gi">+def test_sampling_strategy_float_error_not_in_range(sampling_strategy):</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;it should be in the range&quot;):</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, &quot;under-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sampling_strategy_float_error_not_binary():</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;the type of target is binary&quot;):</span>
<span class="gi">+        sampling_strategy = 0.5</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, &quot;under-sampling&quot;)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(&quot;sampling_method&quot;, [&quot;over-sampling&quot;, &quot;under-sampling&quot;])</span>
<span class="gi">+def test_sampling_strategy_list_error_not_clean_sampling(sampling_method):</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    with pytest.raises(ValueError, match=&quot;cannot be a list for samplers&quot;):</span>
<span class="gi">+        sampling_strategy = [1, 2, 3]</span>
<span class="gi">+        check_sampling_strategy(sampling_strategy, y, sampling_method)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def _sampling_strategy_func(y):</span>
<span class="gi">+    # this function could create an equal number of samples</span>
<span class="gi">+    target_stats = Counter(y)</span>
<span class="gi">+    n_samples = max(target_stats.values())</span>
<span class="gi">+    return {key: int(n_samples) for key in target_stats.keys()}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, sampling_type, expected_sampling_strategy, target&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (&quot;auto&quot;, &quot;under-sampling&quot;, {1: 25, 2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;auto&quot;, &quot;clean-sampling&quot;, {1: 25, 2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;auto&quot;, &quot;over-sampling&quot;, {1: 50, 3: 75}, multiclass_target),</span>
<span class="gi">+        (&quot;all&quot;, &quot;over-sampling&quot;, {1: 50, 2: 0, 3: 75}, multiclass_target),</span>
<span class="gi">+        (&quot;all&quot;, &quot;under-sampling&quot;, {1: 25, 2: 25, 3: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;all&quot;, &quot;clean-sampling&quot;, {1: 25, 2: 25, 3: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;majority&quot;, &quot;under-sampling&quot;, {2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;majority&quot;, &quot;clean-sampling&quot;, {2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;minority&quot;, &quot;over-sampling&quot;, {3: 75}, multiclass_target),</span>
<span class="gi">+        (&quot;not minority&quot;, &quot;over-sampling&quot;, {1: 50, 2: 0}, multiclass_target),</span>
<span class="gi">+        (&quot;not minority&quot;, &quot;under-sampling&quot;, {1: 25, 2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;not minority&quot;, &quot;clean-sampling&quot;, {1: 25, 2: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;not majority&quot;, &quot;over-sampling&quot;, {1: 50, 3: 75}, multiclass_target),</span>
<span class="gi">+        (&quot;not majority&quot;, &quot;under-sampling&quot;, {1: 25, 3: 25}, multiclass_target),</span>
<span class="gi">+        (&quot;not majority&quot;, &quot;clean-sampling&quot;, {1: 25, 3: 25}, multiclass_target),</span>
<span class="gi">+        (</span>
<span class="gi">+            {1: 70, 2: 100, 3: 70},</span>
<span class="gi">+            &quot;over-sampling&quot;,</span>
<span class="gi">+            {1: 20, 2: 0, 3: 45},</span>
<span class="gi">+            multiclass_target,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {1: 30, 2: 45, 3: 25},</span>
<span class="gi">+            &quot;under-sampling&quot;,</span>
<span class="gi">+            {1: 30, 2: 45, 3: 25},</span>
<span class="gi">+            multiclass_target,</span>
<span class="gi">+        ),</span>
<span class="gi">+        ([1], &quot;clean-sampling&quot;, {1: 25}, multiclass_target),</span>
<span class="gi">+        (</span>
<span class="gi">+            _sampling_strategy_func,</span>
<span class="gi">+            &quot;over-sampling&quot;,</span>
<span class="gi">+            {1: 50, 2: 0, 3: 75},</span>
<span class="gi">+            multiclass_target,</span>
<span class="gi">+        ),</span>
<span class="gi">+        (0.5, &quot;over-sampling&quot;, {1: 25}, binary_target),</span>
<span class="gi">+        (0.5, &quot;under-sampling&quot;, {0: 50}, binary_target),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_check_sampling_strategy(</span>
<span class="gi">+    sampling_strategy, sampling_type, expected_sampling_strategy, target</span>
<span class="gi">+):</span>
<span class="gi">+    sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+        sampling_strategy, target, sampling_type</span>
<span class="gi">+    )</span>
<span class="gi">+    assert sampling_strategy_ == expected_sampling_strategy</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_sampling_strategy_callable_args():</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    multiplier = {1: 1.5, 2: 1, 3: 3}</span>
<span class="gi">+</span>
<span class="gi">+    def sampling_strategy_func(y, multiplier):</span>
<span class="gi">+        &quot;&quot;&quot;samples such that each class will be affected by the multiplier.&quot;&quot;&quot;</span>
<span class="gi">+        target_stats = Counter(y)</span>
<span class="gi">+        return {</span>
<span class="gi">+            key: int(values * multiplier[key]) for key, values in target_stats.items()</span>
<span class="gi">+        }</span>
<span class="gi">+</span>
<span class="gi">+    sampling_strategy_ = check_sampling_strategy(</span>
<span class="gi">+        sampling_strategy_func, y, &quot;over-sampling&quot;, multiplier=multiplier</span>
<span class="gi">+    )</span>
<span class="gi">+    assert sampling_strategy_ == {1: 25, 2: 0, 3: 50}</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;sampling_strategy, sampling_type, expected_result&quot;,</span>
<span class="gi">+    [</span>
<span class="gi">+        (</span>
<span class="gi">+            {3: 25, 1: 25, 2: 25},</span>
<span class="gi">+            &quot;under-sampling&quot;,</span>
<span class="gi">+            OrderedDict({1: 25, 2: 25, 3: 25}),</span>
<span class="gi">+        ),</span>
<span class="gi">+        (</span>
<span class="gi">+            {3: 100, 1: 100, 2: 100},</span>
<span class="gi">+            &quot;over-sampling&quot;,</span>
<span class="gi">+            OrderedDict({1: 50, 2: 0, 3: 75}),</span>
<span class="gi">+        ),</span>
<span class="gi">+    ],</span>
<span class="gi">+)</span>
<span class="gi">+def test_sampling_strategy_check_order(</span>
<span class="gi">+    sampling_strategy, sampling_type, expected_result</span>
<span class="gi">+):</span>
<span class="gi">+    # We pass on purpose a non sorted dictionary and check that the resulting</span>
<span class="gi">+    # dictionary is sorted. Refer to issue #428.</span>
<span class="gi">+    y = np.array([1] * 50 + [2] * 100 + [3] * 25)</span>
<span class="gi">+    sampling_strategy_ = check_sampling_strategy(sampling_strategy, y, sampling_type)</span>
<span class="gi">+    assert sampling_strategy_ == expected_result</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_arrays_transformer_plain_list():</span>
<span class="gi">+    X = np.array([[0, 0], [1, 1]])</span>
<span class="gi">+    y = np.array([[0, 0], [1, 1]])</span>
<span class="gi">+</span>
<span class="gi">+    arrays_transformer = ArraysTransformer(X.tolist(), y.tolist())</span>
<span class="gi">+    X_res, y_res = arrays_transformer.transform(X, y)</span>
<span class="gi">+    assert isinstance(X_res, list)</span>
<span class="gi">+    assert isinstance(y_res, list)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_arrays_transformer_numpy():</span>
<span class="gi">+    X = np.array([[0, 0], [1, 1]])</span>
<span class="gi">+    y = np.array([[0, 0], [1, 1]])</span>
<span class="gi">+</span>
<span class="gi">+    arrays_transformer = ArraysTransformer(X, y)</span>
<span class="gi">+    X_res, y_res = arrays_transformer.transform(X, y)</span>
<span class="gi">+    assert isinstance(X_res, np.ndarray)</span>
<span class="gi">+    assert isinstance(y_res, np.ndarray)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_arrays_transformer_pandas():</span>
<span class="gi">+    pd = pytest.importorskip(&quot;pandas&quot;)</span>
<span class="gi">+</span>
<span class="gi">+    X = np.array([[0, 0], [1, 1]])</span>
<span class="gi">+    y = np.array([0, 1])</span>
<span class="gi">+</span>
<span class="gi">+    X_df = pd.DataFrame(X, columns=[&quot;a&quot;, &quot;b&quot;])</span>
<span class="gi">+    X_df = X_df.astype(int)</span>
<span class="gi">+    y_df = pd.DataFrame(y, columns=[&quot;target&quot;])</span>
<span class="gi">+    y_df = y_df.astype(int)</span>
<span class="gi">+    y_s = pd.Series(y, name=&quot;target&quot;, dtype=int)</span>
<span class="gi">+</span>
<span class="gi">+    # DataFrame and DataFrame case</span>
<span class="gi">+    arrays_transformer = ArraysTransformer(X_df, y_df)</span>
<span class="gi">+    X_res, y_res = arrays_transformer.transform(X, y)</span>
<span class="gi">+    assert isinstance(X_res, pd.DataFrame)</span>
<span class="gi">+    assert_array_equal(X_res.columns, X_df.columns)</span>
<span class="gi">+    assert_array_equal(X_res.dtypes, X_df.dtypes)</span>
<span class="gi">+    assert isinstance(y_res, pd.DataFrame)</span>
<span class="gi">+    assert_array_equal(y_res.columns, y_df.columns)</span>
<span class="gi">+    assert_array_equal(y_res.dtypes, y_df.dtypes)</span>
<span class="gi">+</span>
<span class="gi">+    # DataFrames and Series case</span>
<span class="gi">+    arrays_transformer = ArraysTransformer(X_df, y_s)</span>
<span class="gi">+    _, y_res = arrays_transformer.transform(X, y)</span>
<span class="gi">+    assert isinstance(y_res, pd.Series)</span>
<span class="gi">+    assert_array_equal(y_res.name, y_s.name)</span>
<span class="gi">+    assert_array_equal(y_res.dtype, y_s.dtype)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def test_deprecate_positional_args_warns_for_function():</span>
<span class="gi">+    @_deprecate_positional_args</span>
<span class="gi">+    def f1(a, b, *, c=1, d=1):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=r&quot;Pass c=3 as keyword args&quot;):</span>
<span class="gi">+        f1(1, 2, 3)</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=r&quot;Pass c=3, d=4 as keyword args&quot;):</span>
<span class="gi">+        f1(1, 2, 3, 4)</span>
<span class="gi">+</span>
<span class="gi">+    @_deprecate_positional_args</span>
<span class="gi">+    def f2(a=1, *, b=1, c=1, d=1):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=r&quot;Pass b=2 as keyword args&quot;):</span>
<span class="gi">+        f2(1, 2)</span>
<span class="gi">+</span>
<span class="gi">+    # The * is place before a keyword only argument without a default value</span>
<span class="gi">+    @_deprecate_positional_args</span>
<span class="gi">+    def f3(a, *, b, c=1, d=1):</span>
<span class="gi">+        pass</span>
<span class="gi">+</span>
<span class="gi">+    with pytest.warns(FutureWarning, match=r&quot;Pass b=2 as keyword args&quot;):</span>
<span class="gi">+        f3(1, 2)</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@pytest.mark.parametrize(</span>
<span class="gi">+    &quot;estimator, is_neighbor_estimator&quot;, [(NearestNeighbors(), True), (KMeans(), False)]</span>
<span class="gi">+)</span>
<span class="gi">+def test_is_neighbors_object(estimator, is_neighbor_estimator):</span>
<span class="gi">+    assert _is_neighbors_object(estimator) == is_neighbor_estimator</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
    
  </body>
</html>