{"created": 1732308266.2129765, "duration": 8.048333168029785, "exitcode": 1, "root": "/testbed", "environment": {}, "summary": {"failed": 325, "passed": 16, "error": 26, "skipped": 4, "total": 371, "collected": 371}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests", "type": "Package"}]}, {"nodeid": "tests/fake-nested-templates/fake-package", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-nested-templates/fake-project", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-nested-templates", "outcome": "passed", "result": [{"nodeid": "tests/fake-nested-templates/fake-package", "type": "Dir"}, {"nodeid": "tests/fake-nested-templates/fake-project", "type": "Dir"}]}, {"nodeid": "tests/fake-nested-templates-old-style/fake-package", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-nested-templates-old-style", "outcome": "passed", "result": [{"nodeid": "tests/fake-nested-templates-old-style/fake-package", "type": "Dir"}]}, {"nodeid": "tests/fake-repo/fake-project", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo/fake-project", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-bad", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-bad-json/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-bad-json", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-bad-json/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-dict/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-dict", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-dict/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-dir/my-dir/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-dir/my-dir", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-dir/my-dir/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-dir", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-dir/my-dir", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-pre/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-pre", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-pre/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-pre2/{%{cookiecutter.repo_name}%}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-pre2", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-pre2/{%{cookiecutter.repo_name}%}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-replay/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-replay", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-replay/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-tmpl/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-tmpl", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-tmpl/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/fake-repo-tmpl-_cookiecutter/{{cookiecutter.repo_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/fake-repo-tmpl-_cookiecutter", "outcome": "passed", "result": [{"nodeid": "tests/fake-repo-tmpl-_cookiecutter/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/files", "outcome": "passed", "result": []}, {"nodeid": "tests/hooks-abort-render/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/hooks-abort-render/{{cookiecutter.repo_dir}}", "outcome": "passed", "result": []}, {"nodeid": "tests/hooks-abort-render", "outcome": "passed", "result": [{"nodeid": "tests/hooks-abort-render/hooks", "type": "Dir"}, {"nodeid": "tests/hooks-abort-render/{{cookiecutter.repo_dir}}", "type": "Dir"}]}, {"nodeid": "tests/replay/test_dump.py", "outcome": "passed", "result": [{"nodeid": "tests/replay/test_dump.py::test_type_error_if_no_template_name", "type": "Function", "lineno": 34}, {"nodeid": "tests/replay/test_dump.py::test_type_error_if_not_dict_context", "type": "Function", "lineno": 40}, {"nodeid": "tests/replay/test_dump.py::test_value_error_if_key_missing_in_context", "type": "Function", "lineno": 46}, {"nodeid": "tests/replay/test_dump.py::test_ioerror_if_replay_dir_creation_fails", "type": "Function", "lineno": 75}, {"nodeid": "tests/replay/test_dump.py::test_run_json_dump", "type": "Function", "lineno": 83}]}, {"nodeid": "tests/replay/test_load.py", "outcome": "passed", "result": [{"nodeid": "tests/replay/test_load.py::test_type_error_if_no_template_name", "type": "Function", "lineno": 23}, {"nodeid": "tests/replay/test_load.py::test_value_error_if_key_missing_in_context", "type": "Function", "lineno": 29}, {"nodeid": "tests/replay/test_load.py::test_io_error_if_no_replay_file", "type": "Function", "lineno": 36}, {"nodeid": "tests/replay/test_load.py::test_run_json_load", "type": "Function", "lineno": 42}]}, {"nodeid": "tests/replay/test_replay.py", "outcome": "passed", "result": [{"nodeid": "tests/replay/test_replay.py::test_get_replay_file_name[bar]", "type": "Function", "lineno": 9}, {"nodeid": "tests/replay/test_replay.py::test_get_replay_file_name[bar.json]", "type": "Function", "lineno": 9}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs0]", "type": "Function", "lineno": 17}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs1]", "type": "Function", "lineno": 17}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs2]", "type": "Function", "lineno": 17}, {"nodeid": "tests/replay/test_replay.py::test_main_does_not_invoke_dump_but_load", "type": "Function", "lineno": 31}, {"nodeid": "tests/replay/test_replay.py::test_main_does_not_invoke_load_but_dump", "type": "Function", "lineno": 48}]}, {"nodeid": "tests/replay", "outcome": "passed", "result": [{"nodeid": "tests/replay/test_dump.py", "type": "Module"}, {"nodeid": "tests/replay/test_load.py", "type": "Module"}, {"nodeid": "tests/replay/test_replay.py", "type": "Module"}]}, {"nodeid": "tests/repository/test_abbreviation_expansion.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Simple expansion]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Skip expansion (expansion not an abbreviation)]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Expansion prefix]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[expansion_override_builtin]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[expansion_prefix_ignores_suffix]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (github)]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (gitlab)]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (bitbucket)]", "type": "Function", "lineno": 8}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion_prefix_not_0_in_braces", "type": "Function", "lineno": 49}]}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[/path/to/zipfile.zip-False]", "type": "Function", "lineno": 9}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[https://example.com/path/to/zipfile.zip-True]", "type": "Function", "lineno": 9}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[http://example.com/path/to/zipfile.zip-True]", "type": "Function", "lineno": 9}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_repository_url_should_clone", "type": "Function", "lineno": 60}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_repository_url_with_no_context_file", "type": "Function", "lineno": 92}]}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py::test_should_find_existing_cookiecutter", "type": "Function", "lineno": 29}]}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py::test_should_find_existing_cookiecutter", "type": "Function", "lineno": 33}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py::test_local_repo_typo", "type": "Function", "lineno": 50}]}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_finds_local_repo", "type": "Function", "lineno": 9}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_local_repo_with_no_context_raises", "type": "Function", "lineno": 23}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_local_repo_typo", "type": "Function", "lineno": 47}]}, {"nodeid": "tests/repository/test_is_repo_url.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[/path/to/zipfile.zip]", "type": "Function", "lineno": 20}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[https://example.com/path/to/zipfile.zip]", "type": "Function", "lineno": 20}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[http://example.com/path/to/zipfile.zip]", "type": "Function", "lineno": 20}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[gitolite@server:team/repo]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[git@github.com:audreyfeldroy/cookiecutter.git]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[https://github.com/cookiecutter/cookiecutter.git]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[git+https://private.com/gitrepo]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[hg+https://private.com/mercurialrepo]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[https://bitbucket.org/pokoli/cookiecutter.hg]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[file://server/path/to/repo.git]", "type": "Function", "lineno": 41}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[/audreyr/cookiecutter.git]", "type": "Function", "lineno": 61}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[/home/audreyr/cookiecutter]", "type": "Function", "lineno": 61}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template]", "type": "Function", "lineno": 61}, {"nodeid": "tests/repository/test_is_repo_url.py::test_expand_abbreviations", "type": "Function", "lineno": 66}]}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_valid_repository", "type": "Function", "lineno": 7}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_invalid_repository[tests/fake-repo-bad]", "type": "Function", "lineno": 12}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_invalid_repository[tests/unknown-repo]", "type": "Function", "lineno": 12}]}, {"nodeid": "tests/repository", "outcome": "passed", "result": [{"nodeid": "tests/repository/test_abbreviation_expansion.py", "type": "Module"}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py", "type": "Module"}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py", "type": "Module"}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py", "type": "Module"}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py", "type": "Module"}, {"nodeid": "tests/repository/test_is_repo_url.py", "type": "Module"}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py", "type": "Module"}]}, {"nodeid": "tests/test-config", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/custom-extension-post/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/custom-extension-post/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/custom-extension-post", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/custom-extension-post/hooks", "type": "Dir"}, {"nodeid": "tests/test-extensions/custom-extension-post/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-extensions/custom-extension-pre/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/custom-extension-pre/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/custom-extension-pre", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/custom-extension-pre/hooks", "type": "Dir"}, {"nodeid": "tests/test-extensions/custom-extension-pre/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-extensions/default/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/default", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/default/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-extensions/hello_extension", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/local_extension/local_extensions", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/local_extension/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/local_extension", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/local_extension/local_extensions", "type": "Package"}, {"nodeid": "tests/test-extensions/local_extension/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-extensions/unknown/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-extensions/unknown", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/unknown/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-extensions", "outcome": "passed", "result": [{"nodeid": "tests/test-extensions/custom-extension-post", "type": "Dir"}, {"nodeid": "tests/test-extensions/custom-extension-pre", "type": "Dir"}, {"nodeid": "tests/test-extensions/default", "type": "Dir"}, {"nodeid": "tests/test-extensions/hello_extension", "type": "Package"}, {"nodeid": "tests/test-extensions/local_extension", "type": "Dir"}, {"nodeid": "tests/test-extensions/unknown", "type": "Dir"}]}, {"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}/{{cookiecutter.binary_test}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-binaries", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-binaries/input{{cookiecutter.binary_test}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-context", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-not-rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/rendered", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-not-rendered", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-rendered", "type": "Dir"}]}, {"nodeid": "tests/test-generate-copy-without-render", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-copy-without-render/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-not-rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-rendered", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/rendered", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-not-rendered", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}/{{cookiecutter.repo_name}}-rendered", "type": "Dir"}]}, {"nodeid": "tests/test-generate-copy-without-render-override", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-copy-without-render-override/{{cookiecutter.repo_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-files/input{{cookiecutter.food}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-files", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-files/input{{cookiecutter.food}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}/folder", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}/{{cookiecutter.folder_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}/folder", "type": "Dir"}, {"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}/{{cookiecutter.folder_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-files-line-end", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-files-line-end/{{cookiecutter.test_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-generate-files-nontemplated/input", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-files-nontemplated", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-files-nontemplated/input", "type": "Dir"}]}, {"nodeid": "tests/test-generate-files-permissions/input{{cookiecutter.permissions}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-generate-files-permissions", "outcome": "passed", "result": [{"nodeid": "tests/test-generate-files-permissions/input{{cookiecutter.permissions}}", "type": "Dir"}]}, {"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}/folder", "outcome": "passed", "result": []}, {"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}/{{cookiecutter.folder_name}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}", "outcome": "passed", "result": [{"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}/folder", "type": "Dir"}, {"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}/{{cookiecutter.folder_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-output-folder", "outcome": "passed", "result": [{"nodeid": "tests/test-output-folder/{{cookiecutter.test_name}}", "type": "Dir"}]}, {"nodeid": "tests/test-pyhooks/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-pyhooks/input{{cookiecutter.pyhooks}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-pyhooks", "outcome": "passed", "result": [{"nodeid": "tests/test-pyhooks/hooks", "type": "Dir"}, {"nodeid": "tests/test-pyhooks/input{{cookiecutter.pyhooks}}", "type": "Dir"}]}, {"nodeid": "tests/test-pyshellhooks/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-pyshellhooks/input{{pyshellhooks}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-pyshellhooks", "outcome": "passed", "result": [{"nodeid": "tests/test-pyshellhooks/hooks", "type": "Dir"}, {"nodeid": "tests/test-pyshellhooks/input{{pyshellhooks}}", "type": "Dir"}]}, {"nodeid": "tests/test-replay", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks/input{{cookiecutter.shellhooks}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks", "outcome": "passed", "result": [{"nodeid": "tests/test-shellhooks/hooks", "type": "Dir"}, {"nodeid": "tests/test-shellhooks/input{{cookiecutter.shellhooks}}", "type": "Dir"}]}, {"nodeid": "tests/test-shellhooks-empty/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks-empty/input{{cookiecutter.shellhooks}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks-empty", "outcome": "passed", "result": [{"nodeid": "tests/test-shellhooks-empty/hooks", "type": "Dir"}, {"nodeid": "tests/test-shellhooks-empty/input{{cookiecutter.shellhooks}}", "type": "Dir"}]}, {"nodeid": "tests/test-shellhooks-win/hooks", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks-win/input{{cookiecutter.shellhooks}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-shellhooks-win", "outcome": "passed", "result": [{"nodeid": "tests/test-shellhooks-win/hooks", "type": "Dir"}, {"nodeid": "tests/test-shellhooks-win/input{{cookiecutter.shellhooks}}", "type": "Dir"}]}, {"nodeid": "tests/test-templates/extends/templates", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/extends/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/extends", "outcome": "passed", "result": [{"nodeid": "tests/test-templates/extends/templates", "type": "Dir"}, {"nodeid": "tests/test-templates/extends/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-templates/include/templates", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/include/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/include", "outcome": "passed", "result": [{"nodeid": "tests/test-templates/include/templates", "type": "Dir"}, {"nodeid": "tests/test-templates/include/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-templates/no-templates/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/no-templates", "outcome": "passed", "result": [{"nodeid": "tests/test-templates/no-templates/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-templates/super/templates", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/super/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/test-templates/super", "outcome": "passed", "result": [{"nodeid": "tests/test-templates/super/templates", "type": "Dir"}, {"nodeid": "tests/test-templates/super/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/test-templates", "outcome": "passed", "result": [{"nodeid": "tests/test-templates/extends", "type": "Dir"}, {"nodeid": "tests/test-templates/include", "type": "Dir"}, {"nodeid": "tests/test-templates/no-templates", "type": "Dir"}, {"nodeid": "tests/test-templates/super", "type": "Dir"}]}, {"nodeid": "tests/test_abort_generate_on_hook_error.py", "outcome": "passed", "result": [{"nodeid": "tests/test_abort_generate_on_hook_error.py::test_hooks_raises_errors[pre_gen_hook_raises_error]", "type": "Function", "lineno": 12}, {"nodeid": "tests/test_abort_generate_on_hook_error.py::test_hooks_raises_errors[post_gen_hook_raises_error]", "type": "Function", "lineno": 12}]}, {"nodeid": "tests/test_cli.py", "outcome": "passed", "result": [{"nodeid": "tests/test_cli.py::test_cli_version[-V]", "type": "Function", "lineno": 67}, {"nodeid": "tests/test_cli.py::test_cli_version[--version]", "type": "Function", "lineno": 67}, {"nodeid": "tests/test_cli.py::test_cli_error_on_existing_output_directory", "type": "Function", "lineno": 74}, {"nodeid": "tests/test_cli.py::test_cli", "type": "Function", "lineno": 83}, {"nodeid": "tests/test_cli.py::test_cli_verbose", "type": "Function", "lineno": 93}, {"nodeid": "tests/test_cli.py::test_cli_replay", "type": "Function", "lineno": 103}, {"nodeid": "tests/test_cli.py::test_cli_replay_file", "type": "Function", "lineno": 130}, {"nodeid": "tests/test_cli.py::test_cli_replay_generated", "type": "Function", "lineno": 157}, {"nodeid": "tests/test_cli.py::test_cli_exit_on_noinput_and_replay", "type": "Function", "lineno": 173}, {"nodeid": "tests/test_cli.py::test_run_cookiecutter_on_overwrite_if_exists_and_replay[-f]", "type": "Function", "lineno": 215}, {"nodeid": "tests/test_cli.py::test_run_cookiecutter_on_overwrite_if_exists_and_replay[--overwrite-if-exists]", "type": "Function", "lineno": 215}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_does_not_exist[-f]", "type": "Function", "lineno": 245}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_does_not_exist[--overwrite-if-exists]", "type": "Function", "lineno": 245}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_exists[-f]", "type": "Function", "lineno": 259}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_exists[--overwrite-if-exists]", "type": "Function", "lineno": 259}, {"nodeid": "tests/test_cli.py::test_cli_output_dir[-o]", "type": "Function", "lineno": 276}, {"nodeid": "tests/test_cli.py::test_cli_output_dir[--output-dir]", "type": "Function", "lineno": 276}, {"nodeid": "tests/test_cli.py::test_cli_help[-h]", "type": "Function", "lineno": 308}, {"nodeid": "tests/test_cli.py::test_cli_help[--help]", "type": "Function", "lineno": 308}, {"nodeid": "tests/test_cli.py::test_cli_help[help]", "type": "Function", "lineno": 308}, {"nodeid": "tests/test_cli.py::test_user_config", "type": "Function", "lineno": 321}, {"nodeid": "tests/test_cli.py::test_default_user_config_overwrite", "type": "Function", "lineno": 347}, {"nodeid": "tests/test_cli.py::test_default_user_config", "type": "Function", "lineno": 378}, {"nodeid": "tests/test_cli.py::test_echo_undefined_variable_error", "type": "Function", "lineno": 404}, {"nodeid": "tests/test_cli.py::test_echo_unknown_extension_error", "type": "Function", "lineno": 444}, {"nodeid": "tests/test_cli.py::test_local_extension", "type": "Function", "lineno": 461}, {"nodeid": "tests/test_cli.py::test_local_extension_not_available", "type": "Function", "lineno": 479}, {"nodeid": "tests/test_cli.py::test_cli_extra_context", "type": "Function", "lineno": 489}, {"nodeid": "tests/test_cli.py::test_cli_extra_context_invalid_format", "type": "Function", "lineno": 504}, {"nodeid": "tests/test_cli.py::test_debug_file_non_verbose", "type": "Function", "lineno": 524}, {"nodeid": "tests/test_cli.py::test_debug_file_verbose", "type": "Function", "lineno": 550}, {"nodeid": "tests/test_cli.py::test_debug_list_installed_templates", "type": "Function", "lineno": 577}, {"nodeid": "tests/test_cli.py::test_debug_list_installed_templates_failure", "type": "Function", "lineno": 597}, {"nodeid": "tests/test_cli.py::test_directory_repo", "type": "Function", "lineno": 612}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=yes-None-True]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=no-None-False]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=ask-yes-True]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=ask-no-False]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=yes-None-True]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=no-None-False]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=ask-yes-True]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=ask-no-False]", "type": "Function", "lineno": 635}, {"nodeid": "tests/test_cli.py::test_cli_with_json_decoding_error", "type": "Function", "lineno": 674}, {"nodeid": "tests/test_cli.py::test_cli_with_pre_prompt_hook", "type": "Function", "lineno": 693}, {"nodeid": "tests/test_cli.py::test_cli_with_pre_prompt_hook_fail", "type": "Function", "lineno": 705}]}, {"nodeid": "tests/test_cookiecutter_invocation.py", "outcome": "passed", "result": [{"nodeid": "tests/test_cookiecutter_invocation.py::test_should_invoke_main", "type": "Function", "lineno": 25}]}, {"nodeid": "tests/test_cookiecutter_local_no_input.py", "outcome": "passed", "result": [{"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre/]", "type": "Function", "lineno": 34}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre]", "type": "Function", "lineno": 34}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_extra_context", "type": "Function", "lineno": 46}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_templated_context", "type": "Function", "lineno": 57}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_rendered_file", "type": "Function", "lineno": 64}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_dict_values_in_context", "type": "Function", "lineno": 73}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_template_cleanup", "type": "Function", "lineno": 123}]}, {"nodeid": "tests/test_cookiecutter_local_with_input.py", "outcome": "passed", "result": [{"nodeid": "tests/test_cookiecutter_local_with_input.py::test_cookiecutter_local_with_input", "type": "Function", "lineno": 19}, {"nodeid": "tests/test_cookiecutter_local_with_input.py::test_cookiecutter_input_extra_context", "type": "Function", "lineno": 34}]}, {"nodeid": "tests/test_cookiecutter_nested_templates.py", "outcome": "passed", "result": [{"nodeid": "tests/test_cookiecutter_nested_templates.py::test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "type": "Function", "lineno": 9}, {"nodeid": "tests/test_cookiecutter_nested_templates.py::test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "type": "Function", "lineno": 9}]}, {"nodeid": "tests/test_custom_extensions_in_hooks.py", "outcome": "passed", "result": [{"nodeid": "tests/test_custom_extensions_in_hooks.py::test_hook_with_extension[pre_gen_hook]", "type": "Function", "lineno": 29}, {"nodeid": "tests/test_custom_extensions_in_hooks.py::test_hook_with_extension[post_gen_hook]", "type": "Function", "lineno": 29}]}, {"nodeid": "tests/test_default_extensions.py", "outcome": "passed", "result": [{"nodeid": "tests/test_default_extensions.py::test_jinja2_time_extension", "type": "Function", "lineno": 21}, {"nodeid": "tests/test_default_extensions.py::test_jinja2_slugify_extension", "type": "Function", "lineno": 44}, {"nodeid": "tests/test_default_extensions.py::test_jinja2_uuid_extension", "type": "Function", "lineno": 53}]}, {"nodeid": "tests/test_environment.py", "outcome": "passed", "result": [{"nodeid": "tests/test_environment.py::test_env_should_raise_for_unknown_extension", "type": "Function", "lineno": 8}, {"nodeid": "tests/test_environment.py::test_env_should_come_with_default_extensions", "type": "Function", "lineno": 18}]}, {"nodeid": "tests/test_exceptions.py", "outcome": "passed", "result": [{"nodeid": "tests/test_exceptions.py::test_undefined_variable_to_str", "type": "Function", "lineno": 7}]}, {"nodeid": "tests/test_find.py", "outcome": "passed", "result": [{"nodeid": "tests/test_find.py::test_find_template[template with default jinja strings]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_find.py::test_find_template[template with custom jinja strings]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_find.py::test_find_template[template with custom jinja strings but folder with default jinja strings]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_find.py::test_find_template[template missing folder]", "type": "Function", "lineno": 24}]}, {"nodeid": "tests/test_generate_context.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_context.py::test_generate_context[input_params0-expected_context0]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params1-expected_context1]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params2-expected_context2]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params3-expected_context3]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_generate_context.py::test_generate_context_with_json_decoding_error", "type": "Function", "lineno": 60}, {"nodeid": "tests/test_generate_context.py::test_default_context_replacement_in_generate_context", "type": "Function", "lineno": 76}, {"nodeid": "tests/test_generate_context.py::test_generate_context_decodes_non_ascii_chars", "type": "Function", "lineno": 111}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_does_include_unused_variables", "type": "Function", "lineno": 150}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_non_list_value", "type": "Function", "lineno": 159}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_does_not_modify_choices_for_invalid_overwrite", "type": "Function", "lineno": 168}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_invalid_overwrite", "type": "Function", "lineno": 199}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_multichoice_values", "type": "Function", "lineno": 207}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_invalid_multichoice_values", "type": "Function", "lineno": 216}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_error_additional_values", "type": "Function", "lineno": 225}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_dictionaries", "type": "Function", "lineno": 234}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_default_for_choice_variable", "type": "Function", "lineno": 244}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_nested_dict", "type": "Function", "lineno": 253}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrite_context_as_in_nested_dict_with_additional_values", "type": "Function", "lineno": 295}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_nested_dict_additional_values", "type": "Function", "lineno": 311}]}, {"nodeid": "tests/test_generate_copy_without_render.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_copy_without_render.py::test_generate_copy_without_render_extensions", "type": "Function", "lineno": 18}]}, {"nodeid": "tests/test_generate_copy_without_render_override.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_copy_without_render_override.py::test_generate_copy_without_render_extensions", "type": "Function", "lineno": 18}]}, {"nodeid": "tests/test_generate_file.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_file.py::test_generate_file", "type": "Function", "lineno": 43}, {"nodeid": "tests/test_generate_file.py::test_generate_file_jsonify_filter", "type": "Function", "lineno": 57}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[True-10]", "type": "Function", "lineno": 69}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[True-40]", "type": "Function", "lineno": 69}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[False-10]", "type": "Function", "lineno": 69}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[False-40]", "type": "Function", "lineno": 69}, {"nodeid": "tests/test_generate_file.py::test_generate_file_with_true_condition", "type": "Function", "lineno": 82}, {"nodeid": "tests/test_generate_file.py::test_generate_file_with_false_condition", "type": "Function", "lineno": 101}, {"nodeid": "tests/test_generate_file.py::test_generate_file_verbose_template_syntax_error", "type": "Function", "lineno": 128}, {"nodeid": "tests/test_generate_file.py::test_generate_file_does_not_translate_lf_newlines_to_crlf", "type": "Function", "lineno": 140}, {"nodeid": "tests/test_generate_file.py::test_generate_file_does_not_translate_crlf_newlines_to_lf", "type": "Function", "lineno": 158}, {"nodeid": "tests/test_generate_file.py::test_generate_file_handles_mixed_line_endings", "type": "Function", "lineno": 176}]}, {"nodeid": "tests/test_generate_files.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_files.py::test_generate_files_nontemplated_exception", "type": "Function", "lineno": 14}, {"nodeid": "tests/test_generate_files.py::test_generate_files", "type": "Function", "lineno": 28}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_linux_newline", "type": "Function", "lineno": 44}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_jinja2_environment", "type": "Function", "lineno": 62}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_trailing_newline_forced_to_linux_by_context", "type": "Function", "lineno": 83}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_windows_newline", "type": "Function", "lineno": 102}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_windows_newline_forced_to_linux_by_context", "type": "Function", "lineno": 120}, {"nodeid": "tests/test_generate_files.py::test_generate_files_binaries", "type": "Function", "lineno": 139}, {"nodeid": "tests/test_generate_files.py::test_generate_files_absolute_path", "type": "Function", "lineno": 160}, {"nodeid": "tests/test_generate_files.py::test_generate_files_output_dir", "type": "Function", "lineno": 170}, {"nodeid": "tests/test_generate_files.py::test_generate_files_permissions", "type": "Function", "lineno": 186}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_overwrite_if_exists_with_skip_if_file_exists", "type": "Function", "lineno": 230}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_skip_if_file_exists", "type": "Function", "lineno": 256}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_overwrite_if_exists", "type": "Function", "lineno": 280}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_name", "type": "Function", "lineno": 312}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_name_existing_project", "type": "Function", "lineno": 327}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_content", "type": "Function", "lineno": 348}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_dir_name", "type": "Function", "lineno": 363}, {"nodeid": "tests/test_generate_files.py::test_keep_project_dir_on_failure", "type": "Function", "lineno": 382}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_dir_name_existing_project", "type": "Function", "lineno": 394}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_project_dir", "type": "Function", "lineno": 419}]}, {"nodeid": "tests/test_generate_hooks.py", "outcome": "passed", "result": [{"nodeid": "tests/test_generate_hooks.py::test_ignore_hooks_dirs", "type": "Function", "lineno": 31}, {"nodeid": "tests/test_generate_hooks.py::test_run_python_hooks", "type": "Function", "lineno": 42}, {"nodeid": "tests/test_generate_hooks.py::test_run_python_hooks_cwd", "type": "Function", "lineno": 58}, {"nodeid": "tests/test_generate_hooks.py::test_empty_hooks", "type": "Function", "lineno": 72}, {"nodeid": "tests/test_generate_hooks.py::test_oserror_hooks", "type": "Function", "lineno": 89}, {"nodeid": "tests/test_generate_hooks.py::test_run_failing_hook_removes_output_directory", "type": "Function", "lineno": 113}, {"nodeid": "tests/test_generate_hooks.py::test_run_failing_hook_preserves_existing_output_directory", "type": "Function", "lineno": 142}, {"nodeid": "tests/test_generate_hooks.py::test_run_shell_hooks", "type": "Function", "lineno": 172}, {"nodeid": "tests/test_generate_hooks.py::test_run_shell_hooks_win", "type": "Function", "lineno": 194}, {"nodeid": "tests/test_generate_hooks.py::test_ignore_shell_hooks", "type": "Function", "lineno": 216}, {"nodeid": "tests/test_generate_hooks.py::test_deprecate_run_hook_from_repo_dir", "type": "Function", "lineno": 233}]}, {"nodeid": "tests/test_get_config.py", "outcome": "passed", "result": [{"nodeid": "tests/test_get_config.py::test_merge_configs", "type": "Function", "lineno": 11}, {"nodeid": "tests/test_get_config.py::test_get_config", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_get_config.py::test_get_config_does_not_exist", "type": "Function", "lineno": 82}, {"nodeid": "tests/test_get_config.py::test_invalid_config", "type": "Function", "lineno": 91}, {"nodeid": "tests/test_get_config.py::test_get_config_with_defaults", "type": "Function", "lineno": 103}, {"nodeid": "tests/test_get_config.py::test_get_config_empty_config_file", "type": "Function", "lineno": 125}, {"nodeid": "tests/test_get_config.py::test_get_config_invalid_file_with_array_as_top_level_element", "type": "Function", "lineno": 131}, {"nodeid": "tests/test_get_config.py::test_get_config_invalid_file_with_multiple_docs", "type": "Function", "lineno": 142}]}, {"nodeid": "tests/test_get_user_config.py", "outcome": "passed", "result": [{"nodeid": "tests/test_get_user_config.py::test_get_user_config_valid", "type": "Function", "lineno": 69}, {"nodeid": "tests/test_get_user_config.py::test_get_user_config_invalid", "type": "Function", "lineno": 78}, {"nodeid": "tests/test_get_user_config.py::test_get_user_config_nonexistent", "type": "Function", "lineno": 86}, {"nodeid": "tests/test_get_user_config.py::test_specify_config_path", "type": "Function", "lineno": 98}, {"nodeid": "tests/test_get_user_config.py::test_default_config_path", "type": "Function", "lineno": 108}, {"nodeid": "tests/test_get_user_config.py::test_default_config_from_env_variable", "type": "Function", "lineno": 113}, {"nodeid": "tests/test_get_user_config.py::test_force_default_config", "type": "Function", "lineno": 123}, {"nodeid": "tests/test_get_user_config.py::test_expand_user_for_directories_in_config", "type": "Function", "lineno": 133}, {"nodeid": "tests/test_get_user_config.py::test_expand_vars_for_directories_in_config", "type": "Function", "lineno": 148}, {"nodeid": "tests/test_get_user_config.py::test_specify_config_values", "type": "Function", "lineno": 159}]}, {"nodeid": "tests/test_hooks.py::TestFindHooks", "outcome": "passed", "result": [{"nodeid": "tests/test_hooks.py::TestFindHooks::test_find_hook", "type": "Function", "lineno": 87}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_no_hooks", "type": "Function", "lineno": 98}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_unknown_hooks_dir", "type": "Function", "lineno": 103}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_hook_not_found", "type": "Function", "lineno": 108}]}, {"nodeid": "tests/test_hooks.py::TestExternalHooks", "outcome": "passed", "result": [{"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script", "type": "Function", "lineno": 143}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_script", "type": "Function", "lineno": 148}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_script_enoexec", "type": "Function", "lineno": 159}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script_cwd", "type": "Function", "lineno": 173}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script_with_context", "type": "Function", "lineno": 179}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_hook", "type": "Function", "lineno": 207}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_hook", "type": "Function", "lineno": 219}]}, {"nodeid": "tests/test_hooks.py", "outcome": "passed", "result": [{"nodeid": "tests/test_hooks.py::TestFindHooks", "type": "Class"}, {"nodeid": "tests/test_hooks.py::TestExternalHooks", "type": "Class"}, {"nodeid": "tests/test_hooks.py::test_ignore_hook_backup_files", "type": "Function", "lineno": 269}]}, {"nodeid": "tests/test_log.py", "outcome": "passed", "result": [{"nodeid": "tests/test_log.py::test_info_stdout_logging", "type": "Function", "lineno": 72}, {"nodeid": "tests/test_log.py::test_debug_stdout_logging", "type": "Function", "lineno": 89}, {"nodeid": "tests/test_log.py::test_debug_file_logging", "type": "Function", "lineno": 106}]}, {"nodeid": "tests/test_main.py", "outcome": "passed", "result": [{"nodeid": "tests/test_main.py::test_original_cookiecutter_options_preserved_in__cookiecutter", "type": "Function", "lineno": 5}, {"nodeid": "tests/test_main.py::test_replay_dump_template_name", "type": "Function", "lineno": 31}, {"nodeid": "tests/test_main.py::test_replay_load_template_name", "type": "Function", "lineno": 64}, {"nodeid": "tests/test_main.py::test_custom_replay_file", "type": "Function", "lineno": 95}]}, {"nodeid": "tests/test_output_folder.py", "outcome": "passed", "result": [{"nodeid": "tests/test_output_folder.py::test_output_folder", "type": "Function", "lineno": 23}, {"nodeid": "tests/test_output_folder.py::test_exception_when_output_folder_exists", "type": "Function", "lineno": 46}]}, {"nodeid": "tests/test_pre_prompt_hooks.py", "outcome": "passed", "result": [{"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_python_hook", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_python_hook_fail", "type": "Function", "lineno": 33}, {"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_shell_hook", "type": "Function", "lineno": 43}]}, {"nodeid": "tests/test_preferred_encoding.py", "outcome": "passed", "result": [{"nodeid": "tests/test_preferred_encoding.py::test_not_ascii", "type": "Function", "lineno": 11}]}, {"nodeid": "tests/test_prompt.py::TestRenderVariable", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[1-1]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[True-True]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[foo-foo]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[{{cookiecutter.project}}-foobar]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[None-None]", "type": "Function", "lineno": 24}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var0-rendered_var0]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var1-rendered_var1]", "type": "Function", "lineno": 53}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var2-rendered_var2]", "type": "Function", "lineno": 53}]}, {"nodeid": "tests/test_prompt.py::TestPrompt", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config[ASCII default prompt/input]", "type": "Function", "lineno": 76}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config[Unicode default prompt/input]", "type": "Function", "lineno": 76}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_prompts[ASCII default prompt/input]", "type": "Function", "lineno": 94}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context0]", "type": "Function", "lineno": 129}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context1]", "type": "Function", "lineno": 129}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context2]", "type": "Function", "lineno": 129}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_dict", "type": "Function", "lineno": 171}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_dict", "type": "Function", "lineno": 182}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_deep_dict", "type": "Function", "lineno": 199}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_deep_dict_with_human_prompts", "type": "Function", "lineno": 244}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_internal_use_no_human_prompts", "type": "Function", "lineno": 273}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_templated_config", "type": "Function", "lineno": 286}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_dont_prompt_for_private_context_var", "type": "Function", "lineno": 311}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_private_variables_with_two_underscores", "type": "Function", "lineno": 323}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_not_render_private_variables", "type": "Function", "lineno": 360}]}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_invoke_read_user_choice", "type": "Function", "lineno": 385}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_invoke_read_user_variable", "type": "Function", "lineno": 409}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_render_choices", "type": "Function", "lineno": 429}]}, {"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig::test_should_return_first_option_if_no_input", "type": "Function", "lineno": 483}, {"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig::test_should_read_user_choice", "type": "Function", "lineno": 500}]}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_should_invoke_read_user_yes_no[True]", "type": "Function", "lineno": 521}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_should_invoke_read_user_yes_no[False]", "type": "Function", "lineno": 521}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_boolean_parameter_no_input", "type": "Function", "lineno": 545}]}, {"nodeid": "tests/test_prompt.py", "outcome": "passed", "result": [{"nodeid": "tests/test_prompt.py::TestRenderVariable", "type": "Class"}, {"nodeid": "tests/test_prompt.py::TestPrompt", "type": "Class"}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice", "type": "Class"}, {"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig", "type": "Class"}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo", "type": "Class"}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict]", "type": "Function", "lineno": 556}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with choices]", "type": "Function", "lineno": 556}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with dict_key]", "type": "Function", "lineno": 556}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with key_value]", "type": "Function", "lineno": 556}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "type": "Function", "lineno": 581}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "type": "Function", "lineno": 581}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[]", "type": "Function", "lineno": 600}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[/tmp]", "type": "Function", "lineno": 600}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[/foo]", "type": "Function", "lineno": 600}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[]", "type": "Function", "lineno": 622}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[C:/tmp]", "type": "Function", "lineno": 622}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[D:/tmp]", "type": "Function", "lineno": 622}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_rm_repo_dir", "type": "Function", "lineno": 644}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_exit_on_user_no_answer", "type": "Function", "lineno": 660}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_rm_repo_file", "type": "Function", "lineno": 679}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_keep_repo_on_no_reuse", "type": "Function", "lineno": 696}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_keep_repo_on_reuse", "type": "Function", "lineno": 712}, {"nodeid": "tests/test_prompt.py::test_prompt_should_not_ask_if_no_input_and_rm_repo_dir", "type": "Function", "lineno": 732}, {"nodeid": "tests/test_prompt.py::test_prompt_should_not_ask_if_no_input_and_rm_repo_file", "type": "Function", "lineno": 751}]}, {"nodeid": "tests/test_read_repo_password.py", "outcome": "passed", "result": [{"nodeid": "tests/test_read_repo_password.py::test_click_invocation", "type": "Function", "lineno": 5}]}, {"nodeid": "tests/test_read_user_choice.py", "outcome": "passed", "result": [{"nodeid": "tests/test_read_user_choice.py::test_click_invocation[1-hello]", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[2-world]", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[3-foo]", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[4-bar]", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_read_user_choice.py::test_raise_if_options_is_not_a_non_empty_list", "type": "Function", "lineno": 31}]}, {"nodeid": "tests/test_read_user_dict.py", "outcome": "passed", "result": [{"nodeid": "tests/test_read_user_dict.py::test_process_json_invalid_json", "type": "Function", "lineno": 9}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_non_dict", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_valid_json", "type": "Function", "lineno": 25}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_deep_dict", "type": "Function", "lineno": 38}, {"nodeid": "tests/test_read_user_dict.py::test_should_raise_type_error", "type": "Function", "lineno": 74}, {"nodeid": "tests/test_read_user_dict.py::test_should_call_prompt_with_process_json", "type": "Function", "lineno": 83}, {"nodeid": "tests/test_read_user_dict.py::test_should_not_load_json_from_sentinel", "type": "Function", "lineno": 98}, {"nodeid": "tests/test_read_user_dict.py::test_read_user_dict_default_value[\\n]", "type": "Function", "lineno": 111}, {"nodeid": "tests/test_read_user_dict.py::test_read_user_dict_default_value[\\ndefault\\n]", "type": "Function", "lineno": 111}, {"nodeid": "tests/test_read_user_dict.py::test_json_prompt_process_response", "type": "Function", "lineno": 124}]}, {"nodeid": "tests/test_read_user_variable.py", "outcome": "passed", "result": [{"nodeid": "tests/test_read_user_variable.py::test_click_invocation", "type": "Function", "lineno": 16}, {"nodeid": "tests/test_read_user_variable.py::test_input_loop_with_null_default_value", "type": "Function", "lineno": 28}]}, {"nodeid": "tests/test_read_user_yes_no.py", "outcome": "passed", "result": [{"nodeid": "tests/test_read_user_yes_no.py::test_click_invocation", "type": "Function", "lineno": 11}, {"nodeid": "tests/test_read_user_yes_no.py::test_yesno_prompt_process_response", "type": "Function", "lineno": 24}]}, {"nodeid": "tests/test_repo_not_found.py", "outcome": "passed", "result": [{"nodeid": "tests/test_repo_not_found.py::test_should_raise_error_if_repo_does_not_exist", "type": "Function", "lineno": 7}]}, {"nodeid": "tests/test_specify_output_dir.py", "outcome": "passed", "result": [{"nodeid": "tests/test_specify_output_dir.py::test_api_invocation", "type": "Function", "lineno": 47}, {"nodeid": "tests/test_specify_output_dir.py::test_default_output_dir", "type": "Function", "lineno": 64}]}, {"nodeid": "tests/test_templates.py", "outcome": "passed", "result": [{"nodeid": "tests/test_templates.py::test_build_templates[include]", "type": "Function", "lineno": 20}, {"nodeid": "tests/test_templates.py::test_build_templates[no-templates]", "type": "Function", "lineno": 20}, {"nodeid": "tests/test_templates.py::test_build_templates[extends]", "type": "Function", "lineno": 20}, {"nodeid": "tests/test_templates.py::test_build_templates[super]", "type": "Function", "lineno": 20}]}, {"nodeid": "tests/test_time_extension.py", "outcome": "passed", "result": [{"nodeid": "tests/test_time_extension.py::test_tz_is_required", "type": "Function", "lineno": 22}, {"nodeid": "tests/test_time_extension.py::test_utc_default_datetime_format", "type": "Function", "lineno": 28}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[utc]", "type": "Function", "lineno": 35}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[local]", "type": "Function", "lineno": 35}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[Europe/Berlin]", "type": "Function", "lineno": 35}, {"nodeid": "tests/test_time_extension.py::test_environment_datetime_format", "type": "Function", "lineno": 43}, {"nodeid": "tests/test_time_extension.py::test_add_time", "type": "Function", "lineno": 52}, {"nodeid": "tests/test_time_extension.py::test_substract_time", "type": "Function", "lineno": 61}, {"nodeid": "tests/test_time_extension.py::test_offset_with_format", "type": "Function", "lineno": 70}]}, {"nodeid": "tests/test_utils.py", "outcome": "passed", "result": [{"nodeid": "tests/test_utils.py::test_force_delete", "type": "Function", "lineno": 17}, {"nodeid": "tests/test_utils.py::test_rmtree", "type": "Function", "lineno": 32}, {"nodeid": "tests/test_utils.py::test_make_sure_path_exists", "type": "Function", "lineno": 43}, {"nodeid": "tests/test_utils.py::test_make_sure_path_exists_correctly_handle_os_error", "type": "Function", "lineno": 62}, {"nodeid": "tests/test_utils.py::test_work_in", "type": "Function", "lineno": 74}, {"nodeid": "tests/test_utils.py::test_work_in_without_path", "type": "Function", "lineno": 89}, {"nodeid": "tests/test_utils.py::test_create_tmp_repo_dir", "type": "Function", "lineno": 99}]}, {"nodeid": "tests/undefined-variable/dir-name/{{cookiecutter.project_slug}}/{{cookiecutter.foobar}}", "outcome": "passed", "result": []}, {"nodeid": "tests/undefined-variable/dir-name/{{cookiecutter.project_slug}}", "outcome": "passed", "result": [{"nodeid": "tests/undefined-variable/dir-name/{{cookiecutter.project_slug}}/{{cookiecutter.foobar}}", "type": "Dir"}]}, {"nodeid": "tests/undefined-variable/dir-name", "outcome": "passed", "result": [{"nodeid": "tests/undefined-variable/dir-name/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/undefined-variable/file-content/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/undefined-variable/file-content", "outcome": "passed", "result": [{"nodeid": "tests/undefined-variable/file-content/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/undefined-variable/file-name/{{cookiecutter.project_slug}}", "outcome": "passed", "result": []}, {"nodeid": "tests/undefined-variable/file-name", "outcome": "passed", "result": [{"nodeid": "tests/undefined-variable/file-name/{{cookiecutter.project_slug}}", "type": "Dir"}]}, {"nodeid": "tests/undefined-variable", "outcome": "passed", "result": [{"nodeid": "tests/undefined-variable/dir-name", "type": "Dir"}, {"nodeid": "tests/undefined-variable/file-content", "type": "Dir"}, {"nodeid": "tests/undefined-variable/file-name", "type": "Dir"}]}, {"nodeid": "tests/vcs/test_clone.py", "outcome": "passed", "result": [{"nodeid": "tests/vcs/test_clone.py::test_clone_should_raise_if_vcs_not_installed", "type": "Function", "lineno": 10}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_rstrip_trailing_slash_in_repo_url", "type": "Function", "lineno": 21}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_abort_if_user_does_not_want_to_reclone", "type": "Function", "lineno": 40}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_silent_exit_if_ok_to_reuse", "type": "Function", "lineno": 63}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-https://github.com/hello/world.git-world]", "type": "Function", "lineno": 86}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[hg-https://bitbucket.org/foo/bar-bar]", "type": "Function", "lineno": 86}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@host:gitoliterepo-gitoliterepo]", "type": "Function", "lineno": 86}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@gitlab.com:cookiecutter/cookiecutter.git-cookiecutter]", "type": "Function", "lineno": 86}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@github.com:cookiecutter/cookiecutter.git-cookiecutter]", "type": "Function", "lineno": 86}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_repo_typo[fatal: repository 'https://github.com/hackebro/cookiedozer' not found]", "type": "Function", "lineno": 138}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_repo_typo[hg: abort: HTTP Error 404: Not Found]", "type": "Function", "lineno": 138}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_branch_typo[error: pathspec 'unknown_branch' did not match any file(s) known to git]", "type": "Function", "lineno": 166}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_branch_typo[hg: abort: unknown revision 'unknown_branch'!]", "type": "Function", "lineno": 166}, {"nodeid": "tests/vcs/test_clone.py::test_clone_unknown_subprocess_error", "type": "Function", "lineno": 197}]}, {"nodeid": "tests/vcs/test_identify_repo.py", "outcome": "passed", "result": [{"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[hg+https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage.git-git-https://github.com/audreyfeldroy/cookiecutter-pypackage.git]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage-git-https://github.com/audreyfeldroy/cookiecutter-pypackage]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git-git-git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket-hg-https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+git]", "type": "Function", "lineno": 67}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+hg]", "type": "Function", "lineno": 67}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+bar]", "type": "Function", "lineno": 67}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foobar]", "type": "Function", "lineno": 67}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[http://norepotypespecified.com]", "type": "Function", "lineno": 67}]}, {"nodeid": "tests/vcs/test_is_vcs_installed.py", "outcome": "passed", "result": [{"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[-False]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[None-False]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[False-False]", "type": "Function", "lineno": 7}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[/usr/local/bin/git-True]", "type": "Function", "lineno": 7}]}, {"nodeid": "tests/vcs", "outcome": "passed", "result": [{"nodeid": "tests/vcs/test_clone.py", "type": "Module"}, {"nodeid": "tests/vcs/test_identify_repo.py", "type": "Module"}, {"nodeid": "tests/vcs/test_is_vcs_installed.py", "type": "Module"}]}, {"nodeid": "tests/zipfile/test_unzip.py", "outcome": "passed", "result": [{"nodeid": "tests/zipfile/test_unzip.py::test_unzip_local_file", "type": "Function", "lineno": 31}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_environment_password", "type": "Function", "lineno": 45}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_bad_environment_password", "type": "Function", "lineno": 62}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_password_with_noinput", "type": "Function", "lineno": 77}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_password", "type": "Function", "lineno": 92}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_bad_password", "type": "Function", "lineno": 109}, {"nodeid": "tests/zipfile/test_unzip.py::test_empty_zip_file", "type": "Function", "lineno": 126}, {"nodeid": "tests/zipfile/test_unzip.py::test_non_repo_zip_file", "type": "Function", "lineno": 138}, {"nodeid": "tests/zipfile/test_unzip.py::test_bad_zip_file", "type": "Function", "lineno": 150}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url", "type": "Function", "lineno": 162}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_with_empty_chunks", "type": "Function", "lineno": 187}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_existing_cache", "type": "Function", "lineno": 212}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_existing_cache_no_input", "type": "Function", "lineno": 241}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_should_abort_if_no_redownload", "type": "Function", "lineno": 266}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_is_ok_to_reuse", "type": "Function", "lineno": 289}]}, {"nodeid": "tests/zipfile", "outcome": "passed", "result": [{"nodeid": "tests/zipfile/test_unzip.py", "type": "Module"}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/fake-nested-templates", "type": "Dir"}, {"nodeid": "tests/fake-nested-templates-old-style", "type": "Dir"}, {"nodeid": "tests/fake-repo", "type": "Dir"}, {"nodeid": "tests/fake-repo-bad", "type": "Dir"}, {"nodeid": "tests/fake-repo-bad-json", "type": "Dir"}, {"nodeid": "tests/fake-repo-dict", "type": "Dir"}, {"nodeid": "tests/fake-repo-dir", "type": "Dir"}, {"nodeid": "tests/fake-repo-pre", "type": "Dir"}, {"nodeid": "tests/fake-repo-pre2", "type": "Dir"}, {"nodeid": "tests/fake-repo-replay", "type": "Dir"}, {"nodeid": "tests/fake-repo-tmpl", "type": "Dir"}, {"nodeid": "tests/fake-repo-tmpl-_cookiecutter", "type": "Dir"}, {"nodeid": "tests/files", "type": "Dir"}, {"nodeid": "tests/hooks-abort-render", "type": "Dir"}, {"nodeid": "tests/replay", "type": "Dir"}, {"nodeid": "tests/repository", "type": "Dir"}, {"nodeid": "tests/test-config", "type": "Dir"}, {"nodeid": "tests/test-extensions", "type": "Dir"}, {"nodeid": "tests/test-generate-binaries", "type": "Dir"}, {"nodeid": "tests/test-generate-context", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render", "type": "Dir"}, {"nodeid": "tests/test-generate-copy-without-render-override", "type": "Dir"}, {"nodeid": "tests/test-generate-files", "type": "Dir"}, {"nodeid": "tests/test-generate-files-line-end", "type": "Dir"}, {"nodeid": "tests/test-generate-files-nontemplated", "type": "Dir"}, {"nodeid": "tests/test-generate-files-permissions", "type": "Dir"}, {"nodeid": "tests/test-output-folder", "type": "Dir"}, {"nodeid": "tests/test-pyhooks", "type": "Dir"}, {"nodeid": "tests/test-pyshellhooks", "type": "Dir"}, {"nodeid": "tests/test-replay", "type": "Dir"}, {"nodeid": "tests/test-shellhooks", "type": "Dir"}, {"nodeid": "tests/test-shellhooks-empty", "type": "Dir"}, {"nodeid": "tests/test-shellhooks-win", "type": "Dir"}, {"nodeid": "tests/test-templates", "type": "Dir"}, {"nodeid": "tests/test_abort_generate_on_hook_error.py", "type": "Module"}, {"nodeid": "tests/test_cli.py", "type": "Module"}, {"nodeid": "tests/test_cookiecutter_invocation.py", "type": "Module"}, {"nodeid": "tests/test_cookiecutter_local_no_input.py", "type": "Module"}, {"nodeid": "tests/test_cookiecutter_local_with_input.py", "type": "Module"}, {"nodeid": "tests/test_cookiecutter_nested_templates.py", "type": "Module"}, {"nodeid": "tests/test_custom_extensions_in_hooks.py", "type": "Module"}, {"nodeid": "tests/test_default_extensions.py", "type": "Module"}, {"nodeid": "tests/test_environment.py", "type": "Module"}, {"nodeid": "tests/test_exceptions.py", "type": "Module"}, {"nodeid": "tests/test_find.py", "type": "Module"}, {"nodeid": "tests/test_generate_context.py", "type": "Module"}, {"nodeid": "tests/test_generate_copy_without_render.py", "type": "Module"}, {"nodeid": "tests/test_generate_copy_without_render_override.py", "type": "Module"}, {"nodeid": "tests/test_generate_file.py", "type": "Module"}, {"nodeid": "tests/test_generate_files.py", "type": "Module"}, {"nodeid": "tests/test_generate_hooks.py", "type": "Module"}, {"nodeid": "tests/test_get_config.py", "type": "Module"}, {"nodeid": "tests/test_get_user_config.py", "type": "Module"}, {"nodeid": "tests/test_hooks.py", "type": "Module"}, {"nodeid": "tests/test_log.py", "type": "Module"}, {"nodeid": "tests/test_main.py", "type": "Module"}, {"nodeid": "tests/test_output_folder.py", "type": "Module"}, {"nodeid": "tests/test_pre_prompt_hooks.py", "type": "Module"}, {"nodeid": "tests/test_preferred_encoding.py", "type": "Module"}, {"nodeid": "tests/test_prompt.py", "type": "Module"}, {"nodeid": "tests/test_read_repo_password.py", "type": "Module"}, {"nodeid": "tests/test_read_user_choice.py", "type": "Module"}, {"nodeid": "tests/test_read_user_dict.py", "type": "Module"}, {"nodeid": "tests/test_read_user_variable.py", "type": "Module"}, {"nodeid": "tests/test_read_user_yes_no.py", "type": "Module"}, {"nodeid": "tests/test_repo_not_found.py", "type": "Module"}, {"nodeid": "tests/test_specify_output_dir.py", "type": "Module"}, {"nodeid": "tests/test_templates.py", "type": "Module"}, {"nodeid": "tests/test_time_extension.py", "type": "Module"}, {"nodeid": "tests/test_utils.py", "type": "Module"}, {"nodeid": "tests/undefined-variable", "type": "Dir"}, {"nodeid": "tests/vcs", "type": "Dir"}, {"nodeid": "tests/zipfile", "type": "Dir"}]}], "tests": [{"nodeid": "tests/replay/test_dump.py::test_type_error_if_no_template_name", "lineno": 34, "outcome": "failed", "keywords": ["test_type_error_if_no_template_name", "test_dump.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0029958960000002754, "outcome": "passed"}, "call": {"duration": 0.00044901700000021805, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_dump.py", "lineno": 37, "message": "Failed: DID NOT RAISE <class 'TypeError'>"}, "traceback": [{"path": "tests/replay/test_dump.py", "lineno": 37, "message": "Failed"}], "longrepr": "replay_test_dir = 'tests/test-replay/'\ncontext = {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}\n\n    def test_type_error_if_no_template_name(replay_test_dir, context):\n        \"\"\"Test that replay.dump raises if the template_name is not a valid str.\"\"\"\n>       with pytest.raises(TypeError):\nE       Failed: DID NOT RAISE <class 'TypeError'>\n\ntests/replay/test_dump.py:37: Failed"}, "teardown": {"duration": 0.0005526339999999408, "outcome": "passed"}}, {"nodeid": "tests/replay/test_dump.py::test_type_error_if_not_dict_context", "lineno": 40, "outcome": "failed", "keywords": ["test_type_error_if_not_dict_context", "test_dump.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0012880850000001054, "outcome": "passed"}, "call": {"duration": 0.00028480300000000014, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_dump.py", "lineno": 43, "message": "Failed: DID NOT RAISE <class 'TypeError'>"}, "traceback": [{"path": "tests/replay/test_dump.py", "lineno": 43, "message": "Failed"}], "longrepr": "replay_test_dir = 'tests/test-replay/', template_name = 'cookiedozer'\n\n    def test_type_error_if_not_dict_context(replay_test_dir, template_name):\n        \"\"\"Test that replay.dump raises if the context is not of type dict.\"\"\"\n>       with pytest.raises(TypeError):\nE       Failed: DID NOT RAISE <class 'TypeError'>\n\ntests/replay/test_dump.py:43: Failed"}, "teardown": {"duration": 0.00039721399999992357, "outcome": "passed"}}, {"nodeid": "tests/replay/test_dump.py::test_value_error_if_key_missing_in_context", "lineno": 46, "outcome": "failed", "keywords": ["test_value_error_if_key_missing_in_context", "test_dump.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0011239459999998758, "outcome": "passed"}, "call": {"duration": 0.0002691459999999424, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_dump.py", "lineno": 50, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/replay/test_dump.py", "lineno": 50, "message": "Failed"}], "longrepr": "replay_test_dir = 'tests/test-replay/', template_name = 'cookiedozer'\n\n    def test_value_error_if_key_missing_in_context(replay_test_dir, template_name):\n        \"\"\"Test that replay.dump raises if the context does not contain a key \\\n        named 'cookiecutter'.\"\"\"\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/replay/test_dump.py:50: Failed"}, "teardown": {"duration": 0.0003936609999999341, "outcome": "passed"}}, {"nodeid": "tests/replay/test_dump.py::test_ioerror_if_replay_dir_creation_fails", "lineno": 75, "outcome": "failed", "keywords": ["test_ioerror_if_replay_dir_creation_fails", "test_dump.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0020526770000000027, "outcome": "passed"}, "call": {"duration": 0.00029304999999979486, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_dump.py", "lineno": 78, "message": "Failed: DID NOT RAISE <class 'OSError'>"}, "traceback": [{"path": "tests/replay/test_dump.py", "lineno": 78, "message": "Failed"}], "longrepr": "mock_ensure_failure = <MagicMock name='make_sure_path_exists' id='139823955812864'>\nreplay_test_dir = 'tests/test-replay/'\n\n    def test_ioerror_if_replay_dir_creation_fails(mock_ensure_failure, replay_test_dir):\n        \"\"\"Test that replay.dump raises when the replay_dir cannot be created.\"\"\"\n>       with pytest.raises(OSError):\nE       Failed: DID NOT RAISE <class 'OSError'>\n\ntests/replay/test_dump.py:78: Failed"}, "teardown": {"duration": 0.00046216000000010027, "outcome": "passed"}}, {"nodeid": "tests/replay/test_dump.py::test_run_json_dump", "lineno": 83, "outcome": "failed", "keywords": ["test_run_json_dump", "test_dump.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0024248579999999187, "outcome": "passed"}, "call": {"duration": 0.0016734879999997787, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_dump.py", "lineno": 102, "message": "AssertionError: Expected 'make_sure_path_exists' to be called once. Called 0 times."}, "traceback": [{"path": "tests/replay/test_dump.py", "lineno": 102, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='make_sure_path_exists' id='139823971117584'>\nargs = ('tests/test-replay/',), kwargs = {}\nmsg = \"Expected 'make_sure_path_exists' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'make_sure_path_exists' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4e2134f0>\nmock_ensure_success = <MagicMock name='make_sure_path_exists' id='139823971117584'>\nmock_user_config = <MagicMock name='get_user_config' id='139823943977280'>\ntemplate_name = 'cookiedozer'\ncontext = {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}\nreplay_test_dir = 'tests/test-replay/'\nreplay_file = 'tests/test-replay/cookiedozer.json'\n\n    def test_run_json_dump(\n        mocker,\n        mock_ensure_success,\n        mock_user_config,\n        template_name,\n        context,\n        replay_test_dir,\n        replay_file,\n    ):\n        \"\"\"Test that replay.dump runs json.dump under the hood and that the context \\\n        is correctly written to the expected file in the replay_dir.\"\"\"\n        spy_get_replay_file = mocker.spy(replay, 'get_file_name')\n    \n        mock_json_dump = mocker.patch('json.dump', side_effect=json.dump)\n    \n        replay.dump(replay_test_dir, template_name, context)\n    \n        assert not mock_user_config.called\n>       mock_ensure_success.assert_called_once_with(replay_test_dir)\nE       AssertionError: Expected 'make_sure_path_exists' to be called once. Called 0 times.\n\ntests/replay/test_dump.py:102: AssertionError"}, "teardown": {"duration": 0.0005412620000000423, "outcome": "passed"}}, {"nodeid": "tests/replay/test_load.py::test_type_error_if_no_template_name", "lineno": 23, "outcome": "failed", "keywords": ["test_type_error_if_no_template_name", "test_load.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010963920000000016, "outcome": "passed"}, "call": {"duration": 0.0002667400000002651, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_load.py", "lineno": 26, "message": "Failed: DID NOT RAISE <class 'TypeError'>"}, "traceback": [{"path": "tests/replay/test_load.py", "lineno": 26, "message": "Failed"}], "longrepr": "replay_test_dir = 'tests/test-replay/'\n\n    def test_type_error_if_no_template_name(replay_test_dir):\n        \"\"\"Test that replay.load raises if the template_name is not a valid str.\"\"\"\n>       with pytest.raises(TypeError):\nE       Failed: DID NOT RAISE <class 'TypeError'>\n\ntests/replay/test_load.py:26: Failed"}, "teardown": {"duration": 0.00031313400000021474, "outcome": "passed"}}, {"nodeid": "tests/replay/test_load.py::test_value_error_if_key_missing_in_context", "lineno": 29, "outcome": "failed", "keywords": ["test_value_error_if_key_missing_in_context", "test_load.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.001095220999999924, "outcome": "passed"}, "call": {"duration": 0.00028005300000000233, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_load.py", "lineno": 33, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/replay/test_load.py", "lineno": 33, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c832710>\nreplay_test_dir = 'tests/test-replay/'\n\n    def test_value_error_if_key_missing_in_context(mocker, replay_test_dir):\n        \"\"\"Test that replay.load raises if the loaded context does not contain \\\n        'cookiecutter'.\"\"\"\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/replay/test_load.py:33: Failed"}, "teardown": {"duration": 0.0003300249999997895, "outcome": "passed"}}, {"nodeid": "tests/replay/test_load.py::test_io_error_if_no_replay_file", "lineno": 36, "outcome": "failed", "keywords": ["test_io_error_if_no_replay_file", "test_load.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010634959999999971, "outcome": "passed"}, "call": {"duration": 0.00025636200000000997, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_load.py", "lineno": 39, "message": "Failed: DID NOT RAISE <class 'OSError'>"}, "traceback": [{"path": "tests/replay/test_load.py", "lineno": 39, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce12d70>\nreplay_test_dir = 'tests/test-replay/'\n\n    def test_io_error_if_no_replay_file(mocker, replay_test_dir):\n        \"\"\"Test that replay.load raises if it cannot find a replay file.\"\"\"\n>       with pytest.raises(IOError):\nE       Failed: DID NOT RAISE <class 'OSError'>\n\ntests/replay/test_load.py:39: Failed"}, "teardown": {"duration": 0.0003395550000000469, "outcome": "passed"}}, {"nodeid": "tests/replay/test_load.py::test_run_json_load", "lineno": 42, "outcome": "failed", "keywords": ["test_run_json_load", "test_load.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0018058100000000188, "outcome": "passed"}, "call": {"duration": 0.001721446999999987, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError: Expected 'get_file_name' to be called once. Called 0 times."}, "traceback": [{"path": "tests/replay/test_load.py", "lineno": 55, "message": ""}, {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='get_file_name' spec='function' id='139823955812480'>\nargs = ('tests/test-replay/', 'cookiedozer_load'), kwargs = {}\nmsg = \"Expected 'get_file_name' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'get_file_name' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cad5990>\nmock_user_config = <MagicMock name='get_user_config' id='139823946752512'>\ntemplate_name = 'cookiedozer_load'\ncontext = {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}\nreplay_test_dir = 'tests/test-replay/'\nreplay_file = 'tests/test-replay/cookiedozer_load.json'\n\n    def test_run_json_load(\n        mocker, mock_user_config, template_name, context, replay_test_dir, replay_file\n    ):\n        \"\"\"Test that replay.load runs json.load under the hood and that the context \\\n        is correctly loaded from the file in replay_dir.\"\"\"\n        spy_get_replay_file = mocker.spy(replay, 'get_file_name')\n    \n        mock_json_load = mocker.patch('json.load', side_effect=json.load)\n    \n        loaded_context = replay.load(replay_test_dir, template_name)\n    \n        assert not mock_user_config.called\n>       spy_get_replay_file.assert_called_once_with(replay_test_dir, template_name)\n\ntests/replay/test_load.py:55: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = ('tests/test-replay/', 'cookiedozer_load'), kwargs = {}\n\n    def assert_called_once_with(*args, **kwargs):\n>       return mock.assert_called_once_with(*args, **kwargs)\nE       AssertionError: Expected 'get_file_name' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:213: AssertionError"}, "teardown": {"duration": 0.0006274180000001017, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_get_replay_file_name[bar]", "lineno": 9, "outcome": "failed", "keywords": ["test_get_replay_file_name[bar]", "parametrize", "pytestmark", "bar", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0012949319999999709, "outcome": "passed"}, "call": {"duration": 0.0005582189999997489, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 15, "message": "AssertionError: assert None == 'foo/bar.json'"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "replay_file_name = 'bar'\n\n    @pytest.mark.parametrize(\"replay_file_name\", ['bar', 'bar.json'])\n    def test_get_replay_file_name(replay_file_name):\n        \"\"\"Make sure that replay.get_file_name generates a valid json file path.\"\"\"\n        exp_replay_file_path = os.path.join('foo', 'bar.json')\n        replay_file_path = replay.get_file_name('foo', replay_file_name)\n>       assert replay_file_path == exp_replay_file_path\nE       AssertionError: assert None == 'foo/bar.json'\n\ntests/replay/test_replay.py:15: AssertionError"}, "teardown": {"duration": 0.0003288810000001696, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_get_replay_file_name[bar.json]", "lineno": 9, "outcome": "failed", "keywords": ["test_get_replay_file_name[bar.json]", "parametrize", "pytestmark", "bar.json", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010676640000002457, "outcome": "passed"}, "call": {"duration": 0.0003469239999995821, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 15, "message": "AssertionError: assert None == 'foo/bar.json'"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "replay_file_name = 'bar.json'\n\n    @pytest.mark.parametrize(\"replay_file_name\", ['bar', 'bar.json'])\n    def test_get_replay_file_name(replay_file_name):\n        \"\"\"Make sure that replay.get_file_name generates a valid json file path.\"\"\"\n        exp_replay_file_path = os.path.join('foo', 'bar.json')\n        replay_file_path = replay.get_file_name('foo', replay_file_name)\n>       assert replay_file_path == exp_replay_file_path\nE       AssertionError: assert None == 'foo/bar.json'\n\ntests/replay/test_replay.py:15: AssertionError"}, "teardown": {"duration": 0.00031006700000002496, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs0]", "lineno": 17, "outcome": "failed", "keywords": ["test_raise_on_invalid_mode[invalid_kwargs0]", "parametrize", "pytestmark", "invalid_kwargs0", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010086720000002103, "outcome": "passed"}, "call": {"duration": 0.000274961000000129, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 28, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 28, "message": "Failed"}], "longrepr": "invalid_kwargs = {'no_input': True}\n\n    @pytest.mark.parametrize(\n        'invalid_kwargs',\n        (\n            {'no_input': True},\n            {'extra_context': {}},\n            {'no_input': True, 'extra_context': {}},\n        ),\n    )\n    def test_raise_on_invalid_mode(invalid_kwargs):\n        \"\"\"Test `cookiecutter` raise exception on unacceptable `replay` request.\"\"\"\n>       with pytest.raises(exceptions.InvalidModeException):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>\n\ntests/replay/test_replay.py:28: Failed"}, "teardown": {"duration": 0.0003154080000000725, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs1]", "lineno": 17, "outcome": "failed", "keywords": ["test_raise_on_invalid_mode[invalid_kwargs1]", "parametrize", "pytestmark", "invalid_kwargs1", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010343279999998067, "outcome": "passed"}, "call": {"duration": 0.00028041199999995214, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 28, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 28, "message": "Failed"}], "longrepr": "invalid_kwargs = {'extra_context': {}}\n\n    @pytest.mark.parametrize(\n        'invalid_kwargs',\n        (\n            {'no_input': True},\n            {'extra_context': {}},\n            {'no_input': True, 'extra_context': {}},\n        ),\n    )\n    def test_raise_on_invalid_mode(invalid_kwargs):\n        \"\"\"Test `cookiecutter` raise exception on unacceptable `replay` request.\"\"\"\n>       with pytest.raises(exceptions.InvalidModeException):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>\n\ntests/replay/test_replay.py:28: Failed"}, "teardown": {"duration": 0.00033050700000014643, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_raise_on_invalid_mode[invalid_kwargs2]", "lineno": 17, "outcome": "failed", "keywords": ["test_raise_on_invalid_mode[invalid_kwargs2]", "parametrize", "pytestmark", "invalid_kwargs2", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.001015506999999971, "outcome": "passed"}, "call": {"duration": 0.0002659259999999719, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 28, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 28, "message": "Failed"}], "longrepr": "invalid_kwargs = {'extra_context': {}, 'no_input': True}\n\n    @pytest.mark.parametrize(\n        'invalid_kwargs',\n        (\n            {'no_input': True},\n            {'extra_context': {}},\n            {'no_input': True, 'extra_context': {}},\n        ),\n    )\n    def test_raise_on_invalid_mode(invalid_kwargs):\n        \"\"\"Test `cookiecutter` raise exception on unacceptable `replay` request.\"\"\"\n>       with pytest.raises(exceptions.InvalidModeException):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidModeException'>\n\ntests/replay/test_replay.py:28: Failed"}, "teardown": {"duration": 0.0003155970000001673, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_main_does_not_invoke_dump_but_load", "lineno": 31, "outcome": "failed", "keywords": ["test_main_does_not_invoke_dump_but_load", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010617859999997314, "outcome": "passed"}, "call": {"duration": 0.002668738000000115, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 43, "message": "AssertionError: assert False\n +  where False = <MagicMock name='generate_context' id='139823947981216'>.called"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 43, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce9eb00>\n\n    def test_main_does_not_invoke_dump_but_load(mocker):\n        \"\"\"Test `cookiecutter` calling correct functions on `replay`.\"\"\"\n        mock_prompt = mocker.patch('cookiecutter.main.prompt_for_config')\n        mock_gen_context = mocker.patch('cookiecutter.main.generate_context')\n        mock_gen_files = mocker.patch('cookiecutter.main.generate_files')\n        mock_replay_dump = mocker.patch('cookiecutter.main.dump')\n        mock_replay_load = mocker.patch('cookiecutter.main.load')\n    \n        main.cookiecutter('tests/fake-repo-tmpl/', replay=True)\n    \n        assert not mock_prompt.called\n>       assert mock_gen_context.called\nE       AssertionError: assert False\nE        +  where False = <MagicMock name='generate_context' id='139823947981216'>.called\n\ntests/replay/test_replay.py:43: AssertionError"}, "teardown": {"duration": 0.00034936499999993487, "outcome": "passed"}}, {"nodeid": "tests/replay/test_replay.py::test_main_does_not_invoke_load_but_dump", "lineno": 48, "outcome": "failed", "keywords": ["test_main_does_not_invoke_load_but_dump", "test_replay.py", "replay", "tests", "testbed", ""], "setup": {"duration": 0.0010618649999996066, "outcome": "passed"}, "call": {"duration": 0.0025609170000002734, "outcome": "failed", "crash": {"path": "/testbed/tests/replay/test_replay.py", "lineno": 59, "message": "AssertionError: assert False\n +  where False = <MagicMock name='prompt_for_config' id='139823939436352'>.called"}, "traceback": [{"path": "tests/replay/test_replay.py", "lineno": 59, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3d81f0>\n\n    def test_main_does_not_invoke_load_but_dump(mocker):\n        \"\"\"Test `cookiecutter` calling correct functions on non-replay launch.\"\"\"\n        mock_prompt = mocker.patch('cookiecutter.main.prompt_for_config')\n        mock_gen_context = mocker.patch('cookiecutter.main.generate_context')\n        mock_gen_files = mocker.patch('cookiecutter.main.generate_files')\n        mock_replay_dump = mocker.patch('cookiecutter.main.dump')\n        mock_replay_load = mocker.patch('cookiecutter.main.load')\n    \n        main.cookiecutter('tests/fake-repo-tmpl/', replay=False)\n    \n>       assert mock_prompt.called\nE       AssertionError: assert False\nE        +  where False = <MagicMock name='prompt_for_config' id='139823939436352'>.called\n\ntests/replay/test_replay.py:59: AssertionError"}, "teardown": {"duration": 0.0003581580000000528, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Simple expansion]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Simple expansion]", "parametrize", "pytestmark", "Simple expansion", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011635889999999094, "outcome": "passed"}, "call": {"duration": 0.00035805899999985513, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == 'bar'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'foo', abbreviations = {'foo': 'bar'}, expected_result = 'bar'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == 'bar'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003914030000000679, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Skip expansion (expansion not an abbreviation)]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Skip expansion (expansion not an abbreviation)]", "parametrize", "pytestmark", "Skip expansion (expansion not an abbreviation)", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011798629999999477, "outcome": "passed"}, "call": {"duration": 0.00035305199999990933, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == 'baz'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'baz', abbreviations = {'foo': 'bar'}, expected_result = 'baz'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == 'baz'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0004311900000000257, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Expansion prefix]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Expansion prefix]", "parametrize", "pytestmark", "Expansion prefix", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011637209999997289, "outcome": "passed"}, "call": {"duration": 0.0003531030000001323, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == '<a>'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'xx:a', abbreviations = {'xx': '<{0}>'}, expected_result = '<a>'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == '<a>'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.00038040000000005847, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[expansion_override_builtin]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[expansion_override_builtin]", "parametrize", "pytestmark", "expansion_override_builtin", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011809680000003375, "outcome": "passed"}, "call": {"duration": 0.00034964799999981366, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == '<a>'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'gh:a', abbreviations = {'gh': '<{0}>'}, expected_result = '<a>'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == '<a>'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003864979999996798, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[expansion_prefix_ignores_suffix]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[expansion_prefix_ignores_suffix]", "parametrize", "pytestmark", "expansion_prefix_ignores_suffix", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001154250999999995, "outcome": "passed"}, "call": {"duration": 0.0003481439999997171, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == '<>'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'xx:a', abbreviations = {'xx': '<>'}, expected_result = '<>'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == '<>'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003759759999999446, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (github)]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Correct expansion for builtin abbreviations (github)]", "parametrize", "pytestmark", "Correct expansion for builtin abbreviations (github)", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011660710000001018, "outcome": "passed"}, "call": {"duration": 0.0003352769999995786, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == 'https://github.com/pydanny/cookiecutter-django.git'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'gh:pydanny/cookiecutter-django'\nabbreviations = {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}\nexpected_result = 'https://github.com/pydanny/cookiecutter-django.git'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == 'https://github.com/pydanny/cookiecutter-django.git'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003799689999999245, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (gitlab)]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Correct expansion for builtin abbreviations (gitlab)]", "parametrize", "pytestmark", "Correct expansion for builtin abbreviations (gitlab)", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011592640000004373, "outcome": "passed"}, "call": {"duration": 0.0003414489999999937, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == 'https://gitlab.com/pydanny/cookiecutter-django.git'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'gl:pydanny/cookiecutter-django'\nabbreviations = {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}\nexpected_result = 'https://gitlab.com/pydanny/cookiecutter-django.git'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == 'https://gitlab.com/pydanny/cookiecutter-django.git'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003997040000003338, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion[Correct expansion for builtin abbreviations (bitbucket)]", "lineno": 8, "outcome": "failed", "keywords": ["test_abbreviation_expansion[Correct expansion for builtin abbreviations (bitbucket)]", "parametrize", "pytestmark", "Correct expansion for builtin abbreviations (bitbucket)", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011825979999997571, "outcome": "passed"}, "call": {"duration": 0.0003391750000001359, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError: assert None == 'https://bitbucket.org/pydanny/cookiecutter-django'"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "template = 'bb:pydanny/cookiecutter-django'\nabbreviations = {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}\nexpected_result = 'https://bitbucket.org/pydanny/cookiecutter-django'\n\n    @pytest.mark.parametrize(\n        ('template', 'abbreviations', 'expected_result'),\n        [\n            ('foo', {'foo': 'bar'}, 'bar'),\n            ('baz', {'foo': 'bar'}, 'baz'),\n            ('xx:a', {'xx': '<{0}>'}, '<a>'),\n            ('gh:a', {'gh': '<{0}>'}, '<a>'),\n            ('xx:a', {'xx': '<>'}, '<>'),\n            (\n                'gh:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://github.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'gl:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://gitlab.com/pydanny/cookiecutter-django.git',\n            ),\n            (\n                'bb:pydanny/cookiecutter-django',\n                BUILTIN_ABBREVIATIONS,\n                'https://bitbucket.org/pydanny/cookiecutter-django',\n            ),\n        ],\n        ids=(\n            'Simple expansion',\n            'Skip expansion (expansion not an abbreviation)',\n            'Expansion prefix',\n            'expansion_override_builtin',\n            'expansion_prefix_ignores_suffix',\n            'Correct expansion for builtin abbreviations (github)',\n            'Correct expansion for builtin abbreviations (gitlab)',\n            'Correct expansion for builtin abbreviations (bitbucket)',\n        ),\n    )\n    def test_abbreviation_expansion(template, abbreviations, expected_result):\n        \"\"\"Verify abbreviation unpacking.\"\"\"\n        expanded = expand_abbreviations(template, abbreviations)\n>       assert expanded == expected_result\nE       AssertionError: assert None == 'https://bitbucket.org/pydanny/cookiecutter-django'\n\ntests/repository/test_abbreviation_expansion.py:47: AssertionError"}, "teardown": {"duration": 0.0003759290000000526, "outcome": "passed"}}, {"nodeid": "tests/repository/test_abbreviation_expansion.py::test_abbreviation_expansion_prefix_not_0_in_braces", "lineno": 49, "outcome": "failed", "keywords": ["test_abbreviation_expansion_prefix_not_0_in_braces", "test_abbreviation_expansion.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0010064939999998579, "outcome": "passed"}, "call": {"duration": 0.00026047900000003565, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_abbreviation_expansion.py", "lineno": 52, "message": "Failed: DID NOT RAISE <class 'IndexError'>"}, "traceback": [{"path": "tests/repository/test_abbreviation_expansion.py", "lineno": 52, "message": "Failed"}], "longrepr": "def test_abbreviation_expansion_prefix_not_0_in_braces():\n        \"\"\"Verify abbreviation unpacking raises error on incorrect index.\"\"\"\n>       with pytest.raises(IndexError):\nE       Failed: DID NOT RAISE <class 'IndexError'>\n\ntests/repository/test_abbreviation_expansion.py:52: Failed"}, "teardown": {"duration": 0.0003314520000001764, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[/path/to/zipfile.zip-False]", "lineno": 9, "outcome": "failed", "keywords": ["test_zipfile_unzip[/path/to/zipfile.zip-False]", "parametrize", "pytestmark", "/path/to/zipfile.zip-False", "test_determine_repo_dir_clones_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0019975639999998407, "outcome": "passed"}, "call": {"duration": 0.0009883839999997868, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c6caf80>\ntemplate = '/path/to/zipfile.zip', is_url = False\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\n\n    @pytest.mark.parametrize(\n        'template, is_url',\n        [\n            ('/path/to/zipfile.zip', False),\n            ('https://example.com/path/to/zipfile.zip', True),\n            ('http://example.com/path/to/zipfile.zip', True),\n        ],\n    )\n    def test_zipfile_unzip(mocker, template, is_url, user_config_data):\n        \"\"\"Verify zip files correctly handled for different source locations.\n    \n        `unzip()` should be called with correct args when `determine_repo_dir()`\n        is passed a zipfile, or a URL to a zipfile.\n        \"\"\"\n        mock_clone = mocker.patch(\n            'cookiecutter.repository.unzip',\n            return_value='tests/fake-repo-tmpl',\n            autospec=True,\n        )\n    \n>       project_dir, cleanup = repository.determine_repo_dir(\n            template,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n            password=None,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_clones_repo.py:30: TypeError"}, "teardown": {"duration": 0.0004064170000002143, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[https://example.com/path/to/zipfile.zip-True]", "lineno": 9, "outcome": "failed", "keywords": ["test_zipfile_unzip[https://example.com/path/to/zipfile.zip-True]", "parametrize", "pytestmark", "https://example.com/path/to/zipfile.zip-True", "test_determine_repo_dir_clones_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001258863000000332, "outcome": "passed"}, "call": {"duration": 0.0009125739999999993, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c6cbee0>\ntemplate = 'https://example.com/path/to/zipfile.zip', is_url = True\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\n\n    @pytest.mark.parametrize(\n        'template, is_url',\n        [\n            ('/path/to/zipfile.zip', False),\n            ('https://example.com/path/to/zipfile.zip', True),\n            ('http://example.com/path/to/zipfile.zip', True),\n        ],\n    )\n    def test_zipfile_unzip(mocker, template, is_url, user_config_data):\n        \"\"\"Verify zip files correctly handled for different source locations.\n    \n        `unzip()` should be called with correct args when `determine_repo_dir()`\n        is passed a zipfile, or a URL to a zipfile.\n        \"\"\"\n        mock_clone = mocker.patch(\n            'cookiecutter.repository.unzip',\n            return_value='tests/fake-repo-tmpl',\n            autospec=True,\n        )\n    \n>       project_dir, cleanup = repository.determine_repo_dir(\n            template,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n            password=None,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_clones_repo.py:30: TypeError"}, "teardown": {"duration": 0.0003841200000000988, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_zipfile_unzip[http://example.com/path/to/zipfile.zip-True]", "lineno": 9, "outcome": "failed", "keywords": ["test_zipfile_unzip[http://example.com/path/to/zipfile.zip-True]", "parametrize", "pytestmark", "http://example.com/path/to/zipfile.zip-True", "test_determine_repo_dir_clones_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0012222459999997604, "outcome": "passed"}, "call": {"duration": 0.0008889950000003921, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 30, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c56da50>\ntemplate = 'http://example.com/path/to/zipfile.zip', is_url = True\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\n\n    @pytest.mark.parametrize(\n        'template, is_url',\n        [\n            ('/path/to/zipfile.zip', False),\n            ('https://example.com/path/to/zipfile.zip', True),\n            ('http://example.com/path/to/zipfile.zip', True),\n        ],\n    )\n    def test_zipfile_unzip(mocker, template, is_url, user_config_data):\n        \"\"\"Verify zip files correctly handled for different source locations.\n    \n        `unzip()` should be called with correct args when `determine_repo_dir()`\n        is passed a zipfile, or a URL to a zipfile.\n        \"\"\"\n        mock_clone = mocker.patch(\n            'cookiecutter.repository.unzip',\n            return_value='tests/fake-repo-tmpl',\n            autospec=True,\n        )\n    \n>       project_dir, cleanup = repository.determine_repo_dir(\n            template,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n            password=None,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_clones_repo.py:30: TypeError"}, "teardown": {"duration": 0.0003973729999997566, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_repository_url_should_clone", "lineno": 60, "outcome": "failed", "keywords": ["test_repository_url_should_clone", "test_determine_repo_dir_clones_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0012142130000003526, "outcome": "passed"}, "call": {"duration": 0.0009178909999998375, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 73, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 73, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c811ba0>\ntemplate_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\n\n    def test_repository_url_should_clone(mocker, template_url, user_config_data):\n        \"\"\"Verify repository url triggers clone function.\n    \n        `clone()` should be called with correct args when `determine_repo_dir()` is\n        passed a repository template url.\n        \"\"\"\n        mock_clone = mocker.patch(\n            'cookiecutter.repository.clone',\n            return_value='tests/fake-repo-tmpl',\n            autospec=True,\n        )\n    \n>       project_dir, cleanup = repository.determine_repo_dir(\n            template_url,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_clones_repo.py:73: TypeError"}, "teardown": {"duration": 0.0003787140000000022, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_clones_repo.py::test_repository_url_with_no_context_file", "lineno": 92, "outcome": "failed", "keywords": ["test_repository_url_with_no_context_file", "test_determine_repo_dir_clones_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011901839999999275, "outcome": "passed"}, "call": {"duration": 0.0008901520000001995, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 101, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_clones_repo.py", "lineno": 101, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cc00c40>\ntemplate_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\n\n    def test_repository_url_with_no_context_file(mocker, template_url, user_config_data):\n        \"\"\"Verify cloned repository without `cookiecutter.json` file raises error.\"\"\"\n        mocker.patch(\n            'cookiecutter.repository.clone',\n            return_value='tests/fake-repo-bad',\n            autospec=True,\n        )\n    \n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/repository/test_determine_repo_dir_clones_repo.py:101: Failed"}, "teardown": {"duration": 0.00037819600000021936, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py::test_should_find_existing_cookiecutter", "lineno": 29, "outcome": "failed", "keywords": ["test_should_find_existing_cookiecutter", "test_determine_repo_dir_finds_existing_cookiecutter.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0014030700000002838, "outcome": "passed"}, "call": {"duration": 0.0002594910000000006, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py", "lineno": 38, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py", "lineno": 38, "message": "TypeError"}], "longrepr": "template = 'cookiecutter-pytest-plugin'\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\ncloned_cookiecutter_path = '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters/cookiecutter-pytest-plugin'\n\n    def test_should_find_existing_cookiecutter(\n        template, user_config_data, cloned_cookiecutter_path\n    ):\n        \"\"\"\n        Should find folder created by `cloned_cookiecutter_path` and return it.\n    \n        This folder is considered like previously cloned project directory.\n        \"\"\"\n>       project_dir, cleanup = repository.determine_repo_dir(\n            template=template,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_finds_existing_cookiecutter.py:38: TypeError"}, "teardown": {"duration": 0.0003772440000000543, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py::test_should_find_existing_cookiecutter", "lineno": 33, "outcome": "failed", "keywords": ["test_should_find_existing_cookiecutter", "test_determine_repo_dir_finds_subdirectories.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0014124039999998672, "outcome": "passed"}, "call": {"duration": 0.0002620249999996105, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_finds_subdirectories.py", "lineno": 38, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_finds_subdirectories.py", "lineno": 38, "message": "TypeError"}], "longrepr": "template = 'cookiecutter-pytest-plugin'\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\ncloned_cookiecutter_path = '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters/cookiecutter-pytest-plugin/my-dir'\n\n    def test_should_find_existing_cookiecutter(\n        template, user_config_data, cloned_cookiecutter_path\n    ):\n        \"\"\"Find `cookiecutter.json` in sub folder created by `cloned_cookiecutter_path`.\"\"\"\n>       project_dir, cleanup = repository.determine_repo_dir(\n            template=template,\n            abbreviations={},\n            clone_to_dir=user_config_data['cookiecutters_dir'],\n            checkout=None,\n            no_input=True,\n            directory='my-dir',\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repo_dir_finds_subdirectories.py:38: TypeError"}, "teardown": {"duration": 0.00041185400000021133, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repo_dir_finds_subdirectories.py::test_local_repo_typo", "lineno": 50, "outcome": "failed", "keywords": ["test_local_repo_typo", "test_determine_repo_dir_finds_subdirectories.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001231329999999975, "outcome": "passed"}, "call": {"duration": 0.00026951999999980103, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repo_dir_finds_subdirectories.py", "lineno": 53, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/repository/test_determine_repo_dir_finds_subdirectories.py", "lineno": 53, "message": "Failed"}], "longrepr": "template = 'cookiecutter-pytest-plugin'\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\ncloned_cookiecutter_path = '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters/cookiecutter-pytest-plugin/my-dir'\n\n    def test_local_repo_typo(template, user_config_data, cloned_cookiecutter_path):\n        \"\"\"Wrong pointing to `cookiecutter.json` sub-directory should raise.\"\"\"\n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/repository/test_determine_repo_dir_finds_subdirectories.py:53: Failed"}, "teardown": {"duration": 0.00037786999999989135, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_finds_local_repo", "lineno": 9, "outcome": "failed", "keywords": ["test_finds_local_repo", "test_determine_repository_should_use_local_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001047441999999954, "outcome": "passed"}, "call": {"duration": 0.00025387899999973484, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 12, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 12, "message": "TypeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_finds_local_repo0')\n\n    def test_finds_local_repo(tmp_path):\n        \"\"\"A valid local repository should be returned.\"\"\"\n>       project_dir, cleanup = repository.determine_repo_dir(\n            'tests/fake-repo',\n            abbreviations={},\n            clone_to_dir=str(tmp_path),\n            checkout=None,\n            no_input=True,\n        )\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/repository/test_determine_repository_should_use_local_repo.py:12: TypeError"}, "teardown": {"duration": 0.00033552200000031007, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_local_repo_with_no_context_raises", "lineno": 23, "outcome": "failed", "keywords": ["test_local_repo_with_no_context_raises", "test_determine_repository_should_use_local_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0010538589999997683, "outcome": "passed"}, "call": {"duration": 0.0002781370000000116, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 28, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 28, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_local_repo_with_no_contex0')\n\n    def test_local_repo_with_no_context_raises(tmp_path):\n        \"\"\"A local repository without a cookiecutter.json should raise a \\\n        `RepositoryNotFound` exception.\"\"\"\n        template_path = str(Path('tests', 'fake-repo-bad'))\n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/repository/test_determine_repository_should_use_local_repo.py:28: Failed"}, "teardown": {"duration": 0.00033521900000010874, "outcome": "passed"}}, {"nodeid": "tests/repository/test_determine_repository_should_use_local_repo.py::test_local_repo_typo", "lineno": 47, "outcome": "failed", "keywords": ["test_local_repo_typo", "test_determine_repository_should_use_local_repo.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.00110084999999982, "outcome": "passed"}, "call": {"duration": 0.00028942299999989984, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 52, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/repository/test_determine_repository_should_use_local_repo.py", "lineno": 52, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_local_repo_typo1')\n\n    def test_local_repo_typo(tmp_path):\n        \"\"\"An unknown local repository should raise a `RepositoryNotFound` \\\n        exception.\"\"\"\n        template_path = str(Path('tests', 'unknown-repo'))\n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/repository/test_determine_repository_should_use_local_repo.py:52: Failed"}, "teardown": {"duration": 0.0003397480000000286, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[/path/to/zipfile.zip]", "lineno": 20, "outcome": "failed", "keywords": ["test_is_zip_file[/path/to/zipfile.zip]", "/path/to/zipfile.zip", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001124469999999711, "outcome": "passed"}, "call": {"duration": 0.0003653660000000336, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError: assert None is True\n +  where None = is_zip_file('/path/to/zipfile.zip')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError"}], "longrepr": "zipfile = '/path/to/zipfile.zip'\n\n    def test_is_zip_file(zipfile):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_zip_file(zipfile) is True\nE       AssertionError: assert None is True\nE        +  where None = is_zip_file('/path/to/zipfile.zip')\n\ntests/repository/test_is_repo_url.py:23: AssertionError"}, "teardown": {"duration": 0.00034260200000035823, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[https://example.com/path/to/zipfile.zip]", "lineno": 20, "outcome": "failed", "keywords": ["test_is_zip_file[https://example.com/path/to/zipfile.zip]", "https://example.com/path/to/zipfile.zip", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011251709999999804, "outcome": "passed"}, "call": {"duration": 0.00033729100000012835, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError: assert None is True\n +  where None = is_zip_file('https://example.com/path/to/zipfile.zip')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError"}], "longrepr": "zipfile = 'https://example.com/path/to/zipfile.zip'\n\n    def test_is_zip_file(zipfile):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_zip_file(zipfile) is True\nE       AssertionError: assert None is True\nE        +  where None = is_zip_file('https://example.com/path/to/zipfile.zip')\n\ntests/repository/test_is_repo_url.py:23: AssertionError"}, "teardown": {"duration": 0.0003553499999999765, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_zip_file[http://example.com/path/to/zipfile.zip]", "lineno": 20, "outcome": "failed", "keywords": ["test_is_zip_file[http://example.com/path/to/zipfile.zip]", "http://example.com/path/to/zipfile.zip", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011221649999999528, "outcome": "passed"}, "call": {"duration": 0.00035041699999993625, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError: assert None is True\n +  where None = is_zip_file('http://example.com/path/to/zipfile.zip')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 23, "message": "AssertionError"}], "longrepr": "zipfile = 'http://example.com/path/to/zipfile.zip'\n\n    def test_is_zip_file(zipfile):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_zip_file(zipfile) is True\nE       AssertionError: assert None is True\nE        +  where None = is_zip_file('http://example.com/path/to/zipfile.zip')\n\ntests/repository/test_is_repo_url.py:23: AssertionError"}, "teardown": {"duration": 0.000341763000000217, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[gitolite@server:team/repo]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[gitolite@server:team/repo]", "gitolite@server:team/repo", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011304409999999265, "outcome": "passed"}, "call": {"duration": 0.00034540999999999045, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('gitolite@server:team/repo')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'gitolite@server:team/repo'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('gitolite@server:team/repo')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.00036041200000003215, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[git@github.com:audreyfeldroy/cookiecutter.git]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[git@github.com:audreyfeldroy/cookiecutter.git]", "git@github.com:audreyfeldroy/cookiecutter.git", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011400919999999815, "outcome": "passed"}, "call": {"duration": 0.0003495149999999114, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('git@github.com:audreyfeldroy/cookiecutter.git')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'git@github.com:audreyfeldroy/cookiecutter.git'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('git@github.com:audreyfeldroy/cookiecutter.git')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.0003454069999997422, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[https://github.com/cookiecutter/cookiecutter.git]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[https://github.com/cookiecutter/cookiecutter.git]", "https://github.com/cookiecutter/cookiecutter.git", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011458790000000718, "outcome": "passed"}, "call": {"duration": 0.0003406559999996617, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('https://github.com/cookiecutter/cookiecutter.git')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'https://github.com/cookiecutter/cookiecutter.git'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('https://github.com/cookiecutter/cookiecutter.git')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.00036075900000032135, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[git+https://private.com/gitrepo]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[git+https://private.com/gitrepo]", "git+https://private.com/gitrepo", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011400639999998852, "outcome": "passed"}, "call": {"duration": 0.000354085000000115, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('git+https://private.com/gitrepo')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'git+https://private.com/gitrepo'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('git+https://private.com/gitrepo')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.00034234800000021437, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[hg+https://private.com/mercurialrepo]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[hg+https://private.com/mercurialrepo]", "hg+https://private.com/mercurialrepo", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011436780000000368, "outcome": "passed"}, "call": {"duration": 0.00034378000000012676, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('hg+https://private.com/mercurialrepo')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'hg+https://private.com/mercurialrepo'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('hg+https://private.com/mercurialrepo')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.00035346099999999936, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[https://bitbucket.org/pokoli/cookiecutter.hg]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[https://bitbucket.org/pokoli/cookiecutter.hg]", "https://bitbucket.org/pokoli/cookiecutter.hg", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011583129999999109, "outcome": "passed"}, "call": {"duration": 0.00034402800000021827, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('https://bitbucket.org/pokoli/cookiecutter.hg')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'https://bitbucket.org/pokoli/cookiecutter.hg'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('https://bitbucket.org/pokoli/cookiecutter.hg')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.0003458949999997074, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_remote_urls[file://server/path/to/repo.git]", "lineno": 41, "outcome": "failed", "keywords": ["test_is_repo_url_for_remote_urls[file://server/path/to/repo.git]", "file://server/path/to/repo.git", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001164795999999857, "outcome": "passed"}, "call": {"duration": 0.0003614769999997769, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError: assert None is True\n +  where None = is_repo_url('file://server/path/to/repo.git')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 44, "message": "AssertionError"}], "longrepr": "remote_repo_url = 'file://server/path/to/repo.git'\n\n    def test_is_repo_url_for_remote_urls(remote_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(remote_repo_url) is True\nE       AssertionError: assert None is True\nE        +  where None = is_repo_url('file://server/path/to/repo.git')\n\ntests/repository/test_is_repo_url.py:44: AssertionError"}, "teardown": {"duration": 0.0003560479999999977, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[/audreyr/cookiecutter.git]", "lineno": 61, "outcome": "failed", "keywords": ["test_is_repo_url_for_local_urls[/audreyr/cookiecutter.git]", "/audreyr/cookiecutter.git", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011905019999995936, "outcome": "passed"}, "call": {"duration": 0.0003524480000001162, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError: assert None is False\n +  where None = is_repo_url('/audreyr/cookiecutter.git')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError"}], "longrepr": "local_repo_url = '/audreyr/cookiecutter.git'\n\n    def test_is_repo_url_for_local_urls(local_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(local_repo_url) is False\nE       AssertionError: assert None is False\nE        +  where None = is_repo_url('/audreyr/cookiecutter.git')\n\ntests/repository/test_is_repo_url.py:64: AssertionError"}, "teardown": {"duration": 0.0003457839999998491, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[/home/audreyr/cookiecutter]", "lineno": 61, "outcome": "failed", "keywords": ["test_is_repo_url_for_local_urls[/home/audreyr/cookiecutter]", "/home/audreyr/cookiecutter", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001168252999999897, "outcome": "passed"}, "call": {"duration": 0.0003394730000003676, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError: assert None is False\n +  where None = is_repo_url('/home/audreyr/cookiecutter')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError"}], "longrepr": "local_repo_url = '/home/audreyr/cookiecutter'\n\n    def test_is_repo_url_for_local_urls(local_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(local_repo_url) is False\nE       AssertionError: assert None is False\nE        +  where None = is_repo_url('/home/audreyr/cookiecutter')\n\ntests/repository/test_is_repo_url.py:64: AssertionError"}, "teardown": {"duration": 0.0003470520000004029, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_is_repo_url_for_local_urls[c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template]", "lineno": 61, "outcome": "failed", "keywords": ["test_is_repo_url_for_local_urls[c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template]", "c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.001168539000000024, "outcome": "passed"}, "call": {"duration": 0.0003488990000000136, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError: assert None is False\n +  where None = is_repo_url('c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 64, "message": "AssertionError"}], "longrepr": "local_repo_url = 'c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template'\n\n    def test_is_repo_url_for_local_urls(local_repo_url):\n        \"\"\"Verify is_repo_url works.\"\"\"\n>       assert is_repo_url(local_repo_url) is False\nE       AssertionError: assert None is False\nE        +  where None = is_repo_url('c:\\\\users\\\\foo\\\\appdata\\\\local\\\\temp\\\\1\\\\pytest-0\\\\test_default_output_dir0\\\\template')\n\ntests/repository/test_is_repo_url.py:64: AssertionError"}, "teardown": {"duration": 0.0003457020000001698, "outcome": "passed"}}, {"nodeid": "tests/repository/test_is_repo_url.py::test_expand_abbreviations", "lineno": 66, "outcome": "failed", "keywords": ["test_expand_abbreviations", "test_is_repo_url.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011196959999999478, "outcome": "passed"}, "call": {"duration": 0.00034241800000023304, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_is_repo_url.py", "lineno": 73, "message": "AssertionError: assert None is False\n +  where None = is_repo_url('gh:audreyfeldroy/cookiecutter-pypackage')"}, "traceback": [{"path": "tests/repository/test_is_repo_url.py", "lineno": 73, "message": "AssertionError"}], "longrepr": "def test_expand_abbreviations():\n        \"\"\"Validate `repository.expand_abbreviations` correctly translate url.\"\"\"\n        template = 'gh:audreyfeldroy/cookiecutter-pypackage'\n    \n        # This is not a valid repo url just yet!\n        # First `repository.expand_abbreviations` needs to translate it\n>       assert is_repo_url(template) is False\nE       AssertionError: assert None is False\nE        +  where None = is_repo_url('gh:audreyfeldroy/cookiecutter-pypackage')\n\ntests/repository/test_is_repo_url.py:73: AssertionError"}, "teardown": {"duration": 0.0003438380000000407, "outcome": "passed"}}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_valid_repository", "lineno": 7, "outcome": "failed", "keywords": ["test_valid_repository", "test_repository_has_cookiecutter_json.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011177490000000567, "outcome": "passed"}, "call": {"duration": 0.00031648900000025293, "outcome": "failed", "crash": {"path": "/testbed/tests/repository/test_repository_has_cookiecutter_json.py", "lineno": 10, "message": "AssertionError: assert None\n +  where None = repository_has_cookiecutter_json('tests/fake-repo')"}, "traceback": [{"path": "tests/repository/test_repository_has_cookiecutter_json.py", "lineno": 10, "message": "AssertionError"}], "longrepr": "def test_valid_repository():\n        \"\"\"Validate correct response if `cookiecutter.json` file exist.\"\"\"\n>       assert repository_has_cookiecutter_json('tests/fake-repo')\nE       AssertionError: assert None\nE        +  where None = repository_has_cookiecutter_json('tests/fake-repo')\n\ntests/repository/test_repository_has_cookiecutter_json.py:10: AssertionError"}, "teardown": {"duration": 0.0003418020000003352, "outcome": "passed"}}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_invalid_repository[tests/fake-repo-bad]", "lineno": 12, "outcome": "passed", "keywords": ["test_invalid_repository[tests/fake-repo-bad]", "parametrize", "pytestmark", "tests/fake-repo-bad", "test_repository_has_cookiecutter_json.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0012342359999997221, "outcome": "passed"}, "call": {"duration": 0.00023808899999977484, "outcome": "passed"}, "teardown": {"duration": 0.0003202190000002325, "outcome": "passed"}}, {"nodeid": "tests/repository/test_repository_has_cookiecutter_json.py::test_invalid_repository[tests/unknown-repo]", "lineno": 12, "outcome": "passed", "keywords": ["test_invalid_repository[tests/unknown-repo]", "parametrize", "pytestmark", "tests/unknown-repo", "test_repository_has_cookiecutter_json.py", "repository", "tests", "testbed", ""], "setup": {"duration": 0.0011497490000000887, "outcome": "passed"}, "call": {"duration": 0.00023342699999995276, "outcome": "passed"}, "teardown": {"duration": 0.0003531290000000631, "outcome": "passed"}}, {"nodeid": "tests/test_abort_generate_on_hook_error.py::test_hooks_raises_errors[pre_gen_hook_raises_error]", "lineno": 12, "outcome": "failed", "keywords": ["test_hooks_raises_errors[pre_gen_hook_raises_error]", "usefixtures", "parametrize", "pytestmark", "pre_gen_hook_raises_error", "test_abort_generate_on_hook_error.py", "tests", "testbed", ""], "setup": {"duration": 0.0013474119999998813, "outcome": "passed"}, "call": {"duration": 0.0004502089999998127, "outcome": "failed", "crash": {"path": "/testbed/.venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError: .value can only be used after the context manager exits"}, "traceback": [{"path": "tests/test_abort_generate_on_hook_error.py", "lineno": 39, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_hooks_raises_errors_pre_g0')\nabort_pre_gen = 'yes', abort_post_gen = 'no'\n\n    @pytest.mark.parametrize(\n        (\"abort_pre_gen\", \"abort_post_gen\"),\n        ((\"yes\", \"no\"), (\"no\", \"yes\")),\n        ids=(\"pre_gen_hook_raises_error\", \"post_gen_hook_raises_error\"),\n    )\n    @pytest.mark.usefixtures(\"clean_system\")\n    def test_hooks_raises_errors(tmp_path, abort_pre_gen, abort_post_gen):\n        \"\"\"Verify pre- and pos-gen errors raises correct error code from script.\n    \n        This allows developers to make different error codes in their code,\n        for different errors.\n        \"\"\"\n        context = {\n            \"cookiecutter\": {\n                \"repo_dir\": \"foobar\",\n                \"abort_pre_gen\": abort_pre_gen,\n                \"abort_post_gen\": abort_post_gen,\n            }\n        }\n    \n        with pytest.raises(exceptions.FailedHookException) as error:\n            generate.generate_files(\n                repo_dir=\"tests/hooks-abort-render\",\n                context=context,\n                output_dir=str(tmp_path),\n            )\n>           assert error.value.code == 5\n\ntests/test_abort_generate_on_hook_error.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ExceptionInfo for raises contextmanager>\n\n    @property\n    def value(self) -> E:\n        \"\"\"The exception value.\"\"\"\n        assert (\n>           self._excinfo is not None\n        ), \".value can only be used after the context manager exits\"\nE       AssertionError: .value can only be used after the context manager exits\n\n.venv/lib/python3.10/site-packages/_pytest/_code/code.py:548: AssertionError"}, "teardown": {"duration": 0.0006265380000001208, "outcome": "passed"}}, {"nodeid": "tests/test_abort_generate_on_hook_error.py::test_hooks_raises_errors[post_gen_hook_raises_error]", "lineno": 12, "outcome": "failed", "keywords": ["test_hooks_raises_errors[post_gen_hook_raises_error]", "usefixtures", "parametrize", "pytestmark", "post_gen_hook_raises_error", "test_abort_generate_on_hook_error.py", "tests", "testbed", ""], "setup": {"duration": 0.001693685999999861, "outcome": "passed"}, "call": {"duration": 0.00032970300000023656, "outcome": "failed", "crash": {"path": "/testbed/.venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError: .value can only be used after the context manager exits"}, "traceback": [{"path": "tests/test_abort_generate_on_hook_error.py", "lineno": 39, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_hooks_raises_errors_post_0')\nabort_pre_gen = 'no', abort_post_gen = 'yes'\n\n    @pytest.mark.parametrize(\n        (\"abort_pre_gen\", \"abort_post_gen\"),\n        ((\"yes\", \"no\"), (\"no\", \"yes\")),\n        ids=(\"pre_gen_hook_raises_error\", \"post_gen_hook_raises_error\"),\n    )\n    @pytest.mark.usefixtures(\"clean_system\")\n    def test_hooks_raises_errors(tmp_path, abort_pre_gen, abort_post_gen):\n        \"\"\"Verify pre- and pos-gen errors raises correct error code from script.\n    \n        This allows developers to make different error codes in their code,\n        for different errors.\n        \"\"\"\n        context = {\n            \"cookiecutter\": {\n                \"repo_dir\": \"foobar\",\n                \"abort_pre_gen\": abort_pre_gen,\n                \"abort_post_gen\": abort_post_gen,\n            }\n        }\n    \n        with pytest.raises(exceptions.FailedHookException) as error:\n            generate.generate_files(\n                repo_dir=\"tests/hooks-abort-render\",\n                context=context,\n                output_dir=str(tmp_path),\n            )\n>           assert error.value.code == 5\n\ntests/test_abort_generate_on_hook_error.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ExceptionInfo for raises contextmanager>\n\n    @property\n    def value(self) -> E:\n        \"\"\"The exception value.\"\"\"\n        assert (\n>           self._excinfo is not None\n        ), \".value can only be used after the context manager exits\"\nE       AssertionError: .value can only be used after the context manager exits\n\n.venv/lib/python3.10/site-packages/_pytest/_code/code.py:548: AssertionError"}, "teardown": {"duration": 0.000532847000000114, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_version[-V]", "lineno": 67, "outcome": "failed", "keywords": ["test_cli_version[-V]", "-V", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013777289999996611, "outcome": "passed"}, "call": {"duration": 0.0015435039999998068, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 72, "message": "AssertionError: assert False\n +  where False = <built-in method startswith of str object at 0x7f2b4c579390>('Cookiecutter')\n +    where <built-in method startswith of str object at 0x7f2b4c579390> = 'main, version 2.6.0\\n'.startswith\n +      where 'main, version 2.6.0\\n' = <Result okay>.output"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 72, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nversion_cli_flag = '-V'\n\n    def test_cli_version(cli_runner, version_cli_flag):\n        \"\"\"Verify Cookiecutter version output by `cookiecutter` on cli invocation.\"\"\"\n        result = cli_runner(version_cli_flag)\n        assert result.exit_code == 0\n>       assert result.output.startswith('Cookiecutter')\nE       AssertionError: assert False\nE        +  where False = <built-in method startswith of str object at 0x7f2b4c579390>('Cookiecutter')\nE        +    where <built-in method startswith of str object at 0x7f2b4c579390> = 'main, version 2.6.0\\n'.startswith\nE        +      where 'main, version 2.6.0\\n' = <Result okay>.output\n\ntests/test_cli.py:72: AssertionError"}, "teardown": {"duration": 0.00036446999999961704, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_version[--version]", "lineno": 67, "outcome": "failed", "keywords": ["test_cli_version[--version]", "--version", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0012452289999997923, "outcome": "passed"}, "call": {"duration": 0.0008677060000001902, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 72, "message": "AssertionError: assert False\n +  where False = <built-in method startswith of str object at 0x7f2b4c4ca600>('Cookiecutter')\n +    where <built-in method startswith of str object at 0x7f2b4c4ca600> = 'main, version 2.6.0\\n'.startswith\n +      where 'main, version 2.6.0\\n' = <Result okay>.output"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 72, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nversion_cli_flag = '--version'\n\n    def test_cli_version(cli_runner, version_cli_flag):\n        \"\"\"Verify Cookiecutter version output by `cookiecutter` on cli invocation.\"\"\"\n        result = cli_runner(version_cli_flag)\n        assert result.exit_code == 0\n>       assert result.output.startswith('Cookiecutter')\nE       AssertionError: assert False\nE        +  where False = <built-in method startswith of str object at 0x7f2b4c4ca600>('Cookiecutter')\nE        +    where <built-in method startswith of str object at 0x7f2b4c4ca600> = 'main, version 2.6.0\\n'.startswith\nE        +      where 'main, version 2.6.0\\n' = <Result okay>.output\n\ntests/test_cli.py:72: AssertionError"}, "teardown": {"duration": 0.00034427499999978295, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_error_on_existing_output_directory", "lineno": 74, "outcome": "failed", "keywords": ["test_cli_error_on_existing_output_directory", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014183260000000253, "outcome": "passed"}, "call": {"duration": 0.0010762519999998332, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 79, "message": "assert 0 != 0\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 79, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('make_fake_project_dir', 'remove_fake_project_dir')\n    def test_cli_error_on_existing_output_directory(cli_runner):\n        \"\"\"Test cli invocation without `overwrite-if-exists` fail if dir exist.\"\"\"\n        result = cli_runner('tests/fake-repo-pre/', '--no-input')\n>       assert result.exit_code != 0\nE       assert 0 != 0\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:79: AssertionError"}, "teardown": {"duration": 0.0007559409999999822, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli", "lineno": 83, "outcome": "failed", "keywords": ["test_cli", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0015515930000002953, "outcome": "passed"}, "call": {"duration": 0.0011711070000002266, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 90, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli(cli_runner):\n        \"\"\"Test cli invocation work without flags if directory not exist.\"\"\"\n        result = cli_runner('tests/fake-repo-pre/', '--no-input')\n        assert result.exit_code == 0\n        assert os.path.isdir('fake-project')\n>       content = Path(\"fake-project\", \"README.rst\").read_text()\n\ntests/test_cli.py:90: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('fake-project/README.rst'), mode = 'r', buffering = -1\nencoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.000597227000000089, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_verbose", "lineno": 93, "outcome": "failed", "keywords": ["test_cli_verbose", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.002698805000000082, "outcome": "passed"}, "call": {"duration": 0.0013717720000001599, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 100, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_verbose(cli_runner):\n        \"\"\"Test cli invocation display log if called with `verbose` flag.\"\"\"\n        result = cli_runner('tests/fake-repo-pre/', '--no-input', '-v')\n        assert result.exit_code == 0\n        assert os.path.isdir('fake-project')\n>       content = Path(\"fake-project\", \"README.rst\").read_text()\n\ntests/test_cli.py:100: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('fake-project/README.rst'), mode = 'r', buffering = -1\nencoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.0005095909999996984, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_replay", "lineno": 103, "outcome": "failed", "keywords": ["test_cli_replay", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001730934999999878, "outcome": "passed"}, "call": {"duration": 0.001891162999999807, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 113, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 113, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823950146880'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce12bf0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_replay(mocker, cli_runner):\n        \"\"\"Test cli invocation display log with `verbose` and `replay` flags.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--replay', '-v')\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=True,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:113: AssertionError"}, "teardown": {"duration": 0.0005677620000001937, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_replay_file", "lineno": 130, "outcome": "failed", "keywords": ["test_cli_replay_file", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0017589690000003877, "outcome": "passed"}, "call": {"duration": 0.001959434000000204, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 140, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 140, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823971120944'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4e212e30>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_replay_file(mocker, cli_runner):\n        \"\"\"Test cli invocation correctly pass --replay-file option.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--replay-file', '~/custom-replay-file', '-v')\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay='~/custom-replay-file',\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:140: AssertionError"}, "teardown": {"duration": 0.0005065210000001485, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_replay_generated", "lineno": 157, "outcome": "failed", "keywords": ["test_cli_replay_generated", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0016819559999996514, "outcome": "passed"}, "call": {"duration": 0.0012819080000001648, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 171, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'tests/tmp/replay-project/README.md'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 171, "message": "FileNotFoundError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3e7df0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_tmp_dir')\n    def test_cli_replay_generated(mocker, cli_runner):\n        \"\"\"Test cli invocation correctly generates a project with replay.\"\"\"\n        template_path = 'tests/fake-repo-replay/'\n        result = cli_runner(\n            template_path,\n            '--replay-file',\n            'tests/test-replay/valid_replay.json',\n            '-o',\n            'tests/tmp/',\n            '-v',\n        )\n        assert result.exit_code == 0\n>       assert open('tests/tmp/replay-project/README.md').read().strip() == 'replayed'\nE       FileNotFoundError: [Errno 2] No such file or directory: 'tests/tmp/replay-project/README.md'\n\ntests/test_cli.py:171: FileNotFoundError"}, "teardown": {"duration": 0.0003974910000001941, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_exit_on_noinput_and_replay", "lineno": 173, "outcome": "failed", "keywords": ["test_cli_exit_on_noinput_and_replay", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014960990000001395, "outcome": "passed"}, "call": {"duration": 0.0018549690000000396, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 184, "message": "assert 0 == 1\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 184, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cad4730>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_exit_on_noinput_and_replay(mocker, cli_runner):\n        \"\"\"Test cli invocation fail if both `no-input` and `replay` flags passed.\"\"\"\n        mock_cookiecutter = mocker.patch(\n            'cookiecutter.cli.cookiecutter', side_effect=cookiecutter\n        )\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--no-input', '--replay', '-v')\n    \n>       assert result.exit_code == 1\nE       assert 0 == 1\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:184: AssertionError"}, "teardown": {"duration": 0.00044769600000016396, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_run_cookiecutter_on_overwrite_if_exists_and_replay[-f]", "lineno": 215, "outcome": "failed", "keywords": ["test_run_cookiecutter_on_overwrite_if_exists_and_replay[-f]", "usefixtures", "pytestmark", "-f", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0015021950000000395, "outcome": "passed"}, "call": {"duration": 0.001666963999999993, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 228, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 228, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823941079904'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c56fd00>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noverwrite_cli_flag = '-f'\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_run_cookiecutter_on_overwrite_if_exists_and_replay(\n        mocker, cli_runner, overwrite_cli_flag\n    ):\n        \"\"\"Test cli invocation with `overwrite-if-exists` and `replay` flags.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--replay', '-v', overwrite_cli_flag)\n    \n        assert result.exit_code == 0\n    \n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=True,\n            overwrite_if_exists=True,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:228: AssertionError"}, "teardown": {"duration": 0.0005526420000001586, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_run_cookiecutter_on_overwrite_if_exists_and_replay[--overwrite-if-exists]", "lineno": 215, "outcome": "failed", "keywords": ["test_run_cookiecutter_on_overwrite_if_exists_and_replay[--overwrite-if-exists]", "usefixtures", "pytestmark", "--overwrite-if-exists", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0017260429999996774, "outcome": "passed"}, "call": {"duration": 0.0023764600000002467, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 228, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 228, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823955812912'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4d37aa70>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noverwrite_cli_flag = '--overwrite-if-exists'\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_run_cookiecutter_on_overwrite_if_exists_and_replay(\n        mocker, cli_runner, overwrite_cli_flag\n    ):\n        \"\"\"Test cli invocation with `overwrite-if-exists` and `replay` flags.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--replay', '-v', overwrite_cli_flag)\n    \n        assert result.exit_code == 0\n    \n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=True,\n            overwrite_if_exists=True,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:228: AssertionError"}, "teardown": {"duration": 0.0006720270000002415, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_does_not_exist[-f]", "lineno": 245, "outcome": "passed", "keywords": ["test_cli_overwrite_if_exists_when_output_dir_does_not_exist[-f]", "usefixtures", "pytestmark", "-f", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0018652020000002878, "outcome": "passed"}, "call": {"duration": 0.0012770329999995944, "outcome": "passed"}, "teardown": {"duration": 0.000397028000000077, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_does_not_exist[--overwrite-if-exists]", "lineno": 245, "outcome": "passed", "keywords": ["test_cli_overwrite_if_exists_when_output_dir_does_not_exist[--overwrite-if-exists]", "usefixtures", "pytestmark", "--overwrite-if-exists", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0015287819999998398, "outcome": "passed"}, "call": {"duration": 0.001001249000000204, "outcome": "passed"}, "teardown": {"duration": 0.0003850769999997894, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_exists[-f]", "lineno": 259, "outcome": "error", "keywords": ["test_cli_overwrite_if_exists_when_output_dir_exists[-f]", "usefixtures", "pytestmark", "-f", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001352089999999695, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError: [Errno 17] File exists: 'fake-project'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 59, "message": ""}, {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError"}], "longrepr": "request = <SubRequest 'make_fake_project_dir' for <Function test_cli_overwrite_if_exists_when_output_dir_exists[-f]>>\n\n    @pytest.fixture\n    def make_fake_project_dir(request):\n        \"\"\"Create a fake project to be overwritten in the according tests.\"\"\"\n>       os.makedirs('fake-project')\n\ntests/test_cli.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'fake-project', mode = 511, exist_ok = False\n\n    def makedirs(name, mode=0o777, exist_ok=False):\n        \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n    \n        Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n        mkdir, except that any intermediate path segment (not just the rightmost)\n        will be created if it does not exist. If the target directory already\n        exists, raise an OSError if exist_ok is False. Otherwise no exception is\n        raised.  This is recursive.\n    \n        \"\"\"\n        head, tail = path.split(name)\n        if not tail:\n            head, tail = path.split(head)\n        if head and tail and not path.exists(head):\n            try:\n                makedirs(head, exist_ok=exist_ok)\n            except FileExistsError:\n                # Defeats race condition when another thread created the path\n                pass\n            cdir = curdir\n            if isinstance(tail, bytes):\n                cdir = bytes(curdir, 'ASCII')\n            if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\n                return\n        try:\n>           mkdir(name, mode)\nE           FileExistsError: [Errno 17] File exists: 'fake-project'\n\n/usr/lib/python3.10/os.py:225: FileExistsError"}, "teardown": {"duration": 0.00035899099999969764, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_overwrite_if_exists_when_output_dir_exists[--overwrite-if-exists]", "lineno": 259, "outcome": "error", "keywords": ["test_cli_overwrite_if_exists_when_output_dir_exists[--overwrite-if-exists]", "usefixtures", "pytestmark", "--overwrite-if-exists", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014043550000000238, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError: [Errno 17] File exists: 'fake-project'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 59, "message": ""}, {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError"}], "longrepr": "request = <SubRequest 'make_fake_project_dir' for <Function test_cli_overwrite_if_exists_when_output_dir_exists[--overwrite-if-exists]>>\n\n    @pytest.fixture\n    def make_fake_project_dir(request):\n        \"\"\"Create a fake project to be overwritten in the according tests.\"\"\"\n>       os.makedirs('fake-project')\n\ntests/test_cli.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'fake-project', mode = 511, exist_ok = False\n\n    def makedirs(name, mode=0o777, exist_ok=False):\n        \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n    \n        Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n        mkdir, except that any intermediate path segment (not just the rightmost)\n        will be created if it does not exist. If the target directory already\n        exists, raise an OSError if exist_ok is False. Otherwise no exception is\n        raised.  This is recursive.\n    \n        \"\"\"\n        head, tail = path.split(name)\n        if not tail:\n            head, tail = path.split(head)\n        if head and tail and not path.exists(head):\n            try:\n                makedirs(head, exist_ok=exist_ok)\n            except FileExistsError:\n                # Defeats race condition when another thread created the path\n                pass\n            cdir = curdir\n            if isinstance(tail, bytes):\n                cdir = bytes(curdir, 'ASCII')\n            if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\n                return\n        try:\n>           mkdir(name, mode)\nE           FileExistsError: [Errno 17] File exists: 'fake-project'\n\n/usr/lib/python3.10/os.py:225: FileExistsError"}, "teardown": {"duration": 0.00039117800000010305, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_output_dir[-o]", "lineno": 276, "outcome": "failed", "keywords": ["test_cli_output_dir[-o]", "-o", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0015756179999999453, "outcome": "passed"}, "call": {"duration": 0.0017005740000000102, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 285, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 285, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823939480896'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3e6a70>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '-o'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_output_dir__o_0/output'\n\n    def test_cli_output_dir(mocker, cli_runner, output_dir_flag, output_dir):\n        \"\"\"Test cli invocation with `output-dir` flag changes output directory.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, output_dir_flag, output_dir)\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:285: AssertionError"}, "teardown": {"duration": 0.0004659259999999499, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_output_dir[--output-dir]", "lineno": 276, "outcome": "failed", "keywords": ["test_cli_output_dir[--output-dir]", "--output-dir", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001686830000000139, "outcome": "passed"}, "call": {"duration": 0.001567441999999808, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 285, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 285, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823939460816'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3e2650>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '--output-dir'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_output_dir___output_d0/output'\n\n    def test_cli_output_dir(mocker, cli_runner, output_dir_flag, output_dir):\n        \"\"\"Test cli invocation with `output-dir` flag changes output directory.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, output_dir_flag, output_dir)\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:285: AssertionError"}, "teardown": {"duration": 0.00044163600000013403, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_help[-h]", "lineno": 308, "outcome": "passed", "keywords": ["test_cli_help[-h]", "-h", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013995300000000377, "outcome": "passed"}, "call": {"duration": 0.0033769630000000106, "outcome": "passed"}, "teardown": {"duration": 0.00034418300000016444, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_help[--help]", "lineno": 308, "outcome": "passed", "keywords": ["test_cli_help[--help]", "--help", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013467929999997352, "outcome": "passed"}, "call": {"duration": 0.0019264729999997066, "outcome": "passed"}, "teardown": {"duration": 0.0003278049999999588, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_help[help]", "lineno": 308, "outcome": "failed", "keywords": ["test_cli_help[help]", "help", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013326810000000577, "outcome": "passed"}, "call": {"duration": 0.0011736479999999716, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 313, "message": "AssertionError: assert False\n +  where False = <built-in method startswith of str object at 0x7f2b4eab4030>('Usage')\n +    where <built-in method startswith of str object at 0x7f2b4eab4030> = ''.startswith\n +      where '' = <Result okay>.output"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 313, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nhelp_cli_flag = 'help'\n\n    def test_cli_help(cli_runner, help_cli_flag):\n        \"\"\"Test cli invocation display help message with `help` flag.\"\"\"\n        result = cli_runner(help_cli_flag)\n        assert result.exit_code == 0\n>       assert result.output.startswith('Usage')\nE       AssertionError: assert False\nE        +  where False = <built-in method startswith of str object at 0x7f2b4eab4030>('Usage')\nE        +    where <built-in method startswith of str object at 0x7f2b4eab4030> = ''.startswith\nE        +      where '' = <Result okay>.output\n\ntests/test_cli.py:313: AssertionError"}, "teardown": {"duration": 0.00036391099999999454, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_user_config", "lineno": 321, "outcome": "failed", "keywords": ["test_user_config", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014264170000002352, "outcome": "passed"}, "call": {"duration": 0.0015472409999999215, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 330, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 330, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823944281120'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': '/tmp/pytest-of-root/pytest-0/test_user_config0/tests/config.yaml', 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c87b0a0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nuser_config_path = '/tmp/pytest-of-root/pytest-0/test_user_config0/tests/config.yaml'\n\n    def test_user_config(mocker, cli_runner, user_config_path):\n        \"\"\"Test cli invocation works with `config-file` option.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--config-file', user_config_path)\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=user_config_path,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:330: AssertionError"}, "teardown": {"duration": 0.00041050000000009135, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_default_user_config_overwrite", "lineno": 347, "outcome": "failed", "keywords": ["test_default_user_config_overwrite", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001449707999999994, "outcome": "passed"}, "call": {"duration": 0.001547835999999858, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 361, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 361, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823939505024'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': '/tmp/pytest-of-root/pytest-0/test_default_user_config_overw0/tests/config.yaml', 'default_config': True, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3ec640>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nuser_config_path = '/tmp/pytest-of-root/pytest-0/test_default_user_config_overw0/tests/config.yaml'\n\n    def test_default_user_config_overwrite(mocker, cli_runner, user_config_path):\n        \"\"\"Test cli invocation ignores `config-file` if `default-config` passed.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(\n            template_path,\n            '--config-file',\n            user_config_path,\n            '--default-config',\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=user_config_path,\n            default_config=True,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:361: AssertionError"}, "teardown": {"duration": 0.0004487590000001873, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_default_user_config", "lineno": 378, "outcome": "failed", "keywords": ["test_default_user_config", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014241449999996547, "outcome": "passed"}, "call": {"duration": 0.0015247899999994985, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 387, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 387, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823955814112'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': True, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4d37bdc0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    def test_default_user_config(mocker, cli_runner):\n        \"\"\"Test cli invocation accepts `default-config` flag correctly.\"\"\"\n        mock_cookiecutter = mocker.patch('cookiecutter.cli.cookiecutter')\n    \n        template_path = 'tests/fake-repo-pre/'\n        result = cli_runner(template_path, '--default-config')\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            config_file=None,\n            default_config=True,\n            extra_context=None,\n            password=None,\n            directory=None,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:387: AssertionError"}, "teardown": {"duration": 0.0005145640000003837, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_echo_undefined_variable_error", "lineno": 404, "outcome": "failed", "keywords": ["test_echo_undefined_variable_error", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0017173330000002096, "outcome": "passed"}, "call": {"duration": 0.001218416999999583, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 417, "message": "assert 0 == 1\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 417, "message": "AssertionError"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_echo_undefined_variable_e0/output'\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    def test_echo_undefined_variable_error(output_dir, cli_runner):\n        \"\"\"Cli invocation return error if variable undefined in template.\"\"\"\n        template_path = 'tests/undefined-variable/file-name/'\n    \n        result = cli_runner(\n            '--no-input',\n            '--default-config',\n            '--output-dir',\n            output_dir,\n            template_path,\n        )\n    \n>       assert result.exit_code == 1\nE       assert 0 == 1\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:417: AssertionError"}, "teardown": {"duration": 0.0003869390000001971, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_echo_unknown_extension_error", "lineno": 444, "outcome": "failed", "keywords": ["test_echo_unknown_extension_error", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0015275990000001016, "outcome": "passed"}, "call": {"duration": 0.0011065779999999137, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 457, "message": "assert 0 == 1\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 457, "message": "AssertionError"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_echo_unknown_extension_er0/output'\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    def test_echo_unknown_extension_error(output_dir, cli_runner):\n        \"\"\"Cli return error if extension incorrectly defined in template.\"\"\"\n        template_path = 'tests/test-extensions/unknown/'\n    \n        result = cli_runner(\n            '--no-input',\n            '--default-config',\n            '--output-dir',\n            output_dir,\n            template_path,\n        )\n    \n>       assert result.exit_code == 1\nE       assert 0 == 1\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:457: AssertionError"}, "teardown": {"duration": 0.00040366399999935965, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_local_extension", "lineno": 461, "outcome": "failed", "keywords": ["test_local_extension", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001397625000000069, "outcome": "passed"}, "call": {"duration": 0.001244266999999688, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytest-of-root/pytest-0/test_local_extension0/output/Foobar/HISTORY.rst'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 475, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "tmpdir = local('/tmp/pytest-of-root/pytest-0/test_local_extension0')\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    def test_local_extension(tmpdir, cli_runner):\n        \"\"\"Test to verify correct work of extension, included in template.\"\"\"\n        output_dir = str(tmpdir.mkdir('output'))\n        template_path = 'tests/test-extensions/local_extension/'\n    \n        result = cli_runner(\n            '--no-input',\n            '--default-config',\n            '--output-dir',\n            output_dir,\n            template_path,\n        )\n        assert result.exit_code == 0\n>       content = Path(output_dir, 'Foobar', 'HISTORY.rst').read_text()\n\ntests/test_cli.py:475: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('/tmp/pytest-of-root/pytest-0/test_local_extension0/output/Foobar/HISTORY.rst')\nmode = 'r', buffering = -1, encoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytest-of-root/pytest-0/test_local_extension0/output/Foobar/HISTORY.rst'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.00040907399999934313, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_local_extension_not_available", "lineno": 479, "outcome": "failed", "keywords": ["test_local_extension_not_available", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014329530000001256, "outcome": "passed"}, "call": {"duration": 0.00030231800000013465, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 485, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "tmpdir = local('/tmp/pytest-of-root/pytest-0/test_local_extension_not_avail0')\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    def test_local_extension_not_available(tmpdir, cli_runner):\n        \"\"\"Test handling of included but unavailable local extension.\"\"\"\n        context = {'cookiecutter': {'_extensions': ['foobar']}}\n    \n        with pytest.raises(UnknownExtension) as err:\n>           StrictEnvironment(context=context, keep_trailing_newline=True)\n\ntests/test_cli.py:485: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4cc01000>\nkwargs = {'keep_trailing_newline': True, 'undefined': <class 'jinja2.runtime.StrictUndefined'>}\ncontext = {'cookiecutter': {'_extensions': ['foobar']}}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003758609999993112, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_extra_context", "lineno": 489, "outcome": "failed", "keywords": ["test_cli_extra_context", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013540680000003746, "outcome": "passed"}, "call": {"duration": 0.00112046700000068, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 501, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_extra_context(cli_runner):\n        \"\"\"Cli invocation replace content if called with replacement pairs.\"\"\"\n        result = cli_runner(\n            'tests/fake-repo-pre/',\n            '--no-input',\n            '-v',\n            'project_name=Awesomez',\n        )\n        assert result.exit_code == 0\n        assert os.path.isdir('fake-project')\n>       content = Path('fake-project', 'README.rst').read_text()\n\ntests/test_cli.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('fake-project/README.rst'), mode = 'r', buffering = -1\nencoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.0004478169999995174, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_extra_context_invalid_format", "lineno": 504, "outcome": "failed", "keywords": ["test_cli_extra_context_invalid_format", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013754900000000347, "outcome": "passed"}, "call": {"duration": 0.0011586069999998116, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 514, "message": "assert 0 == 2\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 514, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_extra_context_invalid_format(cli_runner):\n        \"\"\"Cli invocation raise error if called with unknown argument.\"\"\"\n        result = cli_runner(\n            'tests/fake-repo-pre/',\n            '--no-input',\n            '-v',\n            'ExtraContextWithNoEqualsSoInvalid',\n        )\n>       assert result.exit_code == 2\nE       assert 0 == 2\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:514: AssertionError"}, "teardown": {"duration": 0.0003807919999996301, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_debug_file_non_verbose", "lineno": 524, "outcome": "failed", "keywords": ["test_debug_file_non_verbose", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014093779999999612, "outcome": "passed"}, "call": {"duration": 0.0010835819999996943, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 541, "message": "AssertionError: assert False\n +  where False = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_non_verbose0/fake-repo.log').exists"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 541, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\ndebug_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_non_verbose0/fake-repo.log')\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_debug_file_non_verbose(cli_runner, debug_file):\n        \"\"\"Test cli invocation writes log to `debug-file` if flag enabled.\n    \n        Case for normal log output.\n        \"\"\"\n        assert not debug_file.exists()\n    \n        result = cli_runner(\n            '--no-input',\n            '--debug-file',\n            str(debug_file),\n            'tests/fake-repo-pre/',\n        )\n        assert result.exit_code == 0\n    \n>       assert debug_file.exists()\nE       AssertionError: assert False\nE        +  where False = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_non_verbose0/fake-repo.log').exists\n\ntests/test_cli.py:541: AssertionError"}, "teardown": {"duration": 0.00043317300000023096, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_debug_file_verbose", "lineno": 550, "outcome": "failed", "keywords": ["test_debug_file_verbose", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.00140367000000019, "outcome": "passed"}, "call": {"duration": 0.0010720019999999053, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 568, "message": "AssertionError: assert False\n +  where False = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_verbose0/fake-repo.log').exists"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 568, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\ndebug_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_verbose0/fake-repo.log')\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_debug_file_verbose(cli_runner, debug_file):\n        \"\"\"Test cli invocation writes log to `debug-file` if flag enabled.\n    \n        Case for verbose log output.\n        \"\"\"\n        assert not debug_file.exists()\n    \n        result = cli_runner(\n            '--verbose',\n            '--no-input',\n            '--debug-file',\n            str(debug_file),\n            'tests/fake-repo-pre/',\n        )\n        assert result.exit_code == 0\n    \n>       assert debug_file.exists()\nE       AssertionError: assert False\nE        +  where False = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_verbose0/fake-repo.log').exists\n\ntests/test_cli.py:568: AssertionError"}, "teardown": {"duration": 0.00041158299999999315, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_debug_list_installed_templates", "lineno": 577, "outcome": "error", "keywords": ["test_debug_list_installed_templates", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014166050000001817, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError: [Errno 17] File exists: 'fake-project'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 59, "message": ""}, {"path": "/usr/lib/python3.10/os.py", "lineno": 225, "message": "FileExistsError"}], "longrepr": "request = <SubRequest 'make_fake_project_dir' for <Function test_debug_list_installed_templates>>\n\n    @pytest.fixture\n    def make_fake_project_dir(request):\n        \"\"\"Create a fake project to be overwritten in the according tests.\"\"\"\n>       os.makedirs('fake-project')\n\ntests/test_cli.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'fake-project', mode = 511, exist_ok = False\n\n    def makedirs(name, mode=0o777, exist_ok=False):\n        \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n    \n        Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n        mkdir, except that any intermediate path segment (not just the rightmost)\n        will be created if it does not exist. If the target directory already\n        exists, raise an OSError if exist_ok is False. Otherwise no exception is\n        raised.  This is recursive.\n    \n        \"\"\"\n        head, tail = path.split(name)\n        if not tail:\n            head, tail = path.split(head)\n        if head and tail and not path.exists(head):\n            try:\n                makedirs(head, exist_ok=exist_ok)\n            except FileExistsError:\n                # Defeats race condition when another thread created the path\n                pass\n            cdir = curdir\n            if isinstance(tail, bytes):\n                cdir = bytes(curdir, 'ASCII')\n            if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\n                return\n        try:\n>           mkdir(name, mode)\nE           FileExistsError: [Errno 17] File exists: 'fake-project'\n\n/usr/lib/python3.10/os.py:225: FileExistsError"}, "teardown": {"duration": 0.00035320799999993824, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_debug_list_installed_templates_failure", "lineno": 597, "outcome": "failed", "keywords": ["test_debug_list_installed_templates_failure", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014578649999998916, "outcome": "passed"}, "call": {"duration": 0.0014630090000000706, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 609, "message": "AssertionError: assert 'Error: Cannot list installed templates.' in ''\n +  where '' = <Result okay>.output"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 609, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\ndebug_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_list_installed_temp1/fake-repo.log')\nuser_config_path = '/tmp/pytest-of-root/pytest-0/test_debug_list_installed_temp1/tests/config.yaml'\n\n    def test_debug_list_installed_templates_failure(\n        cli_runner, debug_file, user_config_path\n    ):\n        \"\"\"Verify --list-installed command error on invocation.\"\"\"\n        os.makedirs(os.path.dirname(user_config_path))\n        Path(user_config_path).write_text('cookiecutters_dir: \"/notarealplace/\"')\n    \n        result = cli_runner(\n            '--list-installed', '--config-file', user_config_path, str(debug_file)\n        )\n    \n>       assert \"Error: Cannot list installed templates.\" in result.output\nE       AssertionError: assert 'Error: Cannot list installed templates.' in ''\nE        +  where '' = <Result okay>.output\n\ntests/test_cli.py:609: AssertionError"}, "teardown": {"duration": 0.00040029500000038354, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_directory_repo", "lineno": 612, "outcome": "failed", "keywords": ["test_directory_repo", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013474410000000603, "outcome": "passed"}, "call": {"duration": 0.001055198999999618, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 624, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_directory_repo(cli_runner):\n        \"\"\"Test cli invocation works with `directory` option.\"\"\"\n        result = cli_runner(\n            'tests/fake-repo-dir/',\n            '--no-input',\n            '-v',\n            '--directory=my-dir',\n        )\n        assert result.exit_code == 0\n        assert os.path.isdir(\"fake-project\")\n>       content = Path(\"fake-project\", \"README.rst\").read_text()\n\ntests/test_cli.py:624: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('fake-project/README.rst'), mode = 'r', buffering = -1\nencoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: 'fake-project/README.rst'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.00046820499999977727, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=yes-None-True]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[-o---accept-hooks=yes-None-True]", "parametrize", "pytestmark", "-o---accept-hooks=yes-None-True", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001962863000000148, "outcome": "passed"}, "call": {"duration": 0.0017306320000001207, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823939448368'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3de8f0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '-o'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks__o___acc0/output'\naccept_hooks_arg = '--accept-hooks=yes', user_input = None, expected = True\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0005051279999994662, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=no-None-False]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[-o---accept-hooks=no-None-False]", "parametrize", "pytestmark", "-o---accept-hooks=no-None-False", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001872510000000105, "outcome": "passed"}, "call": {"duration": 0.0017074329999999804, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823955617120'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': False, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4d349f30>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '-o'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks__o___acc1/output'\naccept_hooks_arg = '--accept-hooks=no', user_input = None, expected = False\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0006795960000003376, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=ask-yes-True]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[-o---accept-hooks=ask-yes-True]", "parametrize", "pytestmark", "-o---accept-hooks=ask-yes-True", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0021495460000000577, "outcome": "passed"}, "call": {"duration": 0.0019439019999998308, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823941085808'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c56d960>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '-o'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks__o___acc2/output'\naccept_hooks_arg = '--accept-hooks=ask', user_input = 'yes', expected = True\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0006812619999996272, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[-o---accept-hooks=ask-no-False]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[-o---accept-hooks=ask-no-False]", "parametrize", "pytestmark", "-o---accept-hooks=ask-no-False", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.002058124000000383, "outcome": "passed"}, "call": {"duration": 0.0018881379999999837, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823948075536'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': False, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cc19960>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '-o'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks__o___acc3/output'\naccept_hooks_arg = '--accept-hooks=ask', user_input = 'no', expected = False\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0005376649999995209, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=yes-None-True]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[--output-dir---accept-hooks=yes-None-True]", "parametrize", "pytestmark", "--output-dir---accept-hooks=yes-None-True", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0020270529999999454, "outcome": "passed"}, "call": {"duration": 0.0016123100000005053, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823947640464'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cbac940>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '--output-dir'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks___output0/output'\naccept_hooks_arg = '--accept-hooks=yes', user_input = None, expected = True\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0004980230000004582, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=no-None-False]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[--output-dir---accept-hooks=no-None-False]", "parametrize", "pytestmark", "--output-dir---accept-hooks=no-None-False", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0019645080000003645, "outcome": "passed"}, "call": {"duration": 0.0016026540000000367, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823951972288'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': False, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cfd1a20>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '--output-dir'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks___output1/output'\naccept_hooks_arg = '--accept-hooks=no', user_input = None, expected = False\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0004872100000001822, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=ask-yes-True]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[--output-dir---accept-hooks=ask-yes-True]", "parametrize", "pytestmark", "--output-dir---accept-hooks=ask-yes-True", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0018503249999994864, "outcome": "passed"}, "call": {"duration": 0.0015779439999992206, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823945149520'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': True, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c94c160>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '--output-dir'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks___output2/output'\naccept_hooks_arg = '--accept-hooks=ask', user_input = 'yes', expected = True\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0005649820000002137, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_accept_hooks[--output-dir---accept-hooks=ask-no-False]", "lineno": 635, "outcome": "failed", "keywords": ["test_cli_accept_hooks[--output-dir---accept-hooks=ask-no-False]", "parametrize", "pytestmark", "--output-dir---accept-hooks=ask-no-False", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.002081989999999756, "outcome": "passed"}, "call": {"duration": 0.0016615509999997613, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 657, "message": "AssertionError: Expected 'cookiecutter' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_cli.py", "lineno": 657, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='cookiecutter' id='139823939458416'>\nargs = ('tests/fake-repo-pre/', None, False)\nkwargs = {'accept_hooks': False, 'config_file': None, 'default_config': False, 'directory': None, ...}\nmsg = \"Expected 'cookiecutter' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3e09d0>\ncli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\noutput_dir_flag = '--output-dir'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_cli_accept_hooks___output3/output'\naccept_hooks_arg = '--accept-hooks=ask', user_input = 'no', expected = False\n\n    @pytest.mark.parametrize(\n        \"accept_hooks_arg,user_input,expected\", cli_accept_hook_arg_testdata\n    )\n    def test_cli_accept_hooks(\n        mocker,\n        cli_runner,\n        output_dir_flag,\n        output_dir,\n        accept_hooks_arg,\n        user_input,\n        expected,\n    ):\n        \"\"\"Test cli invocation works with `accept-hooks` option.\"\"\"\n        mock_cookiecutter = mocker.patch(\"cookiecutter.cli.cookiecutter\")\n    \n        template_path = \"tests/fake-repo-pre/\"\n        result = cli_runner(\n            template_path, output_dir_flag, output_dir, accept_hooks_arg, input=user_input\n        )\n    \n        assert result.exit_code == 0\n>       mock_cookiecutter.assert_called_once_with(\n            template_path,\n            None,\n            False,\n            replay=False,\n            overwrite_if_exists=False,\n            output_dir=output_dir,\n            config_file=None,\n            default_config=False,\n            extra_context=None,\n            password=None,\n            directory=None,\n            skip_if_file_exists=False,\n            accept_hooks=expected,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'cookiecutter' to be called once. Called 0 times.\n\ntests/test_cli.py:657: AssertionError"}, "teardown": {"duration": 0.0005055430000000527, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_with_json_decoding_error", "lineno": 674, "outcome": "failed", "keywords": ["test_cli_with_json_decoding_error", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.001724530999999807, "outcome": "passed"}, "call": {"duration": 0.0011458250000000447, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 680, "message": "assert 0 != 0\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 680, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_with_json_decoding_error(cli_runner):\n        \"\"\"Test cli invocation with a malformed JSON file.\"\"\"\n        template_path = 'tests/fake-repo-bad-json/'\n        result = cli_runner(template_path, '--no-input')\n>       assert result.exit_code != 0\nE       assert 0 != 0\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:680: AssertionError"}, "teardown": {"duration": 0.0003849860000002536, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_with_pre_prompt_hook", "lineno": 693, "outcome": "failed", "keywords": ["test_cli_with_pre_prompt_hook", "usefixtures", "pytestmark", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0014368129999997592, "outcome": "passed"}, "call": {"duration": 0.0010705820000005417, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 701, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('inputfake-project')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 701, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\n\n    @pytest.mark.usefixtures('remove_fake_project_dir')\n    def test_cli_with_pre_prompt_hook(cli_runner):\n        \"\"\"Test cli invocation in a template with pre_prompt hook.\"\"\"\n        template_path = 'tests/test-pyhooks/'\n        result = cli_runner(template_path, '--no-input')\n        assert result.exit_code == 0\n        dir_name = 'inputfake-project'\n>       assert os.path.isdir(dir_name)\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('inputfake-project')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cli.py:701: AssertionError"}, "teardown": {"duration": 0.00037501300000020166, "outcome": "passed"}}, {"nodeid": "tests/test_cli.py::test_cli_with_pre_prompt_hook_fail", "lineno": 705, "outcome": "failed", "keywords": ["test_cli_with_pre_prompt_hook_fail", "test_cli.py", "tests", "testbed", ""], "setup": {"duration": 0.0013030290000006772, "outcome": "passed"}, "call": {"duration": 0.0010416060000002503, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cli.py", "lineno": 712, "message": "assert 0 == 1\n +  where 0 = <Result okay>.exit_code"}, "traceback": [{"path": "tests/test_cli.py", "lineno": 712, "message": "AssertionError"}], "longrepr": "cli_runner = <function cli_runner.<locals>.cli_main at 0x7f2b4c496830>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4cbad9f0>\n\n    def test_cli_with_pre_prompt_hook_fail(cli_runner, monkeypatch):\n        \"\"\"Test cli invocation will fail when a given env var is present.\"\"\"\n        template_path = 'tests/test-pyhooks/'\n        with monkeypatch.context() as m:\n            m.setenv('COOKIECUTTER_FAIL_PRE_PROMPT', '1')\n            result = cli_runner(template_path, '--no-input')\n>       assert result.exit_code == 1\nE       assert 0 == 1\nE        +  where 0 = <Result okay>.exit_code\n\ntests/test_cli.py:712: AssertionError"}, "teardown": {"duration": 0.00034894500000071105, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_invocation.py::test_should_invoke_main", "lineno": 25, "outcome": "failed", "keywords": ["test_should_invoke_main", "usefixtures", "pytestmark", "test_cookiecutter_invocation.py", "tests", "testbed", ""], "setup": {"duration": 0.0015767460000004618, "outcome": "passed"}, "call": {"duration": 0.4348251859999994, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_invocation.py", "lineno": 35, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_invocation.py", "lineno": 35, "message": "AssertionError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3e90c0>\nproject_dir = 'fake-project-templated'\n\n    @pytest.mark.usefixtures('clean_system')\n    def test_should_invoke_main(monkeypatch, project_dir):\n        \"\"\"Should create a project and exit with 0 code on cli invocation.\"\"\"\n        monkeypatch.setenv('PYTHONPATH', '.')\n    \n        exit_code = subprocess.check_call(\n            [sys.executable, '-m', 'cookiecutter.cli', 'tests/fake-repo-tmpl', '--no-input']\n        )\n        assert exit_code == 0\n>       assert os.path.isdir(project_dir)\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_invocation.py:35: AssertionError"}, "teardown": {"duration": 0.0007140959999993868, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre/]", "lineno": 34, "outcome": "failed", "keywords": ["test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre/]", "usefixtures", "parametrize", "pytestmark", "tests/fake-repo-pre/", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.001939672999999864, "outcome": "passed"}, "call": {"duration": 0.00034220600000001156, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/genericpath.py", "lineno": 42, "message": "TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 42, "message": ""}, {"path": "/usr/lib/python3.10/genericpath.py", "lineno": 42, "message": "TypeError"}], "longrepr": "path = 'tests/fake-repo-pre/'\n\n    @pytest.mark.parametrize('path', ['tests/fake-repo-pre/', 'tests/fake-repo-pre'])\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_no_input_return_project_dir(path):\n        \"\"\"Verify `cookiecutter` create project dir on input with or without slash.\"\"\"\n        project_dir = main.cookiecutter(path, no_input=True)\n        assert os.path.isdir('tests/fake-repo-pre/{{cookiecutter.repo_name}}')\n        assert not os.path.isdir('tests/fake-repo-pre/fake-project')\n>       assert os.path.isdir(project_dir)\n\ntests/test_cookiecutter_local_no_input.py:42: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ns = None\n\n    def isdir(s):\n        \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n        try:\n>           st = os.stat(s)\nE           TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n\n/usr/lib/python3.10/genericpath.py:42: TypeError"}, "teardown": {"duration": 0.0009691180000004351, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre]", "lineno": 34, "outcome": "failed", "keywords": ["test_cookiecutter_no_input_return_project_dir[tests/fake-repo-pre]", "usefixtures", "parametrize", "pytestmark", "tests/fake-repo-pre", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0017085429999994517, "outcome": "passed"}, "call": {"duration": 0.0002943160000006273, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/genericpath.py", "lineno": 42, "message": "TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 42, "message": ""}, {"path": "/usr/lib/python3.10/genericpath.py", "lineno": 42, "message": "TypeError"}], "longrepr": "path = 'tests/fake-repo-pre'\n\n    @pytest.mark.parametrize('path', ['tests/fake-repo-pre/', 'tests/fake-repo-pre'])\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_no_input_return_project_dir(path):\n        \"\"\"Verify `cookiecutter` create project dir on input with or without slash.\"\"\"\n        project_dir = main.cookiecutter(path, no_input=True)\n        assert os.path.isdir('tests/fake-repo-pre/{{cookiecutter.repo_name}}')\n        assert not os.path.isdir('tests/fake-repo-pre/fake-project')\n>       assert os.path.isdir(project_dir)\n\ntests/test_cookiecutter_local_no_input.py:42: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ns = None\n\n    def isdir(s):\n        \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n        try:\n>           st = os.stat(s)\nE           TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n\n/usr/lib/python3.10/genericpath.py:42: TypeError"}, "teardown": {"duration": 0.0004759520000003903, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_extra_context", "lineno": 46, "outcome": "failed", "keywords": ["test_cookiecutter_no_input_extra_context", "usefixtures", "pytestmark", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.001536972999999442, "outcome": "passed"}, "call": {"duration": 0.000364444999999769, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_no_input.py", "lineno": 55, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-extra')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 55, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_no_input_extra_context():\n        \"\"\"Verify `cookiecutter` accept `extra_context` argument.\"\"\"\n        main.cookiecutter(\n            'tests/fake-repo-pre',\n            no_input=True,\n            extra_context={'repo_name': 'fake-project-extra'},\n        )\n>       assert os.path.isdir('fake-project-extra')\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-extra')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_no_input.py:55: AssertionError"}, "teardown": {"duration": 0.00046611900000037565, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_templated_context", "lineno": 57, "outcome": "failed", "keywords": ["test_cookiecutter_templated_context", "usefixtures", "pytestmark", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0015033299999993588, "outcome": "passed"}, "call": {"duration": 0.00037448700000020096, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_no_input.py", "lineno": 62, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 62, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_templated_context():\n        \"\"\"Verify Jinja2 templating correctly works in `cookiecutter.json` file.\"\"\"\n        main.cookiecutter('tests/fake-repo-tmpl', no_input=True)\n>       assert os.path.isdir('fake-project-templated')\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_no_input.py:62: AssertionError"}, "teardown": {"duration": 0.0004630469999993281, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_no_input_return_rendered_file", "lineno": 64, "outcome": "failed", "keywords": ["test_cookiecutter_no_input_return_rendered_file", "usefixtures", "pytestmark", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0015496280000002471, "outcome": "passed"}, "call": {"duration": 0.0004352430000000851, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_no_input.py", "lineno": 69, "message": "AssertionError: assert None == '/testbed/fake-project'\n +  where '/testbed/fake-project' = <function abspath at 0x7f2b4e9c2e60>('fake-project')\n +    where <function abspath at 0x7f2b4e9c2e60> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.abspath\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 69, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_no_input_return_rendered_file():\n        \"\"\"Verify Jinja2 templating correctly works in `cookiecutter.json` file.\"\"\"\n        project_dir = main.cookiecutter('tests/fake-repo-pre', no_input=True)\n>       assert project_dir == os.path.abspath('fake-project')\nE       AssertionError: assert None == '/testbed/fake-project'\nE        +  where '/testbed/fake-project' = <function abspath at 0x7f2b4e9c2e60>('fake-project')\nE        +    where <function abspath at 0x7f2b4e9c2e60> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.abspath\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_no_input.py:69: AssertionError"}, "teardown": {"duration": 0.00045788899999976707, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_dict_values_in_context", "lineno": 73, "outcome": "failed", "keywords": ["test_cookiecutter_dict_values_in_context", "usefixtures", "pytestmark", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0014730900000001768, "outcome": "passed"}, "call": {"duration": 0.0004191789999996587, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_no_input.py", "lineno": 78, "message": "AssertionError: assert None == '/testbed/fake-project-dict'\n +  where '/testbed/fake-project-dict' = <function abspath at 0x7f2b4e9c2e60>('fake-project-dict')\n +    where <function abspath at 0x7f2b4e9c2e60> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.abspath\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 78, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_dict_values_in_context():\n        \"\"\"Verify configured dictionary from `cookiecutter.json` correctly unpacked.\"\"\"\n        project_dir = main.cookiecutter('tests/fake-repo-dict', no_input=True)\n>       assert project_dir == os.path.abspath('fake-project-dict')\nE       AssertionError: assert None == '/testbed/fake-project-dict'\nE        +  where '/testbed/fake-project-dict' = <function abspath at 0x7f2b4e9c2e60>('fake-project-dict')\nE        +    where <function abspath at 0x7f2b4e9c2e60> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.abspath\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_no_input.py:78: AssertionError"}, "teardown": {"duration": 0.0004554499999995798, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_no_input.py::test_cookiecutter_template_cleanup", "lineno": 123, "outcome": "failed", "keywords": ["test_cookiecutter_template_cleanup", "usefixtures", "pytestmark", "test_cookiecutter_local_no_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0016654960000002106, "outcome": "passed"}, "call": {"duration": 0.0016881910000003941, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_no_input.py", "lineno": 134, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_no_input.py", "lineno": 134, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c8909a0>\n\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_template_cleanup(mocker):\n        \"\"\"Verify temporary folder for zip unpacking dropped.\"\"\"\n        mocker.patch('tempfile.mkdtemp', return_value='fake-tmp', autospec=True)\n    \n        mocker.patch(\n            'cookiecutter.prompt.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        main.cookiecutter('tests/files/fake-repo-tmpl.zip', no_input=True)\n>       assert os.path.isdir('fake-project-templated')\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-templated')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_no_input.py:134: AssertionError"}, "teardown": {"duration": 0.000520473999999993, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_with_input.py::test_cookiecutter_local_with_input", "lineno": 19, "outcome": "failed", "keywords": ["test_cookiecutter_local_with_input", "usefixtures", "pytestmark", "test_cookiecutter_local_with_input.py", "tests", "testbed", ""], "setup": {"duration": 0.001544162000000071, "outcome": "passed"}, "call": {"duration": 0.00041695799999974525, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_with_input.py", "lineno": 31, "message": "AssertionError: assert False\n +  where False = <function isfile at 0x7f2b4e9c1fc0>('fake-project/README.rst')\n +    where <function isfile at 0x7f2b4e9c1fc0> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isfile\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_with_input.py", "lineno": 31, "message": "AssertionError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3ecd00>\n\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_local_with_input(monkeypatch):\n        \"\"\"Verify simple cookiecutter run results, without extra_context provided.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n        main.cookiecutter('tests/fake-repo-pre/', no_input=False)\n        assert os.path.isdir('tests/fake-repo-pre/{{cookiecutter.repo_name}}')\n        assert not os.path.isdir('tests/fake-repo-pre/fake-project')\n        assert os.path.isdir('fake-project')\n>       assert os.path.isfile('fake-project/README.rst')\nE       AssertionError: assert False\nE        +  where False = <function isfile at 0x7f2b4e9c1fc0>('fake-project/README.rst')\nE        +    where <function isfile at 0x7f2b4e9c1fc0> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isfile\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_with_input.py:31: AssertionError"}, "teardown": {"duration": 0.0005290149999996885, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_local_with_input.py::test_cookiecutter_input_extra_context", "lineno": 34, "outcome": "failed", "keywords": ["test_cookiecutter_input_extra_context", "usefixtures", "pytestmark", "test_cookiecutter_local_with_input.py", "tests", "testbed", ""], "setup": {"duration": 0.0015706730000006885, "outcome": "passed"}, "call": {"duration": 0.00039940499999957524, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_local_with_input.py", "lineno": 47, "message": "AssertionError: assert False\n +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-input-extra')\n +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_cookiecutter_local_with_input.py", "lineno": 47, "message": "AssertionError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3e9510>\n\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_dirs')\n    def test_cookiecutter_input_extra_context(monkeypatch):\n        \"\"\"Verify simple cookiecutter run results, with extra_context provided.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n        main.cookiecutter(\n            'tests/fake-repo-pre',\n            no_input=False,\n            extra_context={'repo_name': 'fake-project-input-extra'},\n        )\n>       assert os.path.isdir('fake-project-input-extra')\nE       AssertionError: assert False\nE        +  where False = <function isdir at 0x7f2b4e9c2050>('fake-project-input-extra')\nE        +    where <function isdir at 0x7f2b4e9c2050> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.isdir\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_cookiecutter_local_with_input.py:47: AssertionError"}, "teardown": {"duration": 0.00046425499999980246, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_nested_templates.py::test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "lineno": 9, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "parametrize", "pytestmark", "fake-nested-templates-fake-project", "test_cookiecutter_nested_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.001520737999999966, "outcome": "passed"}, "call": {"duration": 0.0009991610000001927, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_nested_templates.py", "lineno": 23, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "tests/test_cookiecutter_nested_templates.py", "lineno": 23, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c8128c0>\ntemplate_dir = 'fake-nested-templates', output_dir = 'fake-project'\n\n    @pytest.mark.parametrize(\n        \"template_dir,output_dir\",\n        [\n            [\"fake-nested-templates\", \"fake-project\"],\n            [\"fake-nested-templates-old-style\", \"fake-package\"],\n        ],\n    )\n    def test_cookiecutter_nested_templates(mocker, template_dir: str, output_dir: str):\n        \"\"\"Verify cookiecutter nested configuration files mechanism.\"\"\"\n        mock_generate_files = mocker.patch(\"cookiecutter.main.generate_files\")\n        main_dir = (Path(\"tests\") / template_dir).resolve()\n        main.cookiecutter(f\"{main_dir}\", no_input=True)\n        expected = (Path(main_dir) / output_dir).resolve()\n>       assert mock_generate_files.call_args[1][\"repo_dir\"] == f\"{expected}\"\nE       TypeError: 'NoneType' object is not subscriptable\n\ntests/test_cookiecutter_nested_templates.py:23: TypeError"}, "teardown": {"duration": 0.00039727700000025123, "outcome": "passed"}}, {"nodeid": "tests/test_cookiecutter_nested_templates.py::test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "lineno": 9, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "parametrize", "pytestmark", "fake-nested-templates-old-style-fake-package", "test_cookiecutter_nested_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.0015522499999995887, "outcome": "passed"}, "call": {"duration": 0.0010074260000001445, "outcome": "failed", "crash": {"path": "/testbed/tests/test_cookiecutter_nested_templates.py", "lineno": 23, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "tests/test_cookiecutter_nested_templates.py", "lineno": 23, "message": "TypeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4cad4130>\ntemplate_dir = 'fake-nested-templates-old-style', output_dir = 'fake-package'\n\n    @pytest.mark.parametrize(\n        \"template_dir,output_dir\",\n        [\n            [\"fake-nested-templates\", \"fake-project\"],\n            [\"fake-nested-templates-old-style\", \"fake-package\"],\n        ],\n    )\n    def test_cookiecutter_nested_templates(mocker, template_dir: str, output_dir: str):\n        \"\"\"Verify cookiecutter nested configuration files mechanism.\"\"\"\n        mock_generate_files = mocker.patch(\"cookiecutter.main.generate_files\")\n        main_dir = (Path(\"tests\") / template_dir).resolve()\n        main.cookiecutter(f\"{main_dir}\", no_input=True)\n        expected = (Path(main_dir) / output_dir).resolve()\n>       assert mock_generate_files.call_args[1][\"repo_dir\"] == f\"{expected}\"\nE       TypeError: 'NoneType' object is not subscriptable\n\ntests/test_cookiecutter_nested_templates.py:23: TypeError"}, "teardown": {"duration": 0.0004006930000004516, "outcome": "passed"}}, {"nodeid": "tests/test_custom_extensions_in_hooks.py::test_hook_with_extension[pre_gen_hook]", "lineno": 29, "outcome": "failed", "keywords": ["test_hook_with_extension[pre_gen_hook]", "pre_gen_hook", "test_custom_extensions_in_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018124799999998942, "outcome": "passed"}, "call": {"duration": 0.0002782970000003715, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_custom_extensions_in_hooks.py", "lineno": 43, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'tests/test-extensions/custom-extension-pre'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_hook_with_extension_pre_g0/output'\n\n    def test_hook_with_extension(template, output_dir):\n        \"\"\"Verify custom Jinja2 extension correctly work in hooks and file rendering.\n    \n        Each file in hooks has simple tests inside and will raise error if not\n        correctly rendered.\n        \"\"\"\n        project_dir = main.cookiecutter(\n            template,\n            no_input=True,\n            output_dir=output_dir,\n            extra_context={'project_slug': 'foobar', 'name': 'Cookiemonster'},\n        )\n    \n>       readme = Path(project_dir, 'README.rst').read_text(encoding=\"utf-8\")\n\ntests/test_custom_extensions_in_hooks.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'README.rst')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.00042929599999919077, "outcome": "passed"}}, {"nodeid": "tests/test_custom_extensions_in_hooks.py::test_hook_with_extension[post_gen_hook]", "lineno": 29, "outcome": "failed", "keywords": ["test_hook_with_extension[post_gen_hook]", "post_gen_hook", "test_custom_extensions_in_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001721476000000166, "outcome": "passed"}, "call": {"duration": 0.00027795100000016504, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_custom_extensions_in_hooks.py", "lineno": 43, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'tests/test-extensions/custom-extension-post'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_hook_with_extension_post_0/output'\n\n    def test_hook_with_extension(template, output_dir):\n        \"\"\"Verify custom Jinja2 extension correctly work in hooks and file rendering.\n    \n        Each file in hooks has simple tests inside and will raise error if not\n        correctly rendered.\n        \"\"\"\n        project_dir = main.cookiecutter(\n            template,\n            no_input=True,\n            output_dir=output_dir,\n            extra_context={'project_slug': 'foobar', 'name': 'Cookiemonster'},\n        )\n    \n>       readme = Path(project_dir, 'README.rst').read_text(encoding=\"utf-8\")\n\ntests/test_custom_extensions_in_hooks.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'README.rst')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.00045517299999975336, "outcome": "passed"}}, {"nodeid": "tests/test_default_extensions.py::test_jinja2_time_extension", "lineno": 21, "outcome": "failed", "keywords": ["test_jinja2_time_extension", "test_default_extensions.py", "tests", "testbed", ""], "setup": {"duration": 1449703975.5029106, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 76, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_default_extensions.py", "lineno": 27, "message": ""}, {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 76, "message": "TypeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_jinja2_time_extension0')\n\n    def test_jinja2_time_extension(tmp_path):\n        \"\"\"Verify Jinja2 time extension work correctly.\"\"\"\n        project_dir = cookiecutter(\n            'tests/test-extensions/default/', no_input=True, output_dir=str(tmp_path)\n        )\n>       changelog_file = os.path.join(project_dir, 'HISTORY.rst')\n\ntests/test_default_extensions.py:27: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = None, p = ('HISTORY.rst',)\n\n    def join(a, *p):\n        \"\"\"Join two or more pathname components, inserting '/' as needed.\n        If any component is an absolute path, all previous path components\n        will be discarded.  An empty last part will result in a path that\n        ends with a separator.\"\"\"\n>       a = os.fspath(a)\nE       TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/posixpath.py:76: TypeError"}, "teardown": {"duration": -1449703975.44512, "outcome": "passed"}}, {"nodeid": "tests/test_default_extensions.py::test_jinja2_slugify_extension", "lineno": 44, "outcome": "failed", "keywords": ["test_jinja2_slugify_extension", "test_default_extensions.py", "tests", "testbed", ""], "setup": {"duration": 1449703975.4447951, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 142, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_default_extensions.py", "lineno": 51, "message": ""}, {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 142, "message": "TypeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_jinja2_slugify_extension0')\n\n    def test_jinja2_slugify_extension(tmp_path):\n        \"\"\"Verify Jinja2 slugify extension work correctly.\"\"\"\n        project_dir = cookiecutter(\n            'tests/test-extensions/default/', no_input=True, output_dir=str(tmp_path)\n        )\n    \n>       assert os.path.basename(project_dir) == \"it-s-slugified-foobar\"\n\ntests/test_default_extensions.py:51: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\np = None\n\n    def basename(p):\n        \"\"\"Returns the final component of a pathname\"\"\"\n>       p = os.fspath(p)\nE       TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/posixpath.py:142: TypeError"}, "teardown": {"duration": -1449703975.4269836, "outcome": "passed"}}, {"nodeid": "tests/test_default_extensions.py::test_jinja2_uuid_extension", "lineno": 53, "outcome": "failed", "keywords": ["test_jinja2_uuid_extension", "test_default_extensions.py", "tests", "testbed", ""], "setup": {"duration": 1449703975.4266684, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 76, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_default_extensions.py", "lineno": 59, "message": ""}, {"path": "/usr/lib/python3.10/posixpath.py", "lineno": 76, "message": "TypeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_jinja2_uuid_extension0')\n\n    def test_jinja2_uuid_extension(tmp_path):\n        \"\"\"Verify Jinja2 uuid extension work correctly.\"\"\"\n        project_dir = cookiecutter(\n            'tests/test-extensions/default/', no_input=True, output_dir=str(tmp_path)\n        )\n>       changelog_file = os.path.join(project_dir, 'id')\n\ntests/test_default_extensions.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = None, p = ('id',)\n\n    def join(a, *p):\n        \"\"\"Join two or more pathname components, inserting '/' as needed.\n        If any component is an absolute path, all previous path components\n        will be discarded.  An empty last part will result in a path that\n        ends with a separator.\"\"\"\n>       a = os.fspath(a)\nE       TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/posixpath.py:76: TypeError"}, "teardown": {"duration": -1449703975.3900766, "outcome": "passed"}}, {"nodeid": "tests/test_environment.py::test_env_should_raise_for_unknown_extension", "lineno": 8, "outcome": "failed", "keywords": ["test_env_should_raise_for_unknown_extension", "test_environment.py", "tests", "testbed", ""], "setup": {"duration": 0.0014694210000003594, "outcome": "passed"}, "call": {"duration": 0.00028001299999935725, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_environment.py", "lineno": 14, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "def test_env_should_raise_for_unknown_extension():\n        \"\"\"Test should raise if extension not installed in system.\"\"\"\n        context = {'cookiecutter': {'_extensions': ['foobar']}}\n    \n        with pytest.raises(UnknownExtension) as err:\n>           StrictEnvironment(context=context, keep_trailing_newline=True)\n\ntests/test_environment.py:14: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c6cb130>\nkwargs = {'keep_trailing_newline': True, 'undefined': <class 'jinja2.runtime.StrictUndefined'>}\ncontext = {'cookiecutter': {'_extensions': ['foobar']}}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003414780000001727, "outcome": "passed"}}, {"nodeid": "tests/test_environment.py::test_env_should_come_with_default_extensions", "lineno": 18, "outcome": "failed", "keywords": ["test_env_should_come_with_default_extensions", "test_environment.py", "tests", "testbed", ""], "setup": {"duration": 0.0014259230000002177, "outcome": "passed"}, "call": {"duration": 0.0002631330000006926, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_environment.py", "lineno": 21, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "def test_env_should_come_with_default_extensions():\n        \"\"\"Verify default extensions loaded with StrictEnvironment.\"\"\"\n>       env = StrictEnvironment(keep_trailing_newline=True)\n\ntests/test_environment.py:21: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3f22c0>\nkwargs = {'keep_trailing_newline': True, 'undefined': <class 'jinja2.runtime.StrictUndefined'>}\ncontext = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00034805899999934553, "outcome": "passed"}}, {"nodeid": "tests/test_exceptions.py::test_undefined_variable_to_str", "lineno": 7, "outcome": "passed", "keywords": ["test_undefined_variable_to_str", "test_exceptions.py", "tests", "testbed", ""], "setup": {"duration": 0.0013998050000001427, "outcome": "passed"}, "call": {"duration": 0.00025748000000014315, "outcome": "passed"}, "teardown": {"duration": 0.0003142719999997823, "outcome": "passed"}}, {"nodeid": "tests/test_find.py::test_find_template[template with default jinja strings]", "lineno": 24, "outcome": "failed", "keywords": ["test_find_template[template with default jinja strings]", "parametrize", "pytestmark", "template with default jinja strings", "test_find.py", "tests", "testbed", ""], "setup": {"duration": 0.0016507009999999767, "outcome": "passed"}, "call": {"duration": 0.00038858199999936005, "outcome": "failed", "crash": {"path": "/testbed/tests/test_find.py", "lineno": 72, "message": "AssertionError: assert None == PosixPath('tests/fake-repo-pre/{{cookiecutter.repo_name}}')"}, "traceback": [{"path": "tests/test_find.py", "lineno": 72, "message": "AssertionError"}], "longrepr": "repo_name = 'fake-repo-pre', env = None\nerror_expectation = <contextlib.nullcontext object at 0x7f2b4df600d0>\nexpected = '{{cookiecutter.repo_name}}'\n\n    @pytest.mark.parametrize(\n        \"repo_name,context,error_expectation,expected\",\n        [\n            (\"fake-repo-pre\", {}, does_not_raise(), '{{cookiecutter.repo_name}}'),\n            (\n                \"fake-repo-pre2\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                does_not_raise(),\n                '{%{cookiecutter.repo_name}%}',\n            ),\n            (\n                \"fake-repo-pre\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                pytest.raises(NonTemplatedInputDirException),\n                None,\n            ),\n            (\"fake-repo-bad\", {}, pytest.raises(NonTemplatedInputDirException), None),\n        ],\n        ids=[\n            'template with default jinja strings',\n            'template with custom jinja strings',\n            'template with custom jinja strings but folder with default jinja strings',\n            'template missing folder',\n        ],\n    )\n    def test_find_template(repo_name, env, error_expectation, expected):\n        \"\"\"Verify correctness of `find.find_template` path detection.\"\"\"\n        repo_dir = Path('tests', repo_name)\n    \n        with error_expectation:\n            template = find.find_template(repo_dir, env)\n    \n            test_dir = Path(repo_dir, expected)\n>           assert template == test_dir\nE           AssertionError: assert None == PosixPath('tests/fake-repo-pre/{{cookiecutter.repo_name}}')\n\ntests/test_find.py:72: AssertionError"}, "teardown": {"duration": 0.0004402810000003754, "outcome": "passed"}}, {"nodeid": "tests/test_find.py::test_find_template[template with custom jinja strings]", "lineno": 24, "outcome": "failed", "keywords": ["test_find_template[template with custom jinja strings]", "parametrize", "pytestmark", "template with custom jinja strings", "test_find.py", "tests", "testbed", ""], "setup": {"duration": 0.0016299840000000287, "outcome": "passed"}, "call": {"duration": 0.0003752219999997308, "outcome": "failed", "crash": {"path": "/testbed/tests/test_find.py", "lineno": 72, "message": "AssertionError: assert None == PosixPath('tests/fake-repo-pre2/{%{cookiecutter.repo_name}%}')"}, "traceback": [{"path": "tests/test_find.py", "lineno": 72, "message": "AssertionError"}], "longrepr": "repo_name = 'fake-repo-pre2', env = None\nerror_expectation = <contextlib.nullcontext object at 0x7f2b4df61ba0>\nexpected = '{%{cookiecutter.repo_name}%}'\n\n    @pytest.mark.parametrize(\n        \"repo_name,context,error_expectation,expected\",\n        [\n            (\"fake-repo-pre\", {}, does_not_raise(), '{{cookiecutter.repo_name}}'),\n            (\n                \"fake-repo-pre2\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                does_not_raise(),\n                '{%{cookiecutter.repo_name}%}',\n            ),\n            (\n                \"fake-repo-pre\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                pytest.raises(NonTemplatedInputDirException),\n                None,\n            ),\n            (\"fake-repo-bad\", {}, pytest.raises(NonTemplatedInputDirException), None),\n        ],\n        ids=[\n            'template with default jinja strings',\n            'template with custom jinja strings',\n            'template with custom jinja strings but folder with default jinja strings',\n            'template missing folder',\n        ],\n    )\n    def test_find_template(repo_name, env, error_expectation, expected):\n        \"\"\"Verify correctness of `find.find_template` path detection.\"\"\"\n        repo_dir = Path('tests', repo_name)\n    \n        with error_expectation:\n            template = find.find_template(repo_dir, env)\n    \n            test_dir = Path(repo_dir, expected)\n>           assert template == test_dir\nE           AssertionError: assert None == PosixPath('tests/fake-repo-pre2/{%{cookiecutter.repo_name}%}')\n\ntests/test_find.py:72: AssertionError"}, "teardown": {"duration": 0.0004256780000000404, "outcome": "passed"}}, {"nodeid": "tests/test_find.py::test_find_template[template with custom jinja strings but folder with default jinja strings]", "lineno": 24, "outcome": "failed", "keywords": ["test_find_template[template with custom jinja strings but folder with default jinja strings]", "parametrize", "pytestmark", "template with custom jinja strings but folder with default jinja strings", "test_find.py", "tests", "testbed", ""], "setup": {"duration": 0.0016055240000003579, "outcome": "passed"}, "call": {"duration": 0.00028225800000036827, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_find.py", "lineno": 71, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "repo_name = 'fake-repo-pre', env = None\nerror_expectation = <_pytest.python_api.RaisesContext object at 0x7f2b4df60100>\nexpected = None\n\n    @pytest.mark.parametrize(\n        \"repo_name,context,error_expectation,expected\",\n        [\n            (\"fake-repo-pre\", {}, does_not_raise(), '{{cookiecutter.repo_name}}'),\n            (\n                \"fake-repo-pre2\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                does_not_raise(),\n                '{%{cookiecutter.repo_name}%}',\n            ),\n            (\n                \"fake-repo-pre\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                pytest.raises(NonTemplatedInputDirException),\n                None,\n            ),\n            (\"fake-repo-bad\", {}, pytest.raises(NonTemplatedInputDirException), None),\n        ],\n        ids=[\n            'template with default jinja strings',\n            'template with custom jinja strings',\n            'template with custom jinja strings but folder with default jinja strings',\n            'template missing folder',\n        ],\n    )\n    def test_find_template(repo_name, env, error_expectation, expected):\n        \"\"\"Verify correctness of `find.find_template` path detection.\"\"\"\n        repo_dir = Path('tests', repo_name)\n    \n        with error_expectation:\n            template = find.find_template(repo_dir, env)\n    \n>           test_dir = Path(repo_dir, expected)\n\ntests/test_find.py:71: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>\nargs = (PosixPath('tests/fake-repo-pre'), None)\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.00043014999999968495, "outcome": "passed"}}, {"nodeid": "tests/test_find.py::test_find_template[template missing folder]", "lineno": 24, "outcome": "failed", "keywords": ["test_find_template[template missing folder]", "parametrize", "pytestmark", "template missing folder", "test_find.py", "tests", "testbed", ""], "setup": {"duration": 0.0016538390000002678, "outcome": "passed"}, "call": {"duration": 0.000290965000000476, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_find.py", "lineno": 71, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "repo_name = 'fake-repo-bad', env = None\nerror_expectation = <_pytest.python_api.RaisesContext object at 0x7f2b4df63af0>\nexpected = None\n\n    @pytest.mark.parametrize(\n        \"repo_name,context,error_expectation,expected\",\n        [\n            (\"fake-repo-pre\", {}, does_not_raise(), '{{cookiecutter.repo_name}}'),\n            (\n                \"fake-repo-pre2\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                does_not_raise(),\n                '{%{cookiecutter.repo_name}%}',\n            ),\n            (\n                \"fake-repo-pre\",\n                {\n                    'cookiecutter': {\n                        '_jinja2_env_vars': {\n                            'variable_start_string': '{%{',\n                            'variable_end_string': '}%}',\n                        }\n                    }\n                },\n                pytest.raises(NonTemplatedInputDirException),\n                None,\n            ),\n            (\"fake-repo-bad\", {}, pytest.raises(NonTemplatedInputDirException), None),\n        ],\n        ids=[\n            'template with default jinja strings',\n            'template with custom jinja strings',\n            'template with custom jinja strings but folder with default jinja strings',\n            'template missing folder',\n        ],\n    )\n    def test_find_template(repo_name, env, error_expectation, expected):\n        \"\"\"Verify correctness of `find.find_template` path detection.\"\"\"\n        repo_dir = Path('tests', repo_name)\n    \n        with error_expectation:\n            template = find.find_template(repo_dir, env)\n    \n>           test_dir = Path(repo_dir, expected)\n\ntests/test_find.py:71: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>\nargs = (PosixPath('tests/fake-repo-bad'), None)\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.000548492000000067, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params0-expected_context0]", "lineno": 53, "outcome": "failed", "keywords": ["test_generate_context[input_params0-expected_context0]", "parametrize", "usefixtures", "pytestmark", "input_params0-expected_context0", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0019391760000004865, "outcome": "passed"}, "call": {"duration": 0.00048632400000059306, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 58, "message": "AssertionError: assert None == {'test': {'1': 2, 'some_key': 'some_val'}}\n +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json'})\n +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 58, "message": "AssertionError"}], "longrepr": "input_params = {'context_file': 'tests/test-generate-context/test.json'}\nexpected_context = {'test': {'1': 2, 'some_key': 'some_val'}}\n\n    @pytest.mark.usefixtures('clean_system')\n    @pytest.mark.parametrize('input_params, expected_context', context_data())\n    def test_generate_context(input_params, expected_context):\n        \"\"\"Verify input contexts combinations result in expected content on output.\"\"\"\n>       assert generate.generate_context(**input_params) == expected_context\nE       AssertionError: assert None == {'test': {'1': 2, 'some_key': 'some_val'}}\nE        +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json'})\nE        +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context\n\ntests/test_generate_context.py:58: AssertionError"}, "teardown": {"duration": 0.0004818390000007611, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params1-expected_context1]", "lineno": 53, "outcome": "failed", "keywords": ["test_generate_context[input_params1-expected_context1]", "parametrize", "usefixtures", "pytestmark", "input_params1-expected_context1", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0017326810000000137, "outcome": "passed"}, "call": {"duration": 0.0004931180000005142, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 58, "message": "AssertionError: assert None == {'test': {'1': 3, 'some_key': 'some_val'}}\n +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}})\n +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 58, "message": "AssertionError"}], "longrepr": "input_params = {'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}}\nexpected_context = {'test': {'1': 3, 'some_key': 'some_val'}}\n\n    @pytest.mark.usefixtures('clean_system')\n    @pytest.mark.parametrize('input_params, expected_context', context_data())\n    def test_generate_context(input_params, expected_context):\n        \"\"\"Verify input contexts combinations result in expected content on output.\"\"\"\n>       assert generate.generate_context(**input_params) == expected_context\nE       AssertionError: assert None == {'test': {'1': 3, 'some_key': 'some_val'}}\nE        +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}})\nE        +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context\n\ntests/test_generate_context.py:58: AssertionError"}, "teardown": {"duration": 0.0004634769999993793, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params2-expected_context2]", "lineno": 53, "outcome": "failed", "keywords": ["test_generate_context[input_params2-expected_context2]", "parametrize", "usefixtures", "pytestmark", "input_params2-expected_context2", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.001681284000000005, "outcome": "passed"}, "call": {"duration": 0.00047043699999971267, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 58, "message": "AssertionError: assert None == {'test': {'1': 4, 'some_key': 'some_val'}}\n +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'extra_context': {'1': 4}})\n +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 58, "message": "AssertionError"}], "longrepr": "input_params = {'context_file': 'tests/test-generate-context/test.json', 'extra_context': {'1': 4}}\nexpected_context = {'test': {'1': 4, 'some_key': 'some_val'}}\n\n    @pytest.mark.usefixtures('clean_system')\n    @pytest.mark.parametrize('input_params, expected_context', context_data())\n    def test_generate_context(input_params, expected_context):\n        \"\"\"Verify input contexts combinations result in expected content on output.\"\"\"\n>       assert generate.generate_context(**input_params) == expected_context\nE       AssertionError: assert None == {'test': {'1': 4, 'some_key': 'some_val'}}\nE        +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'extra_context': {'1': 4}})\nE        +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context\n\ntests/test_generate_context.py:58: AssertionError"}, "teardown": {"duration": 0.00047507100000032665, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context[input_params3-expected_context3]", "lineno": 53, "outcome": "failed", "keywords": ["test_generate_context[input_params3-expected_context3]", "parametrize", "usefixtures", "pytestmark", "input_params3-expected_context3", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0016844110000002743, "outcome": "passed"}, "call": {"duration": 0.0004819480000000098, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 58, "message": "AssertionError: assert None == {'test': {'1': 5, 'some_key': 'some_val'}}\n +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}, 'extra_context': {'1': 5}})\n +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 58, "message": "AssertionError"}], "longrepr": "input_params = {'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}, 'extra_context': {'1': 5}}\nexpected_context = {'test': {'1': 5, 'some_key': 'some_val'}}\n\n    @pytest.mark.usefixtures('clean_system')\n    @pytest.mark.parametrize('input_params, expected_context', context_data())\n    def test_generate_context(input_params, expected_context):\n        \"\"\"Verify input contexts combinations result in expected content on output.\"\"\"\n>       assert generate.generate_context(**input_params) == expected_context\nE       AssertionError: assert None == {'test': {'1': 5, 'some_key': 'some_val'}}\nE        +  where None = <function generate_context at 0x7f2b4cfc7250>(**{'context_file': 'tests/test-generate-context/test.json', 'default_context': {'1': 3}, 'extra_context': {'1': 5}})\nE        +    where <function generate_context at 0x7f2b4cfc7250> = generate.generate_context\n\ntests/test_generate_context.py:58: AssertionError"}, "teardown": {"duration": 0.000452923000000105, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context_with_json_decoding_error", "lineno": 60, "outcome": "failed", "keywords": ["test_generate_context_with_json_decoding_error", "usefixtures", "pytestmark", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0015574779999996125, "outcome": "passed"}, "call": {"duration": 0.0002844030000002107, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 64, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.ContextDecodingException'>"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 64, "message": "Failed"}], "longrepr": "@pytest.mark.usefixtures('clean_system')\n    def test_generate_context_with_json_decoding_error():\n        \"\"\"Verify malformed JSON file generates expected error output.\"\"\"\n>       with pytest.raises(ContextDecodingException) as excinfo:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.ContextDecodingException'>\n\ntests/test_generate_context.py:64: Failed"}, "teardown": {"duration": 0.0004320180000005891, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_default_context_replacement_in_generate_context", "lineno": 76, "outcome": "failed", "keywords": ["test_default_context_replacement_in_generate_context", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0013960989999999285, "outcome": "passed"}, "call": {"duration": 0.00039441100000026097, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 109, "message": "AssertionError: assert None == {'choices_template': OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['landscape', 'all', 'portrait'])])}"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 109, "message": "AssertionError"}], "longrepr": "def test_default_context_replacement_in_generate_context():\n        \"\"\"Verify default content settings are correctly replaced by template settings.\n    \n        Make sure that the default for list variables of `orientation` is based on\n        the user config (`choices_template.json`) and not changed to a single value\n        from `default_context`.\n        \"\"\"\n        expected_context = {\n            'choices_template': OrderedDict(\n                [\n                    ('full_name', 'Raphael Pierzina'),\n                    ('github_username', 'hackebrot'),\n                    ('project_name', 'Kivy Project'),\n                    ('repo_name', '{{cookiecutter.project_name|lower}}'),\n                    ('orientation', ['landscape', 'all', 'portrait']),\n                ]\n            )\n        }\n    \n        generated_context = generate.generate_context(\n            context_file='tests/test-generate-context/choices_template.json',\n            default_context={\n                'not_in_template': 'foobar',\n                'project_name': 'Kivy Project',\n                'orientation': 'landscape',\n            },\n            extra_context={\n                'also_not_in_template': 'foobar2',\n                'github_username': 'hackebrot',\n            },\n        )\n    \n>       assert generated_context == expected_context\nE       AssertionError: assert None == {'choices_template': OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['landscape', 'all', 'portrait'])])}\n\ntests/test_generate_context.py:109: AssertionError"}, "teardown": {"duration": 0.00037863900000001394, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_generate_context_decodes_non_ascii_chars", "lineno": 111, "outcome": "failed", "keywords": ["test_generate_context_decodes_non_ascii_chars", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.001490894999999881, "outcome": "passed"}, "call": {"duration": 0.0003978189999997994, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 126, "message": "AssertionError: assert None == {'non_ascii': OrderedDict([('full_name', '\u00e9\u00e8\u00e0')])}"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 126, "message": "AssertionError"}], "longrepr": "def test_generate_context_decodes_non_ascii_chars():\n        \"\"\"Verify `generate_context` correctly decodes non-ascii chars.\"\"\"\n        expected_context = {\n            'non_ascii': OrderedDict(\n                [\n                    ('full_name', '\u00e9\u00e8\u00e0'),\n                ]\n            )\n        }\n    \n        generated_context = generate.generate_context(\n            context_file='tests/test-generate-context/non_ascii.json'\n        )\n    \n>       assert generated_context == expected_context\nE       AssertionError: assert None == {'non_ascii': OrderedDict([('full_name', '\u00e9\u00e8\u00e0')])}\n\ntests/test_generate_context.py:126: AssertionError"}, "teardown": {"duration": 0.00037296699999966876, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_does_include_unused_variables", "lineno": 150, "outcome": "passed", "keywords": ["test_apply_overwrites_does_include_unused_variables", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0015425919999998428, "outcome": "passed"}, "call": {"duration": 0.00029006000000020293, "outcome": "passed"}, "teardown": {"duration": 0.0003574229999996348, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_non_list_value", "lineno": 159, "outcome": "failed", "keywords": ["test_apply_overwrites_sets_non_list_value", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.001517230000000147, "outcome": "passed"}, "call": {"duration": 0.0005925769999999275, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 166, "message": "AssertionError: assert '{{cookiecutter.project_name|lower}}' == 'foobar'\n  \n  - foobar\n  + {{cookiecutter.project_name|lower}}"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 166, "message": "AssertionError"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_sets_non_list_value(template_context):\n        \"\"\"Verify `apply_overwrites_to_context` work with string variables.\"\"\"\n        generate.apply_overwrites_to_context(\n            context=template_context, overwrite_context={'repo_name': 'foobar'}\n        )\n    \n>       assert template_context['repo_name'] == 'foobar'\nE       AssertionError: assert '{{cookiecutter.project_name|lower}}' == 'foobar'\nE         \nE         - foobar\nE         + {{cookiecutter.project_name|lower}}\n\ntests/test_generate_context.py:166: AssertionError"}, "teardown": {"duration": 0.0003552849999994834, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_does_not_modify_choices_for_invalid_overwrite", "lineno": 168, "outcome": "failed", "keywords": ["test_apply_overwrites_does_not_modify_choices_for_invalid_overwrite", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014065800000002682, "outcome": "passed"}, "call": {"duration": 0.0004885990000005336, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 183, "message": "Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.\n Emitted warnings: []."}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 183, "message": "Failed"}], "longrepr": "def test_apply_overwrites_does_not_modify_choices_for_invalid_overwrite():\n        \"\"\"Verify variables overwrite for list if variable not in list ignored.\"\"\"\n        expected_context = {\n            'choices_template': OrderedDict(\n                [\n                    ('full_name', 'Raphael Pierzina'),\n                    ('github_username', 'hackebrot'),\n                    ('project_name', 'Kivy Project'),\n                    ('repo_name', '{{cookiecutter.project_name|lower}}'),\n                    ('orientation', ['all', 'landscape', 'portrait']),\n                ]\n            )\n        }\n    \n>       with pytest.warns(UserWarning, match=\"Invalid default received\"):\nE       Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.\nE        Emitted warnings: [].\n\ntests/test_generate_context.py:183: Failed"}, "teardown": {"duration": 0.00033965500000032733, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_invalid_overwrite", "lineno": 199, "outcome": "failed", "keywords": ["test_apply_overwrites_invalid_overwrite", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014605190000001045, "outcome": "passed"}, "call": {"duration": 0.00028229499999987695, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 202, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 202, "message": "Failed"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_invalid_overwrite(template_context):\n        \"\"\"Verify variables overwrite for list if variable not in list not ignored.\"\"\"\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_generate_context.py:202: Failed"}, "teardown": {"duration": 0.00035077499999935924, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_multichoice_values", "lineno": 207, "outcome": "failed", "keywords": ["test_apply_overwrites_sets_multichoice_values", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014745400000002462, "outcome": "passed"}, "call": {"duration": 0.0006408579999996888, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 214, "message": "AssertionError: assert ['eu', 'us', 'ap'] == ['eu']\n  \n  Left contains 2 more items, first extra item: 'us'\n  \n  Full diff:\n    [\n        'eu',\n  +     'us',\n  +     'ap',\n    ]"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 214, "message": "AssertionError"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_sets_multichoice_values(template_context):\n        \"\"\"Verify variable overwrite for list given multiple valid values.\"\"\"\n        generate.apply_overwrites_to_context(\n            context=template_context,\n            overwrite_context={'deployment_regions': ['eu']},\n        )\n>       assert template_context['deployment_regions'] == ['eu']\nE       AssertionError: assert ['eu', 'us', 'ap'] == ['eu']\nE         \nE         Left contains 2 more items, first extra item: 'us'\nE         \nE         Full diff:\nE           [\nE               'eu',\nE         +     'us',\nE         +     'ap',\nE           ]\n\ntests/test_generate_context.py:214: AssertionError"}, "teardown": {"duration": 0.00036461199999937577, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_invalid_multichoice_values", "lineno": 216, "outcome": "failed", "keywords": ["test_apply_overwrites_invalid_multichoice_values", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0015129510000004842, "outcome": "passed"}, "call": {"duration": 0.00026747300000007357, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 219, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 219, "message": "Failed"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_invalid_multichoice_values(template_context):\n        \"\"\"Verify variable overwrite for list given invalid list entries not ignored.\"\"\"\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_generate_context.py:219: Failed"}, "teardown": {"duration": 0.00035301199999970834, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_error_additional_values", "lineno": 225, "outcome": "failed", "keywords": ["test_apply_overwrites_error_additional_values", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014891629999995715, "outcome": "passed"}, "call": {"duration": 0.0002669429999997419, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 228, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 228, "message": "Failed"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_error_additional_values(template_context):\n        \"\"\"Verify variable overwrite for list given additional entries not ignored.\"\"\"\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_generate_context.py:228: Failed"}, "teardown": {"duration": 0.000350525999999185, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_dictionaries", "lineno": 234, "outcome": "failed", "keywords": ["test_apply_overwrites_in_dictionaries", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.001501812999999963, "outcome": "passed"}, "call": {"duration": 0.0005132830000000865, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 241, "message": "AssertionError: assert ['eu', 'us', 'ap'] == ['eu']\n  \n  Left contains 2 more items, first extra item: 'us'\n  \n  Full diff:\n    [\n        'eu',\n  +     'us',\n  +     'ap',\n    ]"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 241, "message": "AssertionError"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_in_dictionaries(template_context):\n        \"\"\"Verify variable overwrite for lists nested in dictionary variables.\"\"\"\n        generate.apply_overwrites_to_context(\n            context=template_context,\n            overwrite_context={'deployments': {'preprod': ['eu'], 'prod': ['ap']}},\n        )\n>       assert template_context['deployments']['preprod'] == ['eu']\nE       AssertionError: assert ['eu', 'us', 'ap'] == ['eu']\nE         \nE         Left contains 2 more items, first extra item: 'us'\nE         \nE         Full diff:\nE           [\nE               'eu',\nE         +     'us',\nE         +     'ap',\nE           ]\n\ntests/test_generate_context.py:241: AssertionError"}, "teardown": {"duration": 0.0003494479999996969, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_sets_default_for_choice_variable", "lineno": 244, "outcome": "failed", "keywords": ["test_apply_overwrites_sets_default_for_choice_variable", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.001504626000000009, "outcome": "passed"}, "call": {"duration": 0.0005037290000000638, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 251, "message": "AssertionError: assert ['all', 'landscape', 'portrait'] == ['landscape', 'all', 'portrait']\n  \n  At index 0 diff: 'all' != 'landscape'\n  \n  Full diff:\n    [\n  +     'all',\n        'landscape',\n  -     'all',\n        'portrait',\n    ]"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 251, "message": "AssertionError"}], "longrepr": "template_context = OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project_name', 'Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait']), ('deployment_regions', ['eu', 'us', 'ap']), ('deployments', {'preprod': ['eu', 'us', 'ap'], 'prod': ['eu', 'us', 'ap']})])\n\n    def test_apply_overwrites_sets_default_for_choice_variable(template_context):\n        \"\"\"Verify overwritten list member became a default value.\"\"\"\n        generate.apply_overwrites_to_context(\n            context=template_context, overwrite_context={'orientation': 'landscape'}\n        )\n    \n>       assert template_context['orientation'] == ['landscape', 'all', 'portrait']\nE       AssertionError: assert ['all', 'landscape', 'portrait'] == ['landscape', 'all', 'portrait']\nE         \nE         At index 0 diff: 'all' != 'landscape'\nE         \nE         Full diff:\nE           [\nE         +     'all',\nE               'landscape',\nE         -     'all',\nE               'portrait',\nE           ]\n\ntests/test_generate_context.py:251: AssertionError"}, "teardown": {"duration": 0.0003610809999994302, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_nested_dict", "lineno": 253, "outcome": "failed", "keywords": ["test_apply_overwrites_in_nested_dict", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014539520000003137, "outcome": "passed"}, "call": {"duration": 0.0003795730000000219, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 293, "message": "AssertionError: assert None == {'nested_dict': OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project', OrderedDict([('name', 'My Kivy Project'), ('description', 'My Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait'])]))])}"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 293, "message": "AssertionError"}], "longrepr": "def test_apply_overwrites_in_nested_dict():\n        \"\"\"Verify nested dict in default content settings are correctly replaced.\"\"\"\n        expected_context = {\n            'nested_dict': OrderedDict(\n                [\n                    ('full_name', 'Raphael Pierzina'),\n                    ('github_username', 'hackebrot'),\n                    (\n                        'project',\n                        OrderedDict(\n                            [\n                                ('name', 'My Kivy Project'),\n                                ('description', 'My Kivy Project'),\n                                ('repo_name', '{{cookiecutter.project_name|lower}}'),\n                                ('orientation', [\"all\", \"landscape\", \"portrait\"]),\n                            ]\n                        ),\n                    ),\n                ]\n            )\n        }\n    \n        generated_context = generate.generate_context(\n            context_file='tests/test-generate-context/nested_dict.json',\n            default_context={\n                'not_in_template': 'foobar',\n                'project': {\n                    'description': 'My Kivy Project',\n                },\n            },\n            extra_context={\n                'also_not_in_template': 'foobar2',\n                'github_username': 'hackebrot',\n                'project': {\n                    'name': 'My Kivy Project',\n                },\n            },\n        )\n    \n>       assert generated_context == expected_context\nE       AssertionError: assert None == {'nested_dict': OrderedDict([('full_name', 'Raphael Pierzina'), ('github_username', 'hackebrot'), ('project', OrderedDict([('name', 'My Kivy Project'), ('description', 'My Kivy Project'), ('repo_name', '{{cookiecutter.project_name|lower}}'), ('orientation', ['all', 'landscape', 'portrait'])]))])}\n\ntests/test_generate_context.py:293: AssertionError"}, "teardown": {"duration": 0.00036591500000060506, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrite_context_as_in_nested_dict_with_additional_values", "lineno": 295, "outcome": "failed", "keywords": ["test_apply_overwrite_context_as_in_nested_dict_with_additional_values", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014205890000003052, "outcome": "passed"}, "call": {"duration": 0.000606845000000078, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 309, "message": "AssertionError: assert OrderedDict([('key1', 'value1')]) == OrderedDict([('key1', 'value1'), ('key2', 'value2')])\n  \n  Common items:\n  {'key1': 'value1'}\n  Right contains 1 more item:\n  {'key2': 'value2'}\n  \n  Full diff:\n    OrderedDict({\n        'key1': 'value1',\n  -     'key2': 'value2',\n    })"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 309, "message": "AssertionError"}], "longrepr": "def test_apply_overwrite_context_as_in_nested_dict_with_additional_values():\n        \"\"\"Verify nested dict in default content settings are correctly added.\n    \n        The `apply_overwrites_to_context` function should add the extra values to the dict.\n        \"\"\"\n        expected = OrderedDict({\"key1\": \"value1\", \"key2\": \"value2\"})\n        context = OrderedDict({\"key1\": \"value1\"})\n        overwrite_context = OrderedDict({\"key2\": \"value2\"})\n        generate.apply_overwrites_to_context(\n            context,\n            overwrite_context,\n            in_dictionary_variable=True,\n        )\n>       assert context == expected\nE       AssertionError: assert OrderedDict([('key1', 'value1')]) == OrderedDict([('key1', 'value1'), ('key2', 'value2')])\nE         \nE         Common items:\nE         {'key1': 'value1'}\nE         Right contains 1 more item:\nE         {'key2': 'value2'}\nE         \nE         Full diff:\nE           OrderedDict({\nE               'key1': 'value1',\nE         -     'key2': 'value2',\nE           })\n\ntests/test_generate_context.py:309: AssertionError"}, "teardown": {"duration": 0.00034510800000031594, "outcome": "passed"}}, {"nodeid": "tests/test_generate_context.py::test_apply_overwrites_in_nested_dict_additional_values", "lineno": 311, "outcome": "failed", "keywords": ["test_apply_overwrites_in_nested_dict_additional_values", "test_generate_context.py", "tests", "testbed", ""], "setup": {"duration": 0.0014216359999998929, "outcome": "passed"}, "call": {"duration": 0.00039920000000037703, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_context.py", "lineno": 364, "message": "AssertionError: assert None == {'nested_dict_additional': OrderedDict([('mainkey1', 'mainvalue1'), ('mainkey2', OrderedDict([('subkey1', 'subvalue1'), ('subkey2', OrderedDict([('subsubkey1', 'subsubvalue1'), ('subsubkey2', 'subsubvalue2_default'), ('subsubkey3', 'subsubvalue3_extra')])), ('subkey4', 'subvalue4_default'), ('subkey5', 'subvalue5_extra')]))])}"}, "traceback": [{"path": "tests/test_generate_context.py", "lineno": 364, "message": "AssertionError"}], "longrepr": "def test_apply_overwrites_in_nested_dict_additional_values():\n        \"\"\"Verify nested dict in default content settings are correctly added.\"\"\"\n        expected_context = {\n            'nested_dict_additional': OrderedDict(\n                [\n                    ('mainkey1', 'mainvalue1'),\n                    (\n                        'mainkey2',\n                        OrderedDict(\n                            [\n                                ('subkey1', 'subvalue1'),\n                                (\n                                    'subkey2',\n                                    OrderedDict(\n                                        [\n                                            ('subsubkey1', 'subsubvalue1'),\n                                            ('subsubkey2', 'subsubvalue2_default'),\n                                            ('subsubkey3', 'subsubvalue3_extra'),\n                                        ]\n                                    ),\n                                ),\n                                ('subkey4', 'subvalue4_default'),\n                                ('subkey5', 'subvalue5_extra'),\n                            ]\n                        ),\n                    ),\n                ]\n            )\n        }\n    \n        generated_context = generate.generate_context(\n            context_file='tests/test-generate-context/nested_dict_additional.json',\n            default_context={\n                'not_in_template': 'foobar',\n                'mainkey2': {\n                    'subkey2': {\n                        'subsubkey2': 'subsubvalue2_default',\n                    },\n                    'subkey4': 'subvalue4_default',\n                },\n            },\n            extra_context={\n                'also_not_in_template': 'foobar2',\n                'mainkey2': {\n                    'subkey2': {\n                        'subsubkey3': 'subsubvalue3_extra',\n                    },\n                    'subkey5': 'subvalue5_extra',\n                },\n            },\n        )\n    \n>       assert generated_context == expected_context\nE       AssertionError: assert None == {'nested_dict_additional': OrderedDict([('mainkey1', 'mainvalue1'), ('mainkey2', OrderedDict([('subkey1', 'subvalue1'), ('subkey2', OrderedDict([('subsubkey1', 'subsubvalue1'), ('subsubkey2', 'subsubvalue2_default'), ('subsubkey3', 'subsubvalue3_extra')])), ('subkey4', 'subvalue4_default'), ('subkey5', 'subvalue5_extra')]))])}\n\ntests/test_generate_context.py:364: AssertionError"}, "teardown": {"duration": 0.00033717699999957773, "outcome": "passed"}}, {"nodeid": "tests/test_generate_copy_without_render.py::test_generate_copy_without_render_extensions", "lineno": 18, "outcome": "failed", "keywords": ["test_generate_copy_without_render_extensions", "usefixtures", "pytestmark", "test_generate_copy_without_render.py", "tests", "testbed", ""], "setup": {"duration": 0.0016327509999998213, "outcome": "passed"}, "call": {"duration": 0.000287258000000179, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_copy_without_render.py", "lineno": 42, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'test_copy_without_render'"}, "traceback": [{"path": "tests/test_generate_copy_without_render.py", "lineno": 42, "message": "FileNotFoundError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_test_dir')\n    def test_generate_copy_without_render_extensions():\n        \"\"\"Verify correct work of `_copy_without_render` context option.\n    \n        Some files/directories should be rendered during invocation,\n        some just copied, without any modification.\n        \"\"\"\n        generate.generate_files(\n            context={\n                'cookiecutter': {\n                    'repo_name': 'test_copy_without_render',\n                    'render_test': 'I have been rendered!',\n                    '_copy_without_render': [\n                        '*not-rendered',\n                        'rendered/not_rendered.yml',\n                        '*.txt',\n                        '{{cookiecutter.repo_name}}-rendered/README.md',\n                    ],\n                }\n            },\n            repo_dir='tests/test-generate-copy-without-render',\n        )\n    \n>       dir_contents = os.listdir('test_copy_without_render')\nE       FileNotFoundError: [Errno 2] No such file or directory: 'test_copy_without_render'\n\ntests/test_generate_copy_without_render.py:42: FileNotFoundError"}, "teardown": {"duration": 0.00046106700000070333, "outcome": "passed"}}, {"nodeid": "tests/test_generate_copy_without_render_override.py::test_generate_copy_without_render_extensions", "lineno": 18, "outcome": "failed", "keywords": ["test_generate_copy_without_render_extensions", "usefixtures", "pytestmark", "test_generate_copy_without_render_override.py", "tests", "testbed", ""], "setup": {"duration": 0.00162107100000064, "outcome": "passed"}, "call": {"duration": 0.00026684199999937874, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_copy_without_render_override.py", "lineno": 61, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'test_copy_without_render'"}, "traceback": [{"path": "tests/test_generate_copy_without_render_override.py", "lineno": 61, "message": "FileNotFoundError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_test_dir')\n    def test_generate_copy_without_render_extensions():\n        \"\"\"Verify correct work of `_copy_without_render` context option.\n    \n        Some files/directories should be rendered during invocation,\n        some just copied, without any modification.\n        \"\"\"\n        # first run\n        generate.generate_files(\n            context={\n                'cookiecutter': {\n                    'repo_name': 'test_copy_without_render',\n                    'render_test': 'I have been rendered!',\n                    '_copy_without_render': [\n                        '*not-rendered',\n                        'rendered/not_rendered.yml',\n                        '*.txt',\n                        '{{cookiecutter.repo_name}}-rendered/README.md',\n                    ],\n                }\n            },\n            repo_dir='tests/test-generate-copy-without-render-override',\n        )\n    \n        # second run with override flag to True\n        generate.generate_files(\n            context={\n                'cookiecutter': {\n                    'repo_name': 'test_copy_without_render',\n                    'render_test': 'I have been rendered!',\n                    '_copy_without_render': [\n                        '*not-rendered',\n                        'rendered/not_rendered.yml',\n                        '*.txt',\n                        '{{cookiecutter.repo_name}}-rendered/README.md',\n                    ],\n                }\n            },\n            overwrite_if_exists=True,\n            repo_dir='tests/test-generate-copy-without-render',\n        )\n    \n>       dir_contents = os.listdir('test_copy_without_render')\nE       FileNotFoundError: [Errno 2] No such file or directory: 'test_copy_without_render'\n\ntests/test_generate_copy_without_render_override.py:61: FileNotFoundError"}, "teardown": {"duration": 0.0004916289999998824, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file", "lineno": 43, "outcome": "error", "keywords": ["test_generate_file", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.001564212999999981, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3ddb10>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0006142450000004018, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_jsonify_filter", "lineno": 57, "outcome": "error", "keywords": ["test_generate_file_jsonify_filter", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015946989999999772, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c19ef50>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003970589999999774, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[True-10]", "lineno": 69, "outcome": "error", "keywords": ["test_generate_file_random_ascii_string[True-10]", "parametrize", "pytestmark", "True-10", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015620680000001386, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3db100>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004159960000000851, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[True-40]", "lineno": 69, "outcome": "error", "keywords": ["test_generate_file_random_ascii_string[True-40]", "parametrize", "pytestmark", "True-40", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015597920000001153, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c19f820>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003971700000002798, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[False-10]", "lineno": 69, "outcome": "error", "keywords": ["test_generate_file_random_ascii_string[False-10]", "parametrize", "pytestmark", "False-10", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015781859999997039, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3d9d50>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003975469999994985, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_random_ascii_string[False-40]", "lineno": 69, "outcome": "error", "keywords": ["test_generate_file_random_ascii_string[False-40]", "parametrize", "pytestmark", "False-40", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0016135530000003229, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c191d20>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00041086799999945356, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_with_true_condition", "lineno": 82, "outcome": "error", "keywords": ["test_generate_file_with_true_condition", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015622989999997117, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3da770>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00039917400000000214, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_with_false_condition", "lineno": 101, "outcome": "error", "keywords": ["test_generate_file_with_false_condition", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0016018020000005961, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c8925c0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004062660000005991, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_verbose_template_syntax_error", "lineno": 128, "outcome": "error", "keywords": ["test_generate_file_verbose_template_syntax_error", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0016121849999999327, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3e7940>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0003969880000003201, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_does_not_translate_lf_newlines_to_crlf", "lineno": 140, "outcome": "error", "keywords": ["test_generate_file_does_not_translate_lf_newlines_to_crlf", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015869150000007437, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c19dab0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00040513699999955577, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_does_not_translate_crlf_newlines_to_lf", "lineno": 158, "outcome": "error", "keywords": ["test_generate_file_does_not_translate_crlf_newlines_to_lf", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015939669999998074, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3e60e0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00039561600000048713, "outcome": "passed"}}, {"nodeid": "tests/test_generate_file.py::test_generate_file_handles_mixed_line_endings", "lineno": 176, "outcome": "error", "keywords": ["test_generate_file_handles_mixed_line_endings", "test_generate_file.py", "tests", "testbed", ""], "setup": {"duration": 0.0015659530000000643, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_generate_file.py", "lineno": 39, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "@pytest.fixture\n    def env():\n        \"\"\"Fixture. Set Jinja2 environment settings for other tests.\"\"\"\n>       environment = StrictEnvironment()\n\ntests/test_generate_file.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c190610>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00040136199999984967, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_nontemplated_exception", "lineno": 14, "outcome": "failed", "keywords": ["test_generate_files_nontemplated_exception", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.001476907999999888, "outcome": "passed"}, "call": {"duration": 0.0002688959999996854, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 21, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.NonTemplatedInputDirException'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 21, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_nontemplat0')\n\n    def test_generate_files_nontemplated_exception(tmp_path):\n        \"\"\"\n        Verify `generate_files` raises when no directories to render exist.\n    \n        Note: Check `tests/test-generate-files-nontemplated` location to understand.\n        \"\"\"\n>       with pytest.raises(exceptions.NonTemplatedInputDirException):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.NonTemplatedInputDirException'>\n\ntests/test_generate_files.py:21: Failed"}, "teardown": {"duration": 0.0003350409999995918, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files", "lineno": 28, "outcome": "failed", "keywords": ["test_generate_files", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0014892130000001558, "outcome": "passed"}, "call": {"duration": 0.0003901050000001405, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 38, "message": "AssertionError: assert False\n +  where False = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files0/inputpizz\u00e4/simple.txt').exists"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 38, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files0')\n\n    def test_generate_files(tmp_path):\n        \"\"\"Verify directory name correctly rendered with unicode containing context.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        simple_file = Path(tmp_path, 'inputpizz\u00e4/simple.txt')\n>       assert simple_file.exists()\nE       AssertionError: assert False\nE        +  where False = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files0/inputpizz\u00e4/simple.txt').exists\n\ntests/test_generate_files.py:38: AssertionError"}, "teardown": {"duration": 0.0003418020000003352, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_linux_newline", "lineno": 44, "outcome": "failed", "keywords": ["test_generate_files_with_linux_newline", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0014766279999998133, "outcome": "passed"}, "call": {"duration": 0.00038428999999950975, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 54, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_linux0/inputpizz\u00e4/simple-with-newline.txt').is_file"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 54, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_linux0')\n\n    def test_generate_files_with_linux_newline(tmp_path):\n        \"\"\"Verify new line not removed by templating engine after folder generation.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        newline_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline.txt')\n>       assert newline_file.is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_linux0/inputpizz\u00e4/simple-with-newline.txt').is_file\n\ntests/test_generate_files.py:54: AssertionError"}, "teardown": {"duration": 0.0003552630000003276, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_jinja2_environment", "lineno": 62, "outcome": "failed", "keywords": ["test_generate_files_with_jinja2_environment", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0018087769999999281, "outcome": "passed"}, "call": {"duration": 0.0003840459999997492, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 77, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_jinja0/inputpizz\u00e4/simple-with-conditions.txt').is_file"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 77, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_jinja0')\n\n    def test_generate_files_with_jinja2_environment(tmp_path):\n        \"\"\"Extend StrictEnvironment with _jinja2_env_vars cookiecutter template option.\"\"\"\n        generate.generate_files(\n            context={\n                'cookiecutter': {\n                    'food': 'pizz\u00e4',\n                    '_jinja2_env_vars': {'lstrip_blocks': True, 'trim_blocks': True},\n                }\n            },\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        conditions_file = tmp_path.joinpath('inputpizz\u00e4/simple-with-conditions.txt')\n>       assert conditions_file.is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_jinja0/inputpizz\u00e4/simple-with-conditions.txt').is_file\n\ntests/test_generate_files.py:77: AssertionError"}, "teardown": {"duration": 0.0003408639999999963, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_trailing_newline_forced_to_linux_by_context", "lineno": 83, "outcome": "failed", "keywords": ["test_generate_files_with_trailing_newline_forced_to_linux_by_context", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0014679910000001684, "outcome": "passed"}, "call": {"duration": 0.00038819999999972765, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 94, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_trail0/inputpizz\u00e4/simple-with-newline.txt').is_file"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 94, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_trail0')\n\n    def test_generate_files_with_trailing_newline_forced_to_linux_by_context(tmp_path):\n        \"\"\"Verify new line not removed by templating engine after folder generation.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4', '_new_lines': '\\r\\n'}},\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        # assert 'Overwritting endline character with %s' in caplog.messages\n        newline_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline.txt')\n>       assert newline_file.is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_trail0/inputpizz\u00e4/simple-with-newline.txt').is_file\n\ntests/test_generate_files.py:94: AssertionError"}, "teardown": {"duration": 0.00038571800000042344, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_windows_newline", "lineno": 102, "outcome": "failed", "keywords": ["test_generate_files_with_windows_newline", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0014868930000000447, "outcome": "passed"}, "call": {"duration": 0.00038904999999989087, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 112, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo0/inputpizz\u00e4/simple-with-newline-crlf.txt').is_file"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 112, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo0')\n\n    def test_generate_files_with_windows_newline(tmp_path):\n        \"\"\"Verify windows source line end not changed during files generation.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        newline_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline-crlf.txt')\n>       assert newline_file.is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo0/inputpizz\u00e4/simple-with-newline-crlf.txt').is_file\n\ntests/test_generate_files.py:112: AssertionError"}, "teardown": {"duration": 0.0003378630000003824, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_windows_newline_forced_to_linux_by_context", "lineno": 120, "outcome": "failed", "keywords": ["test_generate_files_with_windows_newline_forced_to_linux_by_context", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0014778590000004144, "outcome": "passed"}, "call": {"duration": 0.0003838649999998722, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 130, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo1/inputpizz\u00e4/simple-with-newline-crlf.txt').is_file"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 130, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo1')\n\n    def test_generate_files_with_windows_newline_forced_to_linux_by_context(tmp_path):\n        \"\"\"Verify windows line end changed to linux during files generation.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4', '_new_lines': '\\n'}},\n            repo_dir='tests/test-generate-files',\n            output_dir=tmp_path,\n        )\n    \n        newline_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline-crlf.txt')\n>       assert newline_file.is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_windo1/inputpizz\u00e4/simple-with-newline-crlf.txt').is_file\n\ntests/test_generate_files.py:130: AssertionError"}, "teardown": {"duration": 0.000385711999999927, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_binaries", "lineno": 139, "outcome": "failed", "keywords": ["test_generate_files_binaries", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0016852199999997097, "outcome": "passed"}, "call": {"duration": 0.0006965230000002265, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 150, "message": "AssertionError: assert False\n +  where False = is_binary('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png')\n +    where '/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png' = str(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png'))\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files'), 'logo.png')"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 150, "message": "AssertionError"}], "stdout": "[Errno 2] No such file or directory: '/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png'\n", "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0')\n\n    def test_generate_files_binaries(tmp_path):\n        \"\"\"Verify binary files created during directory generation.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'binary_test': 'binary_files'}},\n            repo_dir='tests/test-generate-binaries',\n            output_dir=tmp_path,\n        )\n    \n        dst_dir = Path(tmp_path, 'inputbinary_files')\n    \n>       assert is_binary(str(Path(dst_dir, 'logo.png')))\nE       AssertionError: assert False\nE        +  where False = is_binary('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png')\nE        +    where '/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png' = str(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png'))\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files/logo.png') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_binaries0/inputbinary_files'), 'logo.png')\n\ntests/test_generate_files.py:150: AssertionError"}, "teardown": {"duration": 0.00034404599999948715, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_absolute_path", "lineno": 160, "outcome": "failed", "keywords": ["test_generate_files_absolute_path", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.001603486000000487, "outcome": "passed"}, "call": {"duration": 0.00045745700000043854, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 168, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0/inputpizz\u00e4/simple.txt').is_file\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0/inputpizz\u00e4/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0'), 'inputpizz\u00e4/simple.txt')"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 168, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0')\n\n    def test_generate_files_absolute_path(tmp_path):\n        \"\"\"Verify usage of absolute path does not change files generation behaviour.\"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir=Path('tests/test-generate-files').absolute(),\n            output_dir=tmp_path,\n        )\n>       assert Path(tmp_path, 'inputpizz\u00e4/simple.txt').is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0/inputpizz\u00e4/simple.txt').is_file\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0/inputpizz\u00e4/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_absolute_p0'), 'inputpizz\u00e4/simple.txt')\n\ntests/test_generate_files.py:168: AssertionError"}, "teardown": {"duration": 0.00035745499999961794, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_output_dir", "lineno": 170, "outcome": "failed", "keywords": ["test_generate_files_output_dir", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.002221201000000228, "outcome": "passed"}, "call": {"duration": 0.0006491459999997673, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 182, "message": "AssertionError: assert False\n +  where False = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir/inputpizz\u00e4/simple.txt').exists\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir/inputpizz\u00e4/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir'), 'inputpizz\u00e4/simple.txt')"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 182, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0')\n\n    def test_generate_files_output_dir(tmp_path):\n        \"\"\"Verify `output_dir` option for `generate_files` changing location correctly.\"\"\"\n        output_dir = Path(tmp_path, 'custom_output_dir')\n        output_dir.mkdir()\n    \n        project_dir = generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir=Path('tests/test-generate-files').absolute(),\n            output_dir=output_dir,\n        )\n    \n>       assert Path(output_dir, 'inputpizz\u00e4/simple.txt').exists()\nE       AssertionError: assert False\nE        +  where False = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir/inputpizz\u00e4/simple.txt').exists\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir/inputpizz\u00e4/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_output_dir0/custom_output_dir'), 'inputpizz\u00e4/simple.txt')\n\ntests/test_generate_files.py:182: AssertionError"}, "teardown": {"duration": 0.0003626939999996637, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_permissions", "lineno": 186, "outcome": "failed", "keywords": ["test_generate_files_permissions", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0015883949999997427, "outcome": "passed"}, "call": {"duration": 0.0004154780000007463, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 199, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0/inputpermissions/simple.txt').is_file\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0/inputpermissions/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0'), 'inputpermissions/simple.txt')"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 199, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0')\n\n    def test_generate_files_permissions(tmp_path):\n        \"\"\"Verify generates files respect source files permissions.\n    \n        simple.txt and script.sh should retain their respective 0o644 and 0o755\n        permissions.\n        \"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'permissions': 'permissions'}},\n            repo_dir='tests/test-generate-files-permissions',\n            output_dir=tmp_path,\n        )\n    \n>       assert Path(tmp_path, 'inputpermissions/simple.txt').is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0/inputpermissions/simple.txt').is_file\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0/inputpermissions/simple.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_permission0'), 'inputpermissions/simple.txt')\n\ntests/test_generate_files.py:199: AssertionError"}, "teardown": {"duration": 0.0003400909999999868, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_overwrite_if_exists_with_skip_if_file_exists", "lineno": 230, "outcome": "failed", "keywords": ["test_generate_files_with_overwrite_if_exists_with_skip_if_file_exists", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.001526898999999915, "outcome": "passed"}, "call": {"duration": 0.0007090910000000505, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 250, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt').is_file\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt'))"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 250, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0')\n\n    def test_generate_files_with_overwrite_if_exists_with_skip_if_file_exists(tmp_path):\n        \"\"\"Verify `skip_if_file_exist` has priority over `overwrite_if_exists`.\"\"\"\n        simple_file = Path(tmp_path, 'inputpizz\u00e4/simple.txt')\n        simple_with_new_line_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline.txt')\n    \n        Path(tmp_path, 'inputpizz\u00e4').mkdir(parents=True)\n        with Path(simple_file).open('w') as f:\n            f.write('temp')\n    \n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir='tests/test-generate-files',\n            overwrite_if_exists=True,\n            skip_if_file_exists=True,\n            output_dir=tmp_path,\n        )\n    \n        assert Path(simple_file).is_file()\n        assert Path(simple_file).exists()\n>       assert Path(simple_with_new_line_file).is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt').is_file\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw0/inputpizz\u00e4/simple-with-newline.txt'))\n\ntests/test_generate_files.py:250: AssertionError"}, "teardown": {"duration": 0.00041776299999973787, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_skip_if_file_exists", "lineno": 256, "outcome": "failed", "keywords": ["test_generate_files_with_skip_if_file_exists", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0015526160000005618, "outcome": "passed"}, "call": {"duration": 0.0005558620000005732, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 265, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.OutputDirExistsException'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 265, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_skip_0')\n\n    def test_generate_files_with_skip_if_file_exists(tmp_path):\n        \"\"\"Verify existed files not removed if error raised with `skip_if_file_exists`.\"\"\"\n        simple_file = Path(tmp_path, 'inputpizz\u00e4/simple.txt')\n        simple_with_new_line_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline.txt')\n    \n        Path(tmp_path, 'inputpizz\u00e4').mkdir(parents=True)\n        Path(simple_file).write_text('temp')\n    \n>       with pytest.raises(exceptions.OutputDirExistsException):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.OutputDirExistsException'>\n\ntests/test_generate_files.py:265: Failed"}, "teardown": {"duration": 0.0003491110000002351, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_generate_files_with_overwrite_if_exists", "lineno": 280, "outcome": "failed", "keywords": ["test_generate_files_with_overwrite_if_exists", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0015184260000005168, "outcome": "passed"}, "call": {"duration": 0.0006996959999998609, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 298, "message": "AssertionError: assert False\n +  where False = is_file()\n +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt').is_file\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt'))"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 298, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1')\n\n    def test_generate_files_with_overwrite_if_exists(tmp_path):\n        \"\"\"Verify overwrite_if_exists overwrites old files.\"\"\"\n        simple_file = Path(tmp_path, 'inputpizz\u00e4/simple.txt')\n        simple_with_new_line_file = Path(tmp_path, 'inputpizz\u00e4/simple-with-newline.txt')\n    \n        Path(tmp_path, 'inputpizz\u00e4').mkdir(parents=True)\n        Path(simple_file).write_text('temp')\n    \n        generate.generate_files(\n            context={'cookiecutter': {'food': 'pizz\u00e4'}},\n            repo_dir='tests/test-generate-files',\n            overwrite_if_exists=True,\n            output_dir=tmp_path,\n        )\n    \n        assert Path(simple_file).is_file()\n        assert Path(simple_file).exists()\n>       assert Path(simple_with_new_line_file).is_file()\nE       AssertionError: assert False\nE        +  where False = is_file()\nE        +    where is_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt').is_file\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_generate_files_with_overw1/inputpizz\u00e4/simple-with-newline.txt'))\n\ntests/test_generate_files.py:298: AssertionError"}, "teardown": {"duration": 0.0003391370000001004, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_name", "lineno": 312, "outcome": "failed", "keywords": ["test_raise_undefined_variable_file_name", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0017405119999995833, "outcome": "passed"}, "call": {"duration": 0.0002784120000001167, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 315, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 315, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_0/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_raise_undefined_variable_file_name(output_dir, undefined_context):\n        \"\"\"Verify correct error raised when file name cannot be rendered.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:315: Failed"}, "teardown": {"duration": 0.0003748540000003686, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_name_existing_project", "lineno": 327, "outcome": "failed", "keywords": ["test_raise_undefined_variable_file_name_existing_project", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.001709060000000484, "outcome": "passed"}, "call": {"duration": 0.0003806960000005688, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 335, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 335, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_1/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_raise_undefined_variable_file_name_existing_project(\n        output_dir, undefined_context\n    ):\n        \"\"\"Verify correct error raised when file name cannot be rendered.\"\"\"\n        testproj_path = Path(output_dir, 'testproject')\n        testproj_path.mkdir()\n    \n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:335: Failed"}, "teardown": {"duration": 0.0003909269999997633, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_file_content", "lineno": 348, "outcome": "failed", "keywords": ["test_raise_undefined_variable_file_content", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0017161409999992827, "outcome": "passed"}, "call": {"duration": 0.00027697500000023467, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 351, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 351, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_2/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_raise_undefined_variable_file_content(output_dir, undefined_context):\n        \"\"\"Verify correct error raised when file content cannot be rendered.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:351: Failed"}, "teardown": {"duration": 0.0003787590000001728, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_dir_name", "lineno": 363, "outcome": "failed", "keywords": ["test_raise_undefined_variable_dir_name", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.001733864000000196, "outcome": "passed"}, "call": {"duration": 0.00027354299999959863, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 366, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 366, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_3/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_raise_undefined_variable_dir_name(output_dir, undefined_context):\n        \"\"\"Verify correct error raised when directory name cannot be rendered.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:366: Failed"}, "teardown": {"duration": 0.0003796779999998279, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_keep_project_dir_on_failure", "lineno": 382, "outcome": "failed", "keywords": ["test_keep_project_dir_on_failure", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0016976260000003407, "outcome": "passed"}, "call": {"duration": 0.0002776930000001343, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 385, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 385, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_keep_project_dir_on_failu0/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_keep_project_dir_on_failure(output_dir, undefined_context):\n        \"\"\"Verify correct error raised when directory name cannot be rendered.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:385: Failed"}, "teardown": {"duration": 0.00038221700000029557, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_dir_name_existing_project", "lineno": 394, "outcome": "failed", "keywords": ["test_raise_undefined_variable_dir_name_existing_project", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0017066220000003796, "outcome": "passed"}, "call": {"duration": 0.0003897090000002379, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 402, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 402, "message": "Failed"}], "longrepr": "output_dir = '/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_4/output'\nundefined_context = {'cookiecutter': {'github_username': 'hackebrot', 'project_slug': 'testproject'}}\n\n    def test_raise_undefined_variable_dir_name_existing_project(\n        output_dir, undefined_context\n    ):\n        \"\"\"Verify correct error raised when directory name cannot be rendered.\"\"\"\n        testproj_path = Path(output_dir, 'testproject')\n        testproj_path.mkdir()\n    \n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:402: Failed"}, "teardown": {"duration": 0.00037655100000044683, "outcome": "passed"}}, {"nodeid": "tests/test_generate_files.py::test_raise_undefined_variable_project_dir", "lineno": 419, "outcome": "failed", "keywords": ["test_raise_undefined_variable_project_dir", "test_generate_files.py", "tests", "testbed", ""], "setup": {"duration": 0.0015362100000002599, "outcome": "passed"}, "call": {"duration": 0.00026794399999996443, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_files.py", "lineno": 422, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_generate_files.py", "lineno": 422, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_raise_undefined_variable_5')\n\n    def test_raise_undefined_variable_project_dir(tmp_path):\n        \"\"\"Verify correct error raised when directory name cannot be rendered.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_generate_files.py:422: Failed"}, "teardown": {"duration": 0.00033426599999941686, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_ignore_hooks_dirs", "lineno": 31, "outcome": "passed", "keywords": ["test_ignore_hooks_dirs", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018056160000003985, "outcome": "passed"}, "call": {"duration": 0.0002664159999996585, "outcome": "passed"}, "teardown": {"duration": 0.0004698379999998892, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_python_hooks", "lineno": 42, "outcome": "failed", "keywords": ["test_run_python_hooks", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017452220000002683, "outcome": "passed"}, "call": {"duration": 0.0003664890000001364, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 55, "message": "AssertionError: assert False\n +  where False = <function exists at 0x7f2b4e9c1f30>('tests/test-pyhooks/inputpyhooks/python_pre.txt')\n +    where <function exists at 0x7f2b4e9c1f30> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.exists\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 55, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_run_python_hooks():\n        \"\"\"Verify pre and post generation python hooks executed and result in output_dir.\n    \n        Each hook should create in target directory. Test verifies that these files\n        created.\n        \"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'pyhooks': 'pyhooks'}},\n            repo_dir='tests/test-pyhooks/',\n            output_dir='tests/test-pyhooks/',\n        )\n>       assert os.path.exists('tests/test-pyhooks/inputpyhooks/python_pre.txt')\nE       AssertionError: assert False\nE        +  where False = <function exists at 0x7f2b4e9c1f30>('tests/test-pyhooks/inputpyhooks/python_pre.txt')\nE        +    where <function exists at 0x7f2b4e9c1f30> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.exists\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_generate_hooks.py:55: AssertionError"}, "teardown": {"duration": 0.0005042829999997167, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_python_hooks_cwd", "lineno": 58, "outcome": "failed", "keywords": ["test_run_python_hooks_cwd", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018101729999999705, "outcome": "passed"}, "call": {"duration": 0.00039571100000035386, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 69, "message": "AssertionError: assert False\n +  where False = <function exists at 0x7f2b4e9c1f30>('inputpyhooks/python_pre.txt')\n +    where <function exists at 0x7f2b4e9c1f30> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.exists\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 69, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_run_python_hooks_cwd():\n        \"\"\"Verify pre and post generation python hooks executed and result in current dir.\n    \n        Each hook should create in target directory. Test verifies that these files\n        created.\n        \"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'pyhooks': 'pyhooks'}}, repo_dir='tests/test-pyhooks/'\n        )\n>       assert os.path.exists('inputpyhooks/python_pre.txt')\nE       AssertionError: assert False\nE        +  where False = <function exists at 0x7f2b4e9c1f30>('inputpyhooks/python_pre.txt')\nE        +    where <function exists at 0x7f2b4e9c1f30> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.exists\nE        +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\n\ntests/test_generate_hooks.py:69: AssertionError"}, "teardown": {"duration": 0.000507269000000754, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_empty_hooks", "lineno": 72, "outcome": "failed", "keywords": ["test_empty_hooks", "usefixtures", "skipif", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001786508000000353, "outcome": "passed"}, "call": {"duration": 0.00027294700000002337, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 81, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 81, "message": "Failed"}], "longrepr": "@pytest.mark.skipif(WINDOWS, reason='OSError.errno=8 is not thrown on Windows')\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_empty_hooks():\n        \"\"\"Verify error is raised on empty hook script. Ignored on windows.\n    \n        OSError.errno=8 is not thrown on Windows when the script is empty\n        because it always runs through shell instead of needing a shebang.\n        \"\"\"\n>       with pytest.raises(FailedHookException) as excinfo:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>\n\ntests/test_generate_hooks.py:81: Failed"}, "teardown": {"duration": 0.0005144030000003852, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_oserror_hooks", "lineno": 89, "outcome": "failed", "keywords": ["test_oserror_hooks", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018727700000003011, "outcome": "passed"}, "call": {"duration": 0.0009214389999998573, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 105, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 105, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c19e920>\n\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_oserror_hooks(mocker):\n        \"\"\"Verify script error passed correctly to cookiecutter error.\n    \n        Here subprocess.Popen function mocked, ie we do not call hook script,\n        just produce expected error.\n        \"\"\"\n        message = 'Out of memory'\n    \n        err = OSError(message)\n        err.errno = errno.ENOMEM\n    \n        prompt = mocker.patch('subprocess.Popen')\n        prompt.side_effect = err\n    \n>       with pytest.raises(FailedHookException) as excinfo:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>\n\ntests/test_generate_hooks.py:105: Failed"}, "teardown": {"duration": 0.0005728490000000974, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_failing_hook_removes_output_directory", "lineno": 113, "outcome": "failed", "keywords": ["test_run_failing_hook_removes_output_directory", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017846500000002763, "outcome": "passed"}, "call": {"duration": 0.0006404319999999686, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 132, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 132, "message": "Failed"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_run_failing_hook_removes_output_directory():\n        \"\"\"Verify project directory not created or removed if hook failed.\"\"\"\n        repo_path = os.path.abspath('tests/test-hooks/')\n        hooks_path = os.path.abspath('tests/test-hooks/hooks')\n    \n        hook_dir = os.path.join(repo_path, 'hooks')\n        template = os.path.join(repo_path, 'input{{cookiecutter.hooks}}')\n        os.mkdir(repo_path)\n        os.mkdir(hook_dir)\n        os.mkdir(template)\n    \n        hook_path = os.path.join(hooks_path, 'pre_gen_project.py')\n    \n        with Path(hook_path).open('w') as f:\n            f.write(\"#!/usr/bin/env python\\n\")\n            f.write(\"import sys; sys.exit(1)\\n\")\n    \n>       with pytest.raises(FailedHookException) as excinfo:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>\n\ntests/test_generate_hooks.py:132: Failed"}, "teardown": {"duration": 0.0005184879999999836, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_failing_hook_preserves_existing_output_directory", "lineno": 142, "outcome": "failed", "keywords": ["test_run_failing_hook_preserves_existing_output_directory", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017790970000000073, "outcome": "passed"}, "call": {"duration": 0.00029289900000062374, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 151, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 151, "message": "FileExistsError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_run_failing_hook_preserves_existing_output_directory():\n        \"\"\"Verify project directory not removed if exist before hook failed.\"\"\"\n        repo_path = os.path.abspath('tests/test-hooks/')\n        hooks_path = os.path.abspath('tests/test-hooks/hooks')\n    \n        hook_dir = os.path.join(repo_path, 'hooks')\n        template = os.path.join(repo_path, 'input{{cookiecutter.hooks}}')\n>       os.mkdir(repo_path)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_generate_hooks.py:151: FileExistsError"}, "teardown": {"duration": 0.0005207729999998634, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_shell_hooks", "lineno": 172, "outcome": "failed", "keywords": ["test_run_shell_hooks", "usefixtures", "skipif", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017870279999998573, "outcome": "passed"}, "call": {"duration": 0.000385285000000124, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 191, "message": "AssertionError: assert False\n +  where False = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_run_shell_hooks0/test-shellhooks/inputshellhooks/shell_pre.txt').exists"}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 191, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_run_shell_hooks0')\n\n    @pytest.mark.skipif(sys.platform.startswith('win'), reason=\"Linux only test\")\n    @pytest.mark.usefixtures('clean_system', 'remove_additional_folders')\n    def test_run_shell_hooks(tmp_path):\n        \"\"\"Verify pre and post generate project shell hooks executed.\n    \n        This test for .sh files.\n        \"\"\"\n        generate.generate_files(\n            context={'cookiecutter': {'shellhooks': 'shellhooks'}},\n            repo_dir='tests/test-shellhooks/',\n            output_dir=tmp_path.joinpath('test-shellhooks'),\n        )\n        shell_pre_file = tmp_path.joinpath(\n            'test-shellhooks', 'inputshellhooks', 'shell_pre.txt'\n        )\n        shell_post_file = tmp_path.joinpath(\n            'test-shellhooks', 'inputshellhooks', 'shell_post.txt'\n        )\n>       assert shell_pre_file.exists()\nE       AssertionError: assert False\nE        +  where False = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_run_shell_hooks0/test-shellhooks/inputshellhooks/shell_pre.txt').exists\n\ntests/test_generate_hooks.py:191: AssertionError"}, "teardown": {"duration": 0.0005079160000001082, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_run_shell_hooks_win", "lineno": 194, "outcome": "skipped", "keywords": ["test_run_shell_hooks_win", "usefixtures", "skipif", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.00020571499999988418, "outcome": "skipped", "longrepr": "('/testbed/tests/test_generate_hooks.py', 195, 'Skipped: Win only test')"}, "teardown": {"duration": 0.00020643600000003204, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_ignore_shell_hooks", "lineno": 216, "outcome": "passed", "keywords": ["test_ignore_shell_hooks", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017870010000002878, "outcome": "passed"}, "call": {"duration": 0.00033043599999960094, "outcome": "passed"}, "teardown": {"duration": 0.00047340500000014885, "outcome": "passed"}}, {"nodeid": "tests/test_generate_hooks.py::test_deprecate_run_hook_from_repo_dir", "lineno": 233, "outcome": "failed", "keywords": ["test_deprecate_run_hook_from_repo_dir", "usefixtures", "pytestmark", "test_generate_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017950149999998999, "outcome": "passed"}, "call": {"duration": 0.0004615880000002903, "outcome": "failed", "crash": {"path": "/testbed/tests/test_generate_hooks.py", "lineno": 240, "message": "Failed: DID NOT WARN. No warnings of type (<class 'DeprecationWarning'>, <class 'PendingDeprecationWarning'>, <class 'FutureWarning'>) were emitted.\n Emitted warnings: []."}, "traceback": [{"path": "tests/test_generate_hooks.py", "lineno": 240, "message": "Failed"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_deprecate_run_hook_from_r0')\n\n    @pytest.mark.usefixtures(\"clean_system\", \"remove_additional_folders\")\n    def test_deprecate_run_hook_from_repo_dir(tmp_path):\n        \"\"\"Test deprecation warning in generate._run_hook_from_repo_dir.\"\"\"\n        repo_dir = \"tests/test-shellhooks/\"\n        project_dir = Path(tmp_path.joinpath('test-shellhooks'))\n        project_dir.mkdir()\n>       with pytest.deprecated_call():\nE       Failed: DID NOT WARN. No warnings of type (<class 'DeprecationWarning'>, <class 'PendingDeprecationWarning'>, <class 'FutureWarning'>) were emitted.\nE        Emitted warnings: [].\n\ntests/test_generate_hooks.py:240: Failed"}, "teardown": {"duration": 0.0005006660000006491, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_merge_configs", "lineno": 11, "outcome": "failed", "keywords": ["test_merge_configs", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015729939999999942, "outcome": "passed"}, "call": {"duration": 0.0005341070000000059, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 51, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/hackebrot/{0}.git', 'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot'}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n +  where None = <function merge_configs at 0x7f2b4d0f0e50>({'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {}, 'replay_dir': '/home/example/some-path-to-replay-files'}, {'abbreviations': {'gl': 'https://gitlab.com/hackebrot/{0}.git', 'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git'}, 'default_context': {'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot'}})\n +    where <function merge_configs at 0x7f2b4d0f0e50> = config.merge_configs"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "def test_merge_configs():\n        \"\"\"Verify default and user config merged in expected way.\"\"\"\n        default = {\n            'cookiecutters_dir': '/home/example/some-path-to-templates',\n            'replay_dir': '/home/example/some-path-to-replay-files',\n            'default_context': {},\n            'abbreviations': {\n                'gh': 'https://github.com/{0}.git',\n                'gl': 'https://gitlab.com/{0}.git',\n                'bb': 'https://bitbucket.org/{0}',\n            },\n        }\n    \n        user_config = {\n            'default_context': {\n                'full_name': 'Raphael Pierzina',\n                'github_username': 'hackebrot',\n            },\n            'abbreviations': {\n                'gl': 'https://gitlab.com/hackebrot/{0}.git',\n                'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git',\n            },\n        }\n    \n        expected_config = {\n            'cookiecutters_dir': '/home/example/some-path-to-templates',\n            'replay_dir': '/home/example/some-path-to-replay-files',\n            'default_context': {\n                'full_name': 'Raphael Pierzina',\n                'github_username': 'hackebrot',\n            },\n            'abbreviations': {\n                'gh': 'https://github.com/{0}.git',\n                'gl': 'https://gitlab.com/hackebrot/{0}.git',\n                'bb': 'https://bitbucket.org/{0}',\n                'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git',\n            },\n        }\n    \n>       assert config.merge_configs(default, user_config) == expected_config\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/hackebrot/{0}.git', 'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot'}, 'replay_dir': '/home/example/some-path-to-replay-files'}\nE        +  where None = <function merge_configs at 0x7f2b4d0f0e50>({'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {}, 'replay_dir': '/home/example/some-path-to-replay-files'}, {'abbreviations': {'gl': 'https://gitlab.com/hackebrot/{0}.git', 'pytest-plugin': 'https://github.com/pytest-dev/pytest-plugin.git'}, 'default_context': {'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot'}})\nE        +    where <function merge_configs at 0x7f2b4d0f0e50> = config.merge_configs\n\ntests/test_get_config.py:51: AssertionError"}, "teardown": {"duration": 0.0003456900000005092, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config", "lineno": 53, "outcome": "failed", "keywords": ["test_get_config", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015658769999999933, "outcome": "passed"}, "call": {"duration": 0.00043078000000029704, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 80, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 80, "message": "AssertionError"}], "longrepr": "def test_get_config():\n        \"\"\"Verify valid config opened and rendered correctly.\"\"\"\n        conf = config.get_config('tests/test-config/valid-config.yaml')\n        expected_conf = {\n            'cookiecutters_dir': '/home/example/some-path-to-templates',\n            'replay_dir': '/home/example/some-path-to-replay-files',\n            'default_context': {\n                'full_name': 'Firstname Lastname',\n                'email': 'firstname.lastname@gmail.com',\n                'github_username': 'example',\n                'project': {\n                    'description': 'description',\n                    'tags': [\n                        'first',\n                        'second',\n                        'third',\n                    ],\n                },\n            },\n            'abbreviations': {\n                'gh': 'https://github.com/{0}.git',\n                'gl': 'https://gitlab.com/{0}.git',\n                'bb': 'https://bitbucket.org/{0}',\n                'helloworld': 'https://github.com/hackebrot/helloworld',\n            },\n        }\n>       assert conf == expected_conf\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\ntests/test_get_config.py:80: AssertionError"}, "teardown": {"duration": 0.0003354400000006308, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config_does_not_exist", "lineno": 82, "outcome": "failed", "keywords": ["test_get_config_does_not_exist", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015606079999992417, "outcome": "passed"}, "call": {"duration": 0.00026957499999991086, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 87, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.ConfigDoesNotExistException'>"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 87, "message": "Failed"}], "longrepr": "def test_get_config_does_not_exist():\n        \"\"\"Check that `exceptions.ConfigDoesNotExistException` is raised when \\\n        attempting to get a non-existent config file.\"\"\"\n        expected_error_msg = 'Config file tests/not-exist.yaml does not exist.'\n>       with pytest.raises(ConfigDoesNotExistException) as exc_info:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.ConfigDoesNotExistException'>\n\ntests/test_get_config.py:87: Failed"}, "teardown": {"duration": 0.00037417500000014314, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_invalid_config", "lineno": 91, "outcome": "failed", "keywords": ["test_invalid_config", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015629840000004336, "outcome": "passed"}, "call": {"duration": 0.00028670699999988614, "outcome": "failed", "crash": {"path": "/testbed/.venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError: .value can only be used after the context manager exits"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 100, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/_pytest/_code/code.py", "lineno": 548, "message": "AssertionError"}], "longrepr": "def test_invalid_config():\n        \"\"\"An invalid config file should raise an `InvalidConfiguration` \\\n        exception.\"\"\"\n        expected_error_msg = (\n            'Unable to parse YAML file tests/test-config/invalid-config.yaml.'\n        )\n        with pytest.raises(InvalidConfiguration) as exc_info:\n            config.get_config('tests/test-config/invalid-config.yaml')\n>           assert expected_error_msg in str(exc_info.value)\n\ntests/test_get_config.py:100: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ExceptionInfo for raises contextmanager>\n\n    @property\n    def value(self) -> E:\n        \"\"\"The exception value.\"\"\"\n        assert (\n>           self._excinfo is not None\n        ), \".value can only be used after the context manager exits\"\nE       AssertionError: .value can only be used after the context manager exits\n\n.venv/lib/python3.10/site-packages/_pytest/_code/code.py:548: AssertionError"}, "teardown": {"duration": 0.0003584130000007235, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config_with_defaults", "lineno": 103, "outcome": "failed", "keywords": ["test_get_config_with_defaults", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015988970000000435, "outcome": "passed"}, "call": {"duration": 0.0004802210000001139, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 123, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_with_defaults0/home/.cookiecutters', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example'}, 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_with_defaults0/home/.cookiecutter_replay'}"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 123, "message": "AssertionError"}], "longrepr": "def test_get_config_with_defaults():\n        \"\"\"A config file that overrides 1 of 3 defaults.\"\"\"\n        conf = config.get_config('tests/test-config/valid-partial-config.yaml')\n        default_cookiecutters_dir = Path('~/.cookiecutters').expanduser()\n        default_replay_dir = Path('~/.cookiecutter_replay').expanduser()\n        expected_conf = {\n            'cookiecutters_dir': str(default_cookiecutters_dir),\n            'replay_dir': str(default_replay_dir),\n            'default_context': {\n                'full_name': 'Firstname Lastname',\n                'email': 'firstname.lastname@gmail.com',\n                'github_username': 'example',\n            },\n            'abbreviations': {\n                'gh': 'https://github.com/{0}.git',\n                'gl': 'https://gitlab.com/{0}.git',\n                'bb': 'https://bitbucket.org/{0}',\n            },\n        }\n>       assert conf == expected_conf\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_with_defaults0/home/.cookiecutters', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example'}, 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_with_defaults0/home/.cookiecutter_replay'}\n\ntests/test_get_config.py:123: AssertionError"}, "teardown": {"duration": 0.0003412489999998769, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config_empty_config_file", "lineno": 125, "outcome": "failed", "keywords": ["test_get_config_empty_config_file", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015878909999997859, "outcome": "passed"}, "call": {"duration": 0.0004105879999993789, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 129, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutter_replay'}\n +  where {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 129, "message": "AssertionError"}], "longrepr": "def test_get_config_empty_config_file():\n        \"\"\"An empty config file results in the default config.\"\"\"\n        conf = config.get_config('tests/test-config/empty-config.yaml')\n>       assert conf == config.DEFAULT_CONFIG\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutter_replay'}\nE        +  where {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_config_empty_config_f0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG\n\ntests/test_get_config.py:129: AssertionError"}, "teardown": {"duration": 0.0003303670000001091, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config_invalid_file_with_array_as_top_level_element", "lineno": 131, "outcome": "failed", "keywords": ["test_get_config_invalid_file_with_array_as_top_level_element", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015845020000000432, "outcome": "passed"}, "call": {"duration": 0.00028278400000036896, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 138, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 138, "message": "Failed"}], "longrepr": "def test_get_config_invalid_file_with_array_as_top_level_element():\n        \"\"\"An exception should be raised if top-level element is array.\"\"\"\n        expected_error_msg = (\n            'Top-level element of YAML file '\n            'tests/test-config/invalid-config-w-array.yaml should be an object.'\n        )\n>       with pytest.raises(InvalidConfiguration) as exc_info:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>\n\ntests/test_get_config.py:138: Failed"}, "teardown": {"duration": 0.0003340689999999924, "outcome": "passed"}}, {"nodeid": "tests/test_get_config.py::test_get_config_invalid_file_with_multiple_docs", "lineno": 142, "outcome": "failed", "keywords": ["test_get_config_invalid_file_with_multiple_docs", "test_get_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0015844799999999992, "outcome": "passed"}, "call": {"duration": 0.000266902999999985, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_config.py", "lineno": 149, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>"}, "traceback": [{"path": "tests/test_get_config.py", "lineno": 149, "message": "Failed"}], "longrepr": "def test_get_config_invalid_file_with_multiple_docs():\n        \"\"\"An exception should be raised if config file contains multiple docs.\"\"\"\n        expected_error_msg = (\n            'Unable to parse YAML file '\n            'tests/test-config/invalid-config-w-multiple-docs.yaml.'\n        )\n>       with pytest.raises(InvalidConfiguration) as exc_info:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>\n\ntests/test_get_config.py:149: Failed"}, "teardown": {"duration": 0.00033332199999946965, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_get_user_config_valid", "lineno": 69, "outcome": "failed", "keywords": ["test_get_user_config_valid", "usefixtures", "pytestmark", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.00182457500000055, "outcome": "passed"}, "call": {"duration": 0.0011023209999994066, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 76, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 76, "message": "AssertionError"}], "longrepr": "user_config_path = '/root/.cookiecutterrc'\ncustom_config = {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\n    @pytest.mark.usefixtures('back_up_rc')\n    def test_get_user_config_valid(user_config_path, custom_config):\n        \"\"\"Validate user config correctly parsed if exist and correctly formatted.\"\"\"\n        shutil.copy('tests/test-config/valid-config.yaml', user_config_path)\n        conf = config.get_user_config()\n    \n>       assert conf == custom_config\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\ntests/test_get_user_config.py:76: AssertionError"}, "teardown": {"duration": 0.0004716550000001263, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_get_user_config_invalid", "lineno": 78, "outcome": "failed", "keywords": ["test_get_user_config_invalid", "usefixtures", "pytestmark", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0017554790000007259, "outcome": "passed"}, "call": {"duration": 0.0009135689999997254, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 83, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 83, "message": "Failed"}], "longrepr": "user_config_path = '/root/.cookiecutterrc'\n\n    @pytest.mark.usefixtures('back_up_rc')\n    def test_get_user_config_invalid(user_config_path):\n        \"\"\"Validate `InvalidConfiguration` raised when provided user config malformed.\"\"\"\n        shutil.copy('tests/test-config/invalid-config.yaml', user_config_path)\n>       with pytest.raises(InvalidConfiguration):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidConfiguration'>\n\ntests/test_get_user_config.py:83: Failed"}, "teardown": {"duration": 0.0004554379999994751, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_get_user_config_nonexistent", "lineno": 86, "outcome": "failed", "keywords": ["test_get_user_config_nonexistent", "usefixtures", "pytestmark", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0017597480000004495, "outcome": "passed"}, "call": {"duration": 0.00044087399999970245, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 90, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutter_replay'}\n +  where None = <function get_user_config at 0x7f2b4d0f1120>()\n +    where <function get_user_config at 0x7f2b4d0f1120> = config.get_user_config\n +  and   {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 90, "message": "AssertionError"}], "longrepr": "@pytest.mark.usefixtures('back_up_rc')\n    def test_get_user_config_nonexistent():\n        \"\"\"Validate default app config returned, if user does not have own config.\"\"\"\n>       assert config.get_user_config() == config.DEFAULT_CONFIG\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutter_replay'}\nE        +  where None = <function get_user_config at 0x7f2b4d0f1120>()\nE        +    where <function get_user_config at 0x7f2b4d0f1120> = config.get_user_config\nE        +  and   {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_get_user_config_nonexiste0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG\n\ntests/test_get_user_config.py:90: AssertionError"}, "teardown": {"duration": 0.00040784400000060117, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_specify_config_path", "lineno": 98, "outcome": "failed", "keywords": ["test_specify_config_path", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.001791790000000404, "outcome": "passed"}, "call": {"duration": 0.0012724089999993637, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError: Expected 'get_config' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 104, "message": ""}, {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='get_config' spec='function' id='139823943850624'>\nargs = ('tests/test-config/valid-config.yaml',), kwargs = {}\nmsg = \"Expected 'get_config' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'get_config' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c831bd0>\ncustom_config_path = 'tests/test-config/valid-config.yaml'\ncustom_config = {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\n    def test_specify_config_path(mocker, custom_config_path, custom_config):\n        \"\"\"Validate provided custom config path should be respected and parsed.\"\"\"\n        spy_get_config = mocker.spy(config, 'get_config')\n    \n        user_config = config.get_user_config(custom_config_path)\n>       spy_get_config.assert_called_once_with(custom_config_path)\n\ntests/test_get_user_config.py:104: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = ('tests/test-config/valid-config.yaml',), kwargs = {}\n\n    def assert_called_once_with(*args, **kwargs):\n>       return mock.assert_called_once_with(*args, **kwargs)\nE       AssertionError: Expected 'get_config' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:213: AssertionError"}, "teardown": {"duration": 0.000584742000000027, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_default_config_path", "lineno": 108, "outcome": "passed", "keywords": ["test_default_config_path", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0021516749999994644, "outcome": "passed"}, "call": {"duration": 0.00028015000000003454, "outcome": "passed"}, "teardown": {"duration": 0.0003153420000003848, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_default_config_from_env_variable", "lineno": 113, "outcome": "failed", "keywords": ["test_default_config_from_env_variable", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.001938358999999501, "outcome": "passed"}, "call": {"duration": 0.0004951059999998009, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 121, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c0933a0>\ncustom_config_path = 'tests/test-config/valid-config.yaml'\ncustom_config = {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\n    def test_default_config_from_env_variable(\n        monkeypatch, custom_config_path, custom_config\n    ):\n        \"\"\"Validate app configuration. User config path should be parsed from sys env.\"\"\"\n        monkeypatch.setenv('COOKIECUTTER_CONFIG', custom_config_path)\n    \n        user_config = config.get_user_config()\n>       assert user_config == custom_config\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git', 'helloworld': 'https://github.com/hackebrot/helloworld'}, 'cookiecutters_dir': '/home/example/some-path-to-templates', 'default_context': {'email': 'firstname.lastname@gmail.com', 'full_name': 'Firstname Lastname', 'github_username': 'example', 'project': {'description': 'description', 'tags': ['first', 'second', 'third']}}, 'replay_dir': '/home/example/some-path-to-replay-files'}\n\ntests/test_get_user_config.py:121: AssertionError"}, "teardown": {"duration": 0.0003777530000004248, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_force_default_config", "lineno": 123, "outcome": "failed", "keywords": ["test_force_default_config", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0018751710000000088, "outcome": "passed"}, "call": {"duration": 0.0013977900000003984, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 130, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutter_replay'}\n +  where {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 130, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c890ca0>\ncustom_config_path = 'tests/test-config/valid-config.yaml'\n\n    def test_force_default_config(mocker, custom_config_path):\n        \"\"\"Validate `default_config=True` should ignore provided custom user config.\"\"\"\n        spy_get_config = mocker.spy(config, 'get_config')\n    \n        user_config = config.get_user_config(custom_config_path, default_config=True)\n    \n>       assert user_config == config.DEFAULT_CONFIG\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutter_replay'}\nE        +  where {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': '/tmp/pytest-of-root/pytest-0/test_force_default_config0/home/.cookiecutter_replay'} = config.DEFAULT_CONFIG\n\ntests/test_get_user_config.py:130: AssertionError"}, "teardown": {"duration": 0.0003893929999998491, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_expand_user_for_directories_in_config", "lineno": 133, "outcome": "failed", "keywords": ["test_expand_user_for_directories_in_config", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0017289600000003347, "outcome": "passed"}, "call": {"duration": 0.0002943980000003066, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 145, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 145, "message": "TypeError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3e0f10>\n\n    def test_expand_user_for_directories_in_config(monkeypatch):\n        \"\"\"Validate user pointers expanded in user configs.\"\"\"\n    \n        def _expanduser(path):\n            return path.replace('~', 'Users/bob')\n    \n        monkeypatch.setattr('os.path.expanduser', _expanduser)\n    \n        config_file = 'tests/test-config/config-expand-user.yaml'\n    \n        user_config = config.get_user_config(config_file)\n>       assert user_config['replay_dir'] == 'Users/bob/replay-files'\nE       TypeError: 'NoneType' object is not subscriptable\n\ntests/test_get_user_config.py:145: TypeError"}, "teardown": {"duration": 0.00039086199999971427, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_expand_vars_for_directories_in_config", "lineno": 148, "outcome": "failed", "keywords": ["test_expand_vars_for_directories_in_config", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.0017196140000006466, "outcome": "passed"}, "call": {"duration": 0.00026685400000037163, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 156, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 156, "message": "TypeError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c19cfd0>\n\n    def test_expand_vars_for_directories_in_config(monkeypatch):\n        \"\"\"Validate environment variables expanded in user configs.\"\"\"\n        monkeypatch.setenv('COOKIES', 'Users/bob/cookies')\n    \n        config_file = 'tests/test-config/config-expand-vars.yaml'\n    \n        user_config = config.get_user_config(config_file)\n>       assert user_config['replay_dir'] == 'Users/bob/cookies/replay-files'\nE       TypeError: 'NoneType' object is not subscriptable\n\ntests/test_get_user_config.py:156: TypeError"}, "teardown": {"duration": 0.0003495380000000381, "outcome": "passed"}}, {"nodeid": "tests/test_get_user_config.py::test_specify_config_values", "lineno": 159, "outcome": "failed", "keywords": ["test_specify_config_values", "test_get_user_config.py", "tests", "testbed", ""], "setup": {"duration": 0.001675835999999542, "outcome": "passed"}, "call": {"duration": 0.0004036319999993765, "outcome": "failed", "crash": {"path": "/testbed/tests/test_get_user_config.py", "lineno": 167, "message": "AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_specify_config_values0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': 'Users/bob/cookies/custom-replay-dir'}"}, "traceback": [{"path": "tests/test_get_user_config.py", "lineno": 167, "message": "AssertionError"}], "longrepr": "def test_specify_config_values():\n        \"\"\"Validate provided custom config values should be respected.\"\"\"\n        replay_dir = 'Users/bob/cookies/custom-replay-dir'\n        custom_config_updated = {**config.DEFAULT_CONFIG, 'replay_dir': replay_dir}\n    \n        user_config = config.get_user_config(default_config={'replay_dir': replay_dir})\n    \n>       assert user_config == custom_config_updated\nE       AssertionError: assert None == {'abbreviations': {'bb': 'https://bitbucket.org/{0}', 'gh': 'https://github.com/{0}.git', 'gl': 'https://gitlab.com/{0}.git'}, 'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/test_specify_config_values0/home/.cookiecutters', 'default_context': OrderedDict(), 'replay_dir': 'Users/bob/cookies/custom-replay-dir'}\n\ntests/test_get_user_config.py:167: AssertionError"}, "teardown": {"duration": 0.00042320399999962177, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_find_hook", "lineno": 87, "outcome": "error", "keywords": ["test_find_hook", "TestFindHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018339680000005742, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: 'tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 82, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestFindHooks object at 0x7f2b4cac0760>\nmethod = <bound method TestFindHooks.test_find_hook of <tests.test_hooks.TestFindHooks object at 0x7f2b4cac0760>>\n\n    def setup_method(self, method):\n        \"\"\"Find hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path)\n\ntests/test_hooks.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'tests/test-hooks', multiple_hooks = False\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: 'tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0003860540000006907, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_no_hooks", "lineno": 98, "outcome": "error", "keywords": ["test_no_hooks", "TestFindHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001803484999999938, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: 'tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 82, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestFindHooks object at 0x7f2b4cac1720>\nmethod = <bound method TestFindHooks.test_no_hooks of <tests.test_hooks.TestFindHooks object at 0x7f2b4cac1720>>\n\n    def setup_method(self, method):\n        \"\"\"Find hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path)\n\ntests/test_hooks.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'tests/test-hooks', multiple_hooks = False\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: 'tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.00035146899999993764, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_unknown_hooks_dir", "lineno": 103, "outcome": "error", "keywords": ["test_unknown_hooks_dir", "TestFindHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017866509999997504, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: 'tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 82, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestFindHooks object at 0x7f2b4cac3b80>\nmethod = <bound method TestFindHooks.test_unknown_hooks_dir of <tests.test_hooks.TestFindHooks object at 0x7f2b4cac3b80>>\n\n    def setup_method(self, method):\n        \"\"\"Find hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path)\n\ntests/test_hooks.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'tests/test-hooks', multiple_hooks = False\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: 'tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.00037462999999959834, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestFindHooks::test_hook_not_found", "lineno": 108, "outcome": "error", "keywords": ["test_hook_not_found", "TestFindHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018097800000003161, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: 'tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 82, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestFindHooks object at 0x7f2b4cac1b10>\nmethod = <bound method TestFindHooks.test_hook_not_found of <tests.test_hooks.TestFindHooks object at 0x7f2b4cac1b10>>\n\n    def setup_method(self, method):\n        \"\"\"Find hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path)\n\ntests/test_hooks.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = 'tests/test-hooks', multiple_hooks = False\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: 'tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0003907010000006039, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script", "lineno": 143, "outcome": "error", "keywords": ["test_run_script", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0019190309999999045, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac24a0>\nmethod = <bound method TestExternalHooks.test_run_script of <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac24a0>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0004134410000000699, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_script", "lineno": 148, "outcome": "error", "keywords": ["test_run_failing_script", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001862881000000094, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac3730>\nmethod = <bound method TestExternalHooks.test_run_failing_script of <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac3730>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.00035589400000013427, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_script_enoexec", "lineno": 159, "outcome": "error", "keywords": ["test_run_failing_script_enoexec", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018446009999992796, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac1900>\nmethod = <bound method TestExternalHooks.test_run_failing_script_enoexec of <tests.test_hooks.TestExternalHooks object at 0x7f2b4cac1900>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.00037054799999935994, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script_cwd", "lineno": 173, "outcome": "error", "keywords": ["test_run_script_cwd", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018371909999999048, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9450>\nmethod = <bound method TestExternalHooks.test_run_script_cwd of <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9450>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.00035355800000047566, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_script_with_context", "lineno": 179, "outcome": "error", "keywords": ["test_run_script_with_context", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018372749999997495, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7dbf70>\nmethod = <bound method TestExternalHooks.test_run_script_with_context of <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7dbf70>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0003579970000000543, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_hook", "lineno": 207, "outcome": "error", "keywords": ["test_run_hook", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001801632000000275, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9060>\nmethod = <bound method TestExternalHooks.test_run_hook of <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9060>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0003517909999999347, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::TestExternalHooks::test_run_failing_hook", "lineno": 219, "outcome": "error", "keywords": ["test_run_failing_hook", "TestExternalHooks", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018199189999998922, "outcome": "failed", "crash": {"path": "/testbed/tests/test_hooks.py", "lineno": 19, "message": "FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'"}, "traceback": [{"path": "tests/test_hooks.py", "lineno": 123, "message": ""}, {"path": "tests/test_hooks.py", "lineno": 19, "message": "FileExistsError"}], "longrepr": "self = <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9960>\nmethod = <bound method TestExternalHooks.test_run_failing_hook of <tests.test_hooks.TestExternalHooks object at 0x7f2b4d7d9960>>\n\n    def setup_method(self, method):\n        \"\"\"External hooks related tests setup fixture.\"\"\"\n>       self.post_hook = make_test_repo(self.repo_path, multiple_hooks=True)\n\ntests/test_hooks.py:123: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nname = '/testbed/tests/test-hooks', multiple_hooks = True\n\n    def make_test_repo(name, multiple_hooks=False):\n        \"\"\"Create test repository for test setup methods.\"\"\"\n        hook_dir = os.path.join(name, 'hooks')\n        template = os.path.join(name, 'input{{hooks}}')\n>       os.mkdir(name)\nE       FileExistsError: [Errno 17] File exists: '/testbed/tests/test-hooks'\n\ntests/test_hooks.py:19: FileExistsError"}, "teardown": {"duration": 0.0003690849999999912, "outcome": "passed"}}, {"nodeid": "tests/test_hooks.py::test_ignore_hook_backup_files", "lineno": 269, "outcome": "passed", "keywords": ["test_ignore_hook_backup_files", "test_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0023192740000004264, "outcome": "passed"}, "call": {"duration": 0.00028883800000034654, "outcome": "passed"}, "teardown": {"duration": 0.0004452669999999159, "outcome": "passed"}}, {"nodeid": "tests/test_log.py::test_info_stdout_logging", "lineno": 72, "outcome": "failed", "keywords": ["test_info_stdout_logging", "test_log.py", "tests", "testbed", ""], "setup": {"duration": 0.0018776310000001573, "outcome": "passed"}, "call": {"duration": 0.000254570000000065, "outcome": "failed", "crash": {"path": "/testbed/tests/test_log.py", "lineno": 75, "message": "AttributeError: 'NoneType' object has no attribute 'handlers'"}, "traceback": [{"path": "tests/test_log.py", "lineno": 75, "message": "AttributeError"}], "longrepr": "caplog = <_pytest.logging.LogCaptureFixture object at 0x7f2b4c3eda50>\ninfo_logger = None\ninfo_messages = ['INFO: Welcome to Cookiecutter', 'INFO: Loading user config from home dir', 'ERROR: Aw, snap! Something went wrong']\n\n    def test_info_stdout_logging(caplog, info_logger, info_messages):\n        \"\"\"Test that stdout logs use info format and level.\"\"\"\n>       [stream_handler] = info_logger.handlers\nE       AttributeError: 'NoneType' object has no attribute 'handlers'\n\ntests/test_log.py:75: AttributeError"}, "teardown": {"duration": 0.00038662900000030476, "outcome": "passed"}}, {"nodeid": "tests/test_log.py::test_debug_stdout_logging", "lineno": 89, "outcome": "failed", "keywords": ["test_debug_stdout_logging", "test_log.py", "tests", "testbed", ""], "setup": {"duration": 0.0018551330000002864, "outcome": "passed"}, "call": {"duration": 0.0002668359999997705, "outcome": "failed", "crash": {"path": "/testbed/tests/test_log.py", "lineno": 92, "message": "AttributeError: 'NoneType' object has no attribute 'handlers'"}, "traceback": [{"path": "tests/test_log.py", "lineno": 92, "message": "AttributeError"}], "longrepr": "caplog = <_pytest.logging.LogCaptureFixture object at 0x7f2b4c3f1090>\ndebug_logger = None\ndebug_messages = ['INFO cookiecutter: Welcome to Cookiecutter', 'DEBUG cookiecutter: Generating project from pytest-plugin', 'INFO cookiecutter.foo: Loading user config from home dir', \"DEBUG cookiecutter.foo.bar: I don't know.\", 'DEBUG cookiecutter.foo.bar: I wanted to save the world.', 'ERROR cookiecutter.foo: Aw, snap! Something went wrong', ...]\n\n    def test_debug_stdout_logging(caplog, debug_logger, debug_messages):\n        \"\"\"Test that stdout logs use debug format and level.\"\"\"\n>       [stream_handler] = debug_logger.handlers\nE       AttributeError: 'NoneType' object has no attribute 'handlers'\n\ntests/test_log.py:92: AttributeError"}, "teardown": {"duration": 0.00038190500000023775, "outcome": "passed"}}, {"nodeid": "tests/test_log.py::test_debug_file_logging", "lineno": 106, "outcome": "failed", "keywords": ["test_debug_file_logging", "test_log.py", "tests", "testbed", ""], "setup": {"duration": 0.0019421230000000733, "outcome": "passed"}, "call": {"duration": 0.00025089000000022565, "outcome": "failed", "crash": {"path": "/testbed/tests/test_log.py", "lineno": 110, "message": "AttributeError: 'NoneType' object has no attribute 'handlers'"}, "traceback": [{"path": "tests/test_log.py", "lineno": 110, "message": "AttributeError"}], "longrepr": "caplog = <_pytest.logging.LogCaptureFixture object at 0x7f2b4c3ecbb0>\ninfo_logger_with_file = None\ndebug_file = PosixPath('/tmp/pytest-of-root/pytest-0/test_debug_file_logging0/pytest-plugin.log')\ndebug_messages = ['INFO cookiecutter: Welcome to Cookiecutter', 'DEBUG cookiecutter: Generating project from pytest-plugin', 'INFO cookiecutter.foo: Loading user config from home dir', \"DEBUG cookiecutter.foo.bar: I don't know.\", 'DEBUG cookiecutter.foo.bar: I wanted to save the world.', 'ERROR cookiecutter.foo: Aw, snap! Something went wrong', ...]\n\n    def test_debug_file_logging(caplog, info_logger_with_file, debug_file, debug_messages):\n        \"\"\"Test that logging to stdout uses a different format and level than \\\n        the the file handler.\"\"\"\n>       [file_handler, stream_handler] = info_logger_with_file.handlers\nE       AttributeError: 'NoneType' object has no attribute 'handlers'\n\ntests/test_log.py:110: AttributeError"}, "teardown": {"duration": 0.0004739679999996582, "outcome": "passed"}}, {"nodeid": "tests/test_main.py::test_original_cookiecutter_options_preserved_in__cookiecutter", "lineno": 5, "outcome": "failed", "keywords": ["test_original_cookiecutter_options_preserved_in__cookiecutter", "test_main.py", "tests", "testbed", ""], "setup": {"duration": 0.002132567000000307, "outcome": "passed"}, "call": {"duration": 0.0008801800000002302, "outcome": "failed", "crash": {"path": "/testbed/tests/test_main.py", "lineno": 24, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "/testbed/tests/test_main.py", "lineno": 24, "message": "TypeError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3f11e0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3f28f0>\nuser_config_file = '/tmp/pytest-of-root/pytest-0/user_dir0/config'\n\n    def test_original_cookiecutter_options_preserved_in__cookiecutter(\n        monkeypatch,\n        mocker,\n        user_config_file,\n    ):\n        \"\"\"Preserve original context options.\n    \n        Tests you can access the original context options via\n        `context['_cookiecutter']`.\n        \"\"\"\n        monkeypatch.chdir('tests/fake-repo-tmpl-_cookiecutter')\n        mock_generate_files = mocker.patch('cookiecutter.main.generate_files')\n        cookiecutter(\n            '.',\n            no_input=True,\n            replay=False,\n            config_file=user_config_file,\n        )\n>       assert mock_generate_files.call_args[1]['context']['_cookiecutter'][\n            'test_list'\n        ] == [1, 2, 3, 4]\nE       TypeError: 'NoneType' object is not subscriptable\n\n/testbed/tests/test_main.py:24: TypeError"}, "teardown": {"duration": 0.000405139999999804, "outcome": "passed"}}, {"nodeid": "tests/test_main.py::test_replay_dump_template_name", "lineno": 31, "outcome": "failed", "keywords": ["test_replay_dump_template_name", "test_main.py", "tests", "testbed", ""], "setup": {"duration": 0.0018400549999997295, "outcome": "passed"}, "call": {"duration": 0.0013729690000001682, "outcome": "failed", "crash": {"path": "/testbed/tests/test_main.py", "lineno": 58, "message": "AssertionError: Expected 'dump' to be called once. Called 0 times."}, "traceback": [{"path": "/testbed/tests/test_main.py", "lineno": 58, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='dump' id='139823937040496'>\nargs = ('/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay', 'fake-repo-tmpl', <ANY>)\nkwargs = {}, msg = \"Expected 'dump' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'dump' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c192fe0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c1913c0>\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\nuser_config_file = '/tmp/pytest-of-root/pytest-0/user_dir0/config'\n\n    def test_replay_dump_template_name(\n        monkeypatch, mocker, user_config_data, user_config_file\n    ):\n        \"\"\"Check that replay_dump is called with a valid template_name.\n    \n        Template name must not be a relative path.\n    \n        Otherwise files such as ``..json`` are created, which are not just cryptic\n        but also later mistaken for replay files of other templates if invoked with\n        '.' and '--replay'.\n    \n        Change the current working directory temporarily to 'tests/fake-repo-tmpl'\n        for this test and call cookiecutter with '.' for the target template.\n        \"\"\"\n        monkeypatch.chdir('tests/fake-repo-tmpl')\n    \n        mock_replay_dump = mocker.patch('cookiecutter.main.dump')\n        mocker.patch('cookiecutter.main.generate_files')\n    \n        cookiecutter(\n            '.',\n            no_input=True,\n            replay=False,\n            config_file=user_config_file,\n        )\n    \n>       mock_replay_dump.assert_called_once_with(\n            user_config_data['replay_dir'],\n            'fake-repo-tmpl',\n            mocker.ANY,\n        )\nE       AssertionError: Expected 'dump' to be called once. Called 0 times.\n\n/testbed/tests/test_main.py:58: AssertionError"}, "teardown": {"duration": 0.00038303000000006193, "outcome": "passed"}}, {"nodeid": "tests/test_main.py::test_replay_load_template_name", "lineno": 64, "outcome": "failed", "keywords": ["test_replay_load_template_name", "test_main.py", "tests", "testbed", ""], "setup": {"duration": 0.0018142890000003575, "outcome": "passed"}, "call": {"duration": 0.002215615999999976, "outcome": "failed", "crash": {"path": "/testbed/tests/test_main.py", "lineno": 90, "message": "AssertionError: Expected 'load' to be called once. Called 0 times."}, "traceback": [{"path": "/testbed/tests/test_main.py", "lineno": 90, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='load' id='139823940185504'>\nargs = ('/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay', 'fake-repo-tmpl')\nkwargs = {}, msg = \"Expected 'load' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'load' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c492e60>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c491ae0>\nuser_config_data = {'cookiecutters_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutters', 'replay_dir': '/tmp/pytest-of-root/pytest-0/user_dir0/cookiecutter_replay'}\nuser_config_file = '/tmp/pytest-of-root/pytest-0/user_dir0/config'\n\n    def test_replay_load_template_name(\n        monkeypatch, mocker, user_config_data, user_config_file\n    ):\n        \"\"\"Check that replay_load is called correctly.\n    \n        Calls require valid template_name that is not a relative path.\n    \n        Change the current working directory temporarily to 'tests/fake-repo-tmpl'\n        for this test and call cookiecutter with '.' for the target template.\n        \"\"\"\n        monkeypatch.chdir('tests/fake-repo-tmpl')\n    \n        mock_replay_load = mocker.patch('cookiecutter.main.load')\n        mocker.patch('cookiecutter.main.generate_context').return_value = {\n            'cookiecutter': {}\n        }\n        mocker.patch('cookiecutter.main.generate_files')\n        mocker.patch('cookiecutter.main.dump')\n    \n        cookiecutter(\n            '.',\n            replay=True,\n            config_file=user_config_file,\n        )\n    \n>       mock_replay_load.assert_called_once_with(\n            user_config_data['replay_dir'],\n            'fake-repo-tmpl',\n        )\nE       AssertionError: Expected 'load' to be called once. Called 0 times.\n\n/testbed/tests/test_main.py:90: AssertionError"}, "teardown": {"duration": 0.00043295800000020535, "outcome": "passed"}}, {"nodeid": "tests/test_main.py::test_custom_replay_file", "lineno": 95, "outcome": "failed", "keywords": ["test_custom_replay_file", "test_main.py", "tests", "testbed", ""], "setup": {"duration": 0.0019738949999998923, "outcome": "passed"}, "call": {"duration": 0.002183868999999561, "outcome": "failed", "crash": {"path": "/testbed/tests/test_main.py", "lineno": 113, "message": "AssertionError: Expected 'load' to be called once. Called 0 times."}, "traceback": [{"path": "/testbed/tests/test_main.py", "lineno": 113, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='load' id='139823935411520'>\nargs = ('.', 'custom-replay-file'), kwargs = {}\nmsg = \"Expected 'load' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'load' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c03ee30>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c005630>\nuser_config_file = '/tmp/pytest-of-root/pytest-0/user_dir0/config'\n\n    def test_custom_replay_file(monkeypatch, mocker, user_config_file):\n        \"\"\"Check that reply.load is called with the custom replay_file.\"\"\"\n        monkeypatch.chdir('tests/fake-repo-tmpl')\n    \n        mock_replay_load = mocker.patch('cookiecutter.main.load')\n        mocker.patch('cookiecutter.main.generate_context').return_value = {\n            'cookiecutter': {}\n        }\n        mocker.patch('cookiecutter.main.generate_files')\n        mocker.patch('cookiecutter.main.dump')\n    \n        cookiecutter(\n            '.',\n            replay='./custom-replay-file',\n            config_file=user_config_file,\n        )\n    \n>       mock_replay_load.assert_called_once_with(\n            '.',\n            'custom-replay-file',\n        )\nE       AssertionError: Expected 'load' to be called once. Called 0 times.\n\n/testbed/tests/test_main.py:113: AssertionError"}, "teardown": {"duration": 0.0004689019999997157, "outcome": "passed"}}, {"nodeid": "tests/test_output_folder.py::test_output_folder", "lineno": 23, "outcome": "failed", "keywords": ["test_output_folder", "usefixtures", "pytestmark", "test_output_folder.py", "tests", "testbed", ""], "setup": {"duration": 0.002234249999999882, "outcome": "passed"}, "call": {"duration": 0.00031064899999933004, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError: [Errno 2] No such file or directory: 'output_folder/something.txt'"}, "traceback": [{"path": "tests/test_output_folder.py", "lineno": 36, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1134, "message": "in read_text"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 1119, "message": "FileNotFoundError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_output_folder')\n    def test_output_folder():\n        \"\"\"Tests should correctly create content, as output_folder does not yet exist.\"\"\"\n        context = generate.generate_context(\n            context_file='tests/test-output-folder/cookiecutter.json'\n        )\n        generate.generate_files(context=context, repo_dir='tests/test-output-folder')\n    \n        something = \"\"\"Hi!\n    My name is Audrey Greenfeld.\n    It is 2014.\n    \"\"\"\n>       something2 = Path('output_folder/something.txt').read_text()\n\ntests/test_output_folder.py:36: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:1134: in read_text\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('output_folder/something.txt'), mode = 'r', buffering = -1\nencoding = 'locale', errors = None, newline = None\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return self._accessor.open(self, mode, buffering, encoding, errors,\n                                   newline)\nE       FileNotFoundError: [Errno 2] No such file or directory: 'output_folder/something.txt'\n\n/usr/lib/python3.10/pathlib.py:1119: FileNotFoundError"}, "teardown": {"duration": 0.0004816270000000955, "outcome": "passed"}}, {"nodeid": "tests/test_output_folder.py::test_exception_when_output_folder_exists", "lineno": 46, "outcome": "failed", "keywords": ["test_exception_when_output_folder_exists", "usefixtures", "pytestmark", "test_output_folder.py", "tests", "testbed", ""], "setup": {"duration": 0.002126366999999796, "outcome": "passed"}, "call": {"duration": 0.0002715369999997108, "outcome": "failed", "crash": {"path": "/testbed/tests/test_output_folder.py", "lineno": 53, "message": "TypeError: 'NoneType' object is not subscriptable"}, "traceback": [{"path": "tests/test_output_folder.py", "lineno": 53, "message": "TypeError"}], "longrepr": "@pytest.mark.usefixtures('clean_system', 'remove_output_folder')\n    def test_exception_when_output_folder_exists():\n        \"\"\"Tests should raise error as output folder created before `generate_files`.\"\"\"\n        context = generate.generate_context(\n            context_file='tests/test-output-folder/cookiecutter.json'\n        )\n>       output_folder = context['cookiecutter']['test_name']\nE       TypeError: 'NoneType' object is not subscriptable\n\ntests/test_output_folder.py:53: TypeError"}, "teardown": {"duration": 0.00044878100000023124, "outcome": "passed"}}, {"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_python_hook", "lineno": 24, "outcome": "failed", "keywords": ["test_run_pre_prompt_python_hook", "test_pre_prompt_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0018517369999999644, "outcome": "passed"}, "call": {"duration": 0.00025715399999981514, "outcome": "failed", "crash": {"path": "/testbed/tests/test_pre_prompt_hooks.py", "lineno": 28, "message": "AttributeError: 'NoneType' object has no attribute 'exists'"}, "traceback": [{"path": "tests/test_pre_prompt_hooks.py", "lineno": 28, "message": "AttributeError"}], "longrepr": "remove_tmp_repo_dir = <function remove_tmp_repo_dir.<locals>._func at 0x7f2b4bfefb50>\n\n    def test_run_pre_prompt_python_hook(remove_tmp_repo_dir):\n        \"\"\"Verify pre_prompt.py runs and creates a copy of cookiecutter.json.\"\"\"\n        new_repo_dir = hooks.run_pre_prompt_hook(repo_dir='tests/test-pyhooks/')\n>       assert new_repo_dir.exists()\nE       AttributeError: 'NoneType' object has no attribute 'exists'\n\ntests/test_pre_prompt_hooks.py:28: AttributeError"}, "teardown": {"duration": 0.0003487260000003545, "outcome": "passed"}}, {"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_python_hook_fail", "lineno": 33, "outcome": "failed", "keywords": ["test_run_pre_prompt_python_hook_fail", "test_pre_prompt_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.0017490819999999019, "outcome": "passed"}, "call": {"duration": 0.0003049200000004859, "outcome": "failed", "crash": {"path": "/testbed/tests/test_pre_prompt_hooks.py", "lineno": 39, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>"}, "traceback": [{"path": "tests/test_pre_prompt_hooks.py", "lineno": 39, "message": "Failed"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c092e60>\n\n    def test_run_pre_prompt_python_hook_fail(monkeypatch):\n        \"\"\"Verify pre_prompt.py will fail when a given env var is present.\"\"\"\n        message = 'Pre-Prompt Hook script failed'\n        with monkeypatch.context() as m:\n            m.setenv('COOKIECUTTER_FAIL_PRE_PROMPT', '1')\n>           with pytest.raises(FailedHookException) as excinfo:\nE           Failed: DID NOT RAISE <class 'cookiecutter.exceptions.FailedHookException'>\n\ntests/test_pre_prompt_hooks.py:39: Failed"}, "teardown": {"duration": 0.0003451299999994717, "outcome": "passed"}}, {"nodeid": "tests/test_pre_prompt_hooks.py::test_run_pre_prompt_shell_hook", "lineno": 43, "outcome": "failed", "keywords": ["test_run_pre_prompt_shell_hook", "skipif", "pytestmark", "test_pre_prompt_hooks.py", "tests", "testbed", ""], "setup": {"duration": 0.001830029999999816, "outcome": "passed"}, "call": {"duration": 0.0002555790000000613, "outcome": "failed", "crash": {"path": "/testbed/tests/test_pre_prompt_hooks.py", "lineno": 48, "message": "AttributeError: 'NoneType' object has no attribute 'exists'"}, "traceback": [{"path": "tests/test_pre_prompt_hooks.py", "lineno": 48, "message": "AttributeError"}], "longrepr": "remove_tmp_repo_dir = <function remove_tmp_repo_dir.<locals>._func at 0x7f2b4bfed090>\n\n    @pytest.mark.skipif(WINDOWS, reason='shell script will not run in Windows')\n    def test_run_pre_prompt_shell_hook(remove_tmp_repo_dir):\n        \"\"\"Verify pre_prompt.sh runs and creates a copy of cookiecutter.json.\"\"\"\n        new_repo_dir = hooks.run_pre_prompt_hook(repo_dir='tests/test-pyshellhooks/')\n>       assert new_repo_dir.exists()\nE       AttributeError: 'NoneType' object has no attribute 'exists'\n\ntests/test_pre_prompt_hooks.py:48: AttributeError"}, "teardown": {"duration": 0.00036560999999935007, "outcome": "passed"}}, {"nodeid": "tests/test_preferred_encoding.py::test_not_ascii", "lineno": 11, "outcome": "passed", "keywords": ["test_not_ascii", "skipif", "pytestmark", "test_preferred_encoding.py", "tests", "testbed", ""], "setup": {"duration": 0.001763003999999846, "outcome": "passed"}, "call": {"duration": 0.00024045100000069652, "outcome": "passed"}, "teardown": {"duration": 0.00031934200000005575, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[1-1]", "lineno": 24, "outcome": "failed", "keywords": ["test_convert_to_str[1-1]", "parametrize", "pytestmark", "1-1", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001989368000000269, "outcome": "passed"}, "call": {"duration": 0.00026674499999934653, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 37, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c500f40>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c892320>\nraw_var = 1, rendered_var = '1'\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            (1, '1'),\n            (True, True),\n            ('foo', 'foo'),\n            ('{{cookiecutter.project}}', 'foobar'),\n            (None, None),\n        ],\n    )\n    def test_convert_to_str(self, mocker, raw_var, rendered_var):\n        \"\"\"Verify simple items correctly rendered to strings.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4cc01cf0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004250289999996326, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[True-True]", "lineno": 24, "outcome": "failed", "keywords": ["test_convert_to_str[True-True]", "parametrize", "pytestmark", "True-True", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002037327000000033, "outcome": "passed"}, "call": {"duration": 0.0002711169999995988, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 37, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55fdc0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce9ce80>\nraw_var = True, rendered_var = True\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            (1, '1'),\n            (True, True),\n            ('foo', 'foo'),\n            ('{{cookiecutter.project}}', 'foobar'),\n            (None, None),\n        ],\n    )\n    def test_convert_to_str(self, mocker, raw_var, rendered_var):\n        \"\"\"Verify simple items correctly rendered to strings.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4ce9e050>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00042989800000015066, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[foo-foo]", "lineno": 24, "outcome": "failed", "keywords": ["test_convert_to_str[foo-foo]", "parametrize", "pytestmark", "foo-foo", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002021470000000747, "outcome": "passed"}, "call": {"duration": 0.0002693969999993939, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 37, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55fb20>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c0915d0>\nraw_var = 'foo', rendered_var = 'foo'\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            (1, '1'),\n            (True, True),\n            ('foo', 'foo'),\n            ('{{cookiecutter.project}}', 'foobar'),\n            (None, None),\n        ],\n    )\n    def test_convert_to_str(self, mocker, raw_var, rendered_var):\n        \"\"\"Verify simple items correctly rendered to strings.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c0936d0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004158430000007485, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[{{cookiecutter.project}}-foobar]", "lineno": 24, "outcome": "failed", "keywords": ["test_convert_to_str[{{cookiecutter.project}}-foobar]", "parametrize", "pytestmark", "{{cookiecutter.project}}-foobar", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001965098999999526, "outcome": "passed"}, "call": {"duration": 0.0002667129999993634, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 37, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55fa60>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3f11e0>\nraw_var = '{{cookiecutter.project}}', rendered_var = 'foobar'\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            (1, '1'),\n            (True, True),\n            ('foo', 'foo'),\n            ('{{cookiecutter.project}}', 'foobar'),\n            (None, None),\n        ],\n    )\n    def test_convert_to_str(self, mocker, raw_var, rendered_var):\n        \"\"\"Verify simple items correctly rendered to strings.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3f1180>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00042273200000053635, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str[None-None]", "lineno": 24, "outcome": "failed", "keywords": ["test_convert_to_str[None-None]", "parametrize", "pytestmark", "None-None", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002000545999999659, "outcome": "passed"}, "call": {"duration": 0.00028264899999985715, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 37, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55fac0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3da830>\nraw_var = None, rendered_var = None\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            (1, '1'),\n            (True, True),\n            ('foo', 'foo'),\n            ('{{cookiecutter.project}}', 'foobar'),\n            (None, None),\n        ],\n    )\n    def test_convert_to_str(self, mocker, raw_var, rendered_var):\n        \"\"\"Verify simple items correctly rendered to strings.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3d9ba0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004035099999999403, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var0-rendered_var0]", "lineno": 53, "outcome": "failed", "keywords": ["test_convert_to_str_complex_variables[raw_var0-rendered_var0]", "parametrize", "pytestmark", "raw_var0-rendered_var0", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001907759999999925, "outcome": "passed"}, "call": {"duration": 0.0002701590000002696, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 67, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55ffd0>\nraw_var = {1: True, 'foo': False}, rendered_var = {'1': True, 'foo': False}\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            ({1: True, 'foo': False}, {'1': True, 'foo': False}),\n            (\n                {'{{cookiecutter.project}}': ['foo', 1], 'bar': False},\n                {'foobar': ['foo', '1'], 'bar': False},\n            ),\n            (['foo', '{{cookiecutter.project}}', None], ['foo', 'foobar', None]),\n        ],\n    )\n    def test_convert_to_str_complex_variables(self, raw_var, rendered_var):\n        \"\"\"Verify tree items correctly rendered.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:67: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3f0df0>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00040694899999937917, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var1-rendered_var1]", "lineno": 53, "outcome": "failed", "keywords": ["test_convert_to_str_complex_variables[raw_var1-rendered_var1]", "parametrize", "pytestmark", "raw_var1-rendered_var1", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019306229999997981, "outcome": "passed"}, "call": {"duration": 0.0002756750000001418, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 67, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55f730>\nraw_var = {'bar': False, '{{cookiecutter.project}}': ['foo', 1]}\nrendered_var = {'bar': False, 'foobar': ['foo', '1']}\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            ({1: True, 'foo': False}, {'1': True, 'foo': False}),\n            (\n                {'{{cookiecutter.project}}': ['foo', 1], 'bar': False},\n                {'foobar': ['foo', '1'], 'bar': False},\n            ),\n            (['foo', '{{cookiecutter.project}}', None], ['foo', 'foobar', None]),\n        ],\n    )\n    def test_convert_to_str_complex_variables(self, raw_var, rendered_var):\n        \"\"\"Verify tree items correctly rendered.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:67: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c025330>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.00039638500000016563, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestRenderVariable::test_convert_to_str_complex_variables[raw_var2-rendered_var2]", "lineno": 53, "outcome": "failed", "keywords": ["test_convert_to_str_complex_variables[raw_var2-rendered_var2]", "parametrize", "pytestmark", "raw_var2-rendered_var2", "TestRenderVariable", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019164310000006068, "outcome": "passed"}, "call": {"duration": 0.0002679639999998429, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 67, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestRenderVariable object at 0x7f2b4c55ef20>\nraw_var = ['foo', '{{cookiecutter.project}}', None]\nrendered_var = ['foo', 'foobar', None]\n\n    @pytest.mark.parametrize(\n        'raw_var, rendered_var',\n        [\n            ({1: True, 'foo': False}, {'1': True, 'foo': False}),\n            (\n                {'{{cookiecutter.project}}': ['foo', 1], 'bar': False},\n                {'foobar': ['foo', '1'], 'bar': False},\n            ),\n            (['foo', '{{cookiecutter.project}}', None], ['foo', 'foobar', None]),\n        ],\n    )\n    def test_convert_to_str_complex_variables(self, raw_var, rendered_var):\n        \"\"\"Verify tree items correctly rendered.\"\"\"\n>       env = environment.StrictEnvironment()\n\ntests/test_prompt.py:67: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c3f1540>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004011830000001382, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config[ASCII default prompt/input]", "lineno": 76, "outcome": "failed", "keywords": ["test_prompt_for_config[ASCII default prompt/input]", "parametrize", "pytestmark", "ASCII default prompt/input", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0018711260000001673, "outcome": "passed"}, "call": {"duration": 0.0004052350000005589, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 93, "message": "AssertionError: assert None == {'full_name': 'Your Name'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 93, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55f580>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c6ca8c0>\ncontext = {'cookiecutter': {'full_name': 'Your Name'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {'cookiecutter': {'full_name': 'Your Name'}},\n            {'cookiecutter': {'full_name': '\u0158ekni \u010di napi\u0161 sv\u00e9 jm\u00e9no'}},\n        ],\n        ids=['ASCII default prompt/input', 'Unicode default prompt/input'],\n    )\n    def test_prompt_for_config(self, monkeypatch, context):\n        \"\"\"Verify `prompt_for_config` call `read_user_variable` on text request.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == context['cookiecutter']\nE       AssertionError: assert None == {'full_name': 'Your Name'}\n\ntests/test_prompt.py:93: AssertionError"}, "teardown": {"duration": 0.0003935620000001805, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config[Unicode default prompt/input]", "lineno": 76, "outcome": "failed", "keywords": ["test_prompt_for_config[Unicode default prompt/input]", "parametrize", "pytestmark", "Unicode default prompt/input", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0018612309999994636, "outcome": "passed"}, "call": {"duration": 0.0003975360000003647, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 93, "message": "AssertionError: assert None == {'full_name': '\u0158ekni \u010di napi\u0161 sv\u00e9 jm\u00e9no'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 93, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55f3a0>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3e31f0>\ncontext = {'cookiecutter': {'full_name': '\u0158ekni \u010di napi\u0161 sv\u00e9 jm\u00e9no'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {'cookiecutter': {'full_name': 'Your Name'}},\n            {'cookiecutter': {'full_name': '\u0158ekni \u010di napi\u0161 sv\u00e9 jm\u00e9no'}},\n        ],\n        ids=['ASCII default prompt/input', 'Unicode default prompt/input'],\n    )\n    def test_prompt_for_config(self, monkeypatch, context):\n        \"\"\"Verify `prompt_for_config` call `read_user_variable` on text request.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == context['cookiecutter']\nE       AssertionError: assert None == {'full_name': '\u0158ekni \u010di napi\u0161 sv\u00e9 jm\u00e9no'}\n\ntests/test_prompt.py:93: AssertionError"}, "teardown": {"duration": 0.0003934750000000875, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_prompts[ASCII default prompt/input]", "lineno": 94, "outcome": "failed", "keywords": ["test_prompt_for_config_with_human_prompts[ASCII default prompt/input]", "parametrize", "pytestmark", "ASCII default prompt/input", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0018838739999997856, "outcome": "passed"}, "call": {"duration": 0.0004237530000006373, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 128, "message": "AssertionError: assert None == {'__prompts__': {'check': 'Checking', 'full_name': 'Name please'}, 'check': ['yes', 'no'], 'full_name': 'Your Name', 'nothing': 'ok'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 128, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e320>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c832620>\ncontext = {'cookiecutter': {'__prompts__': {'check': 'Checking', 'full_name': 'Name please'}, 'check': ['yes', 'no'], 'full_name': 'Your Name', 'nothing': 'ok'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    'nothing': 'ok',\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': 'Checking',\n                    },\n                }\n            },\n        ],\n        ids=['ASCII default prompt/input'],\n    )\n    def test_prompt_for_config_with_human_prompts(self, monkeypatch, context):\n        \"\"\"Verify call `read_user_variable` on request when human-readable prompts.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_yes_no',\n            lambda var, default, prompts, prefix: default,\n        )\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_choice',\n            lambda var, default, prompts, prefix: default,\n        )\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == context['cookiecutter']\nE       AssertionError: assert None == {'__prompts__': {'check': 'Checking', 'full_name': 'Name please'}, 'check': ['yes', 'no'], 'full_name': 'Your Name', 'nothing': 'ok'}\n\ntests/test_prompt.py:128: AssertionError"}, "teardown": {"duration": 0.00039026700000022174, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context0]", "lineno": 129, "outcome": "failed", "keywords": ["test_prompt_for_config_with_human_choices[context0]", "parametrize", "pytestmark", "context0", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001902772000000219, "outcome": "passed"}, "call": {"duration": 0.0003502499999994413, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 170, "message": "TypeError: 'NoneType' object is not iterable"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 170, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e080>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c19dc90>\ncontext = {'cookiecutter': {'__prompts__': {'check': 'Checking'}, 'check': ['yes', 'no'], 'full_name': 'Your Name'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'check': 'Checking',\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'__prompt__': 'Checking', 'yes': 'Yes', 'no': 'No'},\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'no': 'No'},\n                    },\n                }\n            },\n        ],\n    )\n    def test_prompt_for_config_with_human_choices(self, monkeypatch, context):\n        \"\"\"Test prompts when human-readable labels for user choices.\"\"\"\n        runner = click.testing.CliRunner()\n        with runner.isolation(input=\"\\n\\n\\n\"):\n            cookiecutter_dict = prompt.prompt_for_config(context)\n    \n>       assert dict(cookiecutter_dict) == {'full_name': 'Your Name', 'check': 'yes'}\nE       TypeError: 'NoneType' object is not iterable\n\ntests/test_prompt.py:170: TypeError"}, "teardown": {"duration": 0.00041976900000051387, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context1]", "lineno": 129, "outcome": "failed", "keywords": ["test_prompt_for_config_with_human_choices[context1]", "parametrize", "pytestmark", "context1", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019494959999999395, "outcome": "passed"}, "call": {"duration": 0.00033542699999955516, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 170, "message": "TypeError: 'NoneType' object is not iterable"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 170, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55dea0>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3da500>\ncontext = {'cookiecutter': {'__prompts__': {'check': {'__prompt__': 'Checking', 'no': 'No', 'yes': 'Yes'}, 'full_name': 'Name please'}, 'check': ['yes', 'no'], 'full_name': 'Your Name'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'check': 'Checking',\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'__prompt__': 'Checking', 'yes': 'Yes', 'no': 'No'},\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'no': 'No'},\n                    },\n                }\n            },\n        ],\n    )\n    def test_prompt_for_config_with_human_choices(self, monkeypatch, context):\n        \"\"\"Test prompts when human-readable labels for user choices.\"\"\"\n        runner = click.testing.CliRunner()\n        with runner.isolation(input=\"\\n\\n\\n\"):\n            cookiecutter_dict = prompt.prompt_for_config(context)\n    \n>       assert dict(cookiecutter_dict) == {'full_name': 'Your Name', 'check': 'yes'}\nE       TypeError: 'NoneType' object is not iterable\n\ntests/test_prompt.py:170: TypeError"}, "teardown": {"duration": 0.0004179340000005638, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_with_human_choices[context2]", "lineno": 129, "outcome": "failed", "keywords": ["test_prompt_for_config_with_human_choices[context2]", "parametrize", "pytestmark", "context2", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0020887719999995724, "outcome": "passed"}, "call": {"duration": 0.0003470330000006072, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 170, "message": "TypeError: 'NoneType' object is not iterable"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 170, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e1d0>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3f0f70>\ncontext = {'cookiecutter': {'__prompts__': {'check': {'no': 'No'}, 'full_name': 'Name please'}, 'check': ['yes', 'no'], 'full_name': 'Your Name'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        [\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'check': 'Checking',\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'__prompt__': 'Checking', 'yes': 'Yes', 'no': 'No'},\n                    },\n                }\n            },\n            {\n                'cookiecutter': {\n                    'full_name': 'Your Name',\n                    'check': ['yes', 'no'],\n                    '__prompts__': {\n                        'full_name': 'Name please',\n                        'check': {'no': 'No'},\n                    },\n                }\n            },\n        ],\n    )\n    def test_prompt_for_config_with_human_choices(self, monkeypatch, context):\n        \"\"\"Test prompts when human-readable labels for user choices.\"\"\"\n        runner = click.testing.CliRunner()\n        with runner.isolation(input=\"\\n\\n\\n\"):\n            cookiecutter_dict = prompt.prompt_for_config(context)\n    \n>       assert dict(cookiecutter_dict) == {'full_name': 'Your Name', 'check': 'yes'}\nE       TypeError: 'NoneType' object is not iterable\n\ntests/test_prompt.py:170: TypeError"}, "teardown": {"duration": 0.0003880250000003471, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_config_dict", "lineno": 171, "outcome": "failed", "keywords": ["test_prompt_for_config_dict", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002016075000000228, "outcome": "passed"}, "call": {"duration": 0.0004023770000003424, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 181, "message": "AssertionError: assert None == {'details': {'integer': 37, 'key': 'value'}}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 181, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e9e0>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3dd360>\n\n    def test_prompt_for_config_dict(self, monkeypatch):\n        \"\"\"Verify `prompt_for_config` call `read_user_variable` on dict request.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_dict',\n            lambda var, default, prompts, prefix: {\"key\": \"value\", \"integer\": 37},\n        )\n        context = {'cookiecutter': {'details': {}}}\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == {'details': {'key': 'value', 'integer': 37}}\nE       AssertionError: assert None == {'details': {'integer': 37, 'key': 'value'}}\n\ntests/test_prompt.py:181: AssertionError"}, "teardown": {"duration": 0.00039736899999986974, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_dict", "lineno": 182, "outcome": "failed", "keywords": ["test_should_render_dict", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019335140000000806, "outcome": "passed"}, "call": {"duration": 0.00039450899999948774, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 195, "message": "AssertionError: assert None == {'details': {'Slartibartfast': 'Slartibartfast'}, 'project_name': 'Slartibartfast'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 195, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e770>\n\n    def test_should_render_dict(self):\n        \"\"\"Verify template inside dictionary variable rendered.\"\"\"\n        context = {\n            'cookiecutter': {\n                'project_name': 'Slartibartfast',\n                'details': {\n                    '{{cookiecutter.project_name}}': '{{cookiecutter.project_name}}'\n                },\n            }\n        }\n    \n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == {\n            'project_name': 'Slartibartfast',\n            'details': {'Slartibartfast': 'Slartibartfast'},\n        }\nE       AssertionError: assert None == {'details': {'Slartibartfast': 'Slartibartfast'}, 'project_name': 'Slartibartfast'}\n\ntests/test_prompt.py:195: AssertionError"}, "teardown": {"duration": 0.0003750140000002844, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_deep_dict", "lineno": 199, "outcome": "failed", "keywords": ["test_should_render_deep_dict", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019170440000007005, "outcome": "passed"}, "call": {"duration": 0.0004324470000005576, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 229, "message": "AssertionError: assert None == {'details': {'dict_key': {'deep_integer': '42', 'deep_key': 'deep_value', 'deep_list': ['deep value 1', 'Slartibartfast', 'deep value 3'], 'deep_other_name': 'Slartibartfast'}, 'integer_key': '37', 'key': 'value', 'list_key': ['value 1', 'Slartibartfast', 'value 3'], ...}, 'project_name': 'Slartibartfast'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 229, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e500>\n\n    def test_should_render_deep_dict(self):\n        \"\"\"Verify nested structures like dict in dict, rendered correctly.\"\"\"\n        context = {\n            'cookiecutter': {\n                'project_name': \"Slartibartfast\",\n                'details': {\n                    \"key\": \"value\",\n                    \"integer_key\": 37,\n                    \"other_name\": '{{cookiecutter.project_name}}',\n                    \"dict_key\": {\n                        \"deep_key\": \"deep_value\",\n                        \"deep_integer\": 42,\n                        \"deep_other_name\": '{{cookiecutter.project_name}}',\n                        \"deep_list\": [\n                            \"deep value 1\",\n                            \"{{cookiecutter.project_name}}\",\n                            \"deep value 3\",\n                        ],\n                    },\n                    \"list_key\": [\n                        \"value 1\",\n                        \"{{cookiecutter.project_name}}\",\n                        \"value 3\",\n                    ],\n                },\n            }\n        }\n    \n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == {\n            'project_name': \"Slartibartfast\",\n            'details': {\n                \"key\": \"value\",\n                \"integer_key\": \"37\",\n                \"other_name\": \"Slartibartfast\",\n                \"dict_key\": {\n                    \"deep_key\": \"deep_value\",\n                    \"deep_integer\": \"42\",\n                    \"deep_other_name\": \"Slartibartfast\",\n                    \"deep_list\": [\"deep value 1\", \"Slartibartfast\", \"deep value 3\"],\n                },\n                \"list_key\": [\"value 1\", \"Slartibartfast\", \"value 3\"],\n            },\n        }\nE       AssertionError: assert None == {'details': {'dict_key': {'deep_integer': '42', 'deep_key': 'deep_value', 'deep_list': ['deep value 1', 'Slartibartfast', 'deep value 3'], 'deep_other_name': 'Slartibartfast'}, 'integer_key': '37', 'key': 'value', 'list_key': ['value 1', 'Slartibartfast', 'value 3'], ...}, 'project_name': 'Slartibartfast'}\n\ntests/test_prompt.py:229: AssertionError"}, "teardown": {"duration": 0.00037899299999999414, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_deep_dict_with_human_prompts", "lineno": 244, "outcome": "failed", "keywords": ["test_should_render_deep_dict_with_human_prompts", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019471969999997896, "outcome": "passed"}, "call": {"duration": 0.0003927180000005137, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 262, "message": "AssertionError: assert None == {'details': {'dict_key': {'deep_key': 'deep_value'}, 'integer_key': '37', 'key': 'value', 'other_name': 'Slartibartfast'}, 'project_name': 'Slartibartfast'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 262, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e4a0>\n\n    def test_should_render_deep_dict_with_human_prompts(self):\n        \"\"\"Verify dict rendered correctly when human-readable prompts.\"\"\"\n        context = {\n            'cookiecutter': {\n                'project_name': \"Slartibartfast\",\n                'details': {\n                    \"key\": \"value\",\n                    \"integer_key\": 37,\n                    \"other_name\": '{{cookiecutter.project_name}}',\n                    \"dict_key\": {\n                        \"deep_key\": \"deep_value\",\n                    },\n                },\n                '__prompts__': {'project_name': 'Project name'},\n            }\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == {\n            'project_name': \"Slartibartfast\",\n            'details': {\n                \"key\": \"value\",\n                \"integer_key\": \"37\",\n                \"other_name\": \"Slartibartfast\",\n                \"dict_key\": {\n                    \"deep_key\": \"deep_value\",\n                },\n            },\n        }\nE       AssertionError: assert None == {'details': {'dict_key': {'deep_key': 'deep_value'}, 'integer_key': '37', 'key': 'value', 'other_name': 'Slartibartfast'}, 'project_name': 'Slartibartfast'}\n\ntests/test_prompt.py:262: AssertionError"}, "teardown": {"duration": 0.00038274699999973905, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_internal_use_no_human_prompts", "lineno": 273, "outcome": "failed", "keywords": ["test_internal_use_no_human_prompts", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019213809999998333, "outcome": "passed"}, "call": {"duration": 0.0003882510000003947, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 283, "message": "AssertionError: assert None == {'project_name': 'Slartibartfast'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 283, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55f6a0>\n\n    def test_internal_use_no_human_prompts(self):\n        \"\"\"Verify dict rendered correctly when human-readable prompts empty.\"\"\"\n        context = {\n            'cookiecutter': {\n                'project_name': \"Slartibartfast\",\n                '__prompts__': {},\n            }\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == {\n            'project_name': \"Slartibartfast\",\n        }\nE       AssertionError: assert None == {'project_name': 'Slartibartfast'}\n\ntests/test_prompt.py:283: AssertionError"}, "teardown": {"duration": 0.0003685060000000462, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_prompt_for_templated_config", "lineno": 286, "outcome": "failed", "keywords": ["test_prompt_for_templated_config", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0018879800000002334, "outcome": "passed"}, "call": {"duration": 0.0003966819999998705, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 310, "message": "AssertionError: assert None == {'pkg_name': 'anewproject', 'project_name': 'A New Project'}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 310, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55e440>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c01c880>\n\n    def test_prompt_for_templated_config(self, monkeypatch):\n        \"\"\"Verify Jinja2 templating works in unicode prompts.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default, prompts, prefix: default,\n        )\n        context = {\n            'cookiecutter': OrderedDict(\n                [\n                    ('project_name', 'A New Project'),\n                    (\n                        'pkg_name',\n                        '{{ cookiecutter.project_name|lower|replace(\" \", \"\") }}',\n                    ),\n                ]\n            )\n        }\n    \n        exp_cookiecutter_dict = {\n            'project_name': 'A New Project',\n            'pkg_name': 'anewproject',\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == exp_cookiecutter_dict\nE       AssertionError: assert None == {'pkg_name': 'anewproject', 'project_name': 'A New Project'}\n\ntests/test_prompt.py:310: AssertionError"}, "teardown": {"duration": 0.0003675529999993543, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_dont_prompt_for_private_context_var", "lineno": 311, "outcome": "failed", "keywords": ["test_dont_prompt_for_private_context_var", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001888190999999928, "outcome": "passed"}, "call": {"duration": 0.0003855120000002543, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 322, "message": "AssertionError: assert None == {'_copy_without_render': ['*.html']}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 322, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55c970>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f2b4c3e4910>\n\n    def test_dont_prompt_for_private_context_var(self, monkeypatch):\n        \"\"\"Verify `read_user_variable` not called for private context variables.\"\"\"\n        monkeypatch.setattr(\n            'cookiecutter.prompt.read_user_variable',\n            lambda var, default: pytest.fail(\n                'Should not try to read a response for private context var'\n            ),\n        )\n        context = {'cookiecutter': {'_copy_without_render': ['*.html']}}\n        cookiecutter_dict = prompt.prompt_for_config(context)\n>       assert cookiecutter_dict == {'_copy_without_render': ['*.html']}\nE       AssertionError: assert None == {'_copy_without_render': ['*.html']}\n\ntests/test_prompt.py:322: AssertionError"}, "teardown": {"duration": 0.00036905100000073077, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_render_private_variables_with_two_underscores", "lineno": 323, "outcome": "failed", "keywords": ["test_should_render_private_variables_with_two_underscores", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0018865989999996557, "outcome": "passed"}, "call": {"duration": 0.00045715999999984547, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 348, "message": "AssertionError: assert None == OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ('__rendered_hidden_foo', 'hello world'), ('__rendered_hidden_bar', '123')])\n +  where OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ('__rendered_hidden_foo', 'hello world'), ('__rendered_hidden_bar', '123')]) = OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ...])"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 348, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55c340>\n\n    def test_should_render_private_variables_with_two_underscores(self):\n        \"\"\"Test rendering of private variables with two underscores.\n    \n        There are three cases:\n        1. Variables beginning with a single underscore are private and not rendered.\n        2. Variables beginning with a double underscore are private and are rendered.\n        3. Variables beginning with anything other than underscores are not private and\n           are rendered.\n        \"\"\"\n        context = {\n            'cookiecutter': OrderedDict(\n                [\n                    ('foo', 'Hello world'),\n                    ('bar', 123),\n                    ('rendered_foo', '{{ cookiecutter.foo|lower }}'),\n                    ('rendered_bar', 123),\n                    ('_hidden_foo', '{{ cookiecutter.foo|lower }}'),\n                    ('_hidden_bar', 123),\n                    ('__rendered_hidden_foo', '{{ cookiecutter.foo|lower }}'),\n                    ('__rendered_hidden_bar', 123),\n                ]\n            )\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == OrderedDict(\n            [\n                ('foo', 'Hello world'),\n                ('bar', '123'),\n                ('rendered_foo', 'hello world'),\n                ('rendered_bar', '123'),\n                ('_hidden_foo', '{{ cookiecutter.foo|lower }}'),\n                ('_hidden_bar', 123),\n                ('__rendered_hidden_foo', 'hello world'),\n                ('__rendered_hidden_bar', '123'),\n            ]\n        )\nE       AssertionError: assert None == OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ('__rendered_hidden_foo', 'hello world'), ('__rendered_hidden_bar', '123')])\nE        +  where OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ('__rendered_hidden_foo', 'hello world'), ('__rendered_hidden_bar', '123')]) = OrderedDict([('foo', 'Hello world'), ('bar', '123'), ('rendered_foo', 'hello world'), ('rendered_bar', '123'), ('_hidden_foo', '{{ cookiecutter.foo|lower }}'), ('_hidden_bar', 123), ...])\n\ntests/test_prompt.py:348: AssertionError"}, "teardown": {"duration": 0.00038847799999963684, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPrompt::test_should_not_render_private_variables", "lineno": 360, "outcome": "failed", "keywords": ["test_should_not_render_private_variables", "TestPrompt", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001842081000000384, "outcome": "passed"}, "call": {"duration": 0.00037309500000048956, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 377, "message": "AssertionError: assert None == {'_skip_boolean': True, '_skip_float': 123.25, '_skip_integer': 123, '_skip_jinja_template': '{{cookiecutter.project_name}}', ...}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 377, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestPrompt object at 0x7f2b4c55c1f0>\n\n    def test_should_not_render_private_variables(self):\n        \"\"\"Verify private(underscored) variables not rendered by `prompt_for_config`.\n    \n        Private variables designed to be raw, same as context input.\n        \"\"\"\n        context = {\n            'cookiecutter': {\n                'project_name': 'Skip render',\n                '_skip_jinja_template': '{{cookiecutter.project_name}}',\n                '_skip_float': 123.25,\n                '_skip_integer': 123,\n                '_skip_boolean': True,\n                '_skip_nested': True,\n            }\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == context['cookiecutter']\nE       AssertionError: assert None == {'_skip_boolean': True, '_skip_float': 123.25, '_skip_integer': 123, '_skip_jinja_template': '{{cookiecutter.project_name}}', ...}\n\ntests/test_prompt.py:377: AssertionError"}, "teardown": {"duration": 0.00036960399999941274, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_invoke_read_user_choice", "lineno": 385, "outcome": "failed", "keywords": ["test_should_invoke_read_user_choice", "TestReadUserChoice", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.001964282000000317, "outcome": "passed"}, "call": {"duration": 0.0018269589999997393, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 404, "message": "AssertionError: assert False\n +  where False = <MagicMock name='prompt_choice_for_config' id='139823935991728'>.called"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 404, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestReadUserChoice object at 0x7f2b4c55dc30>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c090cd0>\n\n    def test_should_invoke_read_user_choice(self, mocker):\n        \"\"\"Verify correct function called for select(list) variables.\"\"\"\n        prompt_choice = mocker.patch(\n            'cookiecutter.prompt.prompt_choice_for_config',\n            wraps=prompt.prompt_choice_for_config,\n        )\n    \n        read_user_choice = mocker.patch('cookiecutter.prompt.read_user_choice')\n        read_user_choice.return_value = 'all'\n    \n        read_user_variable = mocker.patch('cookiecutter.prompt.read_user_variable')\n    \n        choices = ['landscape', 'portrait', 'all']\n        context = {'cookiecutter': {'orientation': choices}}\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n    \n        assert not read_user_variable.called\n>       assert prompt_choice.called\nE       AssertionError: assert False\nE        +  where False = <MagicMock name='prompt_choice_for_config' id='139823935991728'>.called\n\ntests/test_prompt.py:404: AssertionError"}, "teardown": {"duration": 0.0004137740000000889, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_invoke_read_user_variable", "lineno": 409, "outcome": "failed", "keywords": ["test_should_invoke_read_user_variable", "TestReadUserChoice", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019930679999999867, "outcome": "passed"}, "call": {"duration": 0.001745820000000009, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 425, "message": "AssertionError: Expected 'read_user_variable' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 425, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='read_user_variable' id='139823940176960'>\nargs = ('full_name', 'Your Name', {}, '  [dim][1/1][/] '), kwargs = {}\nmsg = \"Expected 'read_user_variable' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'read_user_variable' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_prompt.TestReadUserChoice object at 0x7f2b4c55caf0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c490760>\n\n    def test_should_invoke_read_user_variable(self, mocker):\n        \"\"\"Verify correct function called for string input variables.\"\"\"\n        read_user_variable = mocker.patch('cookiecutter.prompt.read_user_variable')\n        read_user_variable.return_value = 'Audrey Roy'\n    \n        prompt_choice = mocker.patch('cookiecutter.prompt.prompt_choice_for_config')\n    \n        read_user_choice = mocker.patch('cookiecutter.prompt.read_user_choice')\n    \n        context = {'cookiecutter': {'full_name': 'Your Name'}}\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n    \n        assert not prompt_choice.called\n        assert not read_user_choice.called\n>       read_user_variable.assert_called_once_with(\n            'full_name', 'Your Name', {}, DEFAULT_PREFIX\n        )\nE       AssertionError: Expected 'read_user_variable' to be called once. Called 0 times.\n\ntests/test_prompt.py:425: AssertionError"}, "teardown": {"duration": 0.0004559379999999891, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserChoice::test_should_render_choices", "lineno": 429, "outcome": "failed", "keywords": ["test_should_render_choices", "TestReadUserChoice", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002166670999999454, "outcome": "passed"}, "call": {"duration": 0.0013869780000002052, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 462, "message": "AssertionError: Expected 'read_user_variable' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 462, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='read_user_variable' id='139823935333488'>\nargs = ('project_name', 'A New Project', {}, '  [dim][1/2][/] '), kwargs = {}\nmsg = \"Expected 'read_user_variable' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'read_user_variable' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_prompt.TestReadUserChoice object at 0x7f2b4c55c820>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4be54400>\n\n    def test_should_render_choices(self, mocker):\n        \"\"\"Verify Jinja2 templating engine works inside choices variables.\"\"\"\n        read_user_choice = mocker.patch('cookiecutter.prompt.read_user_choice')\n        read_user_choice.return_value = 'anewproject'\n    \n        read_user_variable = mocker.patch('cookiecutter.prompt.read_user_variable')\n        read_user_variable.return_value = 'A New Project'\n    \n        rendered_choices = ['foo', 'anewproject', 'bar']\n    \n        context = {\n            'cookiecutter': OrderedDict(\n                [\n                    ('project_name', 'A New Project'),\n                    (\n                        'pkg_name',\n                        [\n                            'foo',\n                            '{{ cookiecutter.project_name|lower|replace(\" \", \"\") }}',\n                            'bar',\n                        ],\n                    ),\n                ]\n            )\n        }\n    \n        expected = {\n            'project_name': 'A New Project',\n            'pkg_name': 'anewproject',\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context)\n    \n>       read_user_variable.assert_called_once_with(\n            'project_name', 'A New Project', {}, '  [dim][1/2][/] '\n        )\nE       AssertionError: Expected 'read_user_variable' to be called once. Called 0 times.\n\ntests/test_prompt.py:462: AssertionError"}, "teardown": {"duration": 0.0005236350000004109, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig::test_should_return_first_option_if_no_input", "lineno": 483, "outcome": "failed", "keywords": ["test_should_return_first_option_if_no_input", "TestPromptChoiceForConfig", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0026278240000001674, "outcome": "passed"}, "call": {"duration": 0.0008790310000001966, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 492, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestPromptChoiceForConfig object at 0x7f2b4c7f6ef0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4be550f0>\nchoices = ['landscape', 'portrait', 'all']\ncontext = {'cookiecutter': {'orientation': ['landscape', 'portrait', 'all']}}\n\n    def test_should_return_first_option_if_no_input(self, mocker, choices, context):\n        \"\"\"Verify prompt_choice_for_config return first list option on no_input=True.\"\"\"\n        read_user_choice = mocker.patch('cookiecutter.prompt.read_user_choice')\n    \n        expected_choice = choices[0]\n    \n        actual_choice = prompt.prompt_choice_for_config(\n            cookiecutter_dict=context,\n>           env=environment.StrictEnvironment(),\n            key='orientation',\n            options=choices,\n            no_input=True,  # Suppress user input\n        )\n\ntests/test_prompt.py:492: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4be56a40>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.0004553400000002483, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestPromptChoiceForConfig::test_should_read_user_choice", "lineno": 500, "outcome": "failed", "keywords": ["test_should_read_user_choice", "TestPromptChoiceForConfig", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0023499300000002776, "outcome": "passed"}, "call": {"duration": 0.0008107539999997471, "outcome": "failed", "crash": {"path": "/testbed/cookiecutter/environment.py", "lineno": 28, "message": "TypeError: can only concatenate list (not \"NoneType\") to list"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 510, "message": ""}, {"path": "cookiecutter/environment.py", "lineno": 55, "message": "in __init__"}, {"path": "cookiecutter/environment.py", "lineno": 28, "message": "TypeError"}], "longrepr": "self = <tests.test_prompt.TestPromptChoiceForConfig object at 0x7f2b4c7f7070>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3ee6b0>\nchoices = ['landscape', 'portrait', 'all']\ncontext = {'cookiecutter': {'orientation': ['landscape', 'portrait', 'all']}}\n\n    def test_should_read_user_choice(self, mocker, choices, context):\n        \"\"\"Verify prompt_choice_for_config return user selection on no_input=False.\"\"\"\n        read_user_choice = mocker.patch('cookiecutter.prompt.read_user_choice')\n        read_user_choice.return_value = 'all'\n    \n        expected_choice = 'all'\n    \n        actual_choice = prompt.prompt_choice_for_config(\n            cookiecutter_dict=context,\n>           env=environment.StrictEnvironment(),\n            key='orientation',\n            options=choices,\n            no_input=False,  # Ask the user for input\n        )\n\ntests/test_prompt.py:510: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncookiecutter/environment.py:55: in __init__\n    super().__init__(undefined=StrictUndefined, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <cookiecutter.environment.StrictEnvironment object at 0x7f2b4c024640>\nkwargs = {'undefined': <class 'jinja2.runtime.StrictUndefined'>}, context = {}\ndefault_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Jinja2 Environment object while loading extensions.\n    \n        Does the following:\n    \n        1. Establishes default_extensions (currently just a Time feature)\n        2. Reads extensions set in the cookiecutter.json _extensions key.\n        3. Attempts to load the extensions. Provides useful error if fails.\n        \"\"\"\n        context = kwargs.pop('context', {})\n        default_extensions = ['cookiecutter.extensions.JsonifyExtension',\n            'cookiecutter.extensions.RandomStringExtension',\n            'cookiecutter.extensions.SlugifyExtension',\n            'cookiecutter.extensions.TimeExtension',\n            'cookiecutter.extensions.UUIDExtension']\n>       extensions = default_extensions + self._read_extensions(context)\nE       TypeError: can only concatenate list (not \"NoneType\") to list\n\ncookiecutter/environment.py:28: TypeError"}, "teardown": {"duration": 0.000444053999999916, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_should_invoke_read_user_yes_no[True]", "lineno": 521, "outcome": "failed", "keywords": ["test_should_invoke_read_user_yes_no[True]", "parametrize", "pytestmark", "True", "TestReadUserYesNo", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0021673699999995577, "outcome": "passed"}, "call": {"duration": 0.001316994999999821, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 541, "message": "AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 541, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='read_user_yes_no' id='139823940177248'>\nargs = ('run_as_docker', True, {}, '  [dim][1/1][/] '), kwargs = {}\nmsg = \"Expected 'read_user_yes_no' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_prompt.TestReadUserYesNo object at 0x7f2b4c7f79d0>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c490a60>\nrun_as_docker = True\n\n    @pytest.mark.parametrize(\n        'run_as_docker',\n        (\n            True,\n            False,\n        ),\n    )\n    def test_should_invoke_read_user_yes_no(self, mocker, run_as_docker):\n        \"\"\"Verify correct function called for boolean variables.\"\"\"\n        read_user_yes_no = mocker.patch('cookiecutter.prompt.read_user_yes_no')\n        read_user_yes_no.return_value = run_as_docker\n    \n        read_user_variable = mocker.patch('cookiecutter.prompt.read_user_variable')\n    \n        context = {'cookiecutter': {'run_as_docker': run_as_docker}}\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n    \n        assert not read_user_variable.called\n>       read_user_yes_no.assert_called_once_with(\n            'run_as_docker', run_as_docker, {}, DEFAULT_PREFIX\n        )\nE       AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times.\n\ntests/test_prompt.py:541: AssertionError"}, "teardown": {"duration": 0.0006125770000000585, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_should_invoke_read_user_yes_no[False]", "lineno": 521, "outcome": "failed", "keywords": ["test_should_invoke_read_user_yes_no[False]", "parametrize", "pytestmark", "False", "TestReadUserYesNo", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0026215250000003465, "outcome": "passed"}, "call": {"duration": 0.0015052200000003069, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 541, "message": "AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 541, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='read_user_yes_no' id='139823936018928'>\nargs = ('run_as_docker', False, {}, '  [dim][1/1][/] '), kwargs = {}\nmsg = \"Expected 'read_user_yes_no' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_prompt.TestReadUserYesNo object at 0x7f2b4c7f7160>\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c0991b0>\nrun_as_docker = False\n\n    @pytest.mark.parametrize(\n        'run_as_docker',\n        (\n            True,\n            False,\n        ),\n    )\n    def test_should_invoke_read_user_yes_no(self, mocker, run_as_docker):\n        \"\"\"Verify correct function called for boolean variables.\"\"\"\n        read_user_yes_no = mocker.patch('cookiecutter.prompt.read_user_yes_no')\n        read_user_yes_no.return_value = run_as_docker\n    \n        read_user_variable = mocker.patch('cookiecutter.prompt.read_user_variable')\n    \n        context = {'cookiecutter': {'run_as_docker': run_as_docker}}\n    \n        cookiecutter_dict = prompt.prompt_for_config(context)\n    \n        assert not read_user_variable.called\n>       read_user_yes_no.assert_called_once_with(\n            'run_as_docker', run_as_docker, {}, DEFAULT_PREFIX\n        )\nE       AssertionError: Expected 'read_user_yes_no' to be called once. Called 0 times.\n\ntests/test_prompt.py:541: AssertionError"}, "teardown": {"duration": 0.0006117720000000659, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::TestReadUserYesNo::test_boolean_parameter_no_input", "lineno": 545, "outcome": "failed", "keywords": ["test_boolean_parameter_no_input", "TestReadUserYesNo", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002466318999999828, "outcome": "passed"}, "call": {"duration": 0.00042394699999981356, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 554, "message": "AssertionError: assert None == {'run_as_docker': True}"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 554, "message": "AssertionError"}], "longrepr": "self = <tests.test_prompt.TestReadUserYesNo object at 0x7f2b4c7f69b0>\n\n    def test_boolean_parameter_no_input(self):\n        \"\"\"Verify boolean parameter sent to prompt for config with no input.\"\"\"\n        context = {\n            'cookiecutter': {\n                'run_as_docker': True,\n            }\n        }\n        cookiecutter_dict = prompt.prompt_for_config(context, no_input=True)\n>       assert cookiecutter_dict == context['cookiecutter']\nE       AssertionError: assert None == {'run_as_docker': True}\n\ntests/test_prompt.py:554: AssertionError"}, "teardown": {"duration": 0.0003793930000002277, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict]", "lineno": 556, "outcome": "failed", "keywords": ["test_undefined_variable[Undefined variable in cookiecutter dict]", "parametrize", "pytestmark", "Undefined variable in cookiecutter dict", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0020664510000001357, "outcome": "passed"}, "call": {"duration": 0.0002881270000001379, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 574, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 574, "message": "Failed"}], "longrepr": "context = {'cookiecutter': {'foo': '{{cookiecutter.nope}}'}}\n\n    @pytest.mark.parametrize(\n        'context',\n        (\n            {'cookiecutter': {'foo': '{{cookiecutter.nope}}'}},\n            {'cookiecutter': {'foo': ['123', '{{cookiecutter.nope}}', '456']}},\n            {'cookiecutter': {'foo': {'{{cookiecutter.nope}}': 'value'}}},\n            {'cookiecutter': {'foo': {'key': '{{cookiecutter.nope}}'}}},\n        ),\n        ids=[\n            'Undefined variable in cookiecutter dict',\n            'Undefined variable in cookiecutter dict with choices',\n            'Undefined variable in cookiecutter dict with dict_key',\n            'Undefined variable in cookiecutter dict with key_value',\n        ],\n    )\n    def test_undefined_variable(context):\n        \"\"\"Verify `prompt.prompt_for_config` raises correct error.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_prompt.py:574: Failed"}, "teardown": {"duration": 0.0004063879999991471, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with choices]", "lineno": 556, "outcome": "failed", "keywords": ["test_undefined_variable[Undefined variable in cookiecutter dict with choices]", "parametrize", "pytestmark", "Undefined variable in cookiecutter dict with choices", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0021723219999998378, "outcome": "passed"}, "call": {"duration": 0.00027700100000060957, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 574, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 574, "message": "Failed"}], "longrepr": "context = {'cookiecutter': {'foo': ['123', '{{cookiecutter.nope}}', '456']}}\n\n    @pytest.mark.parametrize(\n        'context',\n        (\n            {'cookiecutter': {'foo': '{{cookiecutter.nope}}'}},\n            {'cookiecutter': {'foo': ['123', '{{cookiecutter.nope}}', '456']}},\n            {'cookiecutter': {'foo': {'{{cookiecutter.nope}}': 'value'}}},\n            {'cookiecutter': {'foo': {'key': '{{cookiecutter.nope}}'}}},\n        ),\n        ids=[\n            'Undefined variable in cookiecutter dict',\n            'Undefined variable in cookiecutter dict with choices',\n            'Undefined variable in cookiecutter dict with dict_key',\n            'Undefined variable in cookiecutter dict with key_value',\n        ],\n    )\n    def test_undefined_variable(context):\n        \"\"\"Verify `prompt.prompt_for_config` raises correct error.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_prompt.py:574: Failed"}, "teardown": {"duration": 0.00040241399999985106, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with dict_key]", "lineno": 556, "outcome": "failed", "keywords": ["test_undefined_variable[Undefined variable in cookiecutter dict with dict_key]", "parametrize", "pytestmark", "Undefined variable in cookiecutter dict with dict_key", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0021157120000001584, "outcome": "passed"}, "call": {"duration": 0.00028708199999982753, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 574, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 574, "message": "Failed"}], "longrepr": "context = {'cookiecutter': {'foo': {'{{cookiecutter.nope}}': 'value'}}}\n\n    @pytest.mark.parametrize(\n        'context',\n        (\n            {'cookiecutter': {'foo': '{{cookiecutter.nope}}'}},\n            {'cookiecutter': {'foo': ['123', '{{cookiecutter.nope}}', '456']}},\n            {'cookiecutter': {'foo': {'{{cookiecutter.nope}}': 'value'}}},\n            {'cookiecutter': {'foo': {'key': '{{cookiecutter.nope}}'}}},\n        ),\n        ids=[\n            'Undefined variable in cookiecutter dict',\n            'Undefined variable in cookiecutter dict with choices',\n            'Undefined variable in cookiecutter dict with dict_key',\n            'Undefined variable in cookiecutter dict with key_value',\n        ],\n    )\n    def test_undefined_variable(context):\n        \"\"\"Verify `prompt.prompt_for_config` raises correct error.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_prompt.py:574: Failed"}, "teardown": {"duration": 0.0003899210000000153, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_undefined_variable[Undefined variable in cookiecutter dict with key_value]", "lineno": 556, "outcome": "failed", "keywords": ["test_undefined_variable[Undefined variable in cookiecutter dict with key_value]", "parametrize", "pytestmark", "Undefined variable in cookiecutter dict with key_value", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0021102610000003352, "outcome": "passed"}, "call": {"duration": 0.0002721829999998704, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 574, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 574, "message": "Failed"}], "longrepr": "context = {'cookiecutter': {'foo': {'key': '{{cookiecutter.nope}}'}}}\n\n    @pytest.mark.parametrize(\n        'context',\n        (\n            {'cookiecutter': {'foo': '{{cookiecutter.nope}}'}},\n            {'cookiecutter': {'foo': ['123', '{{cookiecutter.nope}}', '456']}},\n            {'cookiecutter': {'foo': {'{{cookiecutter.nope}}': 'value'}}},\n            {'cookiecutter': {'foo': {'key': '{{cookiecutter.nope}}'}}},\n        ),\n        ids=[\n            'Undefined variable in cookiecutter dict',\n            'Undefined variable in cookiecutter dict with choices',\n            'Undefined variable in cookiecutter dict with dict_key',\n            'Undefined variable in cookiecutter dict with key_value',\n        ],\n    )\n    def test_undefined_variable(context):\n        \"\"\"Verify `prompt.prompt_for_config` raises correct error.\"\"\"\n>       with pytest.raises(exceptions.UndefinedVariableInTemplate) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UndefinedVariableInTemplate'>\n\ntests/test_prompt.py:574: Failed"}, "teardown": {"duration": 0.00039128399999999175, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "lineno": 581, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates[fake-nested-templates-fake-project]", "parametrize", "pytestmark", "fake-nested-templates-fake-project", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0020255279999998876, "outcome": "passed"}, "call": {"duration": 0.0007134920000000378, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 598, "message": "AssertionError: assert None == '/testbed/tests/fake-nested-templates/fake-project'"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 598, "message": "AssertionError"}], "longrepr": "template_dir = 'fake-nested-templates'\nexpected = PosixPath('/testbed/tests/fake-nested-templates/fake-project')\n\n    @pytest.mark.parametrize(\n        \"template_dir,expected\",\n        [\n            [\"fake-nested-templates\", \"fake-project\"],\n            [\"fake-nested-templates-old-style\", \"fake-package\"],\n        ],\n    )\n    def test_cookiecutter_nested_templates(template_dir: str, expected: str):\n        \"\"\"Test nested_templates generation.\"\"\"\n        from cookiecutter import prompt\n    \n        main_dir = (Path(\"tests\") / template_dir).resolve()\n        cookiecuter_context = json.loads((main_dir / \"cookiecutter.json\").read_text())\n        context = {\"cookiecutter\": cookiecuter_context}\n        output_dir = prompt.choose_nested_template(context, main_dir, no_input=True)\n        expected = (Path(main_dir) / expected).resolve()\n>       assert output_dir == f\"{expected}\"\nE       AssertionError: assert None == '/testbed/tests/fake-nested-templates/fake-project'\n\ntests/test_prompt.py:598: AssertionError"}, "teardown": {"duration": 0.0003958160000001598, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "lineno": 581, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates[fake-nested-templates-old-style-fake-package]", "parametrize", "pytestmark", "fake-nested-templates-old-style-fake-package", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0020266800000001695, "outcome": "passed"}, "call": {"duration": 0.0006627429999994661, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 598, "message": "AssertionError: assert None == '/testbed/tests/fake-nested-templates-old-style/fake-package'"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 598, "message": "AssertionError"}], "longrepr": "template_dir = 'fake-nested-templates-old-style'\nexpected = PosixPath('/testbed/tests/fake-nested-templates-old-style/fake-package')\n\n    @pytest.mark.parametrize(\n        \"template_dir,expected\",\n        [\n            [\"fake-nested-templates\", \"fake-project\"],\n            [\"fake-nested-templates-old-style\", \"fake-package\"],\n        ],\n    )\n    def test_cookiecutter_nested_templates(template_dir: str, expected: str):\n        \"\"\"Test nested_templates generation.\"\"\"\n        from cookiecutter import prompt\n    \n        main_dir = (Path(\"tests\") / template_dir).resolve()\n        cookiecuter_context = json.loads((main_dir / \"cookiecutter.json\").read_text())\n        context = {\"cookiecutter\": cookiecuter_context}\n        output_dir = prompt.choose_nested_template(context, main_dir, no_input=True)\n        expected = (Path(main_dir) / expected).resolve()\n>       assert output_dir == f\"{expected}\"\nE       AssertionError: assert None == '/testbed/tests/fake-nested-templates-old-style/fake-package'\n\ntests/test_prompt.py:598: AssertionError"}, "teardown": {"duration": 0.00040733799999959075, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[]", "lineno": 600, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates_invalid_paths[]", "parametrize", "skipif", "pytestmark", "", "test_prompt.py", "tests", "testbed"], "setup": {"duration": 0.0019888120000004506, "outcome": "passed"}, "call": {"duration": 0.0004496960000004435, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 618, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 618, "message": "Failed"}], "longrepr": "path = ''\n\n    @pytest.mark.skipif(sys.platform.startswith('win'), reason=\"Linux / macos test\")\n    @pytest.mark.parametrize(\n        \"path\",\n        [\n            \"\",\n            \"/tmp\",\n            \"/foo\",\n        ],\n    )\n    def test_cookiecutter_nested_templates_invalid_paths(path: str):\n        \"\"\"Test nested_templates generation.\"\"\"\n        from cookiecutter import prompt\n    \n        main_dir = (Path(\"tests\") / \"fake-nested-templates\").resolve()\n        cookiecuter_context = json.loads((main_dir / \"cookiecutter.json\").read_text())\n        cookiecuter_context[\"templates\"][\"fake-project\"][\"path\"] = path\n        context = {\"cookiecutter\": cookiecuter_context}\n>       with pytest.raises(ValueError) as exc:\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_prompt.py:618: Failed"}, "teardown": {"duration": 0.00042540000000013123, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[/tmp]", "lineno": 600, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates_invalid_paths[/tmp]", "parametrize", "skipif", "pytestmark", "/tmp", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019639859999998066, "outcome": "passed"}, "call": {"duration": 0.0004443009999999248, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 618, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 618, "message": "Failed"}], "longrepr": "path = '/tmp'\n\n    @pytest.mark.skipif(sys.platform.startswith('win'), reason=\"Linux / macos test\")\n    @pytest.mark.parametrize(\n        \"path\",\n        [\n            \"\",\n            \"/tmp\",\n            \"/foo\",\n        ],\n    )\n    def test_cookiecutter_nested_templates_invalid_paths(path: str):\n        \"\"\"Test nested_templates generation.\"\"\"\n        from cookiecutter import prompt\n    \n        main_dir = (Path(\"tests\") / \"fake-nested-templates\").resolve()\n        cookiecuter_context = json.loads((main_dir / \"cookiecutter.json\").read_text())\n        cookiecuter_context[\"templates\"][\"fake-project\"][\"path\"] = path\n        context = {\"cookiecutter\": cookiecuter_context}\n>       with pytest.raises(ValueError) as exc:\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_prompt.py:618: Failed"}, "teardown": {"duration": 0.0003879649999998236, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_paths[/foo]", "lineno": 600, "outcome": "failed", "keywords": ["test_cookiecutter_nested_templates_invalid_paths[/foo]", "parametrize", "skipif", "pytestmark", "/foo", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0019755420000002744, "outcome": "passed"}, "call": {"duration": 0.00044755799999940393, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 618, "message": "Failed: DID NOT RAISE <class 'ValueError'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 618, "message": "Failed"}], "longrepr": "path = '/foo'\n\n    @pytest.mark.skipif(sys.platform.startswith('win'), reason=\"Linux / macos test\")\n    @pytest.mark.parametrize(\n        \"path\",\n        [\n            \"\",\n            \"/tmp\",\n            \"/foo\",\n        ],\n    )\n    def test_cookiecutter_nested_templates_invalid_paths(path: str):\n        \"\"\"Test nested_templates generation.\"\"\"\n        from cookiecutter import prompt\n    \n        main_dir = (Path(\"tests\") / \"fake-nested-templates\").resolve()\n        cookiecuter_context = json.loads((main_dir / \"cookiecutter.json\").read_text())\n        cookiecuter_context[\"templates\"][\"fake-project\"][\"path\"] = path\n        context = {\"cookiecutter\": cookiecuter_context}\n>       with pytest.raises(ValueError) as exc:\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests/test_prompt.py:618: Failed"}, "teardown": {"duration": 0.0004007579999996125, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[]", "lineno": 622, "outcome": "skipped", "keywords": ["test_cookiecutter_nested_templates_invalid_win_paths[]", "parametrize", "skipif", "pytestmark", "", "test_prompt.py", "tests", "testbed"], "setup": {"duration": 0.00021527999999992886, "outcome": "skipped", "longrepr": "('/testbed/tests/test_prompt.py', 623, 'Skipped: Win only test')"}, "teardown": {"duration": 0.00021151599999935655, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[C:/tmp]", "lineno": 622, "outcome": "skipped", "keywords": ["test_cookiecutter_nested_templates_invalid_win_paths[C:/tmp]", "parametrize", "skipif", "pytestmark", "C:/tmp", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.00019544400000004458, "outcome": "skipped", "longrepr": "('/testbed/tests/test_prompt.py', 623, 'Skipped: Win only test')"}, "teardown": {"duration": 0.0002007929999994218, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_cookiecutter_nested_templates_invalid_win_paths[D:/tmp]", "lineno": 622, "outcome": "skipped", "keywords": ["test_cookiecutter_nested_templates_invalid_win_paths[D:/tmp]", "parametrize", "skipif", "pytestmark", "D:/tmp", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.00018954699999973457, "outcome": "skipped", "longrepr": "('/testbed/tests/test_prompt.py', 623, 'Skipped: Win only test')"}, "teardown": {"duration": 0.00020865499999978, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_rm_repo_dir", "lineno": 644, "outcome": "failed", "keywords": ["test_prompt_should_ask_and_rm_repo_dir", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0030016580000005177, "outcome": "passed"}, "call": {"duration": 0.001392599999999966, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 656, "message": "AssertionError: assert False\n +  where False = <MagicMock name='read_user_yes_no' id='139823937083216'>.called"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 656, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c19dea0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_ask_and_rm_0')\n\n    def test_prompt_should_ask_and_rm_repo_dir(mocker, tmp_path):\n        \"\"\"In `prompt_and_delete()`, if the user agrees to delete/reclone the \\\n        repo, the repo should be deleted.\"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', return_value=True\n        )\n        repo_dir = Path(tmp_path, 'repo')\n        repo_dir.mkdir()\n    \n        deleted = prompt.prompt_and_delete(str(repo_dir))\n    \n>       assert mock_read_user.called\nE       AssertionError: assert False\nE        +  where False = <MagicMock name='read_user_yes_no' id='139823937083216'>.called\n\ntests/test_prompt.py:656: AssertionError"}, "teardown": {"duration": 0.00044912099999994126, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_exit_on_user_no_answer", "lineno": 660, "outcome": "failed", "keywords": ["test_prompt_should_ask_and_exit_on_user_no_answer", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0024923120000002186, "outcome": "passed"}, "call": {"duration": 0.0015642260000001684, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 674, "message": "AssertionError: assert False\n +  where False = <MagicMock name='read_user_yes_no' id='139823939480704'>.called"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 674, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3e4df0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_ask_and_exi0')\n\n    def test_prompt_should_ask_and_exit_on_user_no_answer(mocker, tmp_path):\n        \"\"\"In `prompt_and_delete()`, if the user decline to delete/reclone the \\\n        repo, cookiecutter should exit.\"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no',\n            return_value=False,\n        )\n        mock_sys_exit = mocker.patch('sys.exit', return_value=True)\n        repo_dir = Path(tmp_path, 'repo')\n        repo_dir.mkdir()\n    \n        deleted = prompt.prompt_and_delete(str(repo_dir))\n    \n>       assert mock_read_user.called\nE       AssertionError: assert False\nE        +  where False = <MagicMock name='read_user_yes_no' id='139823939480704'>.called\n\ntests/test_prompt.py:674: AssertionError"}, "teardown": {"duration": 0.0005894749999999505, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_rm_repo_file", "lineno": 679, "outcome": "failed", "keywords": ["test_prompt_should_ask_and_rm_repo_file", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002470542999999381, "outcome": "passed"}, "call": {"duration": 0.0014436039999994321, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 692, "message": "assert False\n +  where False = <function read_user_yes_no at 0x7f2b4c0d1f30>.called"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 692, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3deaa0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_ask_and_rm_1')\n\n    def test_prompt_should_ask_and_rm_repo_file(mocker, tmp_path):\n        \"\"\"In `prompt_and_delete()`, if the user agrees to delete/reclone a \\\n        repo file, the repo should be deleted.\"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', return_value=True, autospec=True\n        )\n    \n        repo_file = tmp_path.joinpath('repo.zip')\n        repo_file.write_text('this is zipfile content')\n    \n        deleted = prompt.prompt_and_delete(str(repo_file))\n    \n>       assert mock_read_user.called\nE       assert False\nE        +  where False = <function read_user_yes_no at 0x7f2b4c0d1f30>.called\n\ntests/test_prompt.py:692: AssertionError"}, "teardown": {"duration": 0.00043740999999997143, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_keep_repo_on_no_reuse", "lineno": 696, "outcome": "failed", "keywords": ["test_prompt_should_ask_and_keep_repo_on_no_reuse", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.002319038999999634, "outcome": "passed"}, "call": {"duration": 0.0011819199999996144, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 706, "message": "Failed: DID NOT RAISE <class 'SystemExit'>"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 706, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bff0700>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_ask_and_kee0')\n\n    def test_prompt_should_ask_and_keep_repo_on_no_reuse(mocker, tmp_path):\n        \"\"\"In `prompt_and_delete()`, if the user wants to keep their old \\\n        cloned template repo, it should not be deleted.\"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', return_value=False, autospec=True\n        )\n        repo_dir = Path(tmp_path, 'repo')\n        repo_dir.mkdir()\n    \n>       with pytest.raises(SystemExit):\nE       Failed: DID NOT RAISE <class 'SystemExit'>\n\ntests/test_prompt.py:706: Failed"}, "teardown": {"duration": 0.0004201369999998761, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_ask_and_keep_repo_on_reuse", "lineno": 712, "outcome": "failed", "keywords": ["test_prompt_should_ask_and_keep_repo_on_reuse", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0021704629999996783, "outcome": "passed"}, "call": {"duration": 0.001406269000000293, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 728, "message": "assert False\n +  where False = <function read_user_yes_no at 0x7f2b4bea8550>.called"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 728, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce9d0c0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_ask_and_kee1')\n\n    def test_prompt_should_ask_and_keep_repo_on_reuse(mocker, tmp_path):\n        \"\"\"In `prompt_and_delete()`, if the user wants to keep their old \\\n        cloned template repo, it should not be deleted.\"\"\"\n    \n        def answer(question, default):\n            return 'okay to delete' not in question\n    \n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', side_effect=answer, autospec=True\n        )\n        repo_dir = Path(tmp_path, 'repo')\n        repo_dir.mkdir()\n    \n        deleted = prompt.prompt_and_delete(str(repo_dir))\n    \n>       assert mock_read_user.called\nE       assert False\nE        +  where False = <function read_user_yes_no at 0x7f2b4bea8550>.called\n\ntests/test_prompt.py:728: AssertionError"}, "teardown": {"duration": 0.0008245230000003545, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_not_ask_if_no_input_and_rm_repo_dir", "lineno": 732, "outcome": "failed", "keywords": ["test_prompt_should_not_ask_if_no_input_and_rm_repo_dir", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0028105390000003894, "outcome": "passed"}, "call": {"duration": 0.001364213000000447, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 748, "message": "AssertionError: assert not True\n +  where True = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_0/repo').exists"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 748, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3ee500>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_0')\n\n    def test_prompt_should_not_ask_if_no_input_and_rm_repo_dir(mocker, tmp_path):\n        \"\"\"Prompt should not ask if no input and rm dir.\n    \n        In `prompt_and_delete()`, if `no_input` is True, the call to\n        `prompt.read_user_yes_no()` should be suppressed.\n        \"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', return_value=True, autospec=True\n        )\n        repo_dir = Path(tmp_path, 'repo')\n        repo_dir.mkdir()\n    \n        deleted = prompt.prompt_and_delete(str(repo_dir), no_input=True)\n    \n        assert not mock_read_user.called\n>       assert not repo_dir.exists()\nE       AssertionError: assert not True\nE        +  where True = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_0/repo').exists\n\ntests/test_prompt.py:748: AssertionError"}, "teardown": {"duration": 0.00043106799999925727, "outcome": "passed"}}, {"nodeid": "tests/test_prompt.py::test_prompt_should_not_ask_if_no_input_and_rm_repo_file", "lineno": 751, "outcome": "failed", "keywords": ["test_prompt_should_not_ask_if_no_input_and_rm_repo_file", "test_prompt.py", "tests", "testbed", ""], "setup": {"duration": 0.0024409270000003147, "outcome": "passed"}, "call": {"duration": 0.0012706899999992416, "outcome": "failed", "crash": {"path": "/testbed/tests/test_prompt.py", "lineno": 768, "message": "AssertionError: assert not True\n +  where True = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_1/repo.zip').exists"}, "traceback": [{"path": "tests/test_prompt.py", "lineno": 768, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c006ce0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_1')\n\n    def test_prompt_should_not_ask_if_no_input_and_rm_repo_file(mocker, tmp_path):\n        \"\"\"Prompt should not ask if no input and rm file.\n    \n        In `prompt_and_delete()`, if `no_input` is True, the call to\n        `prompt.read_user_yes_no()` should be suppressed.\n        \"\"\"\n        mock_read_user = mocker.patch(\n            'cookiecutter.prompt.read_user_yes_no', return_value=True, autospec=True\n        )\n    \n        repo_file = tmp_path.joinpath('repo.zip')\n        repo_file.write_text('this is zipfile content')\n    \n        deleted = prompt.prompt_and_delete(str(repo_file), no_input=True)\n    \n        assert not mock_read_user.called\n>       assert not repo_file.exists()\nE       AssertionError: assert not True\nE        +  where True = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_prompt_should_not_ask_if_1/repo.zip').exists\n\ntests/test_prompt.py:768: AssertionError"}, "teardown": {"duration": 0.0005099899999994051, "outcome": "passed"}}, {"nodeid": "tests/test_read_repo_password.py::test_click_invocation", "lineno": 5, "outcome": "failed", "keywords": ["test_click_invocation", "test_read_repo_password.py", "tests", "testbed", ""], "setup": {"duration": 0.0023467840000002127, "outcome": "passed"}, "call": {"duration": 0.001007926999999853, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_repo_password.py", "lineno": 14, "message": "AssertionError: assert None == 'sekrit'\n +  where None = read_repo_password('Password')"}, "traceback": [{"path": "tests/test_read_repo_password.py", "lineno": 14, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3ed0f0>\n\n    def test_click_invocation(mocker):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for password (hidden input) type invocation.\n        \"\"\"\n        prompt = mocker.patch('rich.prompt.Prompt.ask')\n        prompt.return_value = 'sekrit'\n    \n>       assert read_repo_password('Password') == 'sekrit'\nE       AssertionError: assert None == 'sekrit'\nE        +  where None = read_repo_password('Password')\n\ntests/test_read_repo_password.py:14: AssertionError"}, "teardown": {"duration": 0.00037821000000004545, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[1-hello]", "lineno": 17, "outcome": "failed", "keywords": ["test_click_invocation[1-hello]", "parametrize", "pytestmark", "1-hello", "test_read_user_choice.py", "tests", "testbed", ""], "setup": {"duration": 0.0020944160000002654, "outcome": "passed"}, "call": {"duration": 0.0011437440000001686, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError: assert None == 'hello'\n +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])"}, "traceback": [{"path": "tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c07b370>\nuser_choice = 1, expected_value = 'hello'\n\n    @pytest.mark.parametrize('user_choice, expected_value', enumerate(OPTIONS, 1))\n    def test_click_invocation(mocker, user_choice, expected_value):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for choice type invocation.\n        \"\"\"\n        prompt = mocker.patch('rich.prompt.Prompt.ask')\n        prompt.return_value = f'{user_choice}'\n    \n>       assert read_user_choice('varname', OPTIONS) == expected_value\nE       AssertionError: assert None == 'hello'\nE        +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])\n\ntests/test_read_user_choice.py:27: AssertionError"}, "teardown": {"duration": 0.00039527200000044616, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[2-world]", "lineno": 17, "outcome": "failed", "keywords": ["test_click_invocation[2-world]", "parametrize", "pytestmark", "2-world", "test_read_user_choice.py", "tests", "testbed", ""], "setup": {"duration": 0.0020866069999998516, "outcome": "passed"}, "call": {"duration": 0.001005834999999955, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError: assert None == 'world'\n +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])"}, "traceback": [{"path": "tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c831b40>\nuser_choice = 2, expected_value = 'world'\n\n    @pytest.mark.parametrize('user_choice, expected_value', enumerate(OPTIONS, 1))\n    def test_click_invocation(mocker, user_choice, expected_value):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for choice type invocation.\n        \"\"\"\n        prompt = mocker.patch('rich.prompt.Prompt.ask')\n        prompt.return_value = f'{user_choice}'\n    \n>       assert read_user_choice('varname', OPTIONS) == expected_value\nE       AssertionError: assert None == 'world'\nE        +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])\n\ntests/test_read_user_choice.py:27: AssertionError"}, "teardown": {"duration": 0.000386458000000367, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[3-foo]", "lineno": 17, "outcome": "failed", "keywords": ["test_click_invocation[3-foo]", "parametrize", "pytestmark", "3-foo", "test_read_user_choice.py", "tests", "testbed", ""], "setup": {"duration": 0.002071880000000803, "outcome": "passed"}, "call": {"duration": 0.0009395269999998845, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError: assert None == 'foo'\n +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])"}, "traceback": [{"path": "tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c090700>\nuser_choice = 3, expected_value = 'foo'\n\n    @pytest.mark.parametrize('user_choice, expected_value', enumerate(OPTIONS, 1))\n    def test_click_invocation(mocker, user_choice, expected_value):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for choice type invocation.\n        \"\"\"\n        prompt = mocker.patch('rich.prompt.Prompt.ask')\n        prompt.return_value = f'{user_choice}'\n    \n>       assert read_user_choice('varname', OPTIONS) == expected_value\nE       AssertionError: assert None == 'foo'\nE        +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])\n\ntests/test_read_user_choice.py:27: AssertionError"}, "teardown": {"duration": 0.0005451019999993534, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_choice.py::test_click_invocation[4-bar]", "lineno": 17, "outcome": "failed", "keywords": ["test_click_invocation[4-bar]", "parametrize", "pytestmark", "4-bar", "test_read_user_choice.py", "tests", "testbed", ""], "setup": {"duration": 0.002049706999999401, "outcome": "passed"}, "call": {"duration": 0.0008925029999993228, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError: assert None == 'bar'\n +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])"}, "traceback": [{"path": "tests/test_read_user_choice.py", "lineno": 27, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c078d00>\nuser_choice = 4, expected_value = 'bar'\n\n    @pytest.mark.parametrize('user_choice, expected_value', enumerate(OPTIONS, 1))\n    def test_click_invocation(mocker, user_choice, expected_value):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for choice type invocation.\n        \"\"\"\n        prompt = mocker.patch('rich.prompt.Prompt.ask')\n        prompt.return_value = f'{user_choice}'\n    \n>       assert read_user_choice('varname', OPTIONS) == expected_value\nE       AssertionError: assert None == 'bar'\nE        +  where None = read_user_choice('varname', ['hello', 'world', 'foo', 'bar'])\n\ntests/test_read_user_choice.py:27: AssertionError"}, "teardown": {"duration": 0.000421262999999783, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_choice.py::test_raise_if_options_is_not_a_non_empty_list", "lineno": 31, "outcome": "failed", "keywords": ["test_raise_if_options_is_not_a_non_empty_list", "test_read_user_choice.py", "tests", "testbed", ""], "setup": {"duration": 0.001925078000000191, "outcome": "passed"}, "call": {"duration": 0.0002714450000000923, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_choice.py", "lineno": 37, "message": "Failed: DID NOT RAISE <class 'TypeError'>"}, "traceback": [{"path": "tests/test_read_user_choice.py", "lineno": 37, "message": "Failed"}], "longrepr": "def test_raise_if_options_is_not_a_non_empty_list():\n        \"\"\"Test function called by cookiecutter raise expected errors.\n    \n        Test for choice type invocation.\n        \"\"\"\n>       with pytest.raises(TypeError):\nE       Failed: DID NOT RAISE <class 'TypeError'>\n\ntests/test_read_user_choice.py:37: Failed"}, "teardown": {"duration": 0.0003385730000005083, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_invalid_json", "lineno": 9, "outcome": "failed", "keywords": ["test_process_json_invalid_json", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.0018980539999997603, "outcome": "passed"}, "call": {"duration": 0.0002637690000000248, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 12, "message": "Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 12, "message": "Failed"}], "longrepr": "def test_process_json_invalid_json():\n        \"\"\"Test `process_json` for correct error on malformed input.\"\"\"\n>       with pytest.raises(InvalidResponse) as exc_info:\nE       Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>\n\ntests/test_read_user_dict.py:12: Failed"}, "teardown": {"duration": 0.000335387999999881, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_non_dict", "lineno": 17, "outcome": "failed", "keywords": ["test_process_json_non_dict", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.0019005789999999578, "outcome": "passed"}, "call": {"duration": 0.00026004700000026304, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 20, "message": "Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 20, "message": "Failed"}], "longrepr": "def test_process_json_non_dict():\n        \"\"\"Test `process_json` for correct error on non-JSON input.\"\"\"\n>       with pytest.raises(InvalidResponse) as exc_info:\nE       Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>\n\ntests/test_read_user_dict.py:20: Failed"}, "teardown": {"duration": 0.0003267679999998663, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_valid_json", "lineno": 25, "outcome": "failed", "keywords": ["test_process_json_valid_json", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.0018905990000002149, "outcome": "passed"}, "call": {"duration": 0.0003923119999997837, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 33, "message": "assert None == {'bla': ['a', 1, 'b', False], 'name': 'foobar'}\n +  where None = process_json('{\"name\": \"foobar\", \"bla\": [\"a\", 1, \"b\", false]}')"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 33, "message": "AssertionError"}], "longrepr": "def test_process_json_valid_json():\n        \"\"\"Test `process_json` for correct output on JSON input.\n    \n        Test for simple dict with list.\n        \"\"\"\n        user_value = '{\"name\": \"foobar\", \"bla\": [\"a\", 1, \"b\", false]}'\n    \n>       assert process_json(user_value) == {\n            'name': 'foobar',\n            'bla': ['a', 1, 'b', False],\n        }\nE       assert None == {'bla': ['a', 1, 'b', False], 'name': 'foobar'}\nE        +  where None = process_json('{\"name\": \"foobar\", \"bla\": [\"a\", 1, \"b\", false]}')\n\ntests/test_read_user_dict.py:33: AssertionError"}, "teardown": {"duration": 0.00032929199999998104, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_process_json_deep_dict", "lineno": 38, "outcome": "failed", "keywords": ["test_process_json_deep_dict", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.0018983019999998518, "outcome": "passed"}, "call": {"duration": 0.0004271309999994699, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 63, "message": "assert None == {'dict_key': {'deep_integer': 42, 'deep_key': 'deep_value', 'deep_list': ['deep value 1', 'deep value 2', 'deep value 3']}, 'integer_key': 37, 'key': 'value', 'list_key': ['value 1', 'value 2', 'value 3']}\n +  where None = process_json('{\\n        \"key\": \"value\",\\n        \"integer_key\": 37,\\n        \"dict_key\": {\\n            \"deep_key\": \"deep_value\",\\n            \"deep_integer\": 42,\\n            \"deep_list\": [\\n                \"deep value 1\",\\n                \"deep value 2\",\\n                \"deep value 3\"\\n            ]\\n        },\\n        \"list_key\": [\\n            \"value 1\",\\n            \"value 2\",\\n            \"value 3\"\\n        ]\\n    }')"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 63, "message": "AssertionError"}], "longrepr": "def test_process_json_deep_dict():\n        \"\"\"Test `process_json` for correct output on JSON input.\n    \n        Test for dict in dict case.\n        \"\"\"\n        user_value = '''{\n            \"key\": \"value\",\n            \"integer_key\": 37,\n            \"dict_key\": {\n                \"deep_key\": \"deep_value\",\n                \"deep_integer\": 42,\n                \"deep_list\": [\n                    \"deep value 1\",\n                    \"deep value 2\",\n                    \"deep value 3\"\n                ]\n            },\n            \"list_key\": [\n                \"value 1\",\n                \"value 2\",\n                \"value 3\"\n            ]\n        }'''\n    \n>       assert process_json(user_value) == {\n            \"key\": \"value\",\n            \"integer_key\": 37,\n            \"dict_key\": {\n                \"deep_key\": \"deep_value\",\n                \"deep_integer\": 42,\n                \"deep_list\": [\"deep value 1\", \"deep value 2\", \"deep value 3\"],\n            },\n            \"list_key\": [\"value 1\", \"value 2\", \"value 3\"],\n        }\nE       assert None == {'dict_key': {'deep_integer': 42, 'deep_key': 'deep_value', 'deep_list': ['deep value 1', 'deep value 2', 'deep value 3']}, 'integer_key': 37, 'key': 'value', 'list_key': ['value 1', 'value 2', 'value 3']}\nE        +  where None = process_json('{\\n        \"key\": \"value\",\\n        \"integer_key\": 37,\\n        \"dict_key\": {\\n            \"deep_key\": \"deep_value\",\\n            \"deep_integer\": 42,\\n            \"deep_list\": [\\n                \"deep value 1\",\\n                \"deep value 2\",\\n                \"deep value 3\"\\n            ]\\n        },\\n        \"list_key\": [\\n            \"value 1\",\\n            \"value 2\",\\n            \"value 3\"\\n        ]\\n    }')\n\ntests/test_read_user_dict.py:63: AssertionError"}, "teardown": {"duration": 0.00033106400000004754, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_should_raise_type_error", "lineno": 74, "outcome": "failed", "keywords": ["test_should_raise_type_error", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.001969281000000045, "outcome": "passed"}, "call": {"duration": 0.0007846259999997329, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 79, "message": "Failed: DID NOT RAISE <class 'TypeError'>"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 79, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c01db70>\n\n    def test_should_raise_type_error(mocker):\n        \"\"\"Test `default_value` arg verification in `read_user_dict` function.\"\"\"\n        prompt = mocker.patch('cookiecutter.prompt.JsonPrompt.ask')\n    \n>       with pytest.raises(TypeError):\nE       Failed: DID NOT RAISE <class 'TypeError'>\n\ntests/test_read_user_dict.py:79: Failed"}, "teardown": {"duration": 0.0003608140000004312, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_should_call_prompt_with_process_json", "lineno": 83, "outcome": "failed", "keywords": ["test_should_call_prompt_with_process_json", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.001971966000000158, "outcome": "passed"}, "call": {"duration": 0.0013998570000000043, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 93, "message": "TypeError: cannot unpack non-iterable NoneType object"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 93, "message": "TypeError"}], "stdout": "None\n", "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce9e770>\n\n    def test_should_call_prompt_with_process_json(mocker):\n        \"\"\"Test to make sure that `process_json` is actually being used.\n    \n        Verifies generation of a processor for the user input.\n        \"\"\"\n        mock_prompt = mocker.patch('cookiecutter.prompt.JsonPrompt.ask', autospec=True)\n    \n        read_user_dict('name', {'project_slug': 'pytest-plugin'})\n        print(mock_prompt.call_args)\n>       args, kwargs = mock_prompt.call_args\nE       TypeError: cannot unpack non-iterable NoneType object\n\ntests/test_read_user_dict.py:93: TypeError"}, "teardown": {"duration": 0.0003977569999999986, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_should_not_load_json_from_sentinel", "lineno": 98, "outcome": "passed", "keywords": ["test_should_not_load_json_from_sentinel", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.001991922999999396, "outcome": "passed"}, "call": {"duration": 0.00102186400000015, "outcome": "passed"}, "teardown": {"duration": 0.00035077600000033016, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_read_user_dict_default_value[\\n]", "lineno": 111, "outcome": "failed", "keywords": ["test_read_user_dict_default_value[\\n]", "parametrize", "pytestmark", "\\n", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.002070191999999693, "outcome": "passed"}, "call": {"duration": 0.00044196200000001795, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 122, "message": "AssertionError: assert None == {'project_slug': 'pytest-plugin'}"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 122, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c096fb0>\ninput = '\\n'\n\n    @pytest.mark.parametrize(\"input\", [\"\\n\", \"\\ndefault\\n\"])\n    def test_read_user_dict_default_value(mocker, input):\n        \"\"\"Make sure that `read_user_dict` returns the default value.\n    \n        Verify return of a dict variable rather than the display value.\n        \"\"\"\n        runner = click.testing.CliRunner()\n        with runner.isolation(input=input):\n            val = read_user_dict('name', {'project_slug': 'pytest-plugin'})\n    \n>       assert val == {'project_slug': 'pytest-plugin'}\nE       AssertionError: assert None == {'project_slug': 'pytest-plugin'}\n\ntests/test_read_user_dict.py:122: AssertionError"}, "teardown": {"duration": 0.0003630439999993129, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_read_user_dict_default_value[\\ndefault\\n]", "lineno": 111, "outcome": "failed", "keywords": ["test_read_user_dict_default_value[\\ndefault\\n]", "parametrize", "pytestmark", "\\ndefault\\n", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.0020388819999999086, "outcome": "passed"}, "call": {"duration": 0.00042284800000036427, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 122, "message": "AssertionError: assert None == {'project_slug': 'pytest-plugin'}"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 122, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c490730>\ninput = '\\ndefault\\n'\n\n    @pytest.mark.parametrize(\"input\", [\"\\n\", \"\\ndefault\\n\"])\n    def test_read_user_dict_default_value(mocker, input):\n        \"\"\"Make sure that `read_user_dict` returns the default value.\n    \n        Verify return of a dict variable rather than the display value.\n        \"\"\"\n        runner = click.testing.CliRunner()\n        with runner.isolation(input=input):\n            val = read_user_dict('name', {'project_slug': 'pytest-plugin'})\n    \n>       assert val == {'project_slug': 'pytest-plugin'}\nE       AssertionError: assert None == {'project_slug': 'pytest-plugin'}\n\ntests/test_read_user_dict.py:122: AssertionError"}, "teardown": {"duration": 0.0003843720000000772, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_dict.py::test_json_prompt_process_response", "lineno": 124, "outcome": "failed", "keywords": ["test_json_prompt_process_response", "test_read_user_dict.py", "tests", "testbed", ""], "setup": {"duration": 0.00189910900000001, "outcome": "passed"}, "call": {"duration": 0.0005207219999991963, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_dict.py", "lineno": 128, "message": "assert None == {'project_slug': 'something'}\n +  where None = process_response('{\"project_slug\": \"something\"}')\n +    where process_response = <cookiecutter.prompt.JsonPrompt object at 0x7f2b4c027700>.process_response"}, "traceback": [{"path": "tests/test_read_user_dict.py", "lineno": 128, "message": "AssertionError"}], "longrepr": "def test_json_prompt_process_response():\n        \"\"\"Test `JsonPrompt` process_response to convert str to json.\"\"\"\n        jp = JsonPrompt()\n>       assert jp.process_response('{\"project_slug\": \"something\"}') == {\n            'project_slug': 'something'\n        }\nE       assert None == {'project_slug': 'something'}\nE        +  where None = process_response('{\"project_slug\": \"something\"}')\nE        +    where process_response = <cookiecutter.prompt.JsonPrompt object at 0x7f2b4c027700>.process_response\n\ntests/test_read_user_dict.py:128: AssertionError"}, "teardown": {"duration": 0.0003324689999999464, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_variable.py::test_click_invocation", "lineno": 16, "outcome": "failed", "keywords": ["test_click_invocation", "test_read_user_variable.py", "tests", "testbed", ""], "setup": {"duration": 0.0026086269999998635, "outcome": "passed"}, "call": {"duration": 0.0004244980000001064, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_variable.py", "lineno": 24, "message": "AssertionError: assert None == 'Kivy Project'\n +  where None = read_user_variable('project_name', 'Kivy Project')"}, "traceback": [{"path": "tests/test_read_user_variable.py", "lineno": 24, "message": "AssertionError"}], "longrepr": "mock_prompt = <MagicMock name='ask' id='139823940181856'>\n\n    def test_click_invocation(mock_prompt):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for string type invocation.\n        \"\"\"\n        mock_prompt.return_value = DEFAULT\n    \n>       assert read_user_variable(VARIABLE, DEFAULT) == DEFAULT\nE       AssertionError: assert None == 'Kivy Project'\nE        +  where None = read_user_variable('project_name', 'Kivy Project')\n\ntests/test_read_user_variable.py:24: AssertionError"}, "teardown": {"duration": 0.00038913399999973564, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_variable.py::test_input_loop_with_null_default_value", "lineno": 28, "outcome": "failed", "keywords": ["test_input_loop_with_null_default_value", "test_read_user_variable.py", "tests", "testbed", ""], "setup": {"duration": 0.0026294020000001694, "outcome": "passed"}, "call": {"duration": 0.0003928100000001322, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_variable.py", "lineno": 37, "message": "AssertionError: assert None == 'Kivy Project'\n +  where None = read_user_variable('project_name', None)"}, "traceback": [{"path": "tests/test_read_user_variable.py", "lineno": 37, "message": "AssertionError"}], "longrepr": "mock_prompt = <MagicMock name='ask' id='139823933953344'>\n\n    def test_input_loop_with_null_default_value(mock_prompt):\n        \"\"\"Test `Prompt.ask` is run repeatedly until a valid answer is provided.\n    \n        Test for `default_value` parameter equal to None.\n        \"\"\"\n        # Simulate user providing None input initially and then a valid input\n        mock_prompt.side_effect = [None, DEFAULT]\n    \n>       assert read_user_variable(VARIABLE, None) == DEFAULT\nE       AssertionError: assert None == 'Kivy Project'\nE        +  where None = read_user_variable('project_name', None)\n\ntests/test_read_user_variable.py:37: AssertionError"}, "teardown": {"duration": 0.00039070099999971575, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_yes_no.py::test_click_invocation", "lineno": 11, "outcome": "failed", "keywords": ["test_click_invocation", "test_read_user_yes_no.py", "tests", "testbed", ""], "setup": {"duration": 0.0019983830000001035, "outcome": "passed"}, "call": {"duration": 0.0010124400000002254, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_yes_no.py", "lineno": 20, "message": "AssertionError: assert None == 'y'\n +  where None = read_user_yes_no('Is it okay to delete and re-clone it?', 'y')"}, "traceback": [{"path": "tests/test_read_user_yes_no.py", "lineno": 20, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c01cbe0>\n\n    def test_click_invocation(mocker):\n        \"\"\"Test click function called correctly by cookiecutter.\n    \n        Test for boolean type invocation.\n        \"\"\"\n        prompt = mocker.patch('cookiecutter.prompt.YesNoPrompt.ask')\n        prompt.return_value = DEFAULT\n    \n>       assert read_user_yes_no(QUESTION, DEFAULT) == DEFAULT\nE       AssertionError: assert None == 'y'\nE        +  where None = read_user_yes_no('Is it okay to delete and re-clone it?', 'y')\n\ntests/test_read_user_yes_no.py:20: AssertionError"}, "teardown": {"duration": 0.00035723900000039777, "outcome": "passed"}}, {"nodeid": "tests/test_read_user_yes_no.py::test_yesno_prompt_process_response", "lineno": 24, "outcome": "failed", "keywords": ["test_yesno_prompt_process_response", "test_read_user_yes_no.py", "tests", "testbed", ""], "setup": {"duration": 0.0019244780000002848, "outcome": "passed"}, "call": {"duration": 0.00031101800000055135, "outcome": "failed", "crash": {"path": "/testbed/tests/test_read_user_yes_no.py", "lineno": 28, "message": "Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>"}, "traceback": [{"path": "tests/test_read_user_yes_no.py", "lineno": 28, "message": "Failed"}], "longrepr": "def test_yesno_prompt_process_response():\n        \"\"\"Test `YesNoPrompt` process_response to convert str to bool.\"\"\"\n        ynp = YesNoPrompt()\n>       with pytest.raises(InvalidResponse):\nE       Failed: DID NOT RAISE <class 'rich.prompt.InvalidResponse'>\n\ntests/test_read_user_yes_no.py:28: Failed"}, "teardown": {"duration": 0.00032809100000008584, "outcome": "passed"}}, {"nodeid": "tests/test_repo_not_found.py::test_should_raise_error_if_repo_does_not_exist", "lineno": 7, "outcome": "failed", "keywords": ["test_should_raise_error_if_repo_does_not_exist", "test_repo_not_found.py", "tests", "testbed", ""], "setup": {"duration": 0.0019094900000000692, "outcome": "passed"}, "call": {"duration": 0.0002781619999998597, "outcome": "failed", "crash": {"path": "/testbed/tests/test_repo_not_found.py", "lineno": 10, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/test_repo_not_found.py", "lineno": 10, "message": "Failed"}], "longrepr": "def test_should_raise_error_if_repo_does_not_exist():\n        \"\"\"Cookiecutter invocation with non-exist repository should raise error.\"\"\"\n>       with pytest.raises(exceptions.RepositoryNotFound):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/test_repo_not_found.py:10: Failed"}, "teardown": {"duration": 0.000337421999999421, "outcome": "passed"}}, {"nodeid": "tests/test_specify_output_dir.py::test_api_invocation", "lineno": 47, "outcome": "failed", "keywords": ["test_api_invocation", "test_specify_output_dir.py", "tests", "testbed", ""], "setup": {"duration": 0.004229630999999401, "outcome": "passed"}, "call": {"duration": 0.0007955009999998097, "outcome": "failed", "crash": {"path": "/testbed/tests/test_specify_output_dir.py", "lineno": 54, "message": "AssertionError: Expected 'generate_files' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_specify_output_dir.py", "lineno": 54, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='generate_files' id='139823936000192'>, args = ()\nkwargs = {'accept_hooks': True, 'context': {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}, 'keep_project_on_failure': False, 'output_dir': '/tmp/pytest-of-root/pytest-0/test_api_invocation0/output', ...}\nmsg = \"Expected 'generate_files' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'generate_files' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3f3fd0>\ntemplate = '/tmp/pytest-of-root/pytest-0/test_api_invocation0/template'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_api_invocation0/output'\ncontext = {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}\n\n    def test_api_invocation(mocker, template, output_dir, context):\n        \"\"\"Verify output dir location is correctly passed.\"\"\"\n        mock_gen_files = mocker.patch('cookiecutter.main.generate_files')\n    \n        main.cookiecutter(template, output_dir=output_dir)\n    \n>       mock_gen_files.assert_called_once_with(\n            repo_dir=template,\n            context=context,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir=output_dir,\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'generate_files' to be called once. Called 0 times.\n\ntests/test_specify_output_dir.py:54: AssertionError"}, "teardown": {"duration": 0.000689415000000082, "outcome": "passed"}}, {"nodeid": "tests/test_specify_output_dir.py::test_default_output_dir", "lineno": 64, "outcome": "failed", "keywords": ["test_default_output_dir", "test_specify_output_dir.py", "tests", "testbed", ""], "setup": {"duration": 0.004587525999999897, "outcome": "passed"}, "call": {"duration": 0.0008907620000009331, "outcome": "failed", "crash": {"path": "/testbed/tests/test_specify_output_dir.py", "lineno": 71, "message": "AssertionError: Expected 'generate_files' to be called once. Called 0 times."}, "traceback": [{"path": "tests/test_specify_output_dir.py", "lineno": 71, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='generate_files' id='139823933642576'>, args = ()\nkwargs = {'accept_hooks': True, 'context': {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}, 'keep_project_on_failure': False, 'output_dir': '.', ...}\nmsg = \"Expected 'generate_files' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'generate_files' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c07afe0>\ntemplate = '/tmp/pytest-of-root/pytest-0/test_default_output_dir0/template'\ncontext = {'cookiecutter': {'email': 'raphael@hackebrot.de', 'full_name': 'Raphael Pierzina', 'github_username': 'hackebrot', 'version': '0.1.0'}}\n\n    def test_default_output_dir(mocker, template, context):\n        \"\"\"Verify default output dir is current working folder.\"\"\"\n        mock_gen_files = mocker.patch('cookiecutter.main.generate_files')\n    \n        main.cookiecutter(template)\n    \n>       mock_gen_files.assert_called_once_with(\n            repo_dir=template,\n            context=context,\n            overwrite_if_exists=False,\n            skip_if_file_exists=False,\n            output_dir='.',\n            accept_hooks=True,\n            keep_project_on_failure=False,\n        )\nE       AssertionError: Expected 'generate_files' to be called once. Called 0 times.\n\ntests/test_specify_output_dir.py:71: AssertionError"}, "teardown": {"duration": 0.0006123759999994149, "outcome": "passed"}}, {"nodeid": "tests/test_templates.py::test_build_templates[include]", "lineno": 20, "outcome": "failed", "keywords": ["test_build_templates[include]", "parametrize", "pytestmark", "include", "test_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.0025949650000001157, "outcome": "passed"}, "call": {"duration": 0.00027503499999959047, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_templates.py", "lineno": 34, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'include'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_build_templates_include_0/templates'\n\n    @pytest.mark.parametrize(\"template\", [\"include\", \"no-templates\", \"extends\", \"super\"])\n    def test_build_templates(template, output_dir):\n        \"\"\"\n        Verify Templates Design keywords.\n    \n        no-templates is a compatibility tests for repo without `templates` directory\n        \"\"\"\n        project_dir = main.cookiecutter(\n            f'tests/test-templates/{template}',\n            no_input=True,\n            output_dir=output_dir,\n        )\n    \n>       readme = Path(project_dir, 'requirements.txt').read_text()\n\ntests/test_templates.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'requirements.txt')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.0004664949999995116, "outcome": "passed"}}, {"nodeid": "tests/test_templates.py::test_build_templates[no-templates]", "lineno": 20, "outcome": "failed", "keywords": ["test_build_templates[no-templates]", "parametrize", "pytestmark", "no-templates", "test_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.002399204000001376, "outcome": "passed"}, "call": {"duration": 0.0002655770000004054, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_templates.py", "lineno": 34, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'no-templates'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_build_templates_no_templa0/templates'\n\n    @pytest.mark.parametrize(\"template\", [\"include\", \"no-templates\", \"extends\", \"super\"])\n    def test_build_templates(template, output_dir):\n        \"\"\"\n        Verify Templates Design keywords.\n    \n        no-templates is a compatibility tests for repo without `templates` directory\n        \"\"\"\n        project_dir = main.cookiecutter(\n            f'tests/test-templates/{template}',\n            no_input=True,\n            output_dir=output_dir,\n        )\n    \n>       readme = Path(project_dir, 'requirements.txt').read_text()\n\ntests/test_templates.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'requirements.txt')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.00043719999999858317, "outcome": "passed"}}, {"nodeid": "tests/test_templates.py::test_build_templates[extends]", "lineno": 20, "outcome": "failed", "keywords": ["test_build_templates[extends]", "parametrize", "pytestmark", "extends", "test_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.0023641640000011677, "outcome": "passed"}, "call": {"duration": 0.00026722199999973384, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_templates.py", "lineno": 34, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'extends'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_build_templates_extends_0/templates'\n\n    @pytest.mark.parametrize(\"template\", [\"include\", \"no-templates\", \"extends\", \"super\"])\n    def test_build_templates(template, output_dir):\n        \"\"\"\n        Verify Templates Design keywords.\n    \n        no-templates is a compatibility tests for repo without `templates` directory\n        \"\"\"\n        project_dir = main.cookiecutter(\n            f'tests/test-templates/{template}',\n            no_input=True,\n            output_dir=output_dir,\n        )\n    \n>       readme = Path(project_dir, 'requirements.txt').read_text()\n\ntests/test_templates.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'requirements.txt')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.0006138750000008741, "outcome": "passed"}}, {"nodeid": "tests/test_templates.py::test_build_templates[super]", "lineno": 20, "outcome": "failed", "keywords": ["test_build_templates[super]", "parametrize", "pytestmark", "super", "test_templates.py", "tests", "testbed", ""], "setup": {"duration": 0.002874376999999484, "outcome": "passed"}, "call": {"duration": 0.0003019769999994537, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError: expected str, bytes or os.PathLike object, not NoneType"}, "traceback": [{"path": "tests/test_templates.py", "lineno": 34, "message": ""}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 960, "message": "in __new__"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 594, "message": "in _from_parts"}, {"path": "/usr/lib/python3.10/pathlib.py", "lineno": 578, "message": "TypeError"}], "longrepr": "template = 'super'\noutput_dir = '/tmp/pytest-of-root/pytest-0/test_build_templates_super_0/templates'\n\n    @pytest.mark.parametrize(\"template\", [\"include\", \"no-templates\", \"extends\", \"super\"])\n    def test_build_templates(template, output_dir):\n        \"\"\"\n        Verify Templates Design keywords.\n    \n        no-templates is a compatibility tests for repo without `templates` directory\n        \"\"\"\n        project_dir = main.cookiecutter(\n            f'tests/test-templates/{template}',\n            no_input=True,\n            output_dir=output_dir,\n        )\n    \n>       readme = Path(project_dir, 'requirements.txt').read_text()\n\ntests/test_templates.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/lib/python3.10/pathlib.py:960: in __new__\n    self = cls._from_parts(args)\n/usr/lib/python3.10/pathlib.py:594: in _from_parts\n    drv, root, parts = self._parse_args(args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'pathlib.PosixPath'>, args = (None, 'requirements.txt')\n\n    @classmethod\n    def _parse_args(cls, args):\n        # This is useful when you don't want to create an instance, just\n        # canonicalize some constructor arguments.\n        parts = []\n        for a in args:\n            if isinstance(a, PurePath):\n                parts += a._parts\n            else:\n>               a = os.fspath(a)\nE               TypeError: expected str, bytes or os.PathLike object, not NoneType\n\n/usr/lib/python3.10/pathlib.py:578: TypeError"}, "teardown": {"duration": 0.0006133560000005644, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_tz_is_required", "lineno": 22, "outcome": "passed", "keywords": ["test_tz_is_required", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.6638038, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "passed"}, "teardown": {"duration": -1449703972.577232, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_utc_default_datetime_format", "lineno": 28, "outcome": "failed", "keywords": ["test_utc_default_datetime_format", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.5768738, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 31, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4c03f820>\n\n    def test_utc_default_datetime_format(environment):\n        \"\"\"Verify default datetime format can be parsed.\"\"\"\n>       template = environment.from_string(\"{% now 'utc' %}\")\n\ntests/test_time_extension.py:31: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.480195, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[utc]", "lineno": 35, "outcome": "failed", "keywords": ["test_accept_valid_timezones[utc]", "parametrize", "pytestmark", "utc", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.479804, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 39, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4bc84340>\nvalid_tz = 'utc'\n\n    @pytest.mark.parametrize(\"valid_tz\", ['utc', 'local', 'Europe/Berlin'])\n    def test_accept_valid_timezones(environment, valid_tz):\n        \"\"\"Verify that valid timezones are accepted.\"\"\"\n>       template = environment.from_string(f\"{{% now '{valid_tz}', '%Y-%m' %}}\")\n\ntests/test_time_extension.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.4184287, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[local]", "lineno": 35, "outcome": "failed", "keywords": ["test_accept_valid_timezones[local]", "parametrize", "pytestmark", "local", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.418088, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 39, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4c01fc40>\nvalid_tz = 'local'\n\n    @pytest.mark.parametrize(\"valid_tz\", ['utc', 'local', 'Europe/Berlin'])\n    def test_accept_valid_timezones(environment, valid_tz):\n        \"\"\"Verify that valid timezones are accepted.\"\"\"\n>       template = environment.from_string(f\"{{% now '{valid_tz}', '%Y-%m' %}}\")\n\ntests/test_time_extension.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.3579738, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_accept_valid_timezones[Europe/Berlin]", "lineno": 35, "outcome": "failed", "keywords": ["test_accept_valid_timezones[Europe/Berlin]", "parametrize", "pytestmark", "Europe/Berlin", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.3576238, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 39, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4c298220>\nvalid_tz = 'Europe/Berlin'\n\n    @pytest.mark.parametrize(\"valid_tz\", ['utc', 'local', 'Europe/Berlin'])\n    def test_accept_valid_timezones(environment, valid_tz):\n        \"\"\"Verify that valid timezones are accepted.\"\"\"\n>       template = environment.from_string(f\"{{% now '{valid_tz}', '%Y-%m' %}}\")\n\ntests/test_time_extension.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.2969022, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_environment_datetime_format", "lineno": 43, "outcome": "failed", "keywords": ["test_environment_datetime_format", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.2965112, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 48, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4be556c0>\n\n    def test_environment_datetime_format(environment):\n        \"\"\"Verify datetime format can be parsed from environment.\"\"\"\n        environment.datetime_format = '%a, %d %b %Y %H:%M:%S'\n    \n>       template = environment.from_string(\"{% now 'utc' %}\")\n\ntests/test_time_extension.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.2355027, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_add_time", "lineno": 52, "outcome": "failed", "keywords": ["test_add_time", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.2351518, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 57, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4bc7aa10>\n\n    def test_add_time(environment):\n        \"\"\"Verify that added time offset can be parsed.\"\"\"\n        environment.datetime_format = '%a, %d %b %Y %H:%M:%S'\n    \n>       template = environment.from_string(\"{% now 'utc' + 'hours=2,seconds=30' %}\")\n\ntests/test_time_extension.py:57: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.1752667, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_substract_time", "lineno": 61, "outcome": "failed", "keywords": ["test_substract_time", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.1749415, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 66, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4be15c60>\n\n    def test_substract_time(environment):\n        \"\"\"Verify that substracted time offset can be parsed.\"\"\"\n        environment.datetime_format = '%a, %d %b %Y %H:%M:%S'\n    \n>       template = environment.from_string(\"{% now 'utc' - 'minutes=11' %}\")\n\ntests/test_time_extension.py:66: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.1152174, "outcome": "passed"}}, {"nodeid": "tests/test_time_extension.py::test_offset_with_format", "lineno": 70, "outcome": "failed", "keywords": ["test_offset_with_format", "test_time_extension.py", "tests", "testbed", ""], "setup": {"duration": 1449703972.1148713, "outcome": "passed"}, "call": {"duration": 0.0, "outcome": "failed", "crash": {"path": "<unknown>", "lineno": 1, "message": "jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'"}, "traceback": [{"path": "tests/test_time_extension.py", "lineno": 75, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 1108, "message": "in from_string"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 768, "message": "in compile"}, {"path": ".venv/lib/python3.10/site-packages/jinja2/environment.py", "lineno": 939, "message": "in handle_exception"}, {"path": "<unknown>", "lineno": 1, "message": "TemplateSyntaxError"}], "longrepr": "environment = <jinja2.environment.Environment object at 0x7f2b4bc80c40>\n\n    def test_offset_with_format(environment):\n        \"\"\"Verify that offset works together with datetime format.\"\"\"\n        environment.datetime_format = '%d %b %Y %H:%M:%S'\n    \n>       template = environment.from_string(\n            \"{% now 'utc' - 'days=2,minutes=33,seconds=1', '%d %b %Y %H:%M:%S' %}\"\n        )\n\ntests/test_time_extension.py:75: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/jinja2/environment.py:1108: in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:768: in compile\n    self.handle_exception(source=source_hint)\n.venv/lib/python3.10/site-packages/jinja2/environment.py:939: in handle_exception\n    raise rewrite_traceback_stack(source=source)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   ???\nE   jinja2.exceptions.TemplateSyntaxError: expected token 'end of statement block', got 'now'\n\n<unknown>:1: TemplateSyntaxError"}, "teardown": {"duration": -1449703972.053116, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_force_delete", "lineno": 17, "outcome": "failed", "keywords": ["test_force_delete", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.002463784999999774, "outcome": "passed"}, "call": {"duration": 0.0008892280000001307, "outcome": "failed", "crash": {"path": "/testbed/tests/test_utils.py", "lineno": 27, "message": "AssertionError: assert (33060 & 128) == 128\n +  where 33060 = os.stat_result(st_mode=33060, st_ino=3447, st_dev=17, st_nlink=1, st_uid=0, st_gid=0, st_size=9, st_atime=1732308265, st_mtime=1732308265, st_ctime=1732308265).st_mode\n +    where os.stat_result(st_mode=33060, st_ino=3447, st_dev=17, st_nlink=1, st_uid=0, st_gid=0, st_size=9, st_atime=1732308265, st_mtime=1732308265, st_ctime=1732308265) = stat()\n +      where stat = PosixPath('/tmp/pytest-of-root/pytest-0/test_force_delete0/bar').stat\n +  and   128 = stat.S_IWRITE\n +  and   128 = stat.S_IWRITE"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 27, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bb1ffa0>\ntmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_force_delete0')\n\n    def test_force_delete(mocker, tmp_path):\n        \"\"\"Verify `utils.force_delete` makes files writable.\"\"\"\n        ro_file = Path(tmp_path, 'bar')\n        ro_file.write_text(\"Test data\")\n        make_readonly(ro_file)\n    \n        rmtree = mocker.Mock()\n        utils.force_delete(rmtree, ro_file, sys.exc_info())\n    \n>       assert (ro_file.stat().st_mode & stat.S_IWRITE) == stat.S_IWRITE\nE       AssertionError: assert (33060 & 128) == 128\nE        +  where 33060 = os.stat_result(st_mode=33060, st_ino=3447, st_dev=17, st_nlink=1, st_uid=0, st_gid=0, st_size=9, st_atime=1732308265, st_mtime=1732308265, st_ctime=1732308265).st_mode\nE        +    where os.stat_result(st_mode=33060, st_ino=3447, st_dev=17, st_nlink=1, st_uid=0, st_gid=0, st_size=9, st_atime=1732308265, st_mtime=1732308265, st_ctime=1732308265) = stat()\nE        +      where stat = PosixPath('/tmp/pytest-of-root/pytest-0/test_force_delete0/bar').stat\nE        +  and   128 = stat.S_IWRITE\nE        +  and   128 = stat.S_IWRITE\n\ntests/test_utils.py:27: AssertionError"}, "teardown": {"duration": 0.00036670100000080197, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_rmtree", "lineno": 32, "outcome": "failed", "keywords": ["test_rmtree", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.0020591910000007374, "outcome": "passed"}, "call": {"duration": 0.0005657430000010066, "outcome": "failed", "crash": {"path": "/testbed/tests/test_utils.py", "lineno": 41, "message": "AssertionError: assert not True\n +  where True = exists()\n +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0').exists\n +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0'))"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 41, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0')\n\n    def test_rmtree(tmp_path):\n        \"\"\"Verify `utils.rmtree` remove files marked as read-only.\"\"\"\n        file_path = Path(tmp_path, \"bar\")\n        file_path.write_text(\"Test data\")\n        make_readonly(file_path)\n    \n        utils.rmtree(tmp_path)\n    \n>       assert not Path(tmp_path).exists()\nE       AssertionError: assert not True\nE        +  where True = exists()\nE        +    where exists = PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0').exists\nE        +      where PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0') = Path(PosixPath('/tmp/pytest-of-root/pytest-0/test_rmtree0'))\n\ntests/test_utils.py:41: AssertionError"}, "teardown": {"duration": 0.0003271700000002653, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_make_sure_path_exists", "lineno": 43, "outcome": "failed", "keywords": ["test_make_sure_path_exists", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.001997339000000764, "outcome": "passed"}, "call": {"duration": 0.00038931000000097526, "outcome": "failed", "crash": {"path": "/testbed/tests/test_utils.py", "lineno": 59, "message": "AssertionError: assert False\n +  where False = is_dir()\n +    where is_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_make_sure_path_exists0/not_yet_created').is_dir"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 59, "message": "AssertionError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_make_sure_path_exists0')\n\n    def test_make_sure_path_exists(tmp_path):\n        \"\"\"Verify correct True/False response from `utils.make_sure_path_exists`.\n    \n        Should return True if directory exist or created.\n        Should return False if impossible to create directory (for example protected)\n        \"\"\"\n        existing_directory = tmp_path\n        directory_to_create = Path(tmp_path, \"not_yet_created\")\n    \n        utils.make_sure_path_exists(existing_directory)\n        utils.make_sure_path_exists(directory_to_create)\n    \n        # Ensure by base system methods.\n        assert existing_directory.is_dir()\n        assert existing_directory.exists()\n>       assert directory_to_create.is_dir()\nE       AssertionError: assert False\nE        +  where False = is_dir()\nE        +    where is_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_make_sure_path_exists0/not_yet_created').is_dir\n\ntests/test_utils.py:59: AssertionError"}, "teardown": {"duration": 0.0003295110000003376, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_make_sure_path_exists_correctly_handle_os_error", "lineno": 62, "outcome": "failed", "keywords": ["test_make_sure_path_exists_correctly_handle_os_error", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.002094945999999709, "outcome": "passed"}, "call": {"duration": 0.0009225120000007081, "outcome": "failed", "crash": {"path": "/testbed/tests/test_utils.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'OSError'>"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 70, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c01dc30>\n\n    def test_make_sure_path_exists_correctly_handle_os_error(mocker):\n        \"\"\"Verify correct True/False response from `utils.make_sure_path_exists`.\n    \n        Should return True if directory exist or created.\n        Should return False if impossible to create directory (for example protected)\n        \"\"\"\n        mocker.patch(\"pathlib.Path.mkdir\", side_effect=OSError)\n>       with pytest.raises(OSError) as err:\nE       Failed: DID NOT RAISE <class 'OSError'>\n\ntests/test_utils.py:70: Failed"}, "teardown": {"duration": 0.0003628110000004625, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_work_in", "lineno": 74, "outcome": "failed", "keywords": ["test_work_in", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.002007920000000496, "outcome": "passed"}, "call": {"duration": 0.0002881380000001599, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/contextlib.py", "lineno": 135, "message": "TypeError: 'NoneType' object is not an iterator"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 83, "message": ""}, {"path": "/usr/lib/python3.10/contextlib.py", "lineno": 135, "message": "TypeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_work_in0')\n\n    def test_work_in(tmp_path):\n        \"\"\"Verify returning to original folder after `utils.work_in` use.\"\"\"\n        cwd = Path.cwd()\n        ch_to = tmp_path\n    \n        assert ch_to != Path.cwd()\n    \n        # Under context manager we should work in tmp_path.\n>       with utils.work_in(ch_to):\n\ntests/test_utils.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <contextlib._GeneratorContextManager object at 0x7f2b4be56a70>\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n>           return next(self.gen)\nE           TypeError: 'NoneType' object is not an iterator\n\n/usr/lib/python3.10/contextlib.py:135: TypeError"}, "teardown": {"duration": 0.00033781499999996356, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_work_in_without_path", "lineno": 89, "outcome": "failed", "keywords": ["test_work_in_without_path", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.002018426000001128, "outcome": "passed"}, "call": {"duration": 0.00028638299999883543, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/contextlib.py", "lineno": 135, "message": "TypeError: 'NoneType' object is not an iterator"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 94, "message": ""}, {"path": "/usr/lib/python3.10/contextlib.py", "lineno": 135, "message": "TypeError"}], "longrepr": "def test_work_in_without_path():\n        \"\"\"Folder is not changed if no path provided.\"\"\"\n        cwd = Path.cwd()\n    \n>       with utils.work_in():\n\ntests/test_utils.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <contextlib._GeneratorContextManager object at 0x7f2b4c3f16f0>\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n>           return next(self.gen)\nE           TypeError: 'NoneType' object is not an iterator\n\n/usr/lib/python3.10/contextlib.py:135: TypeError"}, "teardown": {"duration": 0.00033447799999919425, "outcome": "passed"}}, {"nodeid": "tests/test_utils.py::test_create_tmp_repo_dir", "lineno": 99, "outcome": "failed", "keywords": ["test_create_tmp_repo_dir", "test_utils.py", "tests", "testbed", ""], "setup": {"duration": 0.0020175849999990447, "outcome": "passed"}, "call": {"duration": 0.0005375399999998365, "outcome": "failed", "crash": {"path": "/testbed/tests/test_utils.py", "lineno": 110, "message": "AttributeError: 'NoneType' object has no attribute 'exists'"}, "traceback": [{"path": "tests/test_utils.py", "lineno": 110, "message": "AttributeError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-root/pytest-0/test_create_tmp_repo_dir0')\n\n    def test_create_tmp_repo_dir(tmp_path):\n        \"\"\"Verify `utils.create_tmp_repo_dir` creates a copy.\"\"\"\n        repo_dir = Path(tmp_path) / 'bar'\n        repo_dir.mkdir()\n        subdirs = ('foo', 'bar', 'foobar')\n        for name in subdirs:\n            (repo_dir / name).mkdir()\n    \n        new_repo_dir = utils.create_tmp_repo_dir(repo_dir)\n    \n>       assert new_repo_dir.exists()\nE       AttributeError: 'NoneType' object has no attribute 'exists'\n\ntests/test_utils.py:110: AttributeError"}, "teardown": {"duration": 0.0003246780000001337, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_raise_if_vcs_not_installed", "lineno": 10, "outcome": "failed", "keywords": ["test_clone_should_raise_if_vcs_not_installed", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0022253209999991697, "outcome": "passed"}, "call": {"duration": 0.0009407489999997409, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 18, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.VCSNotInstalled'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 18, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bb1ea40>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_raise_if_vcs0/clone_dir')\n\n    def test_clone_should_raise_if_vcs_not_installed(mocker, clone_dir):\n        \"\"\"In `clone()`, a `VCSNotInstalled` exception should be raised if no VCS \\\n        is installed.\"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=False)\n    \n        repo_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\n    \n>       with pytest.raises(exceptions.VCSNotInstalled):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.VCSNotInstalled'>\n\ntests/vcs/test_clone.py:18: Failed"}, "teardown": {"duration": 0.0004008479999999537, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_rstrip_trailing_slash_in_repo_url", "lineno": 21, "outcome": "failed", "keywords": ["test_clone_should_rstrip_trailing_slash_in_repo_url", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0021980250000002144, "outcome": "passed"}, "call": {"duration": 0.001633855999999767, "outcome": "failed", "crash": {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError: Expected 'check_output' to be called once. Called 0 times."}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 34, "message": ""}, {"path": "/usr/lib/python3.10/unittest/mock.py", "lineno": 213, "message": "AssertionError"}], "longrepr": "self = <MagicMock name='check_output' spec='function' id='139823935516064'>\nargs = (['git', 'clone', 'https://github.com/foo/bar'],)\nkwargs = {'cwd': PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_rstrip_trail0/clone_dir'), 'stderr': -2}\nmsg = \"Expected 'check_output' to be called once. Called 0 times.\"\n\n    def assert_called_once_with(self, /, *args, **kwargs):\n        \"\"\"assert that the mock was called exactly once and that that call was\n        with the specified arguments.\"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to be called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'check_output' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:940: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c01d090>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_rstrip_trail0/clone_dir')\n\n    def test_clone_should_rstrip_trailing_slash_in_repo_url(mocker, clone_dir):\n        \"\"\"In `clone()`, repo URL's trailing slash should be stripped if one is \\\n        present.\"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n    \n        vcs.clone('https://github.com/foo/bar/', clone_to_dir=clone_dir, no_input=True)\n    \n>       mock_subprocess.assert_called_once_with(\n            ['git', 'clone', 'https://github.com/foo/bar'],\n            cwd=clone_dir,\n            stderr=subprocess.STDOUT,\n        )\n\ntests/vcs/test_clone.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = (['git', 'clone', 'https://github.com/foo/bar'],)\nkwargs = {'cwd': PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_rstrip_trail0/clone_dir'), 'stderr': -2}\n\n    def assert_called_once_with(*args, **kwargs):\n>       return mock.assert_called_once_with(*args, **kwargs)\nE       AssertionError: Expected 'check_output' to be called once. Called 0 times.\n\n/usr/lib/python3.10/unittest/mock.py:213: AssertionError"}, "teardown": {"duration": 0.0006241780000006969, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_abort_if_user_does_not_want_to_reclone", "lineno": 40, "outcome": "failed", "keywords": ["test_clone_should_abort_if_user_does_not_want_to_reclone", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.004252195999999486, "outcome": "passed"}, "call": {"duration": 0.0027600029999987896, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 59, "message": "Failed: DID NOT RAISE <class 'SystemExit'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 59, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c09a200>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_abort_if_use0/clone_dir')\n\n    def test_clone_should_abort_if_user_does_not_want_to_reclone(mocker, clone_dir):\n        \"\"\"In `clone()`, if user doesn't want to reclone, Cookiecutter should exit \\\n        without cloning anything.\"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n        mocker.patch(\n            'cookiecutter.vcs.prompt_and_delete', side_effect=SystemExit, autospec=True\n        )\n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n    \n        # Create repo_dir to trigger prompt_and_delete\n        repo_dir = clone_dir.joinpath('cookiecutter-pytest-plugin')\n        repo_dir.mkdir()\n    \n        repo_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\n    \n>       with pytest.raises(SystemExit):\nE       Failed: DID NOT RAISE <class 'SystemExit'>\n\ntests/vcs/test_clone.py:59: Failed"}, "teardown": {"duration": 0.0004507900000003673, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_silent_exit_if_ok_to_reuse", "lineno": 63, "outcome": "passed", "keywords": ["test_clone_should_silent_exit_if_ok_to_reuse", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002946308000000286, "outcome": "passed"}, "call": {"duration": 0.002372545000000059, "outcome": "passed"}, "teardown": {"duration": 0.0004561039999995131, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-https://github.com/hello/world.git-world]", "lineno": 86, "outcome": "failed", "keywords": ["test_clone_should_invoke_vcs_command[git-https://github.com/hello/world.git-world]", "parametrize", "pytestmark", "git-https://github.com/hello/world.git-world", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002705800999999397, "outcome": "passed"}, "call": {"duration": 0.001905100000000104, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c0/clone_dir/world'"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4ce9e9b0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c0/clone_dir')\nrepo_type = 'git', repo_url = 'https://github.com/hello/world.git'\nrepo_name = 'world'\n\n    @pytest.mark.parametrize(\n        'repo_type, repo_url, repo_name',\n        [\n            ('git', 'https://github.com/hello/world.git', 'world'),\n            ('hg', 'https://bitbucket.org/foo/bar', 'bar'),\n            ('git', 'git@host:gitoliterepo', 'gitoliterepo'),\n            ('git', 'git@gitlab.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n            ('git', 'git@github.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n        ],\n    )\n    def test_clone_should_invoke_vcs_command(\n        mocker, clone_dir, repo_type, repo_url, repo_name\n    ):\n        \"\"\"When `clone()` is called with a git/hg repo, the corresponding VCS \\\n        command should be run via `subprocess.check_output()`.\n    \n        This should take place:\n        * In the correct dir\n        * With the correct args.\n        \"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n        expected_repo_dir = os.path.normpath(os.path.join(clone_dir, repo_name))\n    \n        branch = 'foobar'\n    \n        repo_dir = vcs.clone(\n            repo_url, checkout=branch, clone_to_dir=clone_dir, no_input=True\n        )\n    \n>       assert repo_dir == expected_repo_dir\nE       AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c0/clone_dir/world'\n\ntests/vcs/test_clone.py:121: AssertionError"}, "teardown": {"duration": 0.0004690079999996044, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[hg-https://bitbucket.org/foo/bar-bar]", "lineno": 86, "outcome": "failed", "keywords": ["test_clone_should_invoke_vcs_command[hg-https://bitbucket.org/foo/bar-bar]", "parametrize", "pytestmark", "hg-https://bitbucket.org/foo/bar-bar", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0027446090000005086, "outcome": "passed"}, "call": {"duration": 0.0018179230000008317, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c1/clone_dir/bar'"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4be16560>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c1/clone_dir')\nrepo_type = 'hg', repo_url = 'https://bitbucket.org/foo/bar', repo_name = 'bar'\n\n    @pytest.mark.parametrize(\n        'repo_type, repo_url, repo_name',\n        [\n            ('git', 'https://github.com/hello/world.git', 'world'),\n            ('hg', 'https://bitbucket.org/foo/bar', 'bar'),\n            ('git', 'git@host:gitoliterepo', 'gitoliterepo'),\n            ('git', 'git@gitlab.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n            ('git', 'git@github.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n        ],\n    )\n    def test_clone_should_invoke_vcs_command(\n        mocker, clone_dir, repo_type, repo_url, repo_name\n    ):\n        \"\"\"When `clone()` is called with a git/hg repo, the corresponding VCS \\\n        command should be run via `subprocess.check_output()`.\n    \n        This should take place:\n        * In the correct dir\n        * With the correct args.\n        \"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n        expected_repo_dir = os.path.normpath(os.path.join(clone_dir, repo_name))\n    \n        branch = 'foobar'\n    \n        repo_dir = vcs.clone(\n            repo_url, checkout=branch, clone_to_dir=clone_dir, no_input=True\n        )\n    \n>       assert repo_dir == expected_repo_dir\nE       AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c1/clone_dir/bar'\n\ntests/vcs/test_clone.py:121: AssertionError"}, "teardown": {"duration": 0.00046930899999964026, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@host:gitoliterepo-gitoliterepo]", "lineno": 86, "outcome": "failed", "keywords": ["test_clone_should_invoke_vcs_command[git-git@host:gitoliterepo-gitoliterepo]", "parametrize", "pytestmark", "git-git@host:gitoliterepo-gitoliterepo", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.00277698700000073, "outcome": "passed"}, "call": {"duration": 0.0017754360000008518, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c2/clone_dir/gitoliterepo'"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c19fa60>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c2/clone_dir')\nrepo_type = 'git', repo_url = 'git@host:gitoliterepo'\nrepo_name = 'gitoliterepo'\n\n    @pytest.mark.parametrize(\n        'repo_type, repo_url, repo_name',\n        [\n            ('git', 'https://github.com/hello/world.git', 'world'),\n            ('hg', 'https://bitbucket.org/foo/bar', 'bar'),\n            ('git', 'git@host:gitoliterepo', 'gitoliterepo'),\n            ('git', 'git@gitlab.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n            ('git', 'git@github.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n        ],\n    )\n    def test_clone_should_invoke_vcs_command(\n        mocker, clone_dir, repo_type, repo_url, repo_name\n    ):\n        \"\"\"When `clone()` is called with a git/hg repo, the corresponding VCS \\\n        command should be run via `subprocess.check_output()`.\n    \n        This should take place:\n        * In the correct dir\n        * With the correct args.\n        \"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n        expected_repo_dir = os.path.normpath(os.path.join(clone_dir, repo_name))\n    \n        branch = 'foobar'\n    \n        repo_dir = vcs.clone(\n            repo_url, checkout=branch, clone_to_dir=clone_dir, no_input=True\n        )\n    \n>       assert repo_dir == expected_repo_dir\nE       AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c2/clone_dir/gitoliterepo'\n\ntests/vcs/test_clone.py:121: AssertionError"}, "teardown": {"duration": 0.00047845399999957294, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@gitlab.com:cookiecutter/cookiecutter.git-cookiecutter]", "lineno": 86, "outcome": "failed", "keywords": ["test_clone_should_invoke_vcs_command[git-git@gitlab.com:cookiecutter/cookiecutter.git-cookiecutter]", "parametrize", "pytestmark", "git-git@gitlab.com:cookiecutter/cookiecutter.git-cookiecutter", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002718478000000246, "outcome": "passed"}, "call": {"duration": 0.0017791260000006304, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c3/clone_dir/cookiecutter'"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bc81630>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c3/clone_dir')\nrepo_type = 'git', repo_url = 'git@gitlab.com:cookiecutter/cookiecutter.git'\nrepo_name = 'cookiecutter'\n\n    @pytest.mark.parametrize(\n        'repo_type, repo_url, repo_name',\n        [\n            ('git', 'https://github.com/hello/world.git', 'world'),\n            ('hg', 'https://bitbucket.org/foo/bar', 'bar'),\n            ('git', 'git@host:gitoliterepo', 'gitoliterepo'),\n            ('git', 'git@gitlab.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n            ('git', 'git@github.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n        ],\n    )\n    def test_clone_should_invoke_vcs_command(\n        mocker, clone_dir, repo_type, repo_url, repo_name\n    ):\n        \"\"\"When `clone()` is called with a git/hg repo, the corresponding VCS \\\n        command should be run via `subprocess.check_output()`.\n    \n        This should take place:\n        * In the correct dir\n        * With the correct args.\n        \"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n        expected_repo_dir = os.path.normpath(os.path.join(clone_dir, repo_name))\n    \n        branch = 'foobar'\n    \n        repo_dir = vcs.clone(\n            repo_url, checkout=branch, clone_to_dir=clone_dir, no_input=True\n        )\n    \n>       assert repo_dir == expected_repo_dir\nE       AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c3/clone_dir/cookiecutter'\n\ntests/vcs/test_clone.py:121: AssertionError"}, "teardown": {"duration": 0.0004620030000008768, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command[git-git@github.com:cookiecutter/cookiecutter.git-cookiecutter]", "lineno": 86, "outcome": "failed", "keywords": ["test_clone_should_invoke_vcs_command[git-git@github.com:cookiecutter/cookiecutter.git-cookiecutter]", "parametrize", "pytestmark", "git-git@github.com:cookiecutter/cookiecutter.git-cookiecutter", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0026321029999998302, "outcome": "passed"}, "call": {"duration": 0.0017649960000003517, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c4/clone_dir/cookiecutter'"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 121, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c03da20>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c4/clone_dir')\nrepo_type = 'git', repo_url = 'git@github.com:cookiecutter/cookiecutter.git'\nrepo_name = 'cookiecutter'\n\n    @pytest.mark.parametrize(\n        'repo_type, repo_url, repo_name',\n        [\n            ('git', 'https://github.com/hello/world.git', 'world'),\n            ('hg', 'https://bitbucket.org/foo/bar', 'bar'),\n            ('git', 'git@host:gitoliterepo', 'gitoliterepo'),\n            ('git', 'git@gitlab.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n            ('git', 'git@github.com:cookiecutter/cookiecutter.git', 'cookiecutter'),\n        ],\n    )\n    def test_clone_should_invoke_vcs_command(\n        mocker, clone_dir, repo_type, repo_url, repo_name\n    ):\n        \"\"\"When `clone()` is called with a git/hg repo, the corresponding VCS \\\n        command should be run via `subprocess.check_output()`.\n    \n        This should take place:\n        * In the correct dir\n        * With the correct args.\n        \"\"\"\n        mocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n    \n        mock_subprocess = mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n        )\n        expected_repo_dir = os.path.normpath(os.path.join(clone_dir, repo_name))\n    \n        branch = 'foobar'\n    \n        repo_dir = vcs.clone(\n            repo_url, checkout=branch, clone_to_dir=clone_dir, no_input=True\n        )\n    \n>       assert repo_dir == expected_repo_dir\nE       AssertionError: assert None == '/tmp/pytest-of-root/pytest-0/test_clone_should_invoke_vcs_c4/clone_dir/cookiecutter'\n\ntests/vcs/test_clone.py:121: AssertionError"}, "teardown": {"duration": 0.0004731610000003883, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_repo_typo[fatal: repository 'https://github.com/hackebro/cookiedozer' not found]", "lineno": 138, "outcome": "failed", "keywords": ["test_clone_handles_repo_typo[fatal: repository 'https://github.com/hackebro/cookiedozer' not found]", "parametrize", "pytestmark", "fatal: repository 'https://github.com/hackebro/cookiedozer' not found", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0026009830000006673, "outcome": "passed"}, "call": {"duration": 0.0011818940000001277, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 159, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 159, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3d8070>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_handles_repo_typo_f0/clone_dir')\nerror_message = b\"fatal: repository 'https://github.com/hackebro/cookiedozer' not found\"\n\n    @pytest.mark.parametrize(\n        'error_message',\n        [\n            (b\"fatal: repository 'https://github.com/hackebro/cookiedozer' not found\"),\n            b'hg: abort: HTTP Error 404: Not Found',\n        ],\n    )\n    def test_clone_handles_repo_typo(mocker, clone_dir, error_message):\n        \"\"\"In `clone()`, repository not found errors should raise an \\\n        appropriate exception.\"\"\"\n        # side_effect is set to an iterable here (and below),\n        # because of a Python 3.4 unittest.mock regression\n        # http://bugs.python.org/issue23661\n        mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n            side_effect=[subprocess.CalledProcessError(-1, 'cmd', output=error_message)],\n        )\n    \n        repository_url = 'https://github.com/hackebro/cookiedozer'\n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/vcs/test_clone.py:159: Failed"}, "teardown": {"duration": 0.00043115100000079565, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_repo_typo[hg: abort: HTTP Error 404: Not Found]", "lineno": 138, "outcome": "failed", "keywords": ["test_clone_handles_repo_typo[hg: abort: HTTP Error 404: Not Found]", "parametrize", "pytestmark", "hg: abort: HTTP Error 404: Not Found", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.00312043199999934, "outcome": "passed"}, "call": {"duration": 0.00115916000000027, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 159, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 159, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bb1cbe0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_handles_repo_typo_h0/clone_dir')\nerror_message = b'hg: abort: HTTP Error 404: Not Found'\n\n    @pytest.mark.parametrize(\n        'error_message',\n        [\n            (b\"fatal: repository 'https://github.com/hackebro/cookiedozer' not found\"),\n            b'hg: abort: HTTP Error 404: Not Found',\n        ],\n    )\n    def test_clone_handles_repo_typo(mocker, clone_dir, error_message):\n        \"\"\"In `clone()`, repository not found errors should raise an \\\n        appropriate exception.\"\"\"\n        # side_effect is set to an iterable here (and below),\n        # because of a Python 3.4 unittest.mock regression\n        # http://bugs.python.org/issue23661\n        mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n            side_effect=[subprocess.CalledProcessError(-1, 'cmd', output=error_message)],\n        )\n    \n        repository_url = 'https://github.com/hackebro/cookiedozer'\n>       with pytest.raises(exceptions.RepositoryNotFound) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryNotFound'>\n\ntests/vcs/test_clone.py:159: Failed"}, "teardown": {"duration": 0.00045208200000068643, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_branch_typo[error: pathspec 'unknown_branch' did not match any file(s) known to git]", "lineno": 166, "outcome": "failed", "keywords": ["test_clone_handles_branch_typo[error: pathspec 'unknown_branch' did not match any file(s) known to git]", "parametrize", "pytestmark", "error: pathspec 'unknown_branch' did not match any file(s) known to git", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0026695239999998677, "outcome": "passed"}, "call": {"duration": 0.0011824130000004374, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 184, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryCloneFailed'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 184, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3eca00>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_handles_branch_typo0/clone_dir')\nerror_message = b\"error: pathspec 'unknown_branch' did not match any file(s) known to git\"\n\n    @pytest.mark.parametrize(\n        'error_message',\n        [\n            b\"error: pathspec 'unknown_branch' did not match any file(s) known to git\",\n            b\"hg: abort: unknown revision 'unknown_branch'!\",\n        ],\n    )\n    def test_clone_handles_branch_typo(mocker, clone_dir, error_message):\n        \"\"\"In `clone()`, branch not found errors should raise an \\\n        appropriate exception.\"\"\"\n        mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n            side_effect=[subprocess.CalledProcessError(-1, 'cmd', output=error_message)],\n        )\n    \n        repository_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin'\n>       with pytest.raises(exceptions.RepositoryCloneFailed) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryCloneFailed'>\n\ntests/vcs/test_clone.py:184: Failed"}, "teardown": {"duration": 0.00042806500000125425, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_handles_branch_typo[hg: abort: unknown revision 'unknown_branch'!]", "lineno": 166, "outcome": "failed", "keywords": ["test_clone_handles_branch_typo[hg: abort: unknown revision 'unknown_branch'!]", "parametrize", "pytestmark", "hg: abort: unknown revision 'unknown_branch'!", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0025075060000006033, "outcome": "passed"}, "call": {"duration": 0.0011845919999995402, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 184, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryCloneFailed'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 184, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c2992d0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_handles_branch_typo1/clone_dir')\nerror_message = b\"hg: abort: unknown revision 'unknown_branch'!\"\n\n    @pytest.mark.parametrize(\n        'error_message',\n        [\n            b\"error: pathspec 'unknown_branch' did not match any file(s) known to git\",\n            b\"hg: abort: unknown revision 'unknown_branch'!\",\n        ],\n    )\n    def test_clone_handles_branch_typo(mocker, clone_dir, error_message):\n        \"\"\"In `clone()`, branch not found errors should raise an \\\n        appropriate exception.\"\"\"\n        mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n            side_effect=[subprocess.CalledProcessError(-1, 'cmd', output=error_message)],\n        )\n    \n        repository_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin'\n>       with pytest.raises(exceptions.RepositoryCloneFailed) as err:\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.RepositoryCloneFailed'>\n\ntests/vcs/test_clone.py:184: Failed"}, "teardown": {"duration": 0.0004619429999994651, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_clone.py::test_clone_unknown_subprocess_error", "lineno": 197, "outcome": "failed", "keywords": ["test_clone_unknown_subprocess_error", "test_clone.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002538570000000462, "outcome": "passed"}, "call": {"duration": 0.001137864000000377, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_clone.py", "lineno": 208, "message": "Failed: DID NOT RAISE <class 'subprocess.CalledProcessError'>"}, "traceback": [{"path": "tests/vcs/test_clone.py", "lineno": 208, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bb2a920>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_clone_unknown_subprocess_0/clone_dir')\n\n    def test_clone_unknown_subprocess_error(mocker, clone_dir):\n        \"\"\"In `clone()`, unknown subprocess errors should be raised.\"\"\"\n        mocker.patch(\n            'cookiecutter.vcs.subprocess.check_output',\n            autospec=True,\n            side_effect=[\n                subprocess.CalledProcessError(-1, 'cmd', output=b'Something went wrong')\n            ],\n        )\n    \n>       with pytest.raises(subprocess.CalledProcessError):\nE       Failed: DID NOT RAISE <class 'subprocess.CalledProcessError'>\n\ntests/vcs/test_clone.py:208: Failed"}, "teardown": {"duration": 0.0004184059999996492, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "parametrize", "pytestmark", "git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002488581999999795, "outcome": "passed"}, "call": {"duration": 0.00043699699999955044, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('git', 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\nexp_repo_type = 'git'\nexp_repo_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('git', 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0003900980000004495, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[hg+https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[hg+https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "parametrize", "pytestmark", "hg+https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002389018999998882, "outcome": "passed"}, "call": {"duration": 0.00040652900000104353, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('hg', 'https://bitbucket.org/foo/bar.hg')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('hg+https://bitbucket.org/foo/bar.hg')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'hg+https://bitbucket.org/foo/bar.hg', exp_repo_type = 'hg'\nexp_repo_url = 'https://bitbucket.org/foo/bar.hg'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('hg', 'https://bitbucket.org/foo/bar.hg')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('hg+https://bitbucket.org/foo/bar.hg')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0003915850000009158, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git]", "parametrize", "pytestmark", "https://github.com/pytest-dev/cookiecutter-pytest-plugin.git-git-https://github.com/pytest-dev/cookiecutter-pytest-plugin.git", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0024479829999997094, "outcome": "passed"}, "call": {"duration": 0.0004250219999999416, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('git', 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\nexp_repo_type = 'git'\nexp_repo_url = 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('git', 'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/pytest-dev/cookiecutter-pytest-plugin.git')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0004072920000002256, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg]", "parametrize", "pytestmark", "https://bitbucket.org/foo/bar.hg-hg-https://bitbucket.org/foo/bar.hg", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0022984729999997455, "outcome": "passed"}, "call": {"duration": 0.0004129620000004053, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('hg', 'https://bitbucket.org/foo/bar.hg')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://bitbucket.org/foo/bar.hg')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'https://bitbucket.org/foo/bar.hg', exp_repo_type = 'hg'\nexp_repo_url = 'https://bitbucket.org/foo/bar.hg'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('hg', 'https://bitbucket.org/foo/bar.hg')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://bitbucket.org/foo/bar.hg')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.00040191099999908886, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage.git-git-https://github.com/audreyfeldroy/cookiecutter-pypackage.git]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage.git-git-https://github.com/audreyfeldroy/cookiecutter-pypackage.git]", "parametrize", "pytestmark", "https://github.com/audreyfeldroy/cookiecutter-pypackage.git-git-https://github.com/audreyfeldroy/cookiecutter-pypackage.git", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0023360619999994725, "outcome": "passed"}, "call": {"duration": 0.0004010079999989813, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('git', 'https://github.com/audreyfeldroy/cookiecutter-pypackage.git')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/audreyfeldroy/cookiecutter-pypackage.git')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'https://github.com/audreyfeldroy/cookiecutter-pypackage.git'\nexp_repo_type = 'git'\nexp_repo_url = 'https://github.com/audreyfeldroy/cookiecutter-pypackage.git'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('git', 'https://github.com/audreyfeldroy/cookiecutter-pypackage.git')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/audreyfeldroy/cookiecutter-pypackage.git')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0003921210000008557, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage-git-https://github.com/audreyfeldroy/cookiecutter-pypackage]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[https://github.com/audreyfeldroy/cookiecutter-pypackage-git-https://github.com/audreyfeldroy/cookiecutter-pypackage]", "parametrize", "pytestmark", "https://github.com/audreyfeldroy/cookiecutter-pypackage-git-https://github.com/audreyfeldroy/cookiecutter-pypackage", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002283736000000758, "outcome": "passed"}, "call": {"duration": 0.00040235800000054667, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('git', 'https://github.com/audreyfeldroy/cookiecutter-pypackage')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/audreyfeldroy/cookiecutter-pypackage')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'https://github.com/audreyfeldroy/cookiecutter-pypackage'\nexp_repo_type = 'git'\nexp_repo_url = 'https://github.com/audreyfeldroy/cookiecutter-pypackage'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('git', 'https://github.com/audreyfeldroy/cookiecutter-pypackage')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://github.com/audreyfeldroy/cookiecutter-pypackage')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0003858839999999475, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git-git-git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git-git-git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git]", "parametrize", "pytestmark", "git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git-git-git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002327494999999402, "outcome": "passed"}, "call": {"duration": 0.00038792099999973573, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('git', 'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git'\nexp_repo_type = 'git'\nexp_repo_url = 'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('git', 'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.00037702600000066866, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_known_repo[https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket-hg-https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket]", "lineno": 7, "outcome": "failed", "keywords": ["test_identify_known_repo[https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket-hg-https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket]", "parametrize", "pytestmark", "https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket-hg-https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002287049999999624, "outcome": "passed"}, "call": {"duration": 0.0003994279999997019, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError: assert None == ('hg', 'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket')\n +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket')\n +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 51, "message": "AssertionError"}], "longrepr": "repo_url = 'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket'\nexp_repo_type = 'hg'\nexp_repo_url = 'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket'\n\n    @pytest.mark.parametrize(\n        'repo_url, exp_repo_type, exp_repo_url',\n        [\n            (\n                'git+https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            (\n                'hg+https://bitbucket.org/foo/bar.hg',\n                'hg',\n                'https://bitbucket.org/foo/bar.hg',\n            ),\n            (\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n                'git',\n                'https://github.com/pytest-dev/cookiecutter-pytest-plugin.git',\n            ),\n            ('https://bitbucket.org/foo/bar.hg', 'hg', 'https://bitbucket.org/foo/bar.hg'),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage.git',\n            ),\n            (\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n                'git',\n                'https://github.com/audreyfeldroy/cookiecutter-pypackage',\n            ),\n            (\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n                'git',\n                'git@gitorious.org:cookiecutter-gitorious/cookiecutter-gitorious.git',\n            ),\n            (\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n                'hg',\n                'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket',\n            ),\n        ],\n    )\n    def test_identify_known_repo(repo_url, exp_repo_type, exp_repo_url):\n        \"\"\"Verify different correct repositories url syntax is correctly transformed.\"\"\"\n>       assert vcs.identify_repo(repo_url) == (exp_repo_type, exp_repo_url)\nE       AssertionError: assert None == ('hg', 'https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket')\nE        +  where None = <function identify_repo at 0x7f2b4cc4dfc0>('https://audreyr@bitbucket.org/audreyr/cookiecutter-bitbucket')\nE        +    where <function identify_repo at 0x7f2b4cc4dfc0> = vcs.identify_repo\n\ntests/vcs/test_identify_repo.py:51: AssertionError"}, "teardown": {"duration": 0.0003907729999994558, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+git]", "lineno": 67, "outcome": "failed", "keywords": ["test_identify_raise_on_unknown_repo[foo+git]", "foo+git", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002281963999999803, "outcome": "passed"}, "call": {"duration": 0.0003186769999992123, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed"}], "longrepr": "unknown_repo_type_url = 'foo+git'\n\n    def test_identify_raise_on_unknown_repo(unknown_repo_type_url):\n        \"\"\"Verify different incorrect repositories url syntax trigger error raising.\"\"\"\n>       with pytest.raises(exceptions.UnknownRepoType):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>\n\ntests/vcs/test_identify_repo.py:70: Failed"}, "teardown": {"duration": 0.00035542400000032615, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+hg]", "lineno": 67, "outcome": "failed", "keywords": ["test_identify_raise_on_unknown_repo[foo+hg]", "foo+hg", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002235397999999833, "outcome": "passed"}, "call": {"duration": 0.00027778399999967007, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed"}], "longrepr": "unknown_repo_type_url = 'foo+hg'\n\n    def test_identify_raise_on_unknown_repo(unknown_repo_type_url):\n        \"\"\"Verify different incorrect repositories url syntax trigger error raising.\"\"\"\n>       with pytest.raises(exceptions.UnknownRepoType):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>\n\ntests/vcs/test_identify_repo.py:70: Failed"}, "teardown": {"duration": 0.000350601999999256, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foo+bar]", "lineno": 67, "outcome": "failed", "keywords": ["test_identify_raise_on_unknown_repo[foo+bar]", "foo+bar", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0035217259999988926, "outcome": "passed"}, "call": {"duration": 0.000317754999999309, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed"}], "longrepr": "unknown_repo_type_url = 'foo+bar'\n\n    def test_identify_raise_on_unknown_repo(unknown_repo_type_url):\n        \"\"\"Verify different incorrect repositories url syntax trigger error raising.\"\"\"\n>       with pytest.raises(exceptions.UnknownRepoType):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>\n\ntests/vcs/test_identify_repo.py:70: Failed"}, "teardown": {"duration": 0.0003581149999991595, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[foobar]", "lineno": 67, "outcome": "failed", "keywords": ["test_identify_raise_on_unknown_repo[foobar]", "foobar", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0022573440000002165, "outcome": "passed"}, "call": {"duration": 0.00027372600000141745, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed"}], "longrepr": "unknown_repo_type_url = 'foobar'\n\n    def test_identify_raise_on_unknown_repo(unknown_repo_type_url):\n        \"\"\"Verify different incorrect repositories url syntax trigger error raising.\"\"\"\n>       with pytest.raises(exceptions.UnknownRepoType):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>\n\ntests/vcs/test_identify_repo.py:70: Failed"}, "teardown": {"duration": 0.00035732999999993353, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_identify_repo.py::test_identify_raise_on_unknown_repo[http://norepotypespecified.com]", "lineno": 67, "outcome": "failed", "keywords": ["test_identify_raise_on_unknown_repo[http://norepotypespecified.com]", "http://norepotypespecified.com", "test_identify_repo.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002242903000000851, "outcome": "passed"}, "call": {"duration": 0.0002855620000001835, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>"}, "traceback": [{"path": "tests/vcs/test_identify_repo.py", "lineno": 70, "message": "Failed"}], "longrepr": "unknown_repo_type_url = 'http://norepotypespecified.com'\n\n    def test_identify_raise_on_unknown_repo(unknown_repo_type_url):\n        \"\"\"Verify different incorrect repositories url syntax trigger error raising.\"\"\"\n>       with pytest.raises(exceptions.UnknownRepoType):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.UnknownRepoType'>\n\ntests/vcs/test_identify_repo.py:70: Failed"}, "teardown": {"duration": 0.000415924000000345, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[-False]", "lineno": 7, "outcome": "failed", "keywords": ["test_is_vcs_installed[-False]", "parametrize", "pytestmark", "-False", "test_is_vcs_installed.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002363751999999053, "outcome": "passed"}, "call": {"duration": 0.0011093369999990443, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError: assert None == False\n +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\n +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed"}, "traceback": [{"path": "tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bbde8f0>\nwhich_return = '', result = False\n\n    @pytest.mark.parametrize(\n        'which_return, result',\n        [('', False), (None, False), (False, False), ('/usr/local/bin/git', True)],\n    )\n    def test_is_vcs_installed(mocker, which_return, result):\n        \"\"\"Verify `is_vcs_installed` function correctly handles `which` answer.\"\"\"\n        mocker.patch('cookiecutter.vcs.which', autospec=True, return_value=which_return)\n>       assert vcs.is_vcs_installed('git') == result\nE       AssertionError: assert None == False\nE        +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\nE        +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed\n\ntests/vcs/test_is_vcs_installed.py:15: AssertionError"}, "teardown": {"duration": 0.00040413400000005595, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[None-False]", "lineno": 7, "outcome": "failed", "keywords": ["test_is_vcs_installed[None-False]", "parametrize", "pytestmark", "None-False", "test_is_vcs_installed.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002341911000000252, "outcome": "passed"}, "call": {"duration": 0.0018838549999991017, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError: assert None == False\n +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\n +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed"}, "traceback": [{"path": "tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4be16620>\nwhich_return = None, result = False\n\n    @pytest.mark.parametrize(\n        'which_return, result',\n        [('', False), (None, False), (False, False), ('/usr/local/bin/git', True)],\n    )\n    def test_is_vcs_installed(mocker, which_return, result):\n        \"\"\"Verify `is_vcs_installed` function correctly handles `which` answer.\"\"\"\n        mocker.patch('cookiecutter.vcs.which', autospec=True, return_value=which_return)\n>       assert vcs.is_vcs_installed('git') == result\nE       AssertionError: assert None == False\nE        +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\nE        +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed\n\ntests/vcs/test_is_vcs_installed.py:15: AssertionError"}, "teardown": {"duration": 0.00042795799999950646, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[False-False]", "lineno": 7, "outcome": "failed", "keywords": ["test_is_vcs_installed[False-False]", "parametrize", "pytestmark", "False-False", "test_is_vcs_installed.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.002519128000001203, "outcome": "passed"}, "call": {"duration": 0.0010586019999987428, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError: assert None == False\n +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\n +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed"}, "traceback": [{"path": "tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bbdf730>\nwhich_return = False, result = False\n\n    @pytest.mark.parametrize(\n        'which_return, result',\n        [('', False), (None, False), (False, False), ('/usr/local/bin/git', True)],\n    )\n    def test_is_vcs_installed(mocker, which_return, result):\n        \"\"\"Verify `is_vcs_installed` function correctly handles `which` answer.\"\"\"\n        mocker.patch('cookiecutter.vcs.which', autospec=True, return_value=which_return)\n>       assert vcs.is_vcs_installed('git') == result\nE       AssertionError: assert None == False\nE        +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\nE        +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed\n\ntests/vcs/test_is_vcs_installed.py:15: AssertionError"}, "teardown": {"duration": 0.0003999679999999728, "outcome": "passed"}}, {"nodeid": "tests/vcs/test_is_vcs_installed.py::test_is_vcs_installed[/usr/local/bin/git-True]", "lineno": 7, "outcome": "failed", "keywords": ["test_is_vcs_installed[/usr/local/bin/git-True]", "parametrize", "pytestmark", "/usr/local/bin/git-True", "test_is_vcs_installed.py", "vcs", "tests", "testbed", ""], "setup": {"duration": 0.0023870160000001306, "outcome": "passed"}, "call": {"duration": 0.0010190930000000264, "outcome": "failed", "crash": {"path": "/testbed/tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError: assert None == True\n +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\n +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed"}, "traceback": [{"path": "tests/vcs/test_is_vcs_installed.py", "lineno": 15, "message": "AssertionError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bbded70>\nwhich_return = '/usr/local/bin/git', result = True\n\n    @pytest.mark.parametrize(\n        'which_return, result',\n        [('', False), (None, False), (False, False), ('/usr/local/bin/git', True)],\n    )\n    def test_is_vcs_installed(mocker, which_return, result):\n        \"\"\"Verify `is_vcs_installed` function correctly handles `which` answer.\"\"\"\n        mocker.patch('cookiecutter.vcs.which', autospec=True, return_value=which_return)\n>       assert vcs.is_vcs_installed('git') == result\nE       AssertionError: assert None == True\nE        +  where None = <function is_vcs_installed at 0x7f2b4cc4e050>('git')\nE        +    where <function is_vcs_installed at 0x7f2b4cc4e050> = vcs.is_vcs_installed\n\ntests/vcs/test_is_vcs_installed.py:15: AssertionError"}, "teardown": {"duration": 0.0003931319999992411, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_local_file", "lineno": 31, "outcome": "failed", "keywords": ["test_unzip_local_file", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0024151889999988185, "outcome": "passed"}, "call": {"duration": 0.0010921709999998086, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 42, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 42, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4be17100>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_local_file0/clone_dir')\n\n    def test_unzip_local_file(mocker, clone_dir):\n        \"\"\"Local file reference can be unzipped.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        output_dir = zipfile.unzip(\n            'tests/files/fake-repo-tmpl.zip', is_url=False, clone_to_dir=str(clone_dir)\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:42: AttributeError"}, "teardown": {"duration": 0.00041051299999850244, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_environment_password", "lineno": 45, "outcome": "failed", "keywords": ["test_unzip_protected_local_file_environment_password", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0025607919999988127, "outcome": "passed"}, "call": {"duration": 0.0008934519999996837, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 59, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 59, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bea1480>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_protected_local_fil0/clone_dir')\n\n    def test_unzip_protected_local_file_environment_password(mocker, clone_dir):\n        \"\"\"In `unzip()`, the environment can be used to provide a repo password.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        output_dir = zipfile.unzip(\n            'tests/files/protected-fake-repo-tmpl.zip',\n            is_url=False,\n            clone_to_dir=str(clone_dir),\n            password='sekrit',\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:59: AttributeError"}, "teardown": {"duration": 0.00041382300000059047, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_bad_environment_password", "lineno": 62, "outcome": "failed", "keywords": ["test_unzip_protected_local_file_bad_environment_password", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002675796000000119, "outcome": "passed"}, "call": {"duration": 0.0009501679999992518, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 69, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 69, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bbddea0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_protected_local_fil1/clone_dir')\n\n    def test_unzip_protected_local_file_bad_environment_password(mocker, clone_dir):\n        \"\"\"In `unzip()`, an error occurs if the environment has a bad password.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:69: Failed"}, "teardown": {"duration": 0.0004160329999987056, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_password_with_noinput", "lineno": 77, "outcome": "failed", "keywords": ["test_unzip_protected_local_file_user_password_with_noinput", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002418703000000022, "outcome": "passed"}, "call": {"duration": 0.0009186329999995024, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 84, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 84, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bea1cc0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_protected_local_fil2/clone_dir')\n\n    def test_unzip_protected_local_file_user_password_with_noinput(mocker, clone_dir):\n        \"\"\"Can't unpack a password-protected repo in no_input mode.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:84: Failed"}, "teardown": {"duration": 0.0003998599999999186, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_password", "lineno": 92, "outcome": "failed", "keywords": ["test_unzip_protected_local_file_user_password", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002500460999998566, "outcome": "passed"}, "call": {"duration": 0.001388405000000148, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 106, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 106, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c098220>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_protected_local_fil3/clone_dir')\n\n    def test_unzip_protected_local_file_user_password(mocker, clone_dir):\n        \"\"\"A password-protected local file reference can be unzipped.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n        mocker.patch('cookiecutter.zipfile.read_repo_password', return_value='sekrit')\n    \n        output_dir = zipfile.unzip(\n            'tests/files/protected-fake-repo-tmpl.zip',\n            is_url=False,\n            clone_to_dir=str(clone_dir),\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:106: AttributeError"}, "teardown": {"duration": 0.0004514960000001622, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_protected_local_file_user_bad_password", "lineno": 109, "outcome": "failed", "keywords": ["test_unzip_protected_local_file_user_bad_password", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.00255808800000068, "outcome": "passed"}, "call": {"duration": 0.0013846599999993714, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 119, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 119, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c193580>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_protected_local_fil4/clone_dir')\n\n    def test_unzip_protected_local_file_user_bad_password(mocker, clone_dir):\n        \"\"\"Error in `unzip()`, if user can't provide a valid password.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n        mocker.patch(\n            'cookiecutter.zipfile.read_repo_password', return_value='not-the-right-password'\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:119: Failed"}, "teardown": {"duration": 0.00042725700000012523, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_empty_zip_file", "lineno": 126, "outcome": "failed", "keywords": ["test_empty_zip_file", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002707420000000127, "outcome": "passed"}, "call": {"duration": 0.0009379420000001915, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 133, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 133, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c093520>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_empty_zip_file0/clone_dir')\n\n    def test_empty_zip_file(mocker, clone_dir):\n        \"\"\"In `unzip()`, an empty file raises an error.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:133: Failed"}, "teardown": {"duration": 0.0004081510000002453, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_non_repo_zip_file", "lineno": 138, "outcome": "failed", "keywords": ["test_non_repo_zip_file", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0025438519999987363, "outcome": "passed"}, "call": {"duration": 0.0009059680000014225, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 145, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 145, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c025f30>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_non_repo_zip_file0/clone_dir')\n\n    def test_non_repo_zip_file(mocker, clone_dir):\n        \"\"\"In `unzip()`, a repository must have a top level directory.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:145: Failed"}, "teardown": {"duration": 0.000429975999999499, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_bad_zip_file", "lineno": 150, "outcome": "failed", "keywords": ["test_bad_zip_file", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002637285000000489, "outcome": "passed"}, "call": {"duration": 0.0009250240000007182, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 157, "message": "Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 157, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c03dc30>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_bad_zip_file0/clone_dir')\n\n    def test_bad_zip_file(mocker, clone_dir):\n        \"\"\"In `unzip()`, a corrupted zip file raises an error.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n>       with pytest.raises(InvalidZipRepository):\nE       Failed: DID NOT RAISE <class 'cookiecutter.exceptions.InvalidZipRepository'>\n\ntests/zipfile/test_unzip.py:157: Failed"}, "teardown": {"duration": 0.000405553000000225, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url", "lineno": 162, "outcome": "failed", "keywords": ["test_unzip_url", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0024914389999999287, "outcome": "passed"}, "call": {"duration": 0.0018199650000010337, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 184, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 184, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c3db700>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_url0/clone_dir')\n\n    def test_unzip_url(mocker, clone_dir):\n        \"\"\"In `unzip()`, a url will be downloaded and unzipped.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        request = mocker.MagicMock()\n        request.iter_content.return_value = mock_download()\n    \n        mocker.patch(\n            'cookiecutter.zipfile.requests.get',\n            return_value=request,\n            autospec=True,\n        )\n    \n        output_dir = zipfile.unzip(\n            'https://example.com/path/to/fake-repo-tmpl.zip',\n            is_url=True,\n            clone_to_dir=str(clone_dir),\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:184: AttributeError"}, "teardown": {"duration": 0.000441457999999173, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_with_empty_chunks", "lineno": 187, "outcome": "failed", "keywords": ["test_unzip_url_with_empty_chunks", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0027892699999991777, "outcome": "passed"}, "call": {"duration": 0.001898284000001027, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 209, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 209, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bc87760>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_url_with_empty_chun0/clone_dir')\n\n    def test_unzip_url_with_empty_chunks(mocker, clone_dir):\n        \"\"\"In `unzip()` empty chunk must be ignored.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        request = mocker.MagicMock()\n        request.iter_content.return_value = mock_download_with_empty_chunks()\n    \n        mocker.patch(\n            'cookiecutter.zipfile.requests.get',\n            return_value=request,\n            autospec=True,\n        )\n    \n        output_dir = zipfile.unzip(\n            'https://example.com/path/to/fake-repo-tmpl.zip',\n            is_url=True,\n            clone_to_dir=str(clone_dir),\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:209: AttributeError"}, "teardown": {"duration": 0.0004079770000000593, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_existing_cache", "lineno": 212, "outcome": "failed", "keywords": ["test_unzip_url_existing_cache", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002481954000000286, "outcome": "passed"}, "call": {"duration": 0.002082523000000336, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 238, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 238, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c024100>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_url_existing_cache0/clone_dir')\n\n    def test_unzip_url_existing_cache(mocker, clone_dir):\n        \"\"\"Url should be downloaded and unzipped, old zip file will be removed.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=True, autospec=True\n        )\n    \n        request = mocker.MagicMock()\n        request.iter_content.return_value = mock_download()\n    \n        mocker.patch(\n            'cookiecutter.zipfile.requests.get',\n            return_value=request,\n            autospec=True,\n        )\n    \n        # Create an existing cache of the zipfile\n        existing_zip = clone_dir.joinpath('fake-repo-tmpl.zip')\n        existing_zip.write_text('This is an existing zipfile')\n    \n        output_dir = zipfile.unzip(\n            'https://example.com/path/to/fake-repo-tmpl.zip',\n            is_url=True,\n            clone_to_dir=str(clone_dir),\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:238: AttributeError"}, "teardown": {"duration": 0.000416167000000911, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_url_existing_cache_no_input", "lineno": 241, "outcome": "failed", "keywords": ["test_unzip_url_existing_cache_no_input", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0026004309999994035, "outcome": "passed"}, "call": {"duration": 0.0015060049999995329, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 264, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 264, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4bff0a90>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_url_existing_cache_0/clone_dir')\n\n    def test_unzip_url_existing_cache_no_input(mocker, clone_dir):\n        \"\"\"If no_input is provided, the existing file should be removed.\"\"\"\n        request = mocker.MagicMock()\n        request.iter_content.return_value = mock_download()\n    \n        mocker.patch(\n            'cookiecutter.zipfile.requests.get',\n            return_value=request,\n            autospec=True,\n        )\n    \n        # Create an existing cache of the zipfile\n        existing_zip = clone_dir.joinpath('fake-repo-tmpl.zip')\n        existing_zip.write_text('This is an existing zipfile')\n    \n        output_dir = zipfile.unzip(\n            'https://example.com/path/to/fake-repo-tmpl.zip',\n            is_url=True,\n            clone_to_dir=str(clone_dir),\n            no_input=True,\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:264: AttributeError"}, "teardown": {"duration": 0.00043714799999960974, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_should_abort_if_no_redownload", "lineno": 266, "outcome": "failed", "keywords": ["test_unzip_should_abort_if_no_redownload", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.002791454999998777, "outcome": "passed"}, "call": {"duration": 0.002049936000000585, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 284, "message": "Failed: DID NOT RAISE <class 'SystemExit'>"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 284, "message": "Failed"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c29a1a0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_should_abort_if_no_0/clone_dir')\n\n    def test_unzip_should_abort_if_no_redownload(mocker, clone_dir):\n        \"\"\"Should exit without cloning anything If no redownload.\"\"\"\n        mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', side_effect=SystemExit, autospec=True\n        )\n    \n        mock_requests_get = mocker.patch(\n            'cookiecutter.zipfile.requests.get',\n            autospec=True,\n        )\n    \n        # Create an existing cache of the zipfile\n        existing_zip = clone_dir.joinpath('fake-repo-tmpl.zip')\n        existing_zip.write_text('This is an existing zipfile')\n    \n        zipfile_url = 'https://example.com/path/to/fake-repo-tmpl.zip'\n    \n>       with pytest.raises(SystemExit):\nE       Failed: DID NOT RAISE <class 'SystemExit'>\n\ntests/zipfile/test_unzip.py:284: Failed"}, "teardown": {"duration": 0.0004167319999996977, "outcome": "passed"}}, {"nodeid": "tests/zipfile/test_unzip.py::test_unzip_is_ok_to_reuse", "lineno": 289, "outcome": "failed", "keywords": ["test_unzip_is_ok_to_reuse", "test_unzip.py", "zipfile", "tests", "testbed", ""], "setup": {"duration": 0.0025490529999991907, "outcome": "passed"}, "call": {"duration": 0.0018860740000015142, "outcome": "failed", "crash": {"path": "/testbed/tests/zipfile/test_unzip.py", "lineno": 307, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/zipfile/test_unzip.py", "lineno": 307, "message": "AttributeError"}], "longrepr": "mocker = <pytest_mock.plugin.MockerFixture object at 0x7f2b4c099cc0>\nclone_dir = PosixPath('/tmp/pytest-of-root/pytest-0/test_unzip_is_ok_to_reuse0/clone_dir')\n\n    def test_unzip_is_ok_to_reuse(mocker, clone_dir):\n        \"\"\"Already downloaded zip should not be downloaded again.\"\"\"\n        mock_prompt_and_delete = mocker.patch(\n            'cookiecutter.zipfile.prompt_and_delete', return_value=False, autospec=True\n        )\n    \n        request = mocker.MagicMock()\n    \n        existing_zip = clone_dir.joinpath('fake-repo-tmpl.zip')\n        shutil.copy('tests/files/fake-repo-tmpl.zip', existing_zip)\n    \n        output_dir = zipfile.unzip(\n            'https://example.com/path/to/fake-repo-tmpl.zip',\n            is_url=True,\n            clone_to_dir=str(clone_dir),\n        )\n    \n>       assert output_dir.startswith(tempfile.gettempdir())\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/zipfile/test_unzip.py:307: AttributeError"}, "teardown": {"duration": 0.008167033999999518, "outcome": "passed"}}]}