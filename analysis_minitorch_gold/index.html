
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Select repository - spec2repo</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#select-repository" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="spec2repo" class="md-header__button md-logo" aria-label="spec2repo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            spec2repo
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Select repository
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="spec2repo" class="md-nav__button md-logo" aria-label="spec2repo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    spec2repo
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Setup
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../repos/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboard/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboard
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytests/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Submission Pytests
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Submission Analysis
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#implementation-summary-minitorchgold" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Summary minitorch::gold
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#failed-pytest-outputs-test_autodiffpytest_chain_rule1" class="md-nav__link">
    <span class="md-ellipsis">
      Failed pytest outputs### test_autodiff.py::test_chain_rule1
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Failed pytest outputs### test_autodiff.py::test_chain_rule1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule2" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule3" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_chain_rule4" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_chain_rule4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop1" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop2" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop3" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_autodiffpytest_backprop4" class="md-nav__link">
    <span class="md-ellipsis">
      test_autodiff.py::test_backprop4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv1d_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv1d_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_batch" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv_channel" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv_channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_convpytest_conv2" class="md-nav__link">
    <span class="md-ellipsis">
      test_conv.py::test_conv2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_demo" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_demo
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulepytest_stacked_module" class="md-nav__link">
    <span class="md-ellipsis">
      test_module.py::test_stacked_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_linear" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_linear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_modulespytest_nn_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_modules.py::test_nn_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_avg" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_avg
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_max_pool" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_max_pool
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_drop" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_drop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_nnpytest_log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      test_nn.py::test_log_softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_same_as_python" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_same_as_python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_relu_back" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_relu_back
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_id" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_lt" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_lt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_max" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_eq" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_eq
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sigmoid
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_transitive" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_transitive
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_symmetric" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_symmetric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_other" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_other
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_zip_with" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_zip_with
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum_distribute" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum_distribute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_sum" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_sum
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_prod" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_prod
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_neglist" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_negList
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_operatorspytest_backs" class="md-nav__link">
    <span class="md-ellipsis">
      test_operators.py::test_backs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_central_diff" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_central_diff
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_simple" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_simple
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_argsfn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_args[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_one_derivativefn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_one_derivative[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_scalarpytest_two_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_scalar.py::test_two_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_create" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_argsfn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_args[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_argsfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_args[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_one_derivativefn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_one_derivative[fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_size" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_grad_reducefn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_grad_reduce[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_gradfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_two_grad_broadcastfn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_two_grad_broadcast[fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromlist" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromlist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_back_view" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_back_view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_fromnumpy" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_fromnumpy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_one_dim_2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_one_dim_2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensorpytest_reduce_forward_all_dims" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor.py::test_reduce_forward_all_dims
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_layout" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_layout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_enumeration" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_enumeration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_index" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_index
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_permute" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_permute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_shape_broadcast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_shape_broadcast
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_datapytest_string" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_data.py::test_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_createfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_create[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_argsfast-fn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_args[fast-fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_argsfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_args[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn6" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn6]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn7" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn7]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn8" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn8]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn9" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn9]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn10" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn10]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn11" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn11]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn12" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn12]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_one_derivativefast-fn13" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_one_derivative[fast-fn13]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_gradfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_reducefast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_reduce[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn0" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn0]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn1" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn1]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn2]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn3" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn3]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn4" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn4]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_two_grad_broadcastfast-fn5" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_two_grad_broadcast[fast-fn5]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_permutefast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_permute[fast]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_mm2" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_mm2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_tensor_generalpytest_bmmfast" class="md-nav__link">
    <span class="md-ellipsis">
      test_tensor_general.py::test_bmm[fast]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#diff-to-gold" class="md-nav__link">
    <span class="md-ellipsis">
      Diff to gold
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="select-repository">Select repository</h1>
<p><a class="md-button" href="/analysis_minitorch_gold">minitorch_gold</a></p>
<h2 id="implementation-summary-minitorchgold">Implementation Summary minitorch::gold</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">metric</th>
<th style="text-align: center;">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">no. lines gen</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: left;">no. code toks gen</td>
<td style="text-align: center;">1120</td>
</tr>
<tr>
<td style="text-align: left;">failed</td>
<td style="text-align: center;">211</td>
</tr>
<tr>
<td style="text-align: left;">xfailed</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">passed</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">230</td>
</tr>
<tr>
<td style="text-align: left;">duration</td>
<td style="text-align: center;">205.31s</td>
</tr>
</tbody>
</table>
<h2 id="failed-pytest-outputs-test_autodiffpytest_chain_rule1">Failed pytest outputs### test_autodiff.py::test_chain_rule1</h2>
<details><summary> <pre>test_autodiff.py::test_chain_rule1</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule1() -> None:
        x = minitorch.Scalar(0.0)
        constant = minitorch.Scalar(
            0.0, ScalarHistory(Function1, ctx=Context(), inputs=[x, x])
        )
>       back = constant.chain_rule(d_output=5)

tests/test_autodiff.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule2">test_autodiff.py::test_chain_rule2</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule2</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule2() -> None:
        var = minitorch.Scalar(0.0, ScalarHistory())
        constant = minitorch.Scalar(
            0.0, ScalarHistory(Function1, ctx=Context(), inputs=[var, var])
        )
>       back = constant.chain_rule(d_output=5)

tests/test_autodiff.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule3">test_autodiff.py::test_chain_rule3</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule3</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule3() -> None:
        "Check that constrants are ignored and variables get derivatives."
        constant = 10
        var = minitorch.Scalar(5)

        y = Function2.apply(constant, var)

>       back = y.chain_rule(d_output=5)

tests/test_autodiff.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(60.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_chain_rule4">test_autodiff.py::test_chain_rule4</h3>
<details><summary> <pre>test_autodiff.py::test_chain_rule4</pre></summary><pre>
@pytest.mark.task1_3
    def test_chain_rule4() -> None:
        var1 = minitorch.Scalar(5)
        var2 = minitorch.Scalar(10)

        y = Function2.apply(var1, var2)

>       back = y.chain_rule(d_output=5)

tests/test_autodiff.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(55.000000), d_output = 5

    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        # TODO: Implement for Task 1.3.
>       raise NotImplementedError('Need to implement for Task 1.3')
E       NotImplementedError: Need to implement for Task 1.3

minitorch/scalar.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop1">test_autodiff.py::test_backprop1</h3>
<details><summary> <pre>test_autodiff.py::test_backprop1</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop1() -> None:
        # Example 1: F1(0, v)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
>       var2.backward(d_output=5)

tests/test_autodiff.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(10.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop2">test_autodiff.py::test_backprop2</h3>
<details><summary> <pre>test_autodiff.py::test_backprop2</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop2() -> None:
        # Example 2: F1(0, 0)
        var = minitorch.Scalar(0)
        var2 = Function1.apply(0, var)
        var3 = Function1.apply(0, var2)
>       var3.backward(d_output=5)

tests/test_autodiff.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(20.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop3">test_autodiff.py::test_backprop3</h3>
<details><summary> <pre>test_autodiff.py::test_backprop3</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop3() -> None:
        # Example 3: F1(F1(0, v1), F1(0, v1))
        var1 = minitorch.Scalar(0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(30.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_autodiffpytest_backprop4">test_autodiff.py::test_backprop4</h3>
<details><summary> <pre>test_autodiff.py::test_backprop4</pre></summary><pre>
@pytest.mark.task1_4
    def test_backprop4() -> None:
        # Example 4: F1(F1(0, v1), F1(0, v1))
        var0 = minitorch.Scalar(0)
        var1 = Function1.apply(0, var0)
        var2 = Function1.apply(0, var1)
        var3 = Function1.apply(0, var1)
        var4 = Function1.apply(var2, var3)
>       var4.backward(d_output=5)

tests/test_autodiff.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/scalar.py:189: in backward
    backpropagate(self, d_output)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

variable = Scalar(50.000000), deriv = 5

    def backpropagate(variable: Variable, deriv: Any) -> None:
        """
        Runs backpropagation on the computation graph in order to
        compute derivatives for the leave nodes.

        Args:
            variable: The right-most variable
            deriv  : Its derivative that we want to propagate backward to the leaves.

        No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.
        """
        # TODO: Implement for Task 1.4.
>       raise NotImplementedError('Need to implement for Task 1.4')
E       NotImplementedError: Need to implement for Task 1.4

minitorch/autodiff.py:80: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d_simple">test_conv.py::test_conv1d_simple</h3>
<details><summary> <pre>test_conv.py::test_conv1d_simple</pre></summary><pre>
@pytest.mark.task4_1
    def test_conv1d_simple() -> None:
>       t = minitorch.tensor([0, 1, 2, 3]).view(1, 1, 4)

tests/test_conv.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (4,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d">test_conv.py::test_conv1d</h3>
<details><summary> <pre>test_conv.py::test_conv1d</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(1, 1, 6)), tensors(shape=(1, 1, 4)))

tests/test_conv.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv1d_channel">test_conv.py::test_conv1d_channel</h3>
<details><summary> <pre>test_conv.py::test_conv1d_channel</pre></summary><pre>
@pytest.mark.task4_1
>   @given(tensors(shape=(2, 2, 6)), tensors(shape=(3, 2, 2)))

tests/test_conv.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv">test_conv.py::test_conv</h3>
<details><summary> <pre>test_conv.py::test_conv</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(1, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv_batch">test_conv.py::test_conv_batch</h3>
<details><summary> <pre>test_conv.py::test_conv_batch</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 1, 6, 6)), tensors(shape=(1, 1, 2, 4)))

tests/test_conv.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv_channel">test_conv.py::test_conv_channel</h3>
<details><summary> <pre>test_conv.py::test_conv_channel</pre></summary><pre>
@pytest.mark.task4_2
>   @given(tensors(shape=(2, 2, 6, 6)), tensors(shape=(3, 2, 2, 4)))

tests/test_conv.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2, 2, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_convpytest_conv2">test_conv.py::test_conv2</h3>
<details><summary> <pre>test_conv.py::test_conv2</pre></summary><pre>
@pytest.mark.task4_2
    def test_conv2() -> None:
>       t = minitorch.tensor([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]).view(
            1, 1, 4, 4
        )

tests/test_conv.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_stacked_demo">test_module.py::test_stacked_demo</h3>
<details><summary> <pre>test_module.py::test_stacked_demo</pre></summary><pre>
@pytest.mark.task0_4
    def test_stacked_demo() -> None:
        "Check that each of the properties match"
        mod = ModuleA1()
>       np = dict(mod.named_parameters())

tests/test_module.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ModuleA1(
  (a): ModuleA2()
  (b): ModuleA3(
    (c): ModuleA4()
  )
)

    def named_parameters(self) -> Sequence[Tuple[str, Parameter]]:
        """
        Collect all the parameters of this module and its descendents.


        Returns:
            The name and `Parameter` of each ancestor parameter.
        """
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4

minitorch/module.py:51: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_module">test_module.py::test_module</h3>
<details><summary> <pre>test_module.py::test_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints)

tests/test_module.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_module.py:100: in test_module
    module.eval()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module2(
  (module_c): Module3()
)

    def eval(self) -> None:
        "Set the mode of this module and all descendent modules to `eval`."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4
E       Falsifying example: test_module(
E           size_b=1, size_a=1,
E       )

minitorch/module.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_modulepytest_stacked_module">test_module.py::test_stacked_module</h3>
<details><summary> <pre>test_module.py::test_stacked_module</pre></summary><pre>
@pytest.mark.task0_4
>   @given(med_ints, med_ints, small_floats)

tests/test_module.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_module.py:121: in test_stacked_module
    module.eval()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Module1(
  (module_a): Module2(
    (module_c): Module3()
  )
  (module_b): Module2(
    (module_c): Module3()
  )
)

    def eval(self) -> None:
        "Set the mode of this module and all descendent modules to `eval`."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4
E       Falsifying example: test_stacked_module(
E           val=0.0, size_b=1, size_a=1,
E       )

minitorch/module.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_modulespytest_linear">test_modules.py::test_linear</h3>
<details><summary> <pre>test_modules.py::test_linear</pre></summary><pre>
@given(lists(scalars(), max_size=10), integers(min_value=5, max_value=20))
>   def test_linear(inputs: List[Scalar], out_size: int) -> None:

tests/test_modules.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_modules.py:65: in test_linear
    lin2.forward(mid)
tests/test_modules.py:56: in forward
    y[j] = y[j] + x * self.weights[i][j].value
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.6888437030500962
b = -0.19013172509917142

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_linear(
E           out_size=5, inputs=[],
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_modulespytest_nn_size">test_modules.py::test_nn_size</h3>
<details><summary> <pre>test_modules.py::test_nn_size</pre></summary><pre>
def test_nn_size() -> None:
        model = Network2()
>       assert len(model.parameters()) == (
            len(model.layer1.parameters()) + len(model.layer2.parameters())
        )

tests/test_modules.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Network2(
  (layer1): ScalarLinear()
  (layer2): ScalarLinear()
)

    def parameters(self) -> Sequence[Parameter]:
        "Enumerate over all the parameters of this module and its descendents."
        # TODO: Implement for Task 0.4.
>       raise NotImplementedError('Need to implement for Task 0.4')
E       NotImplementedError: Need to implement for Task 0.4

minitorch/module.py:56: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_avg">test_nn.py::test_avg</h3>
<details><summary> <pre>test_nn.py::test_avg</pre></summary><pre>
@pytest.mark.task4_3
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_max">test_nn.py::test_max</h3>
<details><summary> <pre>test_nn.py::test_max</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(2, 3, 4)))

tests/test_nn.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_max_pool">test_nn.py::test_max_pool</h3>
<details><summary> <pre>test_nn.py::test_max_pool</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_drop">test_nn.py::test_drop</h3>
<details><summary> <pre>test_nn.py::test_drop</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors())

tests/test_nn.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_softmax">test_nn.py::test_softmax</h3>
<details><summary> <pre>test_nn.py::test_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_nnpytest_log_softmax">test_nn.py::test_log_softmax</h3>
<details><summary> <pre>test_nn.py::test_log_softmax</pre></summary><pre>
@pytest.mark.task4_4
>   @given(tensors(shape=(1, 1, 4, 4)))

tests/test_nn.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1, 1, 4, 4)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_same_as_python">test_operators.py::test_same_as_python</h3>
<details><summary> <pre>test_operators.py::test_same_as_python</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats, small_floats)

tests/test_operators.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:37: in test_same_as_python
    assert_close(mul(x, y), x * y)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 0.0

    def mul(x: float, y: float) -> float:
        "$f(x, y) = x * y$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_same_as_python(
E           y=0.0, x=0.0,
E       )

minitorch/operators.py:16: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_relu">test_operators.py::test_relu</h3>
<details><summary> <pre>test_operators.py::test_relu</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:49: in test_relu
    assert relu(a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.0

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_relu(
E           a=1.0,
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_relu_back">test_operators.py::test_relu_back</h3>
<details><summary> <pre>test_operators.py::test_relu_back</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats, small_floats)

tests/test_operators.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:58: in test_relu_back
    assert relu_back(a, b) == b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.0, d = 0.0

    def relu_back(x: float, d: float) -> float:
        r"If $f = relu$ compute $d \times f'(x)$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_relu_back(
E           b=0.0, a=1.0,
E       )

minitorch/operators.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_id">test_operators.py::test_id</h3>
<details><summary> <pre>test_operators.py::test_id</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:66: in test_id
    assert id(a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0

    def id(x: float) -> float:
        "$f(x) = x$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_id(
E           a=0.0,
E       )

minitorch/operators.py:22: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_lt">test_operators.py::test_lt</h3>
<details><summary> <pre>test_operators.py::test_lt</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:73: in test_lt
    assert lt(a - 1.0, a) == 1.0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = -1.0, y = 0.0

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_lt(
E           a=0.0,
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_max">test_operators.py::test_max</h3>
<details><summary> <pre>test_operators.py::test_max</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:80: in test_max
    assert max(a - 1.0, a) == a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = -1.0, y = 0.0

    def max(x: float, y: float) -> float:
        "$f(x) =$ x if x is greater than y else y"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_max(
E           a=0.0,
E       )

minitorch/operators.py:52: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_eq">test_operators.py::test_eq</h3>
<details><summary> <pre>test_operators.py::test_eq</pre></summary><pre>
@pytest.mark.task0_1
>   @given(small_floats)

tests/test_operators.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:89: in test_eq
    assert eq(a, a) == 1.0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 0.0

    def eq(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is equal to y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_eq(
E           a=0.0,
E       )

minitorch/operators.py:46: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sigmoid">test_operators.py::test_sigmoid</h3>
<details><summary> <pre>test_operators.py::test_sigmoid</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats)

tests/test_operators.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0

    @pytest.mark.task0_2
    @given(small_floats)
    def test_sigmoid(a: float) -> None:
        """Check properties of the sigmoid function, specifically
        * It is always between 0.0 and 1.0.
        * one minus sigmoid is the same as sigmoid of the negative
        * It crosses 0 at 0.5
        * It is  strictly increasing.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_sigmoid(
E           a=0.0,
E       )

tests/test_operators.py:111: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_transitive">test_operators.py::test_transitive</h3>
<details><summary> <pre>test_operators.py::test_transitive</pre></summary><pre>
@pytest.mark.task0_2
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = 0.0, b = 0.0, c = 0.0

    @pytest.mark.task0_2
    @given(small_floats, small_floats, small_floats)
    def test_transitive(a: float, b: float, c: float) -> None:
        "Test the transitive property of less-than (a < b and b < c implies a < c)"
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2
E       Falsifying example: test_transitive(
E           c=0.0, b=0.0, a=0.0,
E       )

tests/test_operators.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_symmetric">test_operators.py::test_symmetric</h3>
<details><summary> <pre>test_operators.py::test_symmetric</pre></summary><pre>
@pytest.mark.task0_2
    def test_symmetric() -> None:
        """
        Write a test that ensures that :func:`minitorch.operators.mul` is symmetric, i.e.
        gives the same value regardless of the order of its input.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:129: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_distribute">test_operators.py::test_distribute</h3>
<details><summary> <pre>test_operators.py::test_distribute</pre></summary><pre>
@pytest.mark.task0_2
    def test_distribute() -> None:
        r"""
        Write a test that ensures that your operators distribute, i.e.
        :math:`z \times (x + y) = z \times x + z \times y`
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:139: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_other">test_operators.py::test_other</h3>
<details><summary> <pre>test_operators.py::test_other</pre></summary><pre>
@pytest.mark.task0_2
    def test_other() -> None:
        """
        Write a test that ensures some other property holds for your functions.
        """
        # TODO: Implement for Task 0.2.
>       raise NotImplementedError('Need to implement for Task 0.2')
E       NotImplementedError: Need to implement for Task 0.2

tests/test_operators.py:148: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_zip_with">test_operators.py::test_zip_with</h3>
<details><summary> <pre>test_operators.py::test_zip_with</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats, small_floats)

tests/test_operators.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:160: in test_zip_with
    x1, x2 = addLists([a, b], [c, d])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls1 = [0.0, 0.0], ls2 = [0.0, 0.0]

    def addLists(ls1: Iterable[float], ls2: Iterable[float]) -> Iterable[float]:
        "Add the elements of `ls1` and `ls2` using `zipWith` and `add`"
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_zip_with(
E           d=0.0, c=0.0, b=0.0, a=0.0,
E       )

minitorch/operators.py:175: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sum_distribute">test_operators.py::test_sum_distribute</h3>
<details><summary> <pre>test_operators.py::test_sum_distribute</pre></summary><pre>
@pytest.mark.task0_3
>   @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )

tests/test_operators.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls1 = [0.0, 0.0, 0.0, 0.0, 0.0], ls2 = [0.0, 0.0, 0.0, 0.0, 0.0]

    @pytest.mark.task0_3
    @given(
        lists(small_floats, min_size=5, max_size=5),
        lists(small_floats, min_size=5, max_size=5),
    )
    def test_sum_distribute(ls1: List[float], ls2: List[float]) -> None:
        """
        Write a test that ensures that the sum of `ls1` plus the sum of `ls2`
        is the same as the sum of each element of `ls1` plus each element of `ls2`.
        """
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_sum_distribute(
E           ls2=[0.0, 0.0, 0.0, 0.0, 0.0], ls1=[0.0, 0.0, 0.0, 0.0, 0.0],
E       )

tests/test_operators.py:177: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_sum">test_operators.py::test_sum</h3>
<details><summary> <pre>test_operators.py::test_sum</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:183: in test_sum
    assert_close(sum(ls), sum(ls))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = []

    def sum(ls: Iterable[float]) -> float:
        "Sum up a list using `reduce` and `add`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_sum(
E           ls=[],
E       )

minitorch/operators.py:200: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_prod">test_operators.py::test_prod</h3>
<details><summary> <pre>test_operators.py::test_prod</pre></summary><pre>
@pytest.mark.task0_3
>   @given(small_floats, small_floats, small_floats)

tests/test_operators.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:189: in test_prod
    assert_close(prod([x, y, z]), x * y * z)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = [0.0, 0.0, 0.0]

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_prod(
E           z=0.0, y=0.0, x=0.0,
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_neglist">test_operators.py::test_negList</h3>
<details><summary> <pre>test_operators.py::test_negList</pre></summary><pre>
@pytest.mark.task0_3
>   @given(lists(small_floats))

tests/test_operators.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:195: in test_negList
    check = negList(ls)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = []

    def negList(ls: Iterable[float]) -> Iterable[float]:
        "Use `map` and `neg` to negate each element in `ls`"
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_negList(
E           ls=[],
E       )

minitorch/operators.py:149: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn1">test_operators.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:119: in complex
    operators.relu(operators.relu(a * 10 + 7) * 6 + 5) * 10
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 7.0

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('complex', complex),
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn6">test_operators.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:49: in inv
    return operators.inv(a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 3.5

    def inv(x: float) -> float:
        "$f(x) = 1/x$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('inv', inv),
E       )

minitorch/operators.py:109: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn10">test_operators.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:64: in relu
    return operators.relu(a + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 5.5

    def relu(x: float) -> float:
        """
        $f(x) =$ x if x is greater than 0, else 0

        (See https://en.wikipedia.org/wiki/Rectifier_(neural_networks) .)
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('relu', relu),
E       )

minitorch/operators.py:84: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_one_argsfn11">test_operators.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_operators.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>)

    @given(small_floats)
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_operators.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:212: in test_one_args
    base_fn(t1)
minitorch/testing.py:54: in sig
    return operators.sigmoid(a)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0

    def sigmoid(x: float) -> float:
        r"""
        $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$

        (See https://en.wikipedia.org/wiki/Sigmoid_function )

        Calculate as

        $f(x) =  \frac{1.0}{(1.0 + e^{-x})}$ if x >=0 else $\frac{e^x}{(1.0 + e^{x})}$

        for stability.
        """
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_one_args(
E           t1=0.0, fn=('sig', sig),
E       )

minitorch/operators.py:74: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn2">test_operators.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:100: in eq2
    return operators.eq(a, (b + 5.5))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 5.5

    def eq(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is equal to y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('eq2', eq2),
E       )

minitorch/operators.py:46: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn3">test_operators.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:92: in gt2
    return operators.lt(b, a + 1.2)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, y = 1.2

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('gt2', gt2),
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_two_argsfn4">test_operators.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_operators.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>)

    @given(small_floats, small_floats)
>   @pytest.mark.parametrize("fn", two_arg)

tests/test_operators.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:221: in test_two_args
    base_fn(t1, t2)
minitorch/testing.py:96: in lt2
    return operators.lt(a + 1.2, b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 1.2, y = 0.0

    def lt(x: float, y: float) -> float:
        "$f(x) =$ 1.0 if x is less than y else 0.0"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_two_args(
E           t2=0.0, t1=0.0, fn=('lt2', lt2),
E       )

minitorch/operators.py:40: NotImplementedError
</pre>
</details>
<h3 id="test_operatorspytest_backs">test_operators.py::test_backs</h3>
<details><summary> <pre>test_operators.py::test_backs</pre></summary><pre>
@given(small_floats, small_floats)
>   def test_backs(a: float, b: float) -> None:

tests/test_operators.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_operators.py:226: in test_backs
    relu_back(a, b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = 0.0, d = 0.0

    def relu_back(x: float, d: float) -> float:
        r"If $f = relu$ compute $d \times f'(x)$"
        # TODO: Implement for Task 0.1.
>       raise NotImplementedError('Need to implement for Task 0.1')
E       NotImplementedError: Need to implement for Task 0.1
E       Falsifying example: test_backs(
E           b=0.0, a=0.0,
E       )

minitorch/operators.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_central_diff">test_scalar.py::test_central_diff</h3>
<details><summary> <pre>test_scalar.py::test_central_diff</pre></summary><pre>
@pytest.mark.task1_1
    def test_central_diff() -> None:
>       d = central_difference(operators.id, 5, arg=0)

tests/test_scalar.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

f = <function id at 0x77cdec4d9240>, arg = 0, epsilon = 1e-06, vals = (5,)

    def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -> Any:
        r"""
        Computes an approximation to the derivative of `f` with respect to one arg.

        See :doc:`derivative` or https://en.wikipedia.org/wiki/Finite_difference for more details.

        Args:
            f : arbitrary function from n-scalar args to one value
            *vals : n-float values $x_0 \ldots x_{n-1}$
            arg : the number $i$ of the arg to compute the derivative
            epsilon : a small constant

        Returns:
            An approximation of $f'_i(x_0, \ldots, x_{n-1})$
        """
        # TODO: Implement for Task 1.1.
>       raise NotImplementedError('Need to implement for Task 1.1')
E       NotImplementedError: Need to implement for Task 1.1

minitorch/autodiff.py:26: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_simple">test_scalar.py::test_simple</h3>
<details><summary> <pre>test_scalar.py::test_simple</pre></summary><pre>
@given(small_floats, small_floats)
>   def test_simple(a: float, b: float) -> None:

tests/test_scalar.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:57: in test_simple
    c = Scalar(a) + Scalar(b)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_simple(
E           b=0.0, a=0.0,
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn0">test_scalar.py::test_one_args[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:19: in addConstant
    return 5 + a
minitorch/scalar.py:122: in __radd__
    return self + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('addConstant', addConstant, addConstant),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn1">test_scalar.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:213: in complex
    return (((a * 10 + 7).relu() * 6 + 5).relu() * 10).sigmoid().log() / 50
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 10

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('complex', complex, complex),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn2">test_scalar.py::test_one_args[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:29: in cube
    return a * a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('cube', cube, cube),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn3">test_scalar.py::test_one_args[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:44: in div
    return a / 5
minitorch/scalar.py:89: in __truediv__
    return Mul.apply(self, Inv.apply(b))
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 5

    @staticmethod
    def forward(ctx: Context, a: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('div', div, div),
E       )

minitorch/scalar_functions.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn4">test_scalar.py::test_one_args[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:181: in exp
    return (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 200

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('exp', exp, exp),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn5">test_scalar.py::test_one_args[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:185: in explog
    return (a + 100000).log() + (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('explog', explog, explog),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn6">test_scalar.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:165: in inv
    return 1.0 / (a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 3.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('inv', inv, inv),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn7">test_scalar.py::test_one_args[fn7]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:173: in log
    return (x + 100000).log()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('log', log, log),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn8">test_scalar.py::test_one_args[fn8]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:39: in multConstant
    return 5 * a
minitorch/scalar.py:125: in __rmul__
    return self * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 5

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('multConstant', multConstant, multConstant),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn9">test_scalar.py::test_one_args[fn9]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:14: in neg
    return -a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def __neg__(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('neg', neg, neg),
E       )

minitorch/scalar.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn10">test_scalar.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:177: in relu
    return (x + 5.5).relu()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('relu', relu, relu),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn11">test_scalar.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:169: in sig
    return x.sigmoid()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def sigmoid(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('sig', sig, sig),
E       )

minitorch/scalar.py:137: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn12">test_scalar.py::test_one_args[fn12]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:24: in square
    return a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('square', square, square),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_argsfn13">test_scalar.py::test_one_args[fn13]</h3>
<details><summary> <pre>test_scalar.py::test_one_args[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)

    @given(small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:81: in test_one_args
    assert_close(scalar_fn(t1).data, base_fn(t1.data))
minitorch/testing.py:34: in subConstant
    return a - 5
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_args(
E           t1=Scalar(0.000000), fn=('subConstant', subConstant, subConstant),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn0">test_scalar.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:78: in add2
    return a + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('add2', add2, add2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn1">test_scalar.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:88: in div2
    return a / (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('div2', div2, div2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn2">test_scalar.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:201: in eq2
    return a == (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq2', eq2, eq2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn3">test_scalar.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:205: in gt2
    return a + 1.2 > b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('gt2', gt2, gt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn4">test_scalar.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:209: in lt2
    return a + 1.2 < b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt2', lt2, lt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_argsfn5">test_scalar.py::test_two_args[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_two_args[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_2

tests/test_scalar.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:93: in test_two_args
    assert_close(scalar_fn(t1, t2).data, base_fn(t1.data, t2.data))
minitorch/testing.py:83: in mul2
    return a * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_args(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('mul2', mul2, mul2),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn0">test_scalar.py::test_one_derivative[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:19: in addConstant
    return 5 + a
minitorch/scalar.py:122: in __radd__
    return self + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('addConstant', addConstant, addConstant),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn1">test_scalar.py::test_one_derivative[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:213: in complex
    return (((a * 10 + 7).relu() * 6 + 5).relu() * 10).sigmoid().log() / 50
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 10

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('complex', complex, complex),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn2">test_scalar.py::test_one_derivative[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:29: in cube
    return a * a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('cube', cube, cube),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn3">test_scalar.py::test_one_derivative[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:44: in div
    return a / 5
minitorch/scalar.py:89: in __truediv__
    return Mul.apply(self, Inv.apply(b))
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 5

    @staticmethod
    def forward(ctx: Context, a: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('div', div, div),
E       )

minitorch/scalar_functions.py:121: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn4">test_scalar.py::test_one_derivative[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:181: in exp
    return (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 200

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('exp', exp, exp),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn5">test_scalar.py::test_one_derivative[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:185: in explog
    return (a + 100000).log() + (a - 200).exp()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('explog', explog, explog),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn6">test_scalar.py::test_one_derivative[fn6]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:165: in inv
    return 1.0 / (a + 3.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 3.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('inv', inv, inv),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn7">test_scalar.py::test_one_derivative[fn7]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:173: in log
    return (x + 100000).log()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 100000

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('log', log, log),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn8">test_scalar.py::test_one_derivative[fn8]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:39: in multConstant
    return 5 * a
minitorch/scalar.py:125: in __rmul__
    return self * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 5

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('multConstant', multConstant, multConstant),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn9">test_scalar.py::test_one_derivative[fn9]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:14: in neg
    return -a
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def __neg__(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('neg', neg, neg),
E       )

minitorch/scalar.py:119: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn10">test_scalar.py::test_one_derivative[fn10]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:177: in relu
    return (x + 5.5).relu()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('relu', relu, relu),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn11">test_scalar.py::test_one_derivative[fn11]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:169: in sig
    return x.sigmoid()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000)

    def sigmoid(self) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('sig', sig, sig),
E       )

minitorch/scalar.py:137: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn12">test_scalar.py::test_one_derivative[fn12]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:24: in square
    return a * a
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('square', square, square),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_one_derivativefn13">test_scalar.py::test_one_derivative[fn13]</h3>
<details><summary> <pre>test_scalar.py::test_one_derivative[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)

    @given(small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:108: in test_one_derivative
    derivative_check(scalar_fn, t1)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:34: in subConstant
    return a - 5
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5

    def __sub__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_one_derivative(
E           t1=Scalar(0.000000), fn=('subConstant', subConstant, subConstant),
E       )

minitorch/scalar.py:115: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn0">test_scalar.py::test_two_derivative[fn0]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:78: in add2
    return a + b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = Scalar(0.000000)

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('add2', add2, add2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn1">test_scalar.py::test_two_derivative[fn1]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:88: in div2
    return a / (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('div2', div2, div2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn2">test_scalar.py::test_two_derivative[fn2]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:201: in eq2
    return a == (b + 5.5)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 5.5

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('eq2', eq2, eq2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn3">test_scalar.py::test_two_derivative[fn3]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:205: in gt2
    return a + 1.2 > b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('gt2', gt2, gt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn4">test_scalar.py::test_two_derivative[fn4]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:209: in lt2
    return a + 1.2 < b
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Scalar(0.000000), b = 1.2

    def __add__(self, b: ScalarLike) -> Scalar:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('lt2', lt2, lt2),
E       )

minitorch/scalar.py:96: NotImplementedError
</pre>
</details>
<h3 id="test_scalarpytest_two_derivativefn5">test_scalar.py::test_two_derivative[fn5]</h3>
<details><summary> <pre>test_scalar.py::test_two_derivative[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)

    @given(small_scalars, small_scalars)
>   @pytest.mark.task1_4

tests/test_scalar.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_scalar.py:120: in test_two_derivative
    derivative_check(scalar_fn, t1, t2)
minitorch/scalar.py:201: in derivative_check
    out = f(*scalars)
minitorch/testing.py:83: in mul2
    return a * b
minitorch/scalar.py:86: in __mul__
    return Mul.apply(self, b)
minitorch/scalar_functions.py:63: in apply
    c = cls._forward(ctx, *raw_vals)
minitorch/scalar_functions.py:45: in _forward
    return cls.forward(ctx, *inps)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = Context(no_grad=False, saved_values=()), a = 0.0, b = 0.0

    @staticmethod
    def forward(ctx: Context, a: float, b: float) -> float:
        # TODO: Implement for Task 1.2.
>       raise NotImplementedError('Need to implement for Task 1.2')
E       NotImplementedError: Need to implement for Task 1.2
E       Falsifying example: test_two_derivative(
E           t2=Scalar(0.000000), t1=Scalar(0.000000), fn=('mul2', mul2, mul2),
E       )

minitorch/scalar_functions.py:107: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_create">test_tensor.py::test_create</h3>
<details><summary> <pre>test_tensor.py::test_create</pre></summary><pre>
@given(lists(small_floats, min_size=1))
>   def test_create(t1: List[float]) -> None:

tests/test_tensor.py:16: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor.py:18: in test_create
    t2 = tensor(t1)
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_create(
E           t1=[0.0],
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn0">test_tensor.py::test_one_args[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn1">test_tensor.py::test_one_args[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn2">test_tensor.py::test_one_args[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn3">test_tensor.py::test_one_args[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn4">test_tensor.py::test_one_args[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn5">test_tensor.py::test_one_args[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn6">test_tensor.py::test_one_args[fn6]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn7">test_tensor.py::test_one_args[fn7]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn8">test_tensor.py::test_one_args[fn8]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn9">test_tensor.py::test_one_args[fn9]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn10">test_tensor.py::test_one_args[fn10]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn11">test_tensor.py::test_one_args[fn11]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn12">test_tensor.py::test_one_args[fn12]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_argsfn13">test_tensor.py::test_one_args[fn13]</h3>
<details><summary> <pre>test_tensor.py::test_one_args[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)

    @given(tensors())
>   @pytest.mark.task2_3

tests/test_tensor.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn0">test_tensor.py::test_two_args[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn1">test_tensor.py::test_two_args[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn2">test_tensor.py::test_two_args[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn3">test_tensor.py::test_two_args[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn4">test_tensor.py::test_two_args[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_argsfn5">test_tensor.py::test_two_args[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_args[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_3

tests/test_tensor.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn0">test_tensor.py::test_one_derivative[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn1">test_tensor.py::test_one_derivative[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn2">test_tensor.py::test_one_derivative[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn3">test_tensor.py::test_one_derivative[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn4">test_tensor.py::test_one_derivative[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn5">test_tensor.py::test_one_derivative[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn6">test_tensor.py::test_one_derivative[fn6]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn7">test_tensor.py::test_one_derivative[fn7]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn8">test_tensor.py::test_one_derivative[fn8]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn9">test_tensor.py::test_one_derivative[fn9]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn10">test_tensor.py::test_one_derivative[fn10]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn11">test_tensor.py::test_one_derivative[fn11]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn12">test_tensor.py::test_one_derivative[fn12]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_one_derivativefn13">test_tensor.py::test_one_derivative[fn13]</h3>
<details><summary> <pre>test_tensor.py::test_one_derivative[fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_permute">test_tensor.py::test_permute</h3>
<details><summary> <pre>test_tensor.py::test_permute</pre></summary><pre>
@given(data(), tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_size">test_tensor.py::test_grad_size</h3>
<details><summary> <pre>test_tensor.py::test_grad_size</pre></summary><pre>
def test_grad_size() -> None:
        "Test the size of the gradient (from @WannaFy)"
>       a = tensor([1], requires_grad=True)

tests/test_tensor.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn0">test_tensor.py::test_grad_reduce[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn0]</pre></summary><pre>
fn = ('mean_full_red', <function MathTest.mean_full_red at 0x77cdd00cb910>, <function MathTestVariable.mean_full_red at 0x77cdd00dc040>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn1">test_tensor.py::test_grad_reduce[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn1]</pre></summary><pre>
fn = ('mean_red', <function MathTest.mean_red at 0x77cdd00cb880>, <function MathTestVariable.mean_red at 0x77cdd00cbf40>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_grad_reducefn2">test_tensor.py::test_grad_reduce[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_grad_reduce[fn2]</pre></summary><pre>
fn = ('sum_red', <function MathTest.sum_red at 0x77cdd00cb7f0>, <function MathTestVariable.sum_red at 0x77cdd00cbeb0>)

    @given(tensors())
>   @pytest.mark.task2_4

tests/test_tensor.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn0">test_tensor.py::test_two_grad[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn1">test_tensor.py::test_two_grad[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn2">test_tensor.py::test_two_grad[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn3">test_tensor.py::test_two_grad[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn4">test_tensor.py::test_two_grad[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_gradfn5">test_tensor.py::test_two_grad[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn0">test_tensor.py::test_two_grad_broadcast[fn0]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn1">test_tensor.py::test_two_grad_broadcast[fn1]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn2">test_tensor.py::test_two_grad_broadcast[fn2]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn3">test_tensor.py::test_two_grad_broadcast[fn3]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn4">test_tensor.py::test_two_grad_broadcast[fn4]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_two_grad_broadcastfn5">test_tensor.py::test_two_grad_broadcast[fn5]</h3>
<details><summary> <pre>test_tensor.py::test_two_grad_broadcast[fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)

    @given(shaped_tensors(2))
>   @pytest.mark.task2_4

tests/test_tensor.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_fromlist">test_tensor.py::test_fromlist</h3>
<details><summary> <pre>test_tensor.py::test_fromlist</pre></summary><pre>
def test_fromlist() -> None:
        "Test longer from list conversion"
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_view">test_tensor.py::test_view</h3>
<details><summary> <pre>test_tensor.py::test_view</pre></summary><pre>
def test_view() -> None:
        "Test view"
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_back_view">test_tensor.py::test_back_view</h3>
<details><summary> <pre>test_tensor.py::test_back_view</pre></summary><pre>
@given(tensors())
>   def test_back_view(t1: Tensor) -> None:

tests/test_tensor.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_fromnumpy">test_tensor.py::test_fromnumpy</h3>
<details><summary> <pre>test_tensor.py::test_fromnumpy</pre></summary><pre>
def test_fromnumpy() -> None:
>       t = tensor([[2, 3, 4], [4, 5, 7]])

tests/test_tensor.py:178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim">test_tensor.py::test_reduce_forward_one_dim</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_one_dim_2">test_tensor.py::test_reduce_forward_one_dim_2</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_one_dim_2</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_one_dim_2() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensorpytest_reduce_forward_all_dims">test_tensor.py::test_reduce_forward_all_dims</h3>
<details><summary> <pre>test_tensor.py::test_reduce_forward_all_dims</pre></summary><pre>
@pytest.mark.task2_3
    def test_reduce_forward_all_dims() -> None:
        # shape (3, 2)
>       t = tensor([[2, 3], [4, 6], [5, 7]])

tests/test_tensor.py:219: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_layout">test_tensor_data.py::test_layout</h3>
<details><summary> <pre>test_tensor_data.py::test_layout</pre></summary><pre>
@pytest.mark.task2_1
    def test_layout() -> None:
        "Test basis properties of layout and strides"
        data = [0] * 3 * 5
>       tensor_data = minitorch.TensorData(data, (3, 5), (5, 1))

tests/test_tensor_data.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (3, 5)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_enumeration">test_tensor_data.py::test_enumeration</h3>
<details><summary> <pre>test_tensor_data.py::test_enumeration</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_index">test_tensor_data.py::test_index</h3>
<details><summary> <pre>test_tensor_data.py::test_index</pre></summary><pre>
@pytest.mark.task2_1
>   @given(tensor_data())

tests/test_tensor_data.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_permute">test_tensor_data.py::test_permute</h3>
<details><summary> <pre>test_tensor_data.py::test_permute</pre></summary><pre>
@pytest.mark.task2_1
>   @given(data())

tests/test_tensor_data.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_data.py:83: in test_permute
    td = data.draw(tensor_data())
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_permute(
E           data=data(...),
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_shape_broadcast">test_tensor_data.py::test_shape_broadcast</h3>
<details><summary> <pre>test_tensor_data.py::test_shape_broadcast</pre></summary><pre>
@pytest.mark.task2_2
    def test_shape_broadcast() -> None:
>       c = minitorch.shape_broadcast((1,), (5, 5))

tests/test_tensor_data.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shape1 = (1,), shape2 = (5, 5)

    def shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:
        """
        Broadcast two shapes to create a new union shape.

        Args:
            shape1 : first shape
            shape2 : second shape

        Returns:
            broadcasted shape

        Raises:
            IndexingError : if cannot broadcast
        """
        # TODO: Implement for Task 2.2.
>       raise NotImplementedError('Need to implement for Task 2.2')
E       NotImplementedError: Need to implement for Task 2.2

minitorch/tensor_data.py:105: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_datapytest_string">test_tensor_data.py::test_string</h3>
<details><summary> <pre>test_tensor_data.py::test_string</pre></summary><pre>
@given(tensor_data())
>   def test_string(tensor_data: TensorData) -> None:

tests/test_tensor_data.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_createfast">test_tensor_general.py::test_create[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_create[fast]</pre></summary><pre>
backend = 'fast'

    @given(lists(small_floats, min_size=1))
>   @pytest.mark.parametrize("backend", backend_tests)

tests/test_tensor_general.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:48: in test_create
    t2 = minitorch.tensor(t1, backend=shared[backend])
minitorch/tensor_functions.py:366: in tensor
    return _tensor(cur, tuple(shape2), backend=backend, requires_grad=requires_grad)
minitorch/tensor_functions.py:332: in _tensor
    tensor = minitorch.Tensor.make(ls, shape, backend=backend)
minitorch/tensor.py:264: in make
    return Tensor(TensorData(storage, shape, strides), backend=backend)
minitorch/tensor_data.py:147: in __init__
    self.size = int(prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_create(
E           t1=[0.0], backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn0">test_tensor_general.py::test_one_args[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('addConstant', addConstant, addConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn1">test_tensor_general.py::test_one_args[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('complex', complex, complex), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn2">test_tensor_general.py::test_one_args[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('cube', cube, cube), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn3">test_tensor_general.py::test_one_args[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('div', div, div), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn4">test_tensor_general.py::test_one_args[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('exp', exp, exp), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn5">test_tensor_general.py::test_one_args[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('explog', explog, explog), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn6">test_tensor_general.py::test_one_args[fast-fn6]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('inv', inv, inv), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn7">test_tensor_general.py::test_one_args[fast-fn7]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('log', log, log), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn8">test_tensor_general.py::test_one_args[fast-fn8]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('multConstant', multConstant, multConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn9">test_tensor_general.py::test_one_args[fast-fn9]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('neg', neg, neg), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn10">test_tensor_general.py::test_one_args[fast-fn10]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('relu', relu, relu), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn11">test_tensor_general.py::test_one_args[fast-fn11]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('sig', sig, sig), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn12">test_tensor_general.py::test_one_args[fast-fn12]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...), fn=('square', square, square), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_argsfast-fn13">test_tensor_general.py::test_one_args[fast-fn13]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_args[fast-fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:63: in test_one_args
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('subConstant', subConstant, subConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn0">test_tensor_general.py::test_two_args[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn1">test_tensor_general.py::test_two_args[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn2">test_tensor_general.py::test_two_args[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn3">test_tensor_general.py::test_two_args[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn4">test_tensor_general.py::test_two_args[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_argsfast-fn5">test_tensor_general.py::test_two_args[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_args[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:80: in test_two_args
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_args(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn0">test_tensor_general.py::test_one_derivative[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn0]</pre></summary><pre>
fn = ('addConstant', <function MathTest.addConstant at 0x77cdd00cadd0>, <function MathTest.addConstant at 0x77cdd00cadd0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('addConstant', addConstant, addConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn1">test_tensor_general.py::test_one_derivative[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn1]</pre></summary><pre>
fn = ('complex', <function MathTest.complex at 0x77cdd00cb9a0>, <function MathTestVariable.complex at 0x77cdd00dc280>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('complex', complex, complex), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn2">test_tensor_general.py::test_one_derivative[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn2]</pre></summary><pre>
fn = ('cube', <function MathTest.cube at 0x77cdd00caef0>, <function MathTest.cube at 0x77cdd00caef0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('cube', cube, cube), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn3">test_tensor_general.py::test_one_derivative[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn3]</pre></summary><pre>
fn = ('div', <function MathTest.div at 0x77cdd00cb0a0>, <function MathTest.div at 0x77cdd00cb0a0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('div', div, div), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn4">test_tensor_general.py::test_one_derivative[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn4]</pre></summary><pre>
fn = ('exp', <function MathTest.exp at 0x77cdd00cb370>, <function MathTestVariable.exp at 0x77cdd00cbd90>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('exp', exp, exp), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn5">test_tensor_general.py::test_one_derivative[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn5]</pre></summary><pre>
fn = ('explog', <function MathTest.explog at 0x77cdd00cb400>, <function MathTestVariable.explog at 0x77cdd00cbe20>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('explog', explog, explog), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn6">test_tensor_general.py::test_one_derivative[fast-fn6]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn6]</pre></summary><pre>
fn = ('inv', <function MathTest.inv at 0x77cdd00cb130>, <function MathTestVariable.inv at 0x77cdd00cbb50>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('inv', inv, inv), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn7">test_tensor_general.py::test_one_derivative[fast-fn7]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn7]</pre></summary><pre>
fn = ('log', <function MathTest.log at 0x77cdd00cb250>, <function MathTestVariable.log at 0x77cdd00cbc70>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('log', log, log), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn8">test_tensor_general.py::test_one_derivative[fast-fn8]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn8]</pre></summary><pre>
fn = ('multConstant', <function MathTest.multConstant at 0x77cdd00cb010>, <function MathTest.multConstant at 0x77cdd00cb010>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('multConstant', multConstant, multConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn9">test_tensor_general.py::test_one_derivative[fast-fn9]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn9]</pre></summary><pre>
fn = ('neg', <function MathTest.neg at 0x77cdd00cad40>, <function MathTest.neg at 0x77cdd00cad40>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('neg', neg, neg), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn10">test_tensor_general.py::test_one_derivative[fast-fn10]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn10]</pre></summary><pre>
fn = ('relu', <function MathTest.relu at 0x77cdd00cb2e0>, <function MathTestVariable.relu at 0x77cdd00cbd00>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('relu', relu, relu), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn11">test_tensor_general.py::test_one_derivative[fast-fn11]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn11]</pre></summary><pre>
fn = ('sig', <function MathTest.sig at 0x77cdd00cb1c0>, <function MathTestVariable.sig at 0x77cdd00cbbe0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('sig', sig, sig), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn12">test_tensor_general.py::test_one_derivative[fast-fn12]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn12]</pre></summary><pre>
fn = ('square', <function MathTest.square at 0x77cdd00cae60>, <function MathTest.square at 0x77cdd00cae60>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...), fn=('square', square, square), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_one_derivativefast-fn13">test_tensor_general.py::test_one_derivative[fast-fn13]</h3>
<details><summary> <pre>test_tensor_general.py::test_one_derivative[fast-fn13]</pre></summary><pre>
fn = ('subConstant', <function MathTest.subConstant at 0x77cdd00caf80>, <function MathTest.subConstant at 0x77cdd00caf80>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:96: in test_one_derivative
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_one_derivative(
E           data=data(...),
E           fn=('subConstant', subConstant, subConstant),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn0">test_tensor_general.py::test_two_grad[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn1">test_tensor_general.py::test_two_grad[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn2">test_tensor_general.py::test_two_grad[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn3">test_tensor_general.py::test_two_grad[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn4">test_tensor_general.py::test_two_grad[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_gradfast-fn5">test_tensor_general.py::test_two_grad[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:111: in test_two_grad
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn0">test_tensor_general.py::test_reduce[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn0]</pre></summary><pre>
fn = ('mean_full_red', <function MathTest.mean_full_red at 0x77cdd00cb910>, <function MathTestVariable.mean_full_red at 0x77cdd00dc040>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...),
E           fn=('mean_full_red', mean_full_red, mean_full_red),
E           backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn1">test_tensor_general.py::test_reduce[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn1]</pre></summary><pre>
fn = ('mean_red', <function MathTest.mean_red at 0x77cdd00cb880>, <function MathTestVariable.mean_red at 0x77cdd00cbf40>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...), fn=('mean_red', mean_red, mean_red), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_reducefast-fn2">test_tensor_general.py::test_reduce[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_reduce[fast-fn2]</pre></summary><pre>
fn = ('sum_red', <function MathTest.sum_red at 0x77cdd00cb7f0>, <function MathTestVariable.sum_red at 0x77cdd00cbeb0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:126: in test_reduce
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_reduce(
E           data=data(...), fn=('sum_red', sum_red, sum_red), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn0">test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn0]</pre></summary><pre>
fn = ('add2', <function MathTest.add2 at 0x77cdd00cb490>, <function MathTest.add2 at 0x77cdd00cb490>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn1">test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn1]</pre></summary><pre>
fn = ('div2', <function MathTest.div2 at 0x77cdd00cb5b0>, <function MathTest.div2 at 0x77cdd00cb5b0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn2">test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn2]</pre></summary><pre>
fn = ('eq2', <function MathTest.eq2 at 0x77cdd00cb760>, <function MathTestVariable.eq2 at 0x77cdd00dc0d0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('eq2', eq2, eq2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn3">test_tensor_general.py::test_two_grad_broadcast[fast-fn3]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn3]</pre></summary><pre>
fn = ('gt2', <function MathTest.gt2 at 0x77cdd00cb640>, <function MathTestVariable.gt2 at 0x77cdd00dc160>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn4">test_tensor_general.py::test_two_grad_broadcast[fast-fn4]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn4]</pre></summary><pre>
fn = ('lt2', <function MathTest.lt2 at 0x77cdd00cb6d0>, <function MathTestVariable.lt2 at 0x77cdd00dc1f0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_two_grad_broadcastfast-fn5">test_tensor_general.py::test_two_grad_broadcast[fast-fn5]</h3>
<details><summary> <pre>test_tensor_general.py::test_two_grad_broadcast[fast-fn5]</pre></summary><pre>
fn = ('mul2', <function MathTest.mul2 at 0x77cdd00cb520>, <function MathTest.mul2 at 0x77cdd00cb520>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:317: in test_two_grad_broadcast
    t1, t2 = data.draw(shaped_tensors(2, backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:90: in shaped_tensors
    td = draw(tensor_data(numbers))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_two_grad_broadcast(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_permutefast">test_tensor_general.py::test_permute[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_permute[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:332: in test_permute
    t1 = data.draw(tensors(backend=shared[backend]))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (1,)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_permute(
E           data=data(...), backend='fast',
E       )

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_mm2">test_tensor_general.py::test_mm2</h3>
<details><summary> <pre>test_tensor_general.py::test_mm2</pre></summary><pre>
@pytest.mark.task3_2
    def test_mm2() -> None:
>       a = minitorch.rand((2, 3), backend=FastTensorBackend)

tests/test_tensor_general.py:343: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
minitorch/tensor_functions.py:308: in rand
    vals = [random.random() for _ in range(int(operators.prod(shape)))]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 3)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3

minitorch/operators.py:206: NotImplementedError
</pre>
</details>
<h3 id="test_tensor_generalpytest_bmmfast">test_tensor_general.py::test_bmm[fast]</h3>
<details><summary> <pre>test_tensor_general.py::test_bmm[fast]</pre></summary><pre>
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("backend", matmul_tests)

tests/test_tensor_general.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_tensor_general.py:370: in test_bmm
    a = data.draw(tensors(backend=shared[backend], shape=(D, A, B)))
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1813: in draw
    result = self.conjecture_data.draw(strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:946: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:76: in tensors
    td = draw(tensor_data(numbers, shape=shape))
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
.venv/lib/python3.10/site-packages/hypothesis/internal/conjecture/data.py:941: in draw
    return strategy.do_draw(self)
.venv/lib/python3.10/site-packages/hypothesis/strategies/_internal/core.py:1485: in do_draw
    return self.definition(data.draw, *self.args, **self.kwargs)
tests/tensor_strategies.py:49: in tensor_data
    size = int(minitorch.prod(shape))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ls = (2, 2, 2)

    def prod(ls: Iterable[float]) -> float:
        "Product of a list using `reduce` and `mul`."
        # TODO: Implement for Task 0.3.
>       raise NotImplementedError('Need to implement for Task 0.3')
E       NotImplementedError: Need to implement for Task 0.3
E       Falsifying example: test_bmm(
E           data=data(...), backend='fast',
E       )
E       Draw 1: 2
E       Draw 2: 2
E       Draw 3: 2
E       Draw 4: 2

minitorch/operators.py:206: NotImplementedError
</pre>
</details>

<h2 id="diff-to-gold">Diff to gold</h2>
<table>
    <thead> <th> function </th> <th data-sort-method='none'> impl </th> <th data-sort-method='none'> gold </th> <th data-sort-method='none'> diff </th> </thead>
    </table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../javascripts/tablesort.js"></script>
      
        <script src="../javascripts/button_select.js"></script>
      
    
  </body>
</html>